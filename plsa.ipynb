{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Vocabulary size:6\n",
      "Number of documents:1000\n",
      "EM iteration begins...\n",
      "Initializing...\n",
      "Iteration #1...\n",
      "E step:\n",
      "M step:\n",
      "-79799.39248661422\n",
      "-77854.77497097735\n",
      "Iteration #2...\n",
      "E step:\n",
      "M step:\n",
      "-77854.77497097735\n",
      "-77612.15590647516\n",
      "Iteration #3...\n",
      "E step:\n",
      "M step:\n",
      "-77612.15590647516\n",
      "-77461.18754144666\n",
      "Iteration #4...\n",
      "E step:\n",
      "M step:\n",
      "-77461.18754144666\n",
      "-77315.1136986956\n",
      "Iteration #5...\n",
      "E step:\n",
      "M step:\n",
      "-77315.1136986956\n",
      "-77107.75706737701\n",
      "Iteration #6...\n",
      "E step:\n",
      "M step:\n",
      "-77107.75706737701\n",
      "-76750.57609758285\n",
      "Iteration #7...\n",
      "E step:\n",
      "M step:\n",
      "-76750.57609758285\n",
      "-76113.75240611531\n",
      "Iteration #8...\n",
      "E step:\n",
      "M step:\n",
      "-76113.75240611531\n",
      "-75054.5687100944\n",
      "Iteration #9...\n",
      "E step:\n",
      "M step:\n",
      "-75054.5687100944\n",
      "-73556.02366431645\n",
      "Iteration #10...\n",
      "E step:\n",
      "M step:\n",
      "-73556.02366431645\n",
      "-71912.23733319694\n",
      "Iteration #11...\n",
      "E step:\n",
      "M step:\n",
      "-71912.23733319694\n",
      "-70605.59204852328\n",
      "Iteration #12...\n",
      "E step:\n",
      "M step:\n",
      "-70605.59204852328\n",
      "-69855.63870393403\n",
      "Iteration #13...\n",
      "E step:\n",
      "M step:\n",
      "-69855.63870393403\n",
      "-69511.31394354724\n",
      "Iteration #14...\n",
      "E step:\n",
      "M step:\n",
      "-69511.31394354724\n",
      "-69357.15899430976\n",
      "Iteration #15...\n",
      "E step:\n",
      "M step:\n",
      "-69357.15899430976\n",
      "-69278.17828039668\n",
      "Iteration #16...\n",
      "E step:\n",
      "M step:\n",
      "-69278.17828039668\n",
      "-69230.10218823883\n",
      "Iteration #17...\n",
      "E step:\n",
      "M step:\n",
      "-69230.10218823883\n",
      "-69196.7464112039\n",
      "Iteration #18...\n",
      "E step:\n",
      "M step:\n",
      "-69196.7464112039\n",
      "-69171.82974891255\n",
      "Iteration #19...\n",
      "E step:\n",
      "M step:\n",
      "-69171.82974891255\n",
      "-69152.40071184955\n",
      "Iteration #20...\n",
      "E step:\n",
      "M step:\n",
      "-69152.40071184955\n",
      "-69136.64835370367\n",
      "Iteration #21...\n",
      "E step:\n",
      "M step:\n",
      "-69136.64835370367\n",
      "-69123.37643452929\n",
      "Iteration #22...\n",
      "E step:\n",
      "M step:\n",
      "-69123.37643452929\n",
      "-69111.8450012409\n",
      "Iteration #23...\n",
      "E step:\n",
      "M step:\n",
      "-69111.8450012409\n",
      "-69101.63766857122\n",
      "Iteration #24...\n",
      "E step:\n",
      "M step:\n",
      "-69101.63766857122\n",
      "-69092.53526354485\n",
      "Iteration #25...\n",
      "E step:\n",
      "M step:\n",
      "-69092.53526354485\n",
      "-69084.3934114019\n",
      "Iteration #26...\n",
      "E step:\n",
      "M step:\n",
      "-69084.3934114019\n",
      "-69077.1022570357\n",
      "Iteration #27...\n",
      "E step:\n",
      "M step:\n",
      "-69077.1022570357\n",
      "-69070.59249677756\n",
      "Iteration #28...\n",
      "E step:\n",
      "M step:\n",
      "-69070.59249677756\n",
      "-69064.78393151311\n",
      "Iteration #29...\n",
      "E step:\n",
      "M step:\n",
      "-69064.78393151311\n",
      "-69059.57312769281\n",
      "Iteration #30...\n",
      "E step:\n",
      "M step:\n",
      "-69059.57312769281\n",
      "-69054.8803716119\n",
      "Iteration #31...\n",
      "E step:\n",
      "M step:\n",
      "-69054.8803716119\n",
      "-69050.6924145638\n",
      "Iteration #32...\n",
      "E step:\n",
      "M step:\n",
      "-69050.6924145638\n",
      "-69047.0591651365\n",
      "Iteration #33...\n",
      "E step:\n",
      "M step:\n",
      "-69047.0591651365\n",
      "-69043.97661000703\n",
      "Iteration #34...\n",
      "E step:\n",
      "M step:\n",
      "-69043.97661000703\n",
      "-69041.34111177853\n",
      "Iteration #35...\n",
      "E step:\n",
      "M step:\n",
      "-69041.34111177853\n",
      "-69039.05335651484\n",
      "Iteration #36...\n",
      "E step:\n",
      "M step:\n",
      "-69039.05335651484\n",
      "-69037.05094208516\n",
      "Iteration #37...\n",
      "E step:\n",
      "M step:\n",
      "-69037.05094208516\n",
      "-69035.27599869588\n",
      "Iteration #38...\n",
      "E step:\n",
      "M step:\n",
      "-69035.27599869588\n",
      "-69033.64211104545\n",
      "Iteration #39...\n",
      "E step:\n",
      "M step:\n",
      "-69033.64211104545\n",
      "-69032.05870115364\n",
      "Iteration #40...\n",
      "E step:\n",
      "M step:\n",
      "-69032.05870115364\n",
      "-69030.54285395135\n",
      "Iteration #41...\n",
      "E step:\n",
      "M step:\n",
      "-69030.54285395135\n",
      "-69029.05492130714\n",
      "Iteration #42...\n",
      "E step:\n",
      "M step:\n",
      "-69029.05492130714\n",
      "-69027.5880215178\n",
      "Iteration #43...\n",
      "E step:\n",
      "M step:\n",
      "-69027.5880215178\n",
      "-69026.21740357153\n",
      "Iteration #44...\n",
      "E step:\n",
      "M step:\n",
      "-69026.21740357153\n",
      "-69024.87711948431\n",
      "Iteration #45...\n",
      "E step:\n",
      "M step:\n",
      "-69024.87711948431\n",
      "-69023.5228481396\n",
      "Iteration #46...\n",
      "E step:\n",
      "M step:\n",
      "-69023.5228481396\n",
      "-69022.20559122002\n",
      "Iteration #47...\n",
      "E step:\n",
      "M step:\n",
      "-69022.20559122002\n",
      "-69020.99008993723\n",
      "Iteration #48...\n",
      "E step:\n",
      "M step:\n",
      "-69020.99008993723\n",
      "-69019.90023524575\n",
      "Iteration #49...\n",
      "E step:\n",
      "M step:\n",
      "-69019.90023524575\n",
      "-69018.95717773841\n",
      "Iteration #50...\n",
      "E step:\n",
      "M step:\n",
      "-69018.95717773841\n",
      "-69018.16322599411\n",
      "Iteration #51...\n",
      "E step:\n",
      "M step:\n",
      "-69018.16322599411\n",
      "-69017.48732692748\n",
      "Iteration #52...\n",
      "E step:\n",
      "M step:\n",
      "-69017.48732692748\n",
      "-69016.90010269805\n",
      "Iteration #53...\n",
      "E step:\n",
      "M step:\n",
      "-69016.90010269805\n",
      "-69016.38417144203\n",
      "Iteration #54...\n",
      "E step:\n",
      "M step:\n",
      "-69016.38417144203\n",
      "-69015.92896509693\n",
      "Iteration #55...\n",
      "E step:\n",
      "M step:\n",
      "-69015.92896509693\n",
      "-69015.52681801708\n",
      "Iteration #56...\n",
      "E step:\n",
      "M step:\n",
      "-69015.52681801708\n",
      "-69015.17142633584\n",
      "Iteration #57...\n",
      "E step:\n",
      "M step:\n",
      "-69015.17142633584\n",
      "-69014.85732696982\n",
      "Iteration #58...\n",
      "E step:\n",
      "M step:\n",
      "-69014.85732696982\n",
      "-69014.57970638564\n",
      "Iteration #59...\n",
      "E step:\n",
      "M step:\n",
      "-69014.57970638564\n",
      "-69014.33430605712\n",
      "Iteration #60...\n",
      "E step:\n",
      "M step:\n",
      "-69014.33430605712\n",
      "-69014.11735712785\n",
      "Iteration #61...\n",
      "E step:\n",
      "M step:\n",
      "-69014.11735712785\n",
      "-69013.92552611393\n",
      "Iteration #62...\n",
      "E step:\n",
      "M step:\n",
      "-69013.92552611393\n",
      "-69013.75586684771\n",
      "Iteration #63...\n",
      "E step:\n",
      "M step:\n",
      "-69013.75586684771\n",
      "-69013.60577728649\n",
      "Iteration #64...\n",
      "E step:\n",
      "M step:\n",
      "-69013.60577728649\n",
      "-69013.47296064712\n",
      "Iteration #65...\n",
      "E step:\n",
      "M step:\n",
      "-69013.47296064712\n",
      "-69013.35539052436\n",
      "Iteration #66...\n",
      "E step:\n",
      "M step:\n",
      "-69013.35539052436\n",
      "-69013.25127968134\n",
      "Iteration #67...\n",
      "E step:\n",
      "M step:\n",
      "-69013.25127968134\n",
      "-69013.15905221284\n",
      "Iteration #68...\n",
      "E step:\n",
      "M step:\n",
      "-69013.15905221284\n",
      "-69013.07731877743\n",
      "Iteration #69...\n",
      "E step:\n",
      "M step:\n",
      "-69013.07731877743\n",
      "-69013.0048546152\n",
      "Iteration #70...\n",
      "E step:\n",
      "M step:\n",
      "-69013.0048546152\n",
      "-69012.94058006929\n",
      "Iteration #71...\n",
      "E step:\n",
      "M step:\n",
      "-69012.94058006929\n",
      "-69012.88354335616\n",
      "Iteration #72...\n",
      "E step:\n",
      "M step:\n",
      "-69012.88354335616\n",
      "-69012.83290534424\n",
      "Iteration #73...\n",
      "E step:\n",
      "M step:\n",
      "-69012.83290534424\n",
      "-69012.7879261194\n",
      "Iteration #74...\n",
      "E step:\n",
      "M step:\n",
      "-69012.7879261194\n",
      "-69012.74795313991\n",
      "Iteration #75...\n",
      "E step:\n",
      "M step:\n",
      "-69012.74795313991\n",
      "-69012.71241079879\n",
      "Iteration #76...\n",
      "E step:\n",
      "M step:\n",
      "-69012.71241079879\n",
      "-69012.68079123099\n",
      "Iteration #77...\n",
      "E step:\n",
      "M step:\n",
      "-69012.68079123099\n",
      "-69012.65264622343\n",
      "Iteration #78...\n",
      "E step:\n",
      "M step:\n",
      "-69012.65264622343\n",
      "-69012.62758009169\n",
      "Iteration #79...\n",
      "E step:\n",
      "M step:\n",
      "-69012.62758009169\n",
      "-69012.6052434161\n",
      "Iteration #80...\n",
      "E step:\n",
      "M step:\n",
      "-69012.6052434161\n",
      "-69012.58532753102\n",
      "Iteration #81...\n",
      "E step:\n",
      "M step:\n",
      "-69012.58532753102\n",
      "-69012.5675596751\n",
      "Iteration #82...\n",
      "E step:\n",
      "M step:\n",
      "-69012.5675596751\n",
      "-69012.55169872892\n",
      "Iteration #83...\n",
      "E step:\n",
      "M step:\n",
      "-69012.55169872892\n",
      "-69012.53753146245\n",
      "Iteration #84...\n",
      "E step:\n",
      "M step:\n",
      "-69012.53753146245\n",
      "-69012.52486923683\n",
      "Iteration #85...\n",
      "E step:\n",
      "M step:\n",
      "-69012.52486923683\n",
      "-69012.51354510052\n",
      "Iteration #86...\n",
      "E step:\n",
      "M step:\n",
      "-69012.51354510052\n",
      "-69012.50341123485\n",
      "Iteration #87...\n",
      "E step:\n",
      "M step:\n",
      "-69012.50341123485\n",
      "-69012.49433670599\n",
      "Iteration #88...\n",
      "E step:\n",
      "M step:\n",
      "-69012.49433670599\n",
      "-69012.48620548628\n",
      "Iteration #89...\n",
      "E step:\n",
      "M step:\n",
      "-69012.48620548628\n",
      "-69012.4789147117\n",
      "Iteration #90...\n",
      "E step:\n",
      "M step:\n",
      "-69012.4789147117\n",
      "-69012.47237314704\n",
      "Iteration #91...\n",
      "E step:\n",
      "M step:\n",
      "-69012.47237314704\n",
      "-69012.4664998346\n",
      "Iteration #92...\n",
      "E step:\n",
      "M step:\n",
      "-69012.4664998346\n",
      "-69012.46122290261\n",
      "Iteration #93...\n",
      "E step:\n",
      "M step:\n",
      "-69012.46122290261\n",
      "-69012.45647851583\n",
      "Iteration #94...\n",
      "E step:\n",
      "M step:\n",
      "-69012.45647851583\n",
      "-69012.45220994906\n",
      "Iteration #95...\n",
      "E step:\n",
      "M step:\n",
      "-69012.45220994906\n",
      "-69012.44836677104\n",
      "Iteration #96...\n",
      "E step:\n",
      "M step:\n",
      "-69012.44836677104\n",
      "-69012.44490412388\n",
      "Iteration #97...\n",
      "E step:\n",
      "M step:\n",
      "-69012.44490412388\n",
      "-69012.44178208717\n",
      "Iteration #98...\n",
      "E step:\n",
      "M step:\n",
      "-69012.44178208717\n",
      "-69012.43896511647\n",
      "Iteration #99...\n",
      "E step:\n",
      "M step:\n",
      "-69012.43896511647\n",
      "-69012.43642154687\n",
      "Iteration #100...\n",
      "E step:\n",
      "M step:\n",
      "-69012.43642154687\n",
      "-69012.43412315585\n",
      "Iteration #101...\n",
      "E step:\n",
      "M step:\n",
      "-69012.43412315585\n",
      "-69012.4320447746\n",
      "Iteration #102...\n",
      "E step:\n",
      "M step:\n",
      "-69012.4320447746\n",
      "-69012.43016394647\n",
      "Iteration #103...\n",
      "E step:\n",
      "M step:\n",
      "-69012.43016394647\n",
      "-69012.42846062292\n",
      "Iteration #104...\n",
      "E step:\n",
      "M step:\n",
      "-69012.42846062292\n",
      "-69012.42691689634\n",
      "Iteration #105...\n",
      "E step:\n",
      "M step:\n",
      "-69012.42691689634\n",
      "-69012.42551676174\n",
      "Iteration #106...\n",
      "E step:\n",
      "M step:\n",
      "-69012.42551676174\n",
      "-69012.42424590666\n",
      "Iteration #107...\n",
      "E step:\n",
      "M step:\n",
      "-69012.42424590666\n",
      "-69012.42309152466\n",
      "Iteration #108...\n",
      "E step:\n",
      "M step:\n",
      "-69012.42309152466\n",
      "-69012.4220421506\n",
      "Iteration #109...\n",
      "E step:\n",
      "M step:\n",
      "-69012.4220421506\n",
      "-69012.42108751365\n",
      "Converge\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.likelihoods = []\n",
    "        self.documents_path = documents_path\n",
    "        self.term_doc_matrix = None \n",
    "        self.document_topic_prob = None  # P(z | d) - INITIALIZE TO RANDOM\n",
    "        self.topic_word_prob = None  # P(w | z) - INITiALIZE TO RANDOM\n",
    "        self.topic_prob = None  # P(z | d, w) - NORMALIZED document_topic_prob * topic_word_prob for each doc, each word \n",
    "\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "    def build_corpus(self): # NOT USED, DID IT ALL IN 1 FUNCTION build_vocabulary\n",
    "        return\n",
    "        \"\"\"\n",
    "        Read document, fill in self.documents, a list of list of word\n",
    "        self.documents = [[\"the\", \"day\", \"is\", \"nice\", \"the\", ...], [], []...]\n",
    "        \n",
    "        Update self.number_of_documents\n",
    "        # #############################\n",
    "        # your code here\n",
    "        # #############################\n",
    "        \"\"\"\n",
    "               \n",
    "       \n",
    "        #pass    # REMOVE THIS\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
    "        for example: [\"rain\", \"the\", ...]\n",
    "\n",
    "        Update self.vocabulary_size\n",
    "        # #############################\n",
    "        # your code here\n",
    "        # #############################\n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = dict()\n",
    "        docnumber = 0\n",
    "        with open(self.documents_path, encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line.strip()\n",
    "                words = line.split()\n",
    "                self.documents.append([])\n",
    "                for word in words:\n",
    "                    if word != \"0\" and word != \"1\": # ignore the first word, it is the 0 or 1\n",
    "                        self.documents[docnumber].append(word)\n",
    "                        if vocab.get(word):\n",
    "                            vocab[word] += 1\n",
    "                        else:\n",
    "                            vocab[word] = 1\n",
    "                docnumber = docnumber + 1\n",
    "            self.number_of_documents = len(self.documents)\n",
    "            print(self.number_of_documents)\n",
    "            self.vocabulary = list(vocab.keys())\n",
    "            self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "    def build_term_doc_matrix(self):\n",
    "        \"\"\"\n",
    "        Construct the term-document matrix where each row represents a document, \n",
    "        and each column represents a vocabulary term.\n",
    "\n",
    "        self.term_doc_matrix[i][j] is the count of term j in document i\n",
    "        # ############################\n",
    "        # your code here\n",
    "        # ############################\n",
    "        \"\"\"\n",
    "        doccount = 0\n",
    "        mymatrix = []\n",
    "        for document in self.documents:\n",
    "            # initialize the variables for this doc\n",
    "            mymatrix.append([])\n",
    "            # count the words for this doc\n",
    "            vocab = dict()\n",
    "            for word in document:\n",
    "                if vocab.get(word):\n",
    "                    vocab[word] += 1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "            wordcount = 0\n",
    "            for uniqueword in self.vocabulary:\n",
    "                if vocab.get(uniqueword):\n",
    "                    mymatrix[doccount].append(vocab.get(uniqueword))\n",
    "                else:\n",
    "                    mymatrix[doccount].append(0)\n",
    "            \n",
    "            doccount = doccount + 1\n",
    "        self.term_doc_matrix = mymatrix\n",
    "\n",
    "        \n",
    "        #pass    # REMOVE THIS\n",
    "\n",
    "\n",
    "    def initialize_randomly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Randomly initialize the matrices: document_topic_prob and topic_word_prob\n",
    "        which hold the probability distributions for P(z | d) and P(w | z): self.document_topic_prob, and self.topic_word_prob\n",
    "\n",
    "        Don't forget to normalize! \n",
    "        HINT: you will find numpy's random matrix useful [https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html]\n",
    "        # ############################\n",
    "        # your code here\n",
    "        # ############################\n",
    "        #np.random.random_sample((3, 2)) \n",
    "        \"\"\"\n",
    "        np.random.RandomState()\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        #print(self.document_topic_prob)\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "\n",
    "        #pass    # REMOVE THIS\n",
    "        \n",
    "\n",
    "    def initialize_uniformly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Initializes the matrices: self.document_topic_prob and self.topic_word_prob with a uniform \n",
    "        probability distribution. This is used for testing purposes.\n",
    "\n",
    "        DO NOT CHANGE THIS FUNCTION\n",
    "        \"\"\"\n",
    "        self.document_topic_prob = np.ones((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.ones((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "    def initialize(self, number_of_topics, random=False):\n",
    "        \"\"\" Call the functions to initialize the matrices document_topic_prob and topic_word_prob\n",
    "        \"\"\"\n",
    "        print(\"Initializing...\")\n",
    "\n",
    "        if random:\n",
    "            self.initialize_randomly(number_of_topics)\n",
    "        else:\n",
    "            self.initialize_uniformly(number_of_topics)\n",
    "\n",
    "    def expectation_step(self, number_of_topics):\n",
    "        \"\"\" The E-step updates P(z | w, d)\n",
    "        \"\"\"\n",
    "        return;\n",
    "                \n",
    "\n",
    "    def maximization_step(self, number_of_topics):\n",
    "        print(\"E step:\")\n",
    "        \n",
    "        #self.topic_prob = np.ones((self.number_of_documents, number_of_topics, self.vocabulary_size))\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                #print(self.topic_prob[docindex,topicindex])\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex][topicindex][wordindex] = self.topic_word_prob[topicindex, wordindex] * self.document_topic_prob[docindex, topicindex]\n",
    "                    mysum += self.topic_prob[docindex][topicindex][wordindex]\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex,topicindex,wordindex] = self.topic_prob[docindex,topicindex,wordindex] / mysum\n",
    "\n",
    "        print(\"M step:\")\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for topicindex in range(0, number_of_topics):\n",
    "                mysum = 0\n",
    "                for wordindex in range(0, self.vocabulary_size):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.document_topic_prob[docindex][topicindex] = mysum\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "            \n",
    "        # update P(z | d) self.document_topic_prob\n",
    "        for topicindex in range(0, number_of_topics):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                for docindex in range(0, self.number_of_documents):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.topic_word_prob[topicindex][wordindex] = mysum\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "        \"\"\" Calculate the current log-likelihood of the model using\n",
    "        the model's updated probability matrices\n",
    "        \n",
    "        Append the calculated log-likelihood to self.likelihoods\n",
    "\n",
    "        Likelihood:\n",
    "        For each doc sum:\n",
    "        C(w,d) * log (sum(Prob of that topic * prob of that word in topic)\n",
    "        \n",
    "        loop over docs (variable in self) - docnumber\n",
    "            loop over words (variable in self) - wordnumber\n",
    "                multiply Prob of that topic * prob of that word in topic\n",
    "        log of this\n",
    "                \n",
    "        look up np matrix multplication\n",
    "        \n",
    "        Then sum over docs\n",
    "        # ############################\n",
    "        # your code here\n",
    "        # ############################\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #print (self.document_topic_prob)\n",
    "        #print (self.topic_word_prob)\n",
    "        #print (self.term_doc_matrix)\n",
    "        Matrix_result = np.matmul(self.document_topic_prob, self.topic_word_prob)\n",
    "        #print (Matrix_result)\n",
    "        arr_result = np.sum(Matrix_result, axis = 0)\n",
    "        #arr_result = Matrix_result[0]\n",
    "        #print (arr_result)\n",
    "        #arr_result1 = np.sum(Matrix_result, axis = 1)\n",
    "        #print (arr_result1)\n",
    "        arr_likelyhood = np.log10(arr_result)\n",
    "       \n",
    "        # sum the rows\n",
    "        #arr_result = np.log10(Matrix_result)\n",
    "        #arr_likelyhood = np.sum(arr_result, axis = 0)\n",
    "        \n",
    "        #print (arr_result)\n",
    "        #print (self.term_doc_matrix)\n",
    "        # log the sums\n",
    "        #arr_likelyhood = np.log10(arr_result)\n",
    "        #arr_likelyhood = np.log10(arr_result)\n",
    "        #print (arr_likelyhood)\n",
    "        #print (self.term_doc_matrix)\n",
    "        # multiply by counts\n",
    "        #self.likelihoods = np.matmul(arr_likelyhood.transpose(), self.term_doc_matrix)\n",
    "        newlikelihoods = []\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            #docscore = np.matmul(arr_likelyhood, self.term_doc_matrix[docindex])\n",
    "            #docscore = np.multiply(arr_likelyhood, self.term_doc_matrix[docindex])\n",
    "            docscore = np.multiply(arr_likelyhood, self.term_doc_matrix[docindex])\n",
    "            #print(self.term_doc_matrix[docindex])\n",
    "            #print(arr_likelyhood)\n",
    "            #print(docscore)\n",
    "            docscore = np.sum(docscore)\n",
    "            #print(docscore)\n",
    "            newlikelihoods.append(docscore)\n",
    "        #print(newlikelihoods)\n",
    "        #print(self.likelihoods)\n",
    "        #self.likelihoods = np.matmul(arr_likelyhood, self.term_doc_matrix)\n",
    "        new_likelihood = np.sum(newlikelihoods)\n",
    "        #print(new_likelihood)\n",
    "        self.likelihoods.append(new_likelihood)\n",
    "        \n",
    "        #return \n",
    "        \"\"\"\n",
    "        newlikely = 0\n",
    "        #print(self.document_topic_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            docsum = 0\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    mysum += self.document_topic_prob[docindex][topicindex] * self.topic_word_prob[topicindex][wordindex]\n",
    "                mysum = math.log10(mysum)\n",
    "                mysum = mysum * self.term_doc_matrix[docindex][wordindex]\n",
    "                docsum += mysum\n",
    "            #print(docsum)\n",
    "            newlikely += docsum\n",
    "        self.likelihoods.append(newlikely)\n",
    "        \n",
    "\n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "\n",
    "        \"\"\"\n",
    "        Model topics.\n",
    "        \"\"\"\n",
    "        print (\"EM iteration begins...\")\n",
    "        \n",
    "        # build term-doc matrix\n",
    "        self.build_term_doc_matrix()\n",
    "        \n",
    "        # Create the counter arrays.\n",
    "        \n",
    "        # P(z | d, w)\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "\n",
    "        # P(z | d) P(w | z)\n",
    "        self.initialize(number_of_topics, random=True)\n",
    "\n",
    "        # Run the EM algorithm\n",
    "        self.calculate_likelihood(number_of_topics)\n",
    "\n",
    "        current_likelihood = self.likelihoods[-1]\n",
    "\n",
    "        for iteration in range(0, max_iter):\n",
    "            print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "            self.expectation_step(number_of_topics)\n",
    "            self.maximization_step(number_of_topics)\n",
    "            self.calculate_likelihood(number_of_topics)\n",
    "            \n",
    "            new_likelihood = self.likelihoods[-1]\n",
    "            print(current_likelihood)\n",
    "            print(new_likelihood)\n",
    "            #return\n",
    "            newepsilon = abs(new_likelihood - current_likelihood)\n",
    "            #print(newepsilon)\n",
    "            if (newepsilon <= epsilon):\n",
    "                #return\n",
    "                print(\"Converge\")\n",
    "                break\n",
    "            current_likelihood = new_likelihood\n",
    "\n",
    "            \"\"\"\n",
    "            # ############################\n",
    "            # your code here\n",
    "            # ############################\n",
    "\n",
    "            pass    # REMOVE THIS\n",
    "            \"\"\"\n",
    "        #print(\"Document/Topic\")\n",
    "        #for index in range(0, 15):\n",
    "            #print(self.document_topic_prob[index])\n",
    "        #print(\"Topic/Word\")\n",
    "        #print(self.topic_word_prob)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    documents_path = 'data/test.txt' \n",
    "    #documents_path = 'data/DBLP2.txt'\n",
    "    #documents_path = 'data/test5.txt'\n",
    "    corpus = Corpus(documents_path)  # instantiate corpus\n",
    "    #corpus.build_corpus()\n",
    "    corpus.build_vocabulary()\n",
    "    #print(corpus.vocabulary)\n",
    "    print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "    print(\"Number of documents:\" + str(len(corpus.documents)))\n",
    "    #corpus.build_term_doc_matrix()  # testing only REMOVE\n",
    "    number_of_topics = 2\n",
    "    #max_iterations = 500\n",
    "    max_iterations = 500\n",
    "    epsilon = 0.001\n",
    "    corpus.plsa(number_of_topics, max_iterations, epsilon)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy",
   "language": "python",
   "name": "mypy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
