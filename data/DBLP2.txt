the combination of flit buffer flow control methods and latency insensitive protocols is an effective solution for networks on chip noc since they both rely on backpressure the two techniques are easy to combine while offering complementary advantages low complexity of router design and the ability to cope with long communication channels via automatic wire pipelining we study various alternative implementations of this idea by considering the combination of three different types of flit buffer flow control methods and two different classes of channel repeaters based respectively on flip flops and relay stations we characterize the area and performance of the two most promising alternative implementations for nocs by completing the rtl design and logic synthesis of the repeaters and routers for different channel parallelisms finally we derive high level abstractions of our circuit designs and we use them to perform system level simulations under various scenarios for two distinct noc topologies and various applications based on our comparative analysis and experimental results we propose noc design approach that combines the reduction of the router queues to minimum size with the distribution of flit buffering onto the channels this approach provides precious flexibility during the physical design phase for many nocs particularly in those systems on chip that must be designed to meet tight constraint on the target clock frequency
we present an easy to use model that addresses the practical issues in designing bus based shared memory multiprocessor systems the model relates the shared bus width bus cycle time cache memory the features of program execution and the number of processors on shared bus to metric called request utilization the request utilization is treated as the scaling factor for the effective average waiting processors in computing the queuing delay cycles simulation study shows that the model performs very well in estimating the shared bus response time using the model system designer can quickly decide the number of the processors that shared bus is able to support effectively the size of the cache memory system should use and the bus cycle time that the main memory system should provide with the model we show that the design favors caching the requests for contention based medium instead of speeding up the transfers although the same performance can be respectively achieved by the two techniques in contention free situation
software product lines spls are used to create tailor made software products by managing and composing reusable assets generating software product from the assets of an spl is possible statically before runtime or dynamically at load time or runtime both approaches have benefits and drawbacks with respect to composition flexibility performance and resource consumption which type of composition is preferable should be decided by taking the application scenario into account current tools and languages however force programmer to decide between static and dynamic composition during development in this paper we present an approach that employs code generation to support static and dynamic composition of features of single code base we offer an implementation on top of featurec an extension of the programming language that supports software composition based on features to simplify dynamic composition and to avoid creation of invalid products we furthermore provide means to validate the correctness of composition at runtime automatically instantiate spls in case of stand alone applications and automatically apply interaction code of crosscutting concerns
experience has proved that interactive applications delivered through digital tv must provide personalized information to the viewers in order to be perceived as valuable service due to the limited computational power of dtv receivers either domestic set top boxes or mobile devices most of the existing systems have opted to place the personalization engines in dedicated servers assuming that return channel is always available for bidirectional communication however in domain where most of the information is transmitted through broadcast there are still many cases of intermittent sporadic or null access to return channel in such situations it is impossible for the servers to learn who is watching tv at the moment and so the personalization features become unavailable to solve this problem without sacrificing much personalization quality this paper introduces solutions to run downsized semantic reasoning process in the dtv receivers supported by pre selection of material driven by audience stereotypes in the head end evaluation results are presented to prove the feasibility of this approach and also to assess the quality it achieves in comparison with previous ones
model based testing techniques play vital role in producing quality software however compared to the testing of functional requirements these techniques are not prevalent that much in testing software security this paper presents model based approach to automatic testing of attack scenarios an attack testing framework is proposed to model attack scenarios and test the system with respect to the modeled attack scenarios the techniques adopted in the framework are applicable in general to the systems where the potential attack scenarios can be modeled in formalism based on extended abstract state machines the attack events ie attack test vectors chosen from the attacks happening in real world are converted to the test driver specific events ready to be tested against the attack signatures the proposed framework is implemented and evaluated using the most common attack scenarios the framework is useful to test software with respect to potential attacks which can significantly reduce the risk of security vulnerabilities
in this paper we address the problem of cache replacement for transcoding proxy caching transcoding proxy is proxy that has the functionality of transcoding multimedia object into an appropriate format or resolution for each client we first propose an effective cache replacement algorithm for transcoding proxy in general when new object is to be cached cache replacement algorithms evict some of the cached objects with the least profit to accommodate the new object our algorithm takes into account of the inter relationships among different versions of the same multimedia object and selects the versions to replace according to their aggregate profit which usually differs from simple summation of their individual profits as assumed in the existing algorithms it also considers cache consistency which is not considered in the existing algorithms we then present complexity analysis to show the efficiency of our algorithm finally we give extensive simulation results to compare the performance of our algorithm with some existing algorithms the results show that our algorithm outperforms others in terms of various performance metrics
distributed proof construction protocols have been shown to be valuable for reasoning about authorization decisions in open distributed environments such as pervasive computing spaces unfortunately existing distributed proof protocols offer only limited support for protecting the confidentiality of sensitive facts which limits their utility in many practical scenarios in this paper we propose distributed proof construction protocol in which the release of fact's truth value can be made contingent upon facts managed by other principals in the system we formally prove that our protocol can safely prove conjunctions of facts without leaking the truth values of individual facts even in the face of colluding adversaries and fact release policies with cyclical dependencies this facilitates the definition of context sensitive release policies that enable the conditional use of sensitive facts in distributed proofs
in this paper we introduce novel approach to image completion which we call structure propagation in our system the user manually specifies important missing structure information by extending few curves or line segments from the known to the unknown regions our approach synthesizes image patches along these user specified curves in the unknown region using patches selected around the curves in the known region structure propagation is formulated as global optimization problem by enforcing structure and consistency constraints if only single curve is specified structure propagation is solved using dynamic programming when multiple intersecting curves are specified we adopt the belief propagation algorithm to find the optimal patches after completing structure propagation we fill in the remaining unknown regions using patch based texture synthesis we show that our approach works well on number of examples that are challenging to state of the art techniques
as text documents are explosively increasing in the internet the process of hierarchical document clustering has been proven to be useful for grouping similar documents for versatile applications however most document clustering methods still suffer from challenges in dealing with the problems of high dimensionality scalability accuracy and meaningful cluster labels in this paper we will present an effective fuzzy frequent itemset based hierarchical clustering ihc approach which uses fuzzy association rule mining algorithm to improve the clustering accuracy of frequent itemset based hierarchical clustering fihc method in our approach the key terms will be extracted from the document set and each document is pre processed into the designated representation for the following mining process then fuzzy association rule mining algorithm for text is employed to discover set of highly related fuzzy frequent itemsets which contain key terms to be regarded as the labels of the candidate clusters finally these documents will be clustered into hierarchical cluster tree by referring to these candidate clusters we have conducted experiments to evaluate the performance based on classic hitech re reuters and wap datasets the experimental results show that our approach not only absolutely retains the merits of fihc but also improves the accuracy quality of fihc
this paper presents novel scheme for maintaining accurate information about distributed data in message passing programs we describe static single assignment ssa based algorithms to build up an intermediate representation of sequential program while targeting code generation for distributed memory machines employing the single program multiple data spmd model of programming this ssa based intermediate representation helps in variety of optimizations performed by our automatic parallelizing compiler paradigm which generates message passing programs and targets distributed memory machines in this paper we concentrate on the semantics and implementation of this ssa form for message passing programs while giving some examples of the kind of optimizations they enable we describe in detail the need for various kinds of merge functions to maintain the single assignment property of distributed data we give algorithms for placement and semantics of these merge functions and show how the requirements are substantially different owing to the presence of distributed data and arbitrary array addressing functions this scheme has been incorporated in our compiler framework which can use uniform methods to compile parallelize and optimize sequential program irrespective of the subscripts used in array addressing functions experimental results for number of benchmarks on an ibm sp show significant improvement in the total runtimes owing to some of the optimizations enabled by the ssa based intermediate representation we have observed up to around ndash reduction in total runtimes in our ssa based schemes compared to non ssa based schemes on processors
super resolution reconstruction of face image is the problem of reconstructing high resolution face image from one or more low resolution face images assuming that high and low resolution images share similar intrinsic geometries various recent super resolution methods reconstruct high resolution images based on weights determined from nearest neighbors in the local embedding of low resolution images these methods suffer disadvantages from the finite number of samples and the nature of manifold learning techniques and hence yield unrealistic reconstructed images to address the problem we apply canonical correlation analysis cca which maximizes the correlation between the local neighbor relationships of high and low resolution images we use it separately for reconstruction of global face appearance and facial details experiments using collection of frontal human faces show that the proposed algorithm improves reconstruction quality over existing state of the art super resolution algorithms both visually and using quantitative peak signal to noise ratio assessment
we consider substantial subset ofc named we develop mathematical specification for by formalizing its abstract syntax execution environment well typedness conditions and operational evaluation semantics based on this specification we prove that is type safe by showing that the execution of programs preserves the types up to subtype relationship
web site presents graph like spatial structure composed of pages connected by hyperlinks this structure may represent an environment in which situated agents associated to visitors of the web site user agents are positioned and moved in order to monitor their navigation this paper presents heterogeneous multi agent system supporting the collection of information related to user's behaviour in web site by specific situated reactive user agents the acquired information is then exploited by interface agents supporting advanced adaptive functionalities based on the history of user's movement in the web site environment interface agents also interact with user agents to acquire information on other visitors of the web site and to support context aware form of interaction among web site visitors
in many advanced database applications eg multimedia databases data objects are transformed into high dimensional points and manipulated in high dimensional space one of the most important but costly operations is the similarity join that combines similar points from multiple datasets in this paper we examine the problem of processing nearest neighbor similarity join knn join knn join between two datasets and returns for each point in its most similar points in we propose new index based knn join approach using the idistance as the underlying index structure we first present its basic algorithm and then propose two different enhancements in the first enhancement we optimize the original knn join algorithm by using approximation bounding cubes in the second enhancement we exploit the reduced dimensions of data space we conducted an extensive experimental study using both synthetic and real datasets and the results verify the performance advantage of our schemes over existing knn join algorithms
caches exploits locality of references to reduce memory access latencies and thereby improve processor performance when an operating system switches application task or performs other kernel services the assumption of locality may be violated because the instructions and data may no longer be in the cache when the preempted operation is resumed thus these operations have an additional cache interference cost that must be taken into account when calculating or estimating the performance and responsiveness of the systemin this paper we present simulation framework suitable for examining the cache interference cost in preemptive real time systems using this framework we measure the interference cost for operating system services and set of embedded benchmarksthe simulations show that there are significant performance gap between the best and worst case execution times even for simple hardware architectures also the worst case performance of some software modules was found to be more or less independent of the cache configuration these results can be used to get better understanding of the execution behavior of preemptive real time systems and can serve as guidelines for choosing suitable cache configurations
in this paper we consider the restless bandit problem which is one of the most well studied generalizations of the celebrated stochastic multi armed bandit problem in decision theory in its ultimate generality the restless bandit problem is known to be pspace hard to approximate to any non trivial factor and little progress has been made on this problem despite its significance in modeling activity allocation under uncertainty we make progress on this problem by showing that for an interesting and general subclass that we term monotone bandits surprisingly simple and intuitive greedy policy yields factor approximation such greedy policies are termed index policies and are popular due to their simplicity and their optimality for the stochastic multi armed bandit problem the monotone bandit problem strictly generalizes the stochastic multi armed bandit problem and naturally models multi project scheduling where the state of project becomes increasingly uncertain when the project is not scheduled we develop several novel techniques in the design and analysis of the index policy our algorithm proceeds by introducing novel balance constraint to the dual of well known lp relaxation to the restless bandit problem this is followed by structural characterization of the optimal solution by using both the exact primal as well as dual complementary slackness conditions this yields an interpretation of the dual variables as potential functions from which we derive the index policy and the associated analysis
we introduce straightforward robust and efficient algorithm for rendering high quality soft shadows in dynamic scenes each frame points in the scene visible from the eye are inserted into spatial acceleration structure shadow umbrae are computed by sampling the scene from the light at the image plane coordinates given by the stored points penumbrae are computed at the same set of points per silhouette edge in two steps first the set of points affected by given edge is estimated from the expected light view screen space bounds of the corresponding penumbra second the actual overlap between these points and the penumbra is computed analytically directly from the occluding geometry the umbral and penumbral sources of occlusion are then combined to determine the degree of shadow at the eye view pixel corresponding to each sample point an implementation of this algorithm for the larrabee architecture yields from to frames per second in simulation for scenes from modern game and produces significantly higher image quality than other recent methods in the real time domain
in this paper we present system using computational linguistic techniques to extract metadata for image access we discuss the implementation functionality and evaluation of an image catalogers toolkit developed in the computational linguistics for metadata building climb research project we have tested components of the system including phrase finding for the art and architecture domain functional semantic labeling using machine learning and disambiguation of terms in domain specific text vis vis rich thesaurus of subject terms geographic and artist names we present specific results on disambiguation techniques and on the nature of the ambiguity problem given the thesaurus resources and domain specific text resource with comparison of domain general resources and text our primary user group for evaluation has been the cataloger expert with specific expertise in the fields of painting sculpture and vernacular and landscape architecture
data mining is new technology that helps businesses to predict future trends and behaviours allowing them to make proactive knowledge driven decisions when data mining tools and techniques are applied on the data warehouse based on customer records they search for the hidden patterns and trends these can be further used to improve customer understanding and acquisition customer relationship management crm systems are adopted by the organisations in order to achieve success in the business and also to formulate business strategies which can be formulated based on the predictions given by the data mining tools basically three major areas of data mining research are identified implementation of crm systems evaluation criteria for data mining software and crm systems and methods to improve data quality for data mining the paper is concluded with proposed integrated model for the crm systems evaluation and implementation this paper focuses on these areas where there is need for more explorations and will provide framework for analysis of the data mining research for crm systems
in this paper we propose the novel concept of probabilistic design for multimedia embedded systems which is motivated by the challenge of how to design but not overdesign such systems while systematically incorporating performance requirements of multimedia application uncertainties in execution time and tolerance for reasonable execution failures unlike most present techniques that are based on either worst or average case execution times of application tasks where the former guarantees the completion of each execution but often leads to overdesigned systems and the latter fails to provide any completion guarantees the proposed probabilistic design method takes advantage of unique features mentioned above of multimedia systems to relax the rigid hardware requirements for software implementation and avoid overdesigning the system in essence this relaxation expands the design space and we further develop an off line on line minimum effort algorithm for quick exploration of the enlarged design space at early design stages this is the first step toward our goal of bridging the gap between real time analysis and embedded software implementation for rapid and economic multimedia system design it is our belief that the proposed method has great potential in reducing system resource while meeting performance requirements the experimental results confirm this as we achieve significant saving in system's energy consumption to provide statistical completion ratio guarantee ie the expected number of completions over large number of iterations is greater than given value
column statistics are an important element of cardinality estimation frameworks more accurate estimates allow the optimizer of rdbms to generate better plans and improve the overall system's efficiency this paper introduces filtered statistics which model value distribution over set of rows restricted by predicate this feature available in microsoft sql server can be used to handle column correlation as well as focus on interesting data ranges in particular it fits well for scenarios with logical subtables like flexible schema or multi tenant applications integration with the existing cardinality estimation infrastructure is presented
to keep up with the explosive internet packet processing demands modern network processors nps employ highly parallel multi threaded and multi core architecture in such parallel paradigm accesses to the shared variables in the external memory and the associated memory latency are contained in the critical sections so that they can be executed atomically and sequentially by different threads in the network processor in this paper we present novel program transformation that is used in the intel auto partitioning compiler for ixp to exploit the inherent finer grained parallelism of those critical sections using the software controlled caching mechanism available in the nps consequently those critical sections can be executed in pipelined fashion by different threads thereby effectively hiding the memory latency and improving the performance of network applications experimental results show that the proposed transformation provides impressive speedup up to and scalability up to threads of the performance for the real world network application lgbps efhernet core metro router
we show that if connected graph with nodes has conductance then rumour spreading also known as randomized broadcast successfully broadcasts message within log many rounds with high probability regardless of the source by using the push pull strategy the notation hides polylog factor this result is almost tight since there exists graph of nodes and conductance with diameter log if in addition the network satisfies some kind of uniformity condition on the degrees our analysis implies that both both push and pull by themselves successfully broadcast the message to every node in the same number of rounds
although database design tools have been developed that attempt to automate or semiautomate the design process these tools do not have the capability to capture common sense knowledge about business applications and store it in context specific manner as result they rely on the user to provide great deal of trivial details and do not function as well as human designer who usually has some general knowledge of how an application might work based on his or her common sense knowledge of the real world common sense knowledge could be used by database design system to validate and improve the quality of an existing design or even generate new designs this requires that context specific information about different database design applications be stored and generalized into information about specific application domains eg pharmacy daycare hospital university manufacturing such information should be stored at the appropriate level of generality in hierarchically structured knowledge base so that it can be inherited by the subdomains below for this to occur two types of learning must take place first knowledge about particular application domain that is acquired from specific applications within that domain are generalized into domain node eg entities relationships and attributes from various hospital applications are generalized to hospital node this is referred to as within domain learning second the information common to two or more related application domain nodes is generalized to higher level node for example knowledge from the car rental and video rental domains may be generalized to rental node this is called across domain learning this paper presents methodology for learning across different application domains based on distance measure the parameters used in this methodology were refined by testing on set of representative cases empirical testing provided further validation
in this paper we address the problem of clustering graphs in object oriented databases unlike previous studies which focused only on workload consisting of single operation this study tackles the problem when the workload is set of operations method and queries that occur with certain probability thus the goal is to minimize the expected cost of an operation in the workload while maintaining similarly low cost for each individual operation classto this end we present new clustering policy based on the nearest neighbor graph partitioning algorithm we then demonstrate that this policy provides considerable gains when compared to suite of well known clustering policies proposed in the literature our results are based on two widely referenced object oriented database benchmarks namely the tektronix hypermodel and oo
integrating several legacy software systems together is commonly performed with multiple applications of the adapter design pattern in oo languages such as java the integration is based on specifying bi directional translations between pairs of apis from different systems yet manual development of wrappers to implement these translations is tedious expensive and error prone in this paper we explore how models aspects and generative techniques can be used in conjunction to alleviate the implementation of multiple wrappers briefly the steps are the automatic reverse engineering of relevant concepts in apis to high level models the manual definition of mapping relationships between concepts in different models of apis using an ad hoc dsl the automatic generation of wrappers from these mapping specifications using aop this approach is weighted against manual development of wrappers using an industrial case study criteria are the relative code length and the increase of automation
higher order abstract syntax is simple technique for implementing languages with functional programming object variables and binders are implemented by variables and binders in the host language by using this technique one can avoid implementing common and tricky routines dealing with variables such as capture avoiding substitution however despite the advantages this technique provides it is not commonly used because it is difficult to write sound elimination forms such as folds or catamorphisms for higher order abstract syntax to fold over such data type one must either simultaneously define an inverse operation which may not exist or show that all functions embedded in the data type are parametric in this paper we show how first class polymorphism can be used to guarantee the parametricity of functions embedded in higher order abstract syntax with this restriction we implement library of iteration operators over data structures containing functionals from this implementation we derive fusion laws that functional programmers may use to reason about the iteration operator finally we show how this use of parametric polymorphism corresponds to the sch√ºrmann despeyroux and pfenning method of enforcing parametricity through modal types we do so by using this library to give sound and complete encoding of their calculus into system private char inline graphic mime subtype gif xlink schar private char this encoding can serve as starting point for reasoning about higher order structures in polymorphic languages
we introduce theoretical framework for discovering relationships between two database instances over distinct and unknown schemata this framework is grounded in the context of data exchange we formalize the problem of understanding the relationship between two instances as that of obtaining schema mapping so that minimum repair of this mapping provides perfect description of the target instance given the source instance we show that this definition yields intuitive results when applied on database instances derived from each other by basic operations we study the complexity of decision problems related to this optimality notion in the context of different logical languages and show that even in very restricted cases the problem is of high complexity
mining association rules is an important technique for discovering meaningful patterns in transaction databases many different measures of interestingness have been proposed for association rules however these measures fail to take the probabilistic properties of the mined data into account we start this paper with presenting simple probabilistic framework for transaction data which can be used to simulate transaction data when no associations are present we use such data and real world database from grocery outlet to explore the behavior of confidence and lift two popular interest measures used for rule mining the results show that confidence is systematically influenced by the frequency of the items in the left hand side of rules and that lift performs poorly to filter random noise in transaction data based on the probabilistic framework we develop two new interest measures hyper lift and hyper confidence which can be used to filter or order mined association rules the new measures show significantly better performance than lift for applications where spurious rules are problematic
in spite of impressive gains by pl fortran and cobol remain the languages in which most of the world's production programs are written and will remain so into the foreseeable future there is great deal of theoretical interest in algol and in extensible languages but so far at least they have had little practical impact problem oriented languages may very well become the most important language development area in the next five to ten years in the operating system area all major computer manufacturers set out to produce very ambitious multiprogramming systems and they all ran into similar problems number of university projects though not directly comparable to those of the manufacturers have contributed greatly to better understanding of operating system principles important trends include the increased interest in the development of system measurement and evaluation techniques and increased use of microprogramming for some programming system functions
this paper describes an ambient intelligent prototype known as socio ec socio ec explores the design and implementation of system for sensing and display user modeling and interaction models based on game structure the game structure includes word puzzles levels body states goals and game skills body states are body movements and positions that players must discover in order to complete level and in turn represent learned game skill the paper provides an overview of background concepts and related research we describe the prototype and game structure provide technical description of the prototype and discuss technical issues related to sensing reasoning and display the paper contributes by providing method for constructing group parameters from individual parameters with real time motion capture data and model for mapping the trajectory of participant's actions in order to determine an intensity level used to manage the experience flow of the game and its representation in audio and visual display we conclude with discussion of known and outstanding technical issues and future research
we describe forward rasterization class of rendering algorithms designed for small polygonal primitives the primitive is efficiently rasterized by interpolation between its vertices the interpolation factors are chosen to guarantee that each pixel covered by the primitive receives at least one sample which avoids holes the location of the samples is recorded with subpixel accuracy using pair of offsets which are then used to reconstruct resample the output image offset reconstruction has good static and temporal antialiasing properties we present two forward rasterization algorithms one that renders quadrilaterals and is suitable for scenes modeled with depth images like in image based rendering by warping and one that renders triangles and is suitable for scenes modeled conventionally when compared to conventional rasterization forward rasterization is more efficient for small primitives and has better temporal antialiasing properties
there is common misconception that the automobile industry is slow to adapt new technologies such as artificial intelligence ai and soft computing the reality is that many new technologies are deployed and brought to the public through the vehicles that they drive this paper provides an overview and sampling of many of the ways that the automotive industry has utilized ai soft computing and other intelligent system technologies in such diverse domains like manufacturing diagnostics on board systems warranty analysis and design
as memory hierarchy becomes deeper and shared by more processors locality increasingly determines system performance as rigorous and precise locality model reuse distance has been used in program optimizations performance prediction memory disambiguation and locality phase prediction however the high cost of measurement has been severely impeding its uses in scenarios requiring high efficiency such as product compilers performance debugging run time optimizationswe recently discovered the statistical connection between time and reuse distance which led to an efficient way to approximate reuse distance using time however not exposed are some algorithmic and implementation techniques that are vital for the efficiency and scalability of the approximation model this paper presents these techniques it describes an algorithm that approximates reuse distance on arbitrary scales it explains portable scheme that employs memory controller to accelerate the measure of time distance it uncovers the algorithm and proof of trace generator that can facilitate various locality studies
retrieving images from large image collection has been an active area of research most of the existing works have focused on content representation in this paper we address the issue of identifying relevant images quickly this is important in order to meet the users performance requirements we propose framework for fast image retrieval based on object shapes extracted from objects within images the framework builds hierarchy of approximations on object shapes such that shape representation at higher level is coarser representation of shape at the lower level in other words multiple shapes at lower level can be mapped into single shape at higher level in this way the hierarchy serves to partition the database at various granularities given query shape by searching only the relevant paths in the hierarchy large portion of the database can thus be pruned away we propose the angle mapping am method to transform shape from one level to another higher level am essentially replaces some edges of shape by smaller number of edges based on the angles between the edges thus reducing the complexity of the original shape based on the framework we also propose two hierarchical structures to facilitate speedy retrieval the first called hierarchical partitioning on shape representation hpsr uses the shape representation as the indexing key the second called hierarchical partitioning on angle vector hpav captures the angle information from the shape representation we conducted an extensive study on both methods to see their quality and efficiency our experiments on sets of images each of which has objects around from to showed that the framework can provide speedy image retrieval without sacrificing on the quality both proposed schemes can improve the efficiency by as much as hundreds of times to sequential scanning the improvement grows as image database size objects per image or object dimension increase
with the advent of extensive wireless networks that blanket physically compact urban enclaves such as office complexes shopping centers or university campuses it is possible to create software applications that provide location based mobile online services one such application is campuswiki which integrates location information into wiki structure in the design science research reported in this paper we employed form of action research in which we engaged users as participants in an iterative process of designing and evaluating campuswiki two qualitative studies were undertaken early in the design process in which semi structured interviews were used to assess potential users reactions to campuswiki through this research the designers were able to assess whether their intentions matched the mental models of potential users of the application the results showed that although many of the perceived benefits were as designed by the developers misunderstanding of the location aware feature led users to unanticipated concerns and expectations these findings are important in guiding designers and implementers on the desirable and possibly undesirable features of such systems
existing template independent web data extraction approaches adopt highly ineffective decoupled strategies attempting to do data record detection and attribute labeling in two separate phases in this paper we propose an integrated web data extraction paradigm with hierarchical models the proposed model is called dynamic hierarchical markov random fields dhmrfs dhmrfs take structural uncertainty into consideration and define joint distribution of both model structure and class labels the joint distribution is an exponential family distribution as conditional model dhmrfs relax the independence assumption as made in directed models since exact inference is intractable variational method is developed to learn the model's parameters and to find the map model structure and label assignments we apply dhmrfs to real world web data extraction task experimental results show that integrated web data extraction models can achieve significant improvements on both record detection and attribute labeling compared to decoupled models in diverse web data extraction dhmrfs can potentially address the blocky artifact issue which is suffered by fixed structured hierarchical models
as the total amount of traffic data in networks has been growing at an alarming rate there is currently substantial body of research that attempts to mine traffic data with the purpose of obtaining useful information for instance there are some investigations into the detection of internet worms and intrusions by discovering abnormal traffic patterns however since network traffic data contain information about the internet usage patterns of users network users privacy may be compromised during the mining process in this paper we propose an efficient and practical method that preserves privacy during sequential pattern mining on network traffic data in order to discover frequent sequential patterns without violating privacy our method uses the repository server model which operates as single mining server and the retention replacement technique which changes the answer to query probabilistically in addition our method accelerates the overall mining process by maintaining the meta tables in each site so as to determine quickly whether candidate patterns have ever occurred in the site or not extensive experiments with real world network traffic data revealed the correctness and the efficiency of the proposed method
this paper describes llvm low level virtual machine compiler framework designed to support transparent lifelongprogram analysis and transformation for arbitrary programs by providing high level information to compilertransformations at compile time link time run time and inidle time between runsllvm defines common low levelcode representation in static single assignment ssa form with several novel features simple language independenttype system that exposes the primitives commonly used toimplement high level language features an instruction fortyped address arithmetic and simple mechanism that canbe used to implement the exception handling features ofhigh level languages and setjmp longjmp in uniformlyand efficientlythe llvm compiler framework and coderepresentation together provide combination of key capabilitiesthat are important for practical lifelong analysis andtransformation of programsto our knowledge no existingcompilation approach provides all these capabilitieswe describethe design of the llvm representation and compilerframework and evaluate the design in three ways thesize and effectiveness of the representation including thetype information it provides compiler performance forseveral interprocedural problems and illustrative examplesof the benefits llvm provides for several challengingcompiler problems
caching frequently accessed data items on the client side is an effective technique for improving performance in mobile environment classical cache invalidation strategies are not suitable for mobile environments due to frequent disconnections and mobility of the clients one attractive cache invalidation technique is based on invalidation reports irs however the ir based cache invalidation solution has two major drawbacks which have not been addressed in previous research first there is long query latency associated with this solution since client cannot answer the query until the next ir interval second when the server updates hot data item all clients have to query the server and get the data from the server separately which wastes large amount of bandwidth in this paper we propose an ir based cache invalidation algorithm which can significantly reduce the query latency and efficiently utilize the broadcast bandwidth detailed analytical analysis and simulation experiments are carried out to evaluate the proposed methodology compared to previous ir based schemes our scheme can significantly improve the throughput and reduce the query latency the number of uplink request and the broadcast bandwidth requirements
physical database design is important for query performance in shared nothing parallel database system in which data is horizontally partitioned among multiple independent nodes we seek to automate the process of data partitioning given workload of sql statements we seek to determine automatically how to partition the base data across multiple nodes to achieve overall optimal or close to optimal performance for that workload previous attempts use heuristic rules to make those decisions these approaches fail to consider all of the interdependent aspects of query performance typically modeled by today's sophisticated query optimizerswe present comprehensive solution to the problem that has been tightly integrated with the optimizer of commercial shared nothing parallel database system our approach uses the query optimizer itself both to recommend candidate partitions for each table that will benefit each query in the workload and to evaluate various combinations of these candidates we compare rank based enumeration method with random based one our experimental results show that the former is more effective
considering the constraint brought by mobility and resources it is important for routing protocols to efficiently deliver data in intermittently connected mobile network icmn different from previous works that use the knowledge of previous encounters to predict the future contact we propose storagefriendly region based protocol namely rena in this paper instead of using temporal information rena builds routing tables based on regional movement history which avoids excessive storage for tracking encounter history we validate the generality of rena through time variant community mobility model with parameters extracted from the mit wlan trace and the vehicular network based on bus routes of the city of helsinki the comprehensive simulation results show that rena is not only storage friendly but also more efficient than the epidemic routing the restricted replication protocol snw and the encounter based protocol rapid under various conditions
we propose distributed on demand power management protocol for collecting data in sensor networks the protocol aims to reduce power consumption while supporting fluctuating demand in the network and provide local routing information and synchronicity without global control energy savings are achieved by powering down nodes during idle times identified through dynamic scheduling we present real implementation on wireless sensor nodes based on novel two level architecture we evaluate our approach through measurements and simulation and show how the protocol allows adaptive scheduling and enables smooth trade off between energy savings and latency an example current measurement shows an energy savings of on an intermediate node
we study new research problem where an implicit information retrieval query is inferred from eye movements measured when the user is reading and used to retrieve new documents in the training phase the user's interest is known and we learn mapping from how the user looks at term to the role of the term in the implicit query assuming the mapping is universal that is the same for all queries in given domain we can use it to construct queries even for new topics for which no learning data is available we constructed controlled experimental setting to show that when the system has no prior information as to what the user is searching the eye movements help significantly in the search this is the case in proactive search for instance where the system monitors the reading behaviour of the user in new topic in contrast during search or reading session where the set of inspected documents is biased towards being relevant stronger strategy is to search for content wise similar documents than to use the eye movements
in recent years network of workstations pcs so called now are becoming appealing vehicles for cost effective parallel computing due to the commodity nature of workstations and networking equipment lan environments are gradually becoming heterogeneous the diverse sources of heterogeneity in now systems pose challenge on the design of efficient communication algorithms for this class of systems in this paper we propose efficient algorithms for multiple multicast on heterogeneous now systems focusing on heterogeneity in processing speeds of workstations pcs multiple multicast is an important operation in many scientific and industrial applications multicast on heterogeneous systems has not been investigated until recently our work distinguishes itself from others in two aspects in contrast to the blocking communication model used in prior works we model communication in heterogeneous cluster more accurately by non blocking communication model and design multicast algorithms that can fully take advantage of non blocking communication while prior works focus on single multicast problem we propose efficient algorithms for general multiple multicast in which single multicast is special case on heterogeneous now systems to our knowledge our work is the earliest effort that addresses multiple multicast for heterogeneous now systems these algorithms are evaluated using network simulator for heterogeneous now systems our experimental results on system of up to nodes show that some of the algorithms outperform others in many cases the best algorithm achieves completion time that is within times of the lower bound
supporting quality of service qos in wireless networks has been very rich and interesting area of research many significant advances have been made in supporting qos in single wireless networks however the support for the qos across multiple heterogeneous wireless networks will be required in the future wireless networks in connections spanning multiple wireless networks the end to end qos will depend on several factors such as mobility and connection patterns of users and the qos policies in each of the wireless networks the end to end qos is also affected by multiple decisions that must be made by several different network entities for resource allocation the paper has two objectives one is to demonstrate the decision making process for resource allocation in multiple heterogeneous wireless networks and the second is to present novel concept of composite qos in such wireless environment more specifically we present an architecture for multiple heterogeneous wireless networks decision making process for resource request and allocation simulation model to study composite qos and several interesting results we also present potential implications of composite qos on users and network service providers we also show how the qos ideas presented in this paper can be used by wireless carriers for improved qos support and management the paper can form the basis for significant further research in dss for emerging wireless networks supporting qos for range of sophisticated and resource intensive mobile applications
this work addresses the problem of optimizing the deployment of sensors in order to ensure the quality of the readings of the value of interest in given critical geographic region as usual we assume that each sensor is capable of reading particular physical phenomenon eg concentration of toxic materials in the air and transmitting it to server or peer however the key assumptions considered in this work are each sensor is capable of moving where the motion may be remotely controlled and the spatial range for which the individual sensor's reading is guaranteed to be of desired quality is limited in scenarios like disaster management and homeland security in case some of the sensors dispersed in larger geographic area report value higher than certain threshold one may want to ensure quality of the readings for the affected region this in turn implies that one may want to ensure that there are enough sensors there and consequently guide subset of the rest of the sensors towards the affected region in this paper we explore variants of the problem of optimizing the guidance of the mobile sensors towards the affected geographic region and we present algorithms for their solutions
we describe in place reconfiguration ipr for lut based fpgas an algorithm that maximizes identical configuration bits for complementary inputs of lut thereby reducing the propagation of faults seen at pair of complementary inputs based on ipr we develop fault tolerant logic resynthesis algorithm which decreases the circuit fault rate while preserving functionality and topology of the lut based logic network since the topology is preserved the resynthesis algorithm can be applied post layout and without changes in physical design compared to the state of the art academic technology mapper berkeley abc ipr reduces the relative fault rate by and increases mttf by with the same area and performance and ipr combined with previous fault tolerant logic resynthesis algorithm rose reduces the relative fault rate by and increases mttf by with less area but same performance the above improvement assumes stochastic single fault and more improvement is expected for multi fault models
in this paper an interactive and realistic virtual head oriented to human computer interaction and social robotics is presented it has been designed following hybrid approach taking robotic characteristics into account and searching for convergence between these characteristics real facial actions and animation techniques an initial head model is first obtained from real person using laser scanner then the model is animated using hierarchical skeleton based procedure the proposed rig structure is close to real facial muscular anatomy and its behaviour follows the facial action coding system speech synthesis and visual human face tracking capabilities are also integrated for providing the head with further interaction ability using the said hybrid approach the head can be readily linked to social robot architecture the opinions of number of persons interacting with this social avatar have been evaluated and are reported in the paper as against their reactions when interacting with social robot with mechatronic face results show the suitability of the avatar for on screen real time interfacing in human computer interaction the proposed technique could also be helpful in the future for designing and parameterizing mechatronic human like heads for social robots
the emerging paradigm of electronic services promises to bring to distributed computation and services the flexibility that the web has brought to the sharing of documents an understanding of fundamental properties of service composition is required in order to take full advantage of the paradigm this paper examines proposals and standards for services from the perspectives of xml data management workflow and process models key areas for study are identified including behavioral service signatures verification and synthesis techniques for composite services analysis of service data manipulation commands and xml analysis applied to service specifications we give sample of the relevant results and techniques in each of these areas
generic database replication algorithms do not scale linearly in throughput as all update deletion and insertion udi queries must be applied to every database replica the throughput is therefore limited to the point where the number of udi queries alone is sufficient to overload one server in such scenarios partial replication of database can help as udi queries are executed only by subset of all servers in this paper we propose globetp system that employs partial replication to improve database throughput globetp exploits the fact that web application's query workload is composed of small set of read and write templates using knowledge of these templates and their respective execution costs globetp provides database table placements that produce significant improvements in database throughput we demonstrate the efficiency of this technique using two different industry standard benchmarks in our experiments globetp increases the throughput by to compared to full replication while using identical hardware configuration furthermore adding single query cache improves the throughput by another to
many artificial intelligence tasks such as automated question answering reasoning or heterogeneous database integration involve verification of semantic category eg coffee is drink red is color while steak is not drink and big is not color we present novel algorithm to automatically validate semantic category contrary to the methods suggested earlier our approach does not rely on any manually codified knowledge but instead capitalizes on the diversity of topics and word usage on the world wide web we have tested our approach within our online fact seeking question answering environment when tested on the trec questions that expect the answer to belong to specific semantic category our approach has improved the accuracy by up to depending on the model and metrics used
in the rank join problem we are given set of relations and scoring function and the goal is to return the join results with the top scores it is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic these conditions allow for efficient algorithms that solve the rank join problem without reading all of the input in this article we present thorough analysis of such rank join algorithms strong point of our analysis is that it is based on more general problem statement than previous work making it more relevant to the execution model that is employed by database systems one of our results indicates that the well known hrjn algorithm has shortcomings because it does not stop reading its input as soon as possible we find that it is np hard to overcome this weakness in the general case but cases of limited query complexity are tractable we prove the latter with an algorithm that infers provably tight bounds on the potential benefit of reading more input in order to stop as soon as possible as result the algorithm achieves cost that is within constant factor of optimal
in this paper we present method for organizing and indexing logo digital libraries like the ones of the patent and trademark offices we propose an efficient queried by example retrieval system which is able to retrieve logos by similarity from large databases of logo images logos are compactly described by variant of the shape context descriptor these descriptors are then indexed by locality sensitive hashing data structure aiming to perform approximate nn search in high dimensional spaces in sub linear time the experiments demonstrate the effectiveness and efficiency of this system on realistic datasets as the tobacco logo database
early applications of smart cards have focused in the area of personal security recently there has been an increasing demand for networked multi application cards in this new scenario enhanced application specific on card java applets and complex cryptographic services are executed through the smart card java virtual machine jvm in order to support such computation intensive applications contemporary smart cards are designed with built in microprocessors and memory as smart cards are highly area constrained environments with memory cpu and peripherals competing for very small die space the vm execution engine of choice is often small slow interpreter in addition support for multiple applications and cryptographic services demands high performance vm execution engine the above necessitates the optimization of the jvm for java cardsin this paper we present the concept of an annotation aware interpreter that optimizes the interpreted execution of java code using java bytecode superoperators sos sos are groups of bytecode operations that are executed as specialized vm instruction simultaneous translation of all the bytecode operations in an so reduces the bytecode dispatch cost and the number of stack accesses data transfer to from the java operand stack and stack pointer updates furthermore sos help improve native code quality without hindering class file portability annotation attributes in the class files mark the occurrences of valuable sos thereby dispensing the expensive task of searching and selecting sos at runtime besides our annotation based approach incurs minimal memory overhead as opposed to just in time jit compilerswe obtain an average speedup of using an interpreter customized with the top sos formed from operation folding patterns further we show that greater speedups could be achieved by statically adding to the interpreter application specific sos formed by top basic blocks the effectiveness of our approach is evidenced by performance improvements of upto obtained using sos formed from optimized basic blocks
engineering knowledge is specific kind of knowledge that is oriented to the production of particular classes of artifacts is typically related to disciplined design methods and takes place in tool intensive contexts as consequence representing engineering knowledge requires the elaboration of complex models that combine functional and structural representations of the resulting artifacts with process and methodological knowledge the different categories used in the engineering domain vary in their status and in the way they should be manipulated when building applications that support engineering processes these categories include artifacts activities methods and models this paper surveys existing models of engineering knowledge and discusses an upper ontology that abstracts the categories that crosscut different engineering domains such an upper model can be reused for particular engineering disciplines the process of creating such elaborations is reported on the particular case study of software engineering as concrete application example
recent works have shown the benefits of keyword proximity search in querying xml documents in addition to text documents for example given query keywords over shakespeare's plays in xml the user might be interested in knowing how the keywords cooccur in this paper we focus on xml trees and define xml keyword proximity queries to return the possibly heterogeneous set of minimum connecting trees mcts of the matches to the individual keywords in the query we consider efficiently executing keyword proximity queries on labeled trees xml in various settings when the xml database has been preprocessed and when no indices are available on the xml database we perform detailed experimental evaluation to study the benefits of our approach and show that our algorithms considerably outperform prior algorithms and other applicable approaches
many current research efforts address the problem of personalizing the web experience for each user with respect to user's identity and or context in this paper we propose new high level model for the specification of web applications that takes into account the manner in which users interact with the application for supplying appropriate contents or gathering profile data we therefore consider entire behaviors rather than single properties as the smallest information units allowing for automatic restructuring of application components for this purpose high level event condition action eca paradigm is proposed which enables capturing arbitrary and timed clicking behaviors also the architecture and components of first prototype implementation are discussed
one reason that researchers may wish to demonstrate that an external software quality attribute can be measured consistently is so that they can validate prediction system for the attribute however attempts at validating prediction systems for external subjective quality attributes have tended to rely on experts indicating that the values provided by the prediction systems informally agree with the experts intuition about the attribute these attempts are undertaken without pre defined scale on which it is known that the attribute can be measured consistently consequently valid unbiased estimate of the predictive capability of the prediction system cannot be given because the experts measurement process is not independent of the prediction system's values usually no justification is given for not checking to see if the experts can measure the attribute consistently it seems to be assumed that subjective measurement isn't proper measurement or subjective measurement cannot be quantified or no one knows the true values of the attributes anyway and they cannot be estimated however even though the classification of software systems or software artefacts quality attributes is subjective it is possible to quantify experts measurements in terms of conditional probabilities it is then possible using statistical approach to assess formally whether the experts measurements can be considered consistent if the measurements are consistent it is also possible to identify estimates of the true values which are independent of the prediction system these values can then be used to assess the predictive capability of the prediction system in this paper we use bayesian inference markov chain monte carlo simulation and missing data imputation to develop statistical tests for consistent measurement of subjective ordinal scale attributes
for robots operating in real world environments the ability to deal with dynamic entities such as humans animals vehicles or other robots is of fundamental importance the variability of dynamic objects however is large in general which makes it hard to manually design suitable models for their appearance and dynamics in this paper we present an unsupervised learning approach to this model building problem we describe an exemplar based model for representing the time varying appearance of objects in planar laser scans as well as clustering procedure that builds set of object classes from given observation sequences extensive experiments in real environments demonstrate that our system is able to autonomously learn useful models for eg pedestrians skaters or cyclists without being provided with external class information
we consider the problem of establishing route and sending packets between source destination pair in ad hoc networks composed of rational selfish nodes whose purpose is to maximize their own utility in order to motivate nodes to follow the protocol specification we use side payments that are made to the forwarding nodes our goal is to design fully distributed algorithm such that node is always better off participating in the protocol execution individual rationality ii node is always better off behaving according to the protocol specification truthfulness iii messages are routed along the most energy efficient least cost path and iv the message complexity is reasonably low we introduce the commit protocol for individually rational truthful and energy efficient routing in ad hoc networks to the best of our knowledge this is the first ad hoc routing protocol with these features commit is based on the vcg payment scheme in conjunction with novel game theoretic technique to achieve truthfulness for the sender node by means of simulation we show that the inevitable economic inefficiency is small as an aside our work demonstrates the advantage of using cross layer approach to solving problems leveraging the existence of an underlying topology control protocol we are able to simplify the design and analysis of our routing protocol and to reduce its message complexity on the other hand our investigation of the routing problem in presence of selfish nodes disclosed new metric under which topology control protocols can be evaluated the cost of cooperation
modern presentation software is still built around interaction metaphors adapted from traditional slide projectors we provide an analysis of the problems in this application genre that presentation authors face and present fly presentation tool that is based on the idea of planar information structures inspired by the natural human thought processes of data chunking association and spatial memory fly explores authoring of presentation documents evaluation of paper prototype showed that the planar ui is easily grasped by users and leads to presentations more closely resembling the information structure of the original content thus providing better authoring support than the slide metaphor our software prototype confirmed these results and outperformed powerpoint in second study for tasks such as prototyping presentations and generating meaningful overviews users reported that this interface helped them better to express their concepts and expressed significant preference for fly over the traditional slide model
we study generalization of the constraint satisfaction problem csp the periodic constraint satisfaction problem an input instance of the periodic csp is finite set of generating constraints over structured variable set that implicitly specifies larger possibly infinite set of constraints the problem is to decide whether or not the larger set of constraints has satisfying assignment this model is natural for studying constraint networks consisting of constraints obeying high degree of regularity or symmetry our main contribution is the identification of two broad polynomial time tractable subclasses of the periodic csp
suppose we are given graph and set of terminals we consider the problem of constructing graph eh that approximately preserves the congestion of every multicommodity flow with endpoints supported in we refer to such graph as flow sparsifier we prove that there exist flow sparsifiers that simultaneously preserve the congestion of all multicommodity flows within an log log log factor where this bound improves to if excludes any fixed minor this is strengthening of previous results which consider the problem of finding graph eh cut sparsifier that approximately preserves the value of minimum cuts separating any partition of the terminals indirectly our result also allows us to give construction for better quality cut sparsifiers and flow sparsifiers thereby we immediately improve all approximation ratios derived using vertex sparsification in we also prove an log log lower bound for how well flow sparsifier can simultaneously approximate the congestion of every multicommodity flow in the original graph the proof of this theorem relies on technique which we refer to as oblivious dual certifcates for proving super constant congestion lower bounds against many multicommodity flows at once our result implies that approximation algorithms for multicommodity flow type problems designed by black box reduction to uniform case on nodes see for examples must incur super constant cost in the approximation ratio
similarity search is important in information retrieval applications where objects are usually represented as vectors of high dimensionality this paper proposes new dimensionality reduction technique and an indexing mechanism for high dimensional datasets the proposed technique reduces the dimensions for which coordinates are less than critical value with respect to each data vector this flexible datawise dimensionality reduction contributes to improving indexing mechanisms for high dimensional datasets that are in skewed distributions in all coordinates to apply the proposed technique to information retrieval cva file compact va file which is revised version of the va file is developed by using cva file the size of index files is reduced further while the tightness of the index bounds is held maximally the effectiveness is confirmed by synthetic and real data
many of today's high level parallel languages support dynamic fine grained parallelism these languages allow the user to expose all the parallelism in the program which is typically of much higher degree than the number of processors hence an efficient scheduling algorithm is required to assign computations to processors at runtime besides having low overheads and good load balancing it is important for the scheduling algorithm to minimize the space usage of the parallel program this paper presents scheduling algorithm that is provably space efficient and time efficient for nested parallel languages in addition to proving the space and time bounds of the parallel schedule generated by the algorithm we demonstrate that it is efficient in practice we have implemented runtime system that uses our algorithm to schedule parallel threads the results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques without compromising performance
in this paper we study generalization of standard property testing where the algorithms are required to be more tolerant with respect to objects that do not have but are close to having the property specifically tolerant property testing algorithm is required to accept objects that are close to having given property and reject objects that are far from having for some parameters another related natural extension of standard property testing that we study is distance approximation here the algorithm should output an estimate of the distance of the object to where this estimate is sufficiently close to the true distance of the object to we first formalize the notions of tolerant property testing and distance approximation and discuss the relationship between the two tasks as well as their relationship to standard property testing we then apply these new notions to the study of two problems tolerant testing of clustering and distance approximation for monotonicity we present and analyze algorithms whose query complexity is either polylogarithmic or independent of the size of the input
despite their popularity and importance pointer based programs remain major challenge for program verification in this paper we propose an automated verification system that is concise precise and expressive for ensuring the safety of pointer based programs our approach uses user definable shape predicates to allow programmers to describe wide range of data structures with their associated size properties to support automatic verification we design new entailment checking procedure that can handle well founded inductive predicates using unfold fold reasoning we have proven the soundness and termination of our verification system and have built prototype system
this paper introduces simple real time distributed computing model for message passing systems which reconciles the distributed computing and the real time systems perspective by just replacing instantaneous computing steps with computing steps of non zero duration we obtain model that both facilitates real time scheduling analysis and retains compatibility with classic distributed computing analysis techniques and results we provide general simulations and validity conditions for transforming algorithms from the classic synchronous model to our real time model and vice versa and investigate whether which properties of real systems are inaccurately or even wrongly captured when resorting to zero step time models we revisit the well studied problem of deterministic drift and failure free internal clock synchronization for this purpose and show that no clock synchronization algorithm with constant running time can achieve optimal precision in our real time model since such an algorithm is known for the classic model this is an instance of problem where the standard distributed computing analysis gives too optimistic results we prove that optimal precision is only achievable with algorithms that take time in our model and establish several additional algorithms and lower bounds
advances in microsensor and radio technology will enable small but smart sensors to be deployed for wide range of environmental monitoring applications the low per node cost will allow these wireless networks of sensors and actuators to be densely distributed the nodes in these dense networks will coordinate to perform the distributed sensing and actuation tasks moreover as described in this paper the nodes can also coordinate to exploit the redundancy provided by high density so as to extend overall system lifetime the large number of nodes deployed in these systems will preclude manual configuration and the environmental dynamics will preclude design time preconfiguration therefore nodes will have to self configure to establish topology that provides communication under stringent energy constraints ascent builds on the notion that as density increases only subset of the nodes are necessary to establish routing forwarding backbone in ascent each node assesses its connectivity and adapts its participation in the multihop network topology based on the measured operating region this paper motivates and describes the ascent algorithm and presents analysis simulation and experimental measurements we show that the system achieves linear increase in energy savings as function of the density and the convergence time required in case of node failures while still providing adequate connectivity
an error occurs when software cannot complete requested action as result of some problem with its input configuration or environment high quality error report allows user to understand and correct the problem unfortunately the quality of error reports has been decreasing as software becomes more complex and layered end users take the cryptic error messages given to them by programsand struggle to fix their problems using search engines and support websites developers cannot improve their error messages when they receive an ambiguous or otherwise insufficient error indicator from black box software component we introduce clarify system that improves error reporting by classifying application behavior clarify uses minimally invasive monitoring to generate behavior profile which is summary of the program's execution history machine learning classifier uses the behavior profile to classify the application's behavior thereby enabling more precise error report than the output of the application itself we evaluate prototype clarify system on ambiguous error messages generated by large modern applications like gcc la tex and the linux kernel for performance cost of less than on user applications and on the linux kernel the proto type correctly disambiguates at least of application behaviors that result in ambiguous error reports this accuracy does not degrade significantly with more behaviors clarify classifier for la tex error messages is at most less accurate than classifier for latex error messages finally we show that without any human effort to build classifier clarify can provide nearest neighbor software support where users who experience problem are told about other users who might have had the same problem on average of the users that clarify identifies have experienced the same problem
distributional measures of lexical similarity and kernel methods for classification are well known tools in natural language processing we bring these two methods together by introducing distributional kernels that compare co occurrence probability distributions we demonstrate the effectiveness of these kernels by presenting state of the art results on datasets for three semantic classification compound noun interpretation identification of semantic relations between nominals and semantic classification of verbs finally we consider explanations for the impressive performance of distributional kernels and sketch some promising generalisations
we survey recent results on wireless networks that are based on analogies with various branches of physics we address among others the problems of optimally arranging the flow of traffic in wireless sensor networks finding minimum cost routes performing load balancing optimizing and analyzing cooperative transmissions calculating the capacity finding routes that avoid bottlenecks and developing distributed anycasting protocols the results are based on establishing analogies between wireless networks and settings from various branches of physics such as electrostatics optics percolation theory diffusion and others many of the results we present hinge on the assumption that the network is massive ie it consists of so many nodes that it can be described in terms of novel macroscopic view the macroscopic view is not as detailed as the standard microscopic one but nevertheless contains enough details to permit meaningful optimization
when meeting someone new the first impression is often influenced by someone's physical appearance and other types of prejudice in this paper we present touchmedare an interactive canvas which aims to provide an experience when meeting new people while preventing visual prejudice and lowering potential thresholds the focus of the designed experience was to stimulate people to get acquainted through the interactive canvas touchmedare consists of flexible opaque canvas which plays music when touched simultaneously from both sides dynamic variation of this bodily contact is reflected through real time adaptations of the musical compositions two redesigns were qualitatively and quantitatively evaluated and final version was placed in the lowlands festival as case study evaluation results showed that some explanation was needed for the initial interaction with the installation on the other hand after this initial unfamiliarity passed results showed that making bodily contact through the installation did help people to get acquainted with each other and increased their social interaction
the standard formalism for explaining abstract types is existential quantification while it provides sufficient model for type abstraction in entirely statically typed languages it proves to be too weak for languages enriched with forms of dynamic typing where parametricity is violated as an alternative approach to type abstraction that addresses this shortcoming we present calculus for dynamic type generation it features an explicit construct for generating new type names and relies on coercions for managing abstraction boundaries between generated types and their designated representation sealing is represented as generalized form of these coercions the calculus maintains abstractions dynamically without restricting type analysis
many applications require randomized ordering of input data examples include algorithms for online aggregation data mining and various randomized algorithms most existing work seems to assume that accessing the records from large database in randomized order is not difficult problem however it turns out to be extremely difficult in practice using existing methods randomization is either extremely expensive at the front end as data are loaded or at the back end as data are queried this paper presents simple file structure which supports both efficient online random shuffling of large database as well as efficient online sampling or randomization of the database when it is queried the key innovation of our method is the introduction of small degree of carefully controlled rigorously monitored nonrandomness into the file
moments before the launch of every space vehicle engineering discipline specialists must make critical go no go decision the cost of false positive allowing launch in spite of fault or false negative stopping potentially successful launch can be measured in the tens of millions of dollars not including the cost in morale and other more intangible detriments the aerospace corporation is responsible for providing engineering assessments critical to the go no go decision for every department of defense space vehicle these assessments are made by constantly monitoring streaming telemetry data in the hours before launch we will introduce viztree novel time series visualization tool to aid the aerospace analysts who must make these engineering assessments viztree was developed at the university of california riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry the use of single tool for both aspects of the task allows natural and intuitive transfer of mined knowledge to the monitoring task our visualization approach works by transforming the time series into symbolic representation and encoding the data in modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties we demonstrate the utility of our system by comparing it with state of the art batch algorithms on several real and synthetic datasets
numerous context aware mobile communication systems have emerged for individuals and groups calling for the identification of critical success factors related to the design of such systems at different levels the effective system design cannot be achieved without the understanding of situated user behaviour in using context aware systems drawing on activity theory this article advances cross level but coherent conceptualisations of context awareness as enabled by emerging systems grounded in the activities of using context aware systems these conceptualisations provide implications for system design at individual and group levels in terms of critical success factors including contextualisation interactivity and personalisation
many materials including water plastic and metal have specular surface characteristics specular reflections have commonly been considered nuisance for the recovery of object shape however the way that reflections are distorted across the surface depends crucially on curvature suggesting that they could in fact be useful source of information indeed observers can have vivid impression of shape when an object is perfectly mirrored ie the image contains nothing but specular reflections this leads to the question what are the underlying mechanisms of our visual system to extract this shape information from perfectly mirrored object in this paper we propose biologically motivated recurrent model for the extraction of visual features relevant for the perception of shape information from images of mirrored objects we qualitatively and quantitatively analyze the results of computational model simulations and show that bidirectional recurrent information processing leads to better results than pure feedforward processing furthermore we utilize the model output to create rough nonphotorealistic sketch representation of mirrored object which emphasizes image features that are mandatory for shape perception eg occluding contour and regions of high curvature moreover this sketch illustrates that the model generates representation of object features independent of the surrounding scene reflected in the mirrored object
we address the problem of answering conjunctive queries over extended entity relationship schemata which we call eer extended er schemata with is among entities and relationships and cardinality constraints this is common setting in conceptual data modelling where reasoning over incomplete data with respect to knowledge base is required we adopt semantics for eer schemata based on their relational representation we identify wide class of eer schemata for which query answering is tractable in data complexity the crucial condition for tractability is the separability between maximum cardinality constraints represented as key constraints in relational form and the other constraints we provide by means of graph based representation syntactic condition for separability we show that our conditions is not only sufficient but also necessary thus precisely identifying the class of separable schemata we present an algorithm based on query rewriting that is capable of dealing with such eer schemata while achieving tractability we show that further negative constraints can be added to the eer formalism while still keeping query answering tractable we show that our formalism is general enough to properly generalise the most widely adopted knowledge representation languages
we present set of algorithms and an associated display system capable of producing correctly rendered eye contact between three dimensionally transmitted remote participant and group of observers in teleconferencing system the participant's face is scanned in at hz and transmitted in real time to an autostereoscopic horizontal parallax display displaying him or her over more than deg field of view observable to multiple observers to render the geometry with correct perspective we create fast vertex shader based on lookup table for projecting scene vertices to range of subject angles heights and distances we generalize the projection mathematics to arbitrarily shaped display surfaces which allows us to employ curved concave display surface to focus the high speed imagery to individual observers to achieve two way eye contact we capture video from cross polarized camera reflected to the position of the virtual participant's eyes and display this video feed on large screen in front of the real participant replicating the viewpoint of their virtual self to achieve correct vertical perspective we further leverage this image to track the position of each audience member's eyes allowing the display to render correct vertical perspective for each of the viewers around the device the result is one to many teleconferencing system able to reproduce the effects of gaze attention and eye contact generally missing in traditional teleconferencing systems
common deficiency of discretized datasets is that detail beyond the resolution of the dataset has been irrecoverably lost this lack of detail becomes immediately apparent once one attempts to zoom into the dataset and only recovers blur here we describe method that generates the missing detail from any available and plausible high resolution data using texture synthesis since the detail generation process is guided by the underlying image or volume data and is designed to fill in plausible detail in accordance with the coarse structure and properties of the zoomed in neighborhood we refer to our method as constrained texture synthesis regular zooms become semantic zooms where each level of detail stems from data source attuned to that resolution we demonstrate our approach by medical application the visualization of human liver but its principles readily apply to any scenario as long as data at all resolutions are available we will first present viewing application called the virtual microscope and then extend our technique to volumetric viewing
we present new technique that employs support vector machines svms and gaussian mixture densities gmds to create generative discriminative object classification technique using local image features in the past several approaches to fuse the advantages of generative and discriminative approaches were presented often leading to improved robustness and recognition accuracy support vector machines are well known discriminative classification framework but similar to other discriminative approaches suffer from lack of robustness with respect to noise and overfitting gaussian mixtures on the contrary are widely used generative technique we present method to directly fuse both approaches effectively allowing to fully exploit the advantages of both the fusion of svms and gmds is done by representing svms in the framework of gmds without changing the training and without changing the decision boundary the new classifier is evaluated on the pascal voc data additionally we perform experiments on the usps dataset and on four tasks from the uci machine learning repository to obtain additional insights into the properties of the proposed approach it is shown that for the relatively rare cases where svms have problems the combined method outperforms both individual ones
in this paper processor scheduling policies that save processors are introduced and studied in multiprogrammed parallel system processor saving scheduling policy purposefully keeps some of the available processors idle in the presence of work to be done the conditions under which processor saving policies can be more effective than their greedy counterparts ie policies that never leave processors idle in the presence of work to be done are examined sensitivity analysis is performed with respect to application speedup system size coefficient of variation of the applications execution time variability in the arrival process and multiclass workloads analytical simulation and experimental results show that processor saving policies outperform their greedy counterparts under variety of system and workload characteristics
principles of the unitesk test development technology based on the use of formal models of target software are presented this technology was developed by the redverst group in the institute for system programming russian academy of sciences ispras lsqb rsqb which obtained rich experience in testing and verification of complex commercial software
recovering from malicious attacks in survival database systems is vital in mission critical information systems traditional rollback and re execute techniques are too time consuming and can not be applied in survival environments in this paper two efficient approaches transaction dependency based and data dependency based are proposed comparing to transaction dependency based approach data dependency recovery approaches need not undo innocent operations in malicious and affected transactions even some benign blind writes on bad data item speed up recovery process
information system engineering has become under increasing pressure to come up with software solutions that endow systems with the agility that is required to evolve in continually changing business and technological environment in this paper we suggest that software engineering has contribution to make in terms of concepts and techniques that have been recently developed for parallel program design and software architectures we show how such mechanisms can be encapsulated in new modelling primitive coordination contract that can be used for extending component based development approaches in order to manage such levels of change
this article provides detailed implementation study on the behavior of web serves that serve static requests where the load fluctuates over time transient overload various external factors are considered including wan delays and losses and different client behavior models we find that performance can be dramatically improved via kernel level modification to the web server to change the scheduling policy at the server from the standard fair processor sharing scheduling to srpt shortest remaining processing time scheduling we find that srpt scheduling induces no penalties in particular throughput is not sacrificed and requests for long files experience only negligibly higher response times under srpt than they did under the original fair scheduling
recent work demonstrates the potential for extracting patterns from users behavior as detected by sensors since there is currently no generalized framework for reasoning about activity aware applications designers can only rely on the existing systems for guidance however these systems often use custom domain specific definition of activity pattern consequently the guidelines designers can extract from individual systems are limited to the specific application domains of those applications in this paper we introduce five high level guidelines or commandments for designing activity aware applications by considering the issues we outlined in this paper designers will be able to avoid common mistakes inherent in designing activity aware applications
cast shadows are an informative cue to the shape of objects they are particularly valuable for discovering object's concavities which are not available from other cues such as occluding boundaries we propose new method for recovering shape from shadows which we call shadow carving given conservative estimate of the volume occupied by an object it is possible to identify and carve away regions of this volume that are inconsistent with the observed pattern of shadows we prove theorem that guarantees that when these regions are carved away from the shape the shape still remains conservative shadow carving overcomes limitations of previous studies on shape from shadows because it is robust with respect to errors in shadows detection and it allows the reconstruction of objects in the round rather than just bas reliefs we propose reconstruction system to recover shape from silhouettes and shadow carving the silhouettes are used to reconstruct the initial conservative estimate of the object's shape and shadow carving is used to carve out the concavities we have simulated our reconstruction system with commercial rendering package to explore the design parameters and assess the accuracy of the reconstruction we have also implemented our reconstruction scheme in table top system and present the results of scanning of several objects
information from which knowledge can be discovered is frequently distributed due to having been recorded at different times or to having arisen from different sources such information is often subject to both imprecision and uncertainty the dempster shafer representation of evidence offers way of representing uncertainty in the presence of imprecision and may therefore be used to provide mechanism for storing imprecise and uncertain information in databases we consider an extended relational data model that allows the imprecision and uncertainty associated with attribute values to be quantified using mass function distribution when query is executed it may be necessary to combine imprecise and uncertain data from distributed sources in order to answer that query mechanism is therefore required both for combining the data and for generating measures of uncertainty to be attached to the imprecise combined data in this paper we provide such mechanism based on aggregation of evidence we show first how this mechanism can be used to resolve inconsistencies and hence provide an essential database capability to perform the operations necessary to respond to queries on imprecise and uncertain data we go on to exploit the aggregation operator in an attribute driven approach to provide information on properties of and patterns in the data this is fundamental to rule discovery and hence such an aggregation operator provides facility that is central requirement in providing distributed information system with the capability to perform the operations necessary for knowledge discovery
one view of computational learning theory is that of learner acquiring the knowledge of teacher we introduce formal model of learning capturing the idea that teachers may have gaps in their knowledge the goal of the learner is still to acquire the knowledge of the teacher but now the learner must also identify the gaps this is the notion of learning from consistently ignorant teacher we consider the impact of knowledge gaps on learning for example monotone dnf and dimensional boxes and show that learning is still possible negatively we show that knowledge gaps make learning conjunctions of horn clauses as hard as learning dnf we also present general results describing when known learning algorithms can be used to obtain learning algorithms using consistently ignorant teacher
in this paper we study the challenges and evaluate the effectiveness of data collected from the web for recommendations we provide experimental results including user study showing that our methods produce good recommendations in realistic applications we propose new evaluation metric that takes into account the difficulty of prediction we show that the new metric aligns well with the results from user study
this paper proposes new clothing segmentation method using foreground clothing and background non clothing estimation based on the constrained delaunay triangulation cdt without any pre defined clothing model in our method the clothing is extracted by graph cuts where the foreground seeds and background seeds are determined automatically the foreground seeds are found by torso detection based on dominant colors determination and the background seeds are estimated based on cdt with the determined seeds the color distributions of the foreground and background are modeled by gaussian mixture models and filtered by cdt based noise suppression algorithm for more robust and accurate segmentation experimental results show that our clothing segmentation method is able to extract different clothing from static images with variations in backgrounds and lighting conditions
in this paper we address the issue of feeding future superscalar processor cores with enough instructions hardware techniques targeting an increase in the instruction fetch bandwidth have been proposed such as the trace cache microarchitecture we present microarchitecture solution based on register file holding basic blocks of instructions this solution places the instruction memory hierarchy out of the cycle determining path we call our approach instruction register file irf we estimate our approach with simplescalar based simulator run on the mediabench benchmark suite and compare to the trace cache performance on the same benchmarks we show that on this benchmark suite an irf based processor fetching up to three basic blocks per cycle outperforms trace cache based processor fetching instructions long traces by on the average
the authors propose method for personalizing the flexible widget layout fwl by adjusting the desirability of widgets with pairwise comparison method and show its implementation and that it actually works personalization of graphical user interfaces guis is important from perspective of usability and it is challenge in the field of model based user interface designs the fwl is model and optimization based layout framework of guis offering possibility for personalization but it has not actually realized it with any concrete method yet in this paper the authors implement method for personalization as dialog box and incorporate it into the existing system of the fwl thus users can personalize layouts generated by the fwl system at run time
within the context of the relational model general technique for establishing that the translation of view update defined by constant complement is independent of the choice of complement is presented in contrast to previous results the uniqueness is not limited to order based updates those constructed from insertions and deletions nor is it limited to those well behaved complements which define closed update strategies rather the approach is based upon optimizing the change of information in the main schema which the view update entails the only requirement is that the view and its complement together possess property called semantic bijectivity relative to the information measure it is furthermore established that very wide range of views have this property this results formalizes the intuition long observed in examples that it is difficult to find different complements which define distinct but reasonable update strategies
in wireless networks bandwidth is relatively scarce especially for supporting on demand media streaming in wired networks multicast stream merging is well known technique for scalable on demand streaming also caching proxies are widely used on the internet to offload servers and reduce network traffic this paper uses simulation to examine caching hierarchy for wireless streaming video distribution in combination with multicast stream merging the main purpose is to gain insight into the filtering effects caused by caching and merging using request frequencies entropy and inter reference times as metrics we illustrate how merging caching and traffic aggregation affect the traffic characteristics at each level the simulation results provide useful insights into caching performance in video streaming hierarchy
the power of high level languages lies in their abstraction over hardware and software complexity leading to greater security better reliability and lower development costs however opaque abstractions are often show stoppers for systems programmers forcing them to either break the abstraction or more often simply give up and use different language this paper addresses the challenge of opening up high level language to allow practical low level programming without forsaking integrity or performance the contribution of this paper is three fold we draw together common threads in diverse literature we identify framework for extending high level languages for low level programming and we show the power of this approach through concrete case studies our framework leverages just three core ideas extending semantics via intrinsic methods extending types via unboxing and architectural width primitives and controlling semantics via scoped semantic regimes we develop these ideas through the context of rich literature and substantial practical experience we show that they provide the power necessary to implement substantial artifacts such as high performance virtual machine while preserving the software engineering benefits of the host language the time has come for high level low level programming to be taken more seriously more projects now use high level languages for systems programming increasing architectural heterogeneity and parallelism heighten the need for abstraction and new generation of high level languages are under development and ripe to be influenced
collaboration and information sharing between organizations that share common goal is becoming increasingly important effective sharing promotes efficiency and productivity as well as enhances customer service with internet connectivity widely available sharing and access to information is relatively simple to implement however the abundance causes another problem the difficulty of determining where truly useful and relevant information is housed information resources such as data documents multimedia objects and services stored in different agencies need to be easily discovered and shared we propose collaborative semantic and pragmatic annotation environment where resources of each agency can be annotated by users in the government social network this collaborative annotation captures not only the semantics but also the pragmatics of the resources such as who when where how and why the resources are used the benefits of semantic and pragmatic annotation tags will include an ability to filter discover and search new and dynamic as well as hidden resources to navigate between resources in search by traversing semantic relationships and to recommend the most relevant government information distributed over different agencies distributed architecture of tagging system is shown and tag based search is illustrated
number of supervised learning methods have been introduced in the last decade unfortunately the last comprehensive empirical evaluation of supervised learning was the statlog project in the early we present large scale empirical comparison between ten supervised learning methods svms neural nets logistic regression naive bayes memory based learning random forests decision trees bagged trees boosted trees and boosted stumps we also examine the effect that calibrating the models via platt scaling and isotonic regression has on their performance an important aspect of our study is the use of variety of performance criteria to evaluate the learning methods
model driven development using languages such as uml and bon often makes use of multiple diagrams eg class and sequence diagrams when modeling systems these diagrams presenting different views of system of interest may be inconsistent metamodel provides unifying framework in which to ensure and check consistency while at the same time providing the means to distinguish between valid and invalid models that is conformance two formal specifications of the metamodel for an object oriented modeling language are presented and it is shown how to use these specifications for model conformance and multiview consistency checking comparisons are made in terms of completeness and the level of automation each provide for checking multiview consistency and model conformance the lessons learned from applying formal techniques to the problems of metamodeling model conformance and multiview consistency checking are summarized
disk caching algorithm is presented that uses an adaptive prefetching scheme to optimize the system performance in disk controllers for traces with different data localities the algorithm uses on line measurements of disk transfer times and of inter page fault rates to adjust the level of prefetching dynamically and its performance is evaluated through trace driven simulations using real workloads the results confirm the effectiveness and efficiency of the new adaptive prefetching algorithm
grids offer dramatic increase in the number of available processing and storing resources that can be delivered to applications however efficient job submission and management continue being far from accessible to ordinary scientists and engineers due to their dynamic and complex nature this paper describes new globus based framework that allows an easier and more efficient execution of jobs in submit and forget fashion the framework automatically performs the steps involved in job submission and also watches over its efficient execution in order to obtain reasonable degree of performance job execution is adapted to dynamic resource conditions and application demands adaptation is achieved by supporting automatic application migration following performance degradation better resource discovery requirement change owner decision or remote resource failure the framework is currently functional on any grid testbed based on globus because it does not require new system software to be installed in the resources the paper also includes practical experiences of the behavior of our framework on the trgp and ucm cab testbeds
debugging refers to the laborious process of finding causes of program failures often such failures are introduced when program undergoes changes and evolves from stable version to new modified version in this paper we propose an automated approach for debugging evolving programs given two programs reference stable program and new modified program and an input that fails on the modified program our approach uses concrete as well as symbolic execution to synthesize new inputs that differ marginally from the failing input in their control flow behavior comparison of the execution traces of the failing input and the new inputs provides critical clues to the root cause of the failure notable feature of our approach is that it handles hard to explain bugs like code missing errors by pointing to the relevant code in the reference program we have implemented our approach in tool called darwin we have conducted experiments with several real life case studies including real world web servers and the libpng library for manipulating png images our experience from these experiments points to the efficacy of darwin in pinpointing bugs moreover while localizing given observable error the new inputs synthesized by darwin can reveal other undiscovered errors
os kernels have been written in weakly typed or non typed programming languages for example therefore it is extremely hard to verify even simple memory safety of the kernels the difficulty could be resolved by writing os kernels in strictly typed programming languages but existing strictly typed languages are not flexible enough to implement important os facilities eg memory management and multi thread management facilities to address the problem we designed and implemented talk new strictly and statically typed assembly language which is flexible enough to implement os facilities and wrote an os kernel with talk in our approach the safety of the kernel can be verified automatically through static type checking at the level of binary executables without source code
existing research has most often relied on simulation and considered the uniform traffic distribution when investigating the performance properties of multicomputer networks eg the torus however there are numerous parallel applications that generate non uniform traffic patterns such as hot spot furthermore much more attention has been paid to capturing the impact of non uniform traffic on network performance resulting in the development of number of analytical models for predicting message latency in the presence of hot spots in the network for instance analytical models have been reported for the adaptively routed torus with uni directional as well as bi directional channels however models for the deterministically routed torus have considered uni directional channels only in an effort to fill in this gap this paper describes an analytical model for the deterministically routed torus with bi directional channels when subjected to hot spot traffic the modelling approach adopted for deterministic routing is totally different from that for adaptive routing due to the inherently different nature of the two types of routing the validity of the model is demonstrated by comparing analytical results against those obtained through extensive simulation experiments
the task of obtaining an optimal set of parameters to fit mixture model has many applications in science and engineering domains and is computationally challenging problem novel algorithm using convolution based smoothing approach to construct hierarchy or family of smoothed log likelihood surfaces is proposed this approach smooths the likelihood function and applies the em algorithm to obtain promising solution on the smoothed surface using the most promising solutions as initial guesses the em algorithm is applied again on the original likelihood though the results are demonstrated using only two levels the method can potentially be applied to any number of levels in the hierarchy theoretical insight demonstrates that the smoothing approach indeed reduces the overall gradient of modified version of the likelihood surface this optimization procedure effectively eliminates extensive searching in non promising regions of the parameter space results on some benchmark datasets demonstrate significant improvements of the proposed algorithm compared to other approaches empirical results on the reduction in the number of local maxima and improvements in the initialization procedures are provided
an object detection method from line drawings is presented the method adopts the local neighborhood structure as the elementary descriptor which is formed by grouping several nearest neighbor lines curves around one reference with this representation both the appearance and the geometric structure of the line drawing are well described the detection algorithm is hypothesis test scheme the top most similar local structures in the drawing are firstly obtained for each local structure of the model and the transformation parameters are estimated for each of the candidates such as object center scale and rotation factors by treating each estimation result as point in the parameter space dense region around the ground truth is then formed provided that there exist model in the drawing the mean shift method is used to detect the dense regions and the significant modes are accepted as the occurrence of object instances
we describe our experiences of designing digital community display with members of rural community these experiences are highlighted by the development of printed and digital postcard features for the wray photo display public photosharing display designed with the community which was trialled during popular village fair where both local residents and visitors interacted with the system this trial allowed us to examine the relative popularity and differences in usage between printed and digital postcard and offer insights into the uses of these features with community generated content and potential problems encountered
the interconnection network considered in this paper is the generalized base hypercube that is an attractive variance of the well known hypercube the generalized base hypercube is superior to the hypercube in many criteria such as diameter connectivity and fault diameter in this paper we study the hamiltonian connectivity and pancyclicity of the generalized base hypercube by the algorithmic approach we show that generalized base hypercube is hamiltonian connected for that is there exists hamiltonian path joining each pair of vertices in generalized base hypercube for we also show that generalized base hypercube is pancyclic for that is it embeds cycles of all lengths ranging from to the order of the graph for
in this paper we approach the problem of constructing ensembles of classifiers from the point of view of instance selection instance selection is aimed at obtaining subset of the instances available for training capable of achieving at least the same performance as the whole training set in this way instance selection algorithms try to keep the performance of the classifiers while reducing the number of instances in the training set meanwhile boosting methods construct an ensemble of classifiers iteratively focusing each new member on the most difficult instances by means of biased distribution of the training instances in this work we show how these two methodologies can be combined advantageously we can use instance selection algorithms for boosting using as objective to optimize the training error weighted by the biased distribution of the instances given by the boosting method our method can be considered as boosting by instance selection instance selection has mostly been developed and used for nearest neighbor nn classifiers so as first step our methodology is suited to construct ensembles of nn classifiers constructing ensembles of classifiers by means of instance selection has the important feature of reducing the space complexity of the final ensemble as only subset of the instances is selected for each classifier however the methodology is not restricted to nn classifier other classifiers such as decision trees and support vector machines svms may also benefit from smaller training set as they produce simpler classifiers if an instance selection algorithm is performed before training in the experimental section we show that the proposed approach is able to produce better and simpler ensembles than random subspace method rsm method for nn and standard ensemble methods for and svms
we present an expectation maximization learning algorithm em for estimating the parameters of partially constrained bayesian trees the bayesian trees considered here consist of an unconstrained subtree and set of constrained subtrees in this tree structure constraints are imposed on some of the parameters of the parametrized conditional distributions such that all conditional distributions within the same subtree share the same constraint we propose learning method that uses the unconstrained subtree to guide the process of discovering set of relevant constrained substructures substructure discovery and constraint enforcement are simultaneously accomplished using an em algorithm we show how our tree substructure discovery method can be applied to the problem of learning representative pose models from set of unsegmented video sequences our experiments demonstrate the potential of the proposed method for human motion classification
in multimedia retrieval query is typically interactively refined towards the optimal answers by exploiting user feedback however in existing work in each iteration the refined query is re evaluated this is not only inefficient but fails to exploit the answers that may be common between iterations furthermore it may also take too many iterations to get the optimal answers in this paper we introduce new approach called optrfs optimizing relevance feedback search by query prediction for iterative relevance feedback search optrfs aims to take users to view the optimal results as fast as possible it optimizes relevance feedback search by both shortening the searching time during each iteration and reducing the number of iterations optrfs predicts the potential candidates for the next iteration and maintains this small set for efficient sequential scan by doing so repeated candidate accesses ie random accesses can be saved hence reducing the searching time for the next iteration in addition efficient scan on the overlap before the next search starts also tightens the search space with smaller pruning radius as step forward optrfs also predicts the optimal query which corresponds to optimal answers based on the early executed iterations queries by doing so some intermediate iterations can be saved hence reducing the total number of iterations by taking the correlations among the early executed iterations into consideration optrfs investigates linear regression exponential smoothing and linear exponential smoothing to predict the next refined query so as to decide the overlap of candidates between two consecutive iterations considering the special features of relevance feedback optrfs further introduces adaptive linear exponential smoothing to self adjust the parameters for more accurate prediction we implemented optrfs and our experimental study on real life data sets show that it can reduce the total cost of relevance feedback search significantly some interesting features of relevance feedback search are also discovered and discussed
automated software engineering methods support the construction maintenance and analysis of both new and legacy systems their application is commonplace in desktop and enterprise class systems due to the productivity and reliability benefits they afford the contribution of this article is to present an applied foundation for extending the use of such methods to the flourishing domain of wireless sensor networks the objective is to enable developers to construct tools that aid in understanding both the static and dynamic properties of reactive event based systems we present static analysis and instrumentation toolkit for the nesc language the defacto standard for sensor network development we highlight the novel aspects of the toolkit analyze its performance and provide representative case studies that illustrate its use
real scale semantic web applications such as knowledge portals and marketplaces require the management of large volumes of metadata ie information describing the available web content and services better knowledge about their meaning usage accessibility or quality will considerably facilitate an automated processing of web resources the resource description framework rdf enables the creation and exchange of metadata as normal web data although voluminous rdf descriptions are already appearing sufficiently expressive declarative languages for querying both rdf descriptions and schemas are still missing in this paper we propose new rdf query language called rql it is typed functional language la oql and relies on formal model for directed labeled graphs permitting the interpretation of superimposed resource descriptions by means of one or more rdf schemas rql adapts the functionality of semistructured xml query languages to the peculiarities of rdf but foremost it enables to uniformly query both resource descriptions and schemas we illustrate the rql syntax semantics and typing system by means of set of example queries and report on the performance of our persistent rdf store employed by the rql interpreter
the web is now being used as general platform for hosting distributed applications like wikis bulletin board messaging systems and collaborative editing environments data from multiple applications originating at multiple sources all intermix in single web browser making sensitive data stored in the browser subject to broad milieu of attacks cross site scripting cross site request forgery and others the fundamental problem is that existing web infrastructure provides no means for enforcing end to end security on data to solve this we design an architecture using mandatory access control mac enforcement we overcome the limitations of traditional mac systems implemented solely at the operating system layer by unifying mac enforcement across virtual machine operating system networking and application layers we implement our architecture using xen virtual machine management selinux at the operating system layer labeled ipsec for networking and our own label enforcing web browser called flowwolf we tested our implementation and find that it performs well supporting data intermixing while still providing end to end security guarantees
paper augmented digital documents padds are digital documents that can be manipulated either on computer screen or on paper padds and the infrastructure supporting them can be seen as bridge between the digital and the paper worlds as digital documents padds are easy to edit distribute and archive as paper documents padds are easy to navigate annotate and well accepted in social settings the chimeric nature of padds make them well suited for many tasks such as proofreading editing and annotation of large format document like blueprintswe are presenting an architecture which supports the seamless manipulation of padds using today's technologies and reports on the lessons we learned while implementing the first padd system
we present statistical method called covering topic score cts to predict query performance for information retrieval estimation is based on how well the topic of user's query is covered by documents retrieved from certain retrieval system our approach is conceptually simple and intuitive and can be easily extended to incorporate features beyond bag of words such as phrases and proximity of terms experiments demonstrate that cts significantly correlates with query performance in variety of trec test collections and in particular cts gains more prediction power benefiting from features of phrases and proximity of terms we compare cts with previous state of the art methods for query performance prediction including clarity score and robustness score our experimental results show that cts consistently performs better than or at least as well as these other methods in addition to its high effectiveness cts is also shown to have very low computational complexity meaning that it can be practical for real applications
sensor networks consist of many small sensing devices that monitor an environment and communicate using wireless links the lifetime of these networks is severely curtailed by the limited battery power of the sensors one line of research in sensor network lifetime management has examined sensor selection techniques in which applications judiciously choose which sensors data should be retrieved and are worth the expended energy in the past many ad hoc approaches for sensor selection have been proposed in this paper we argue that sensor selection should be based upon tradeoff between application perceived benefit and energy consumption of the selected sensor setwe propose framework wherein the application can specify the utility of measuring data nearly concurrently at each set of sensors he goal is then to select sequence of sets to measure whose total utility is maximized while not exceeding the available energy alternatively we may look for the most cost effective sensor set maximizing the product of utility and system lifetimethis approach is very generic and permits us to model many applications of sensor networks we proceed to study two important classes of utility functions submodular and supermodular functions we show that the optimum solution for submodular functions can be found in polynomial time while optimizing the costeffectiveness of supermodular functions is np hard for practically important subclass of supermodular functions we present an lp based solution if nodes can send for different amounts of time and show that we can achieve an logn approximation ratio if each node has to send for the same amount of timefinally we study scenarios in which the quality of measurements is naturally expressed in terms of distances from targets we show that the utility based approach is analogous to penalty based approach in those scenarios and present preliminary results on some practically important special cases
while total order broadcast or atomic broadcast primitives have received lot of attention this paper concentrates on total order multicast to multiple groups in the context of asynchronous distributed systems in which processes may suffer crash failures multicast to multiple groups means that each message is sent to subset of the process groups composing the system distinct messages possibly having distinct destination groups total order means that all message deliveries must be totally ordered this paper investigates consensus based approach to solve this problem and proposes corresponding protocol to implement this multicast primitive this protocol is based on two underlying building blocks namely uniform reliable multicast and uniform consensus its design characteristics lie in the two following properties the first one is minimality property more precisely only the sender of message and processes of its destination groups have to participate in the total order multicast of the message the second property is locality property no execution of consensus has to involve processes belonging to distinct groups ie consensus is executed on per group basis this locality property is particularly useful when one is interested in using the total order multicast primitive in large scale distributed systems in addition to correctness proof an improvement that reduces the cost of the protocol is also suggested
as collaboration in virtual environments becomes more object focused and closely coupled the frequency of conflicts in accessing shared objects can increase in addition two kinds of concurrency control surprises become more disruptive to the collaboration undo surprises can occur when previously visible change is undone because of an access conflict intention surprises can happen when concurrent action by remote session changes the structure of shared object at the same perceived time as local access of that object such that the local user might not get what they expect because they have not had time to visually process the change hierarchy of three concurrency control mechanisms is presented in descending order of collaborative surprises which allows the concurrency scheme to be tailored to the tolerance for such surprises one mechanism is semioptimistic the other two are pessimistic designed for peer to peer vitual environments in which several threads have access to the shared scene graph these algorithms are straightforward and relatively simple they can be implemented using and java under windows and unix on both desktop and immersive systems in series of usability experiments the average performance of the most conservative concurrency control mechanism on local lan was found to be quite acceptable
using moving parabolic approximations mpa we reconstruct an improved point based model of curve or surface represented as an unorganized point cloud while also estimating the differential properties of the underlying smooth manifold we present optimization algorithms to solve these mpa models and examples which show that our reconstructions of the curve or surface and estimates of the normals and curvature information are accurate for precise point clouds and robust in the presence of noise
transactional memory tm is promising paradigm for concurrent programming this paper is an overview of our recent theoretical work on defining theory of tm we first recall some tm correctness properties and then overview results on the inherent power and limitations of tms
time sequences which are ordered sets of observations have been studied in various database applications in this paper we introduce new class of time sequences where each observation is represented by an interval rather than number such sequences may arise in many situations for instance we may not be able to determine the exact value at time point due to uncertainty or aggregation such observation may be represented better by range of possible values similarity search with interval time sequences as both query and data sequences poses new challenge for research we first address the issue of dis similarity measures for interval time sequences we choose an norm based measure because it effectively quantifies the degree of overlapping and remoteness between two intervals and is invariant irrespective of the position of an interval when it is enclosed within another interval we next propose an efficient indexing technique for fast retrieval of similar interval time sequences from large databases more specifically we propose to extract segment based feature vector for each sequence and to map each feature vector to either point or hyper rectangle in multi dimensional feature space we then show how we can use existing multi dimensional index structures such as the tree for efficient query processing the proposed method guarantees no false dismissals experimental results show that for synthetic and real stock data it is superior to sequential scanning in performance and scales well with the data size
application integration can be carried out on three different levels the data source level the business logic level and the user interface level with ontologies based integration on the data source level dating back to the and semantic web services for integrating on the business logic level coming of age it is time for the next logical step employing ontologies for integration on the user interface level such an approach supports both the developer in terms of reduced development times and the user in terms of better usability of integrated applications in this paper we introduce framework employing ontologies for integrating applications on the user interface level the acm portal is published by the association for computing machinery copyright acm inc terms of usage privacy policy code of ethics contact us useful downloads adobe acrobat quicktime windows media player real player
the handling of user preferences is becoming an increasingly important issue in present day information systems among others preferences are used for information filtering and extraction to reduce the volume of data presented to the user they are also used to keep track of user profiles and formulate policies to improve and automate decision makingwe propose here simple logical framework for formulating preferences as preference formulas the framework does not impose any restrictions on the preference relations and allows arbitrary operation and predicate signatures in preference formulas it also makes the composition of preference relations straightforward we propose simple natural embedding of preference formulas into relational algebra and sql through single winnow operator parameterized by preference formula the embedding makes possible the formulation of complex preference queries for example involving aggregation by piggybacking on existing sql constructs it also leads in natural way to the definition of further preference related concepts like ranking finally we present general algebraic laws governing the winnow operator and its interactions with other relational algebra operators the preconditions on the applicability of the laws are captured by logical formulas the laws provide formal foundation for the algebraic optimization of preference queries we demonstrate the usefulness of our approach through numerous examples
block correlations are common semantic patterns in storage systems these correlations can be exploited for improving the effectiveness of storage caching prefetching data layout and disk scheduling unfortunately information about block correlations is not available at the storage system level previous approaches for discovering file correlations in file systems do not scale well enough to be used for discovering block correlations in storage systems in this paper we propose miner an algorithm which uses data mining technique called frequent sequence mining to discover block correlations in storage systems miner runs reasonably fast with feasible space requirement indicating that it is practical tool for dynamically inferring correlations in storage system moreover we have also evaluated the benefits of block correlation directed prefetching and data layout through experiments our results using real system workloads show that correlation directed prefetching and data layout can reduce average response time by compared to the base case and compared to the commonly used sequential prefetching scheme
this paper describes onechip third generation reconfigurable processor architecture that integrates reconfigurable functional unit rfu into superscalar reduced instruction set computer risc processor's pipeline the architecture allows dynamic scheduling and dynamic reconfiguration it also provides support for pre loading configurations and for least recently used lru configuration managementto evaluate the performance of the onechip architecture several off the shelf software applications were compiled and executed on sim onechip an architecture simulator for onechip that includes software environment for programming the system the architecture is compared to similar one but without dynamic scheduling and without an rfu onechip achieves performance improvement and shows speedup range from up to for the different applications and data sizes used the results show that dynamic scheduling helps performance the most on average and that the rfu will always improve performance the best when most of the execution is in the rfu
most correlation clustering algorithms rely on principal component analysis pca as correlation analysis tool the correlation of each cluster is learned by applying pca to set of sample points since pca is rather sensitive to outliers if small fraction of these points does not correspond to the correct correlation of the cluster the algorithms are usually misled or even fail to detect the correct results in this paper we evaluate the influence of outliers on pca and propose general framework for increasing the robustness of pca in order to determine the correct correlation of each cluster we further show how our framework can be applied to pca based correlation clustering algorithms thorough experimental evaluation shows the benefit of our framework on several synthetic and real world data sets
existing dram controllers employ rigid non adaptive scheduling and buffer management policies when servicing prefetch requests some controllers treat prefetch requests the same as demand requests others always prioritize demand requests over prefetch requests however none of these rigid policies result in the best performance because they do not take into account the usefulness of prefetch requests if prefetch requests are useless treating prefetches and demands equally can lead to significant performance loss and extra bandwidth consumption in contrast if prefetch requests are useful prioritizing demands over prefetches can hurt performance by reducing dram throughput and delaying the service of useful requests this paper proposes new low cost memory controller called prefetch aware dram controller padc that aims to maximize the benefit of useful prefetches and minimize the harm caused by useless prefetches to accomplish this padc estimates the usefulness of prefetch requests and dynamically adapts its scheduling and buffer management policies based on the estimates the key idea is to adaptively prioritize between demand and prefetch requests and drop useless prefetches to free up memory system resources based on the accuracy of the prefetcher our evaluation shows that padc significantly outperforms previous memory controllers with rigid prefetch handling policies on both single and multi core systems with variety of prefetching algorithms across wide range of multiprogrammed spec cpu workloads it improves system performance by on core system and by on an core system while reducing dram bandwidth consumption by and respectively
most hardware predictors are table based eg two level branch predictors and have exponential size growth in the number of input bits or features eg previous branch outcomes this growth severely limits the amount of predictive information that such predictors can use to avoid exponential growth we introduce the idea of dynamic feature selection for building hardware predictors that can use large amount of predictive information based on this idea we design the dynamic decision tree ddt predictor which exhibits only linear size growth in the number of features our initial evaluation in branch prediction shows that the general purpose ddt using only branch history features is comparable on average to conventional branch predictors opening the door to practically using large numbers of additional features
referential integrity is an essential global constraint in relational database that maintains it in complete and consistent state in this work we assume the database may violate referential integrity and relations may be denormalized we propose set of quality metrics defined at four granularity levels database relation attribute and value that measure referential completeness and consistency quality metrics are efficiently computed with standard sql queries that incorporate two query optimizations left outer joins on foreign keys and early foreign key grouping experiments evaluate our proposed metrics and sql query optimizations on real and synthetic databases showing they can help in detecting and explaining referential errors
cluster based replication solutions are an attractive mechanism to provide both high availability and scalability for the database backend within the multi tier information systems of service oriented businesses an important issue that has not yet received sufficient attention is how database replicas that have failed can be reintegrated into the system or how completely new replicas can be added in order to increase the capacity of the system ideally recovery takes place online ie while transaction processing continues at the replicas that are already running in this paper we present complete online recovery solution for database clusters one important issue is to find an efficient way to transfer the data the joining replica needs in this paper we present two data transfer strategies the first transfers the latest copy of each data item the second transfers the updates rejoining replica has missed during its downtime second challenge is to coordinate this transfer with ongoing transaction processing such that the joining node does not miss any updates we present coordination protocol that can be used with postgres replication tool which uses group communication system for replica control we have implemented and compared our transfer solutions against set of parameters and present heuristics which allow an automatic selection of the optimal strategy for given configuration
influence of items on some other items might not be the same as the association between these sets of items many tasks of data analysis are based on expressing influence of items on other items in this paper we introduce the notion of an overall influence of set of items on another set of items we also propose an extension to the notion of overall association between two items in database using the notion of overall influence we have designed two algorithms for influence analysis involving specific items in database as the number of databases increases on yearly basis we have adopted incremental approach in these algorithms experimental results are reported for both synthetic and real world databases
the growing number of information security breaches in electronic and computing systems calls for new design paradigms that consider security as primary design objective this is particularly relevant in the embedded domain where the security solution should be customized to the needs of the target system while considering other design objectives such as cost performance and power due to the increasing complexity and shrinking design cycles of embedded software most embedded systems present host of software vulnerabilities that can be exploited by security attacks many attacks are initiated by causing violation in the properties of data eg integrity privacy access control rules etc associated with trusted program that is executing on the system leading to range of undesirable effectsin this work we develop general framework that provides security assurance against wide class of security attacks our work is based on the observation that program's permissible behaviorwith respect to data accesses can be characterized by certain properties we present hardware software approach wherein such properties can be encoded as data attributes and enforced as security policies during program execution these policies may be application specific eg access control for certain data structures compiler generated eg enforcing that variables are accessed only within their scope or universally applicable to all programs eg disallowing writes to unallocated memory we show how an embedded system architecture can support such policies by enhancing the memory hierarchy to represent the attributes of each datum as security tags that are linked to it through its lifetime and ii adding configurable hardware checker that interprets the semantics of the tags and enforces the desired security policies we evaluated the effectiveness of the proposed architecture in enforcing various security policies for several embedded benchmarks our experiments in the context of the simplescalar framework demonstrate that the proposed solution ensures run time validation of program data properties with minimal execution time overheads
given two sets of moving objects future timestamp tq and distance threshold spatio temporal join retrieves all pairs of objects that are within distance at tq the selectivity of join equals the number of retrieved pairs divided by the cardinality of the cartesian product this paper develops model for spatio temporal join selectivity estimation based on rigorous probabilistic analysis and reveals the factors that affect the selectivity initially we solve the problem for id point and rectangle objects whose location and velocities distribute uniformly and then extend the results to multi dimensional spaces finally we deal with non uniform distributions using specialized spatio temporal histogram extensive experiments confirm that the proposed formulae are highly accurate average error below
we give semantics to polymorphic effect analysis that tracks possibly thrown exceptions and possible non termination for higher order language the semantics is defined using partial equivalence relations over standard monadic domain theoretic model of the original language and establishes the correctness of both the analysis itself and of the contextual program transformations that it enables
the need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available unfortunately the visualizations in existing systems do not satisfactorily resolve the basic dilemma of being readable both for the global structure of thenetwork and also for detailed analysis of local communities to address this problem we present nodetrix hybrid representation for networks that combines the advantages of two traditional representations node link diagrams are used to show the global structure of network while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities key contribution is set of interaction techniques these allow analysts to create nodetrix visualization by dragging selections to and from node link and matrix forms and to flexibly manipulate the nodetrix representation to explore the dataset andcreate meaningful summary visualizations of their findings finally we present case study applying nodetrix to the analysis of the infovis coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results
the artifacts constituting software system often drift apart over time we have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting rather than removing the drift between design and implementation more specifically the technique helps an engineer compare artifacts by summarizing where one artifact such as design is consistent with and inconsistent with another artifact such as source the technique can be applied to help software engineer evolve structural mental model of system to the point that it is good enough to be used for reasoning about task at hand the software reflexion model technique has been applied to support variety of tasks including design conformance change assessment and an experimental reengineering of the million lines of code microsoft excel product in this paper we provide formal characterization of the reflexion model technique discuss practical aspects of the approach relate experiences of applying the approach and tools and place the technique into the context of related work
abstraction and slicing are both techniques for reducing the size of the state space to be inspected during verification in this paper we present new model checking procedure for infinite state concurrent systems that interleaves automatic abstraction refinement which splits states according to new predicates obtained by craig interpolation with slicing which removes irrelevant states and transitions from the abstraction the effects of abstraction and slicing complement each other as the refinement progresses the increasing accuracy of the abstract model allows for more precise slice the resulting smaller representation gives room for additional predicates in the abstraction the procedure terminates when an error path in the abstraction can be concretized which proves that the system is erroneous or when the slice becomes empty which proves that the system is correct
compressing the instructions of an embedded program is important for cost sensitive low power control oriented embedded computing number of compression schemes have been proposed to reduce program size however the increased instruction density has an accompanying performance cost because the instructions must be decompressed before execution in this paper we investigate the performance penalty of hardware managed code compression algorithm recently introduced in ibm's powerpc this scheme is the first to combine many previously proposed code compression techniques making it an ideal candidate for study we find that code compression with appropriate hardware optimizations does not have to incur much performance loss furthermore our studies show this holds for architectures with wide range of memory configurations and issue widths surprisingly we find that performance increase over native code is achievable in many situations
detecting code clones has many software engineering applications existing approaches either do not scale to large code bases or are not robust against minor code modifications in this paper we present an efficient algorithm for identifying similar subtrees and apply it to tree representations of source code our algorithm is based on novel characterization of subtrees with numerical vectors in the euclidean space mathbb and an efficient algorithm to cluster these vectors wrt the euclidean distance metric subtrees with vectors in one cluster are considered similar we have implemented our tree similarity algorithm as clone detection tool called deckard and evaluated it on large code bases written in and java including the linux kernel and jdk our experiments show that deckard is both scalable and accurate it is also language independent applicable to any language with formally specified grammar
this paper presents an initial study of multimodal collaborative platform concerning user preferences and interaction technique adequacy towards task true collaborative interactions are missing aspect of the majority of nowadays multi user system on par with the lack of support towards impaired users in order to surpass these obstacles we provide an accessible platform for co located collaborative environments which aims at not only improving the ways users interact within them but also at exploring novel interaction patterns brief study regarding set of interaction techniques and tasks was conducted in order to assess the most suited modalities in certain settings we discuss the results drawn from this study detail some related conclusions and present future work directions
password authenticated key exchange pake protocols allow parties to share secret keys in an authentic manner based on an easily memorizable password recently lu and cao proposed three party password authenticated key exchange protocol so called pake based on ideas of the abdalla and pointcheval two party spake extended to three parties pake can be seen to have structure alternative to that of another three party pake protocol pake by abdalla and pointcheval furthermore simple improvement to pake was proposed very recently by chung and ku to resist the kind of attacks that applied to earlier versions of pake in this paper we show that pake falls to unknown key share attacks by any other client and undetectable online dictionary attacks by any adversary the latter attack equally applies to the recently improved pake indeed the provable security approach should be taken when designing pakes and furthermore our results highlight that extra cautions still be exercised when defining models and constructing proofs in this direction
efficient construction of inverted indexes is essential to provision of search over large collections of text data in this article we review the principal approaches to inversion analyze their theoretical cost and present experimental results we identify the drawbacks of existing inversion approaches and propose single pass inversion method that in contrast to previous approaches does not require the complete vocabulary of the indexed collection in main memory can operate within limited resources and does not sacrifice speed with high temporary storage requirements we show that the performance of the single pass approach can be improved by constructing inverted files in segments reducing the cost of disk accesses during inversion of large volumes of data
materialized xpath access control views are commonly used for enforcing access control when access control rules defining materialized xml access control view change the view must be adapted to reflect these changes the process of updating materialized view after its definition changes is referred to as view adaptation while xpath security views have been widely reported in literature the problem of view adaptation for xpath security views has not been addressed view adaptation results in view downtime during which users are denied access to security views to prevent unauthorized access thus efficient view adaptation is important for making xpath security views pragmatic in this work we show how to adapt an xpath access control view incrementally by re using the existing view which reduces computation and communication costs significantly and results in less downtime for the end user empirical evaluations confirm that the incremental view adaptation algorithms presented in this paper are efficient and scalable
class imbalance where the classes in dataset are not represented equally is common occurrence in machine learning classification models built with such datasets are often not practical since most machine learning algorithms would tend to perform poorly on the minority class instances we present unique evolutionary computing based data sampling approach as an effective solution for the class imbalance problem the genetic algorithm based approach evolutionary sampling works as majority undersampling technique where instances from the majority class are selectively removed this preserves the relative integrity of the majority class while maintaining the original minority class group our research prototype evann also implements genetic algorithm based optimization of modeling parameters for the machine learning algorithms considered in our study an extensive empirical investigation involving four real world datasets is performed comparing the proposed approach to other existing data sampling techniques that target the class imbalance problem our results demonstrate that evolutionary sampling both with and without learner optimization performs relatively better than other data sampling techniques detailed coverage of our case studies in this paper lends itself toward empirical replication
high level languages are growing in popularity however decades of software development have produced large libraries of fast time tested meritorious code that are impractical to recreate from scratch cross language bindings can expose low level code to high level languages unfortunately writing bindings by hand is tedious and error prone while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect we present an improved binding generation strategy based on static analysis of unannotated library source code we characterize three high level idioms that are not uniquely expressible in c's low level type system array parameters resource managers and multiple return values we describe suite of interprocedural analyses that recover this high level information and we show how the results can be used in binding generator for the python programming language in experiments with four large libraries we find that our approach avoids the mistakes characteristic of hand written bindings while offering level of python integration unmatched by prior automated approaches among the thousands of functions in the public interfaces of these libraries roughly exhibit the behaviors detected by our static analyses
the ability to predict at compile time the likelihood of particular branch being taken provides valuable information for several optimizations including global instruction scheduling code layout function inlining interprocedural register allocation and many high level optimizations previous attempts at static branch prediction have either used simple heuristics which can be quite inaccurate or put the burden onto the programmer by using execution profiling data or source code hintsthis paper presents new approach to static branch prediction called value range propagation this method tracks the weighted value ranges of variables through program much like constant propagation these value ranges may be either numeric of symbolic in nature branch prediction is then performed by simply consulting the value range of the appropriate variable heuristics are used as fallback for cases where the value range of the variable cannot be determined statically in the process value range propagationsubsumes both constant propagation and copy propagationexperimental results indicate that this approach produces significantly more accurate predictions than the best existing heuristic techniques the value range propagation method can be implemented over any ldquo factored rdquo dataflow representation with static single assignment property such as ssa form or dependence flow graph where the variables have been renamed to achieve single assignment experimental results indicate that the technique maintains the linear runtime behavior of constant propagation experienced in practice
in this paper we study the problem of effective keyword search over xml documents we begin by introducing the notion of valuable lowest common ancestor vlca to accurately and effectively answer keyword queries over xml documents we then propose the concept of compact vlca cvlca and compute the meaningful compact connected trees rooted as cvlcas as the answers of keyword queries to efficiently compute cvlcas we devise an effective optimization strategy for speeding up the computation and exploit the key properties of cvlca in the design of the stack based algorithm for answering keyword queries we have conducted an extensive experimental study and the experimental results show that our proposed approach achieves both high efficiency and effectiveness when compared with existing proposals
decision support systems help the decision making process with the use of olap on line analytical processing and data warehouses these systems allow the analysis of corporate data as olap and data warehousing evolve more and more complex data is being used xml extensible markup language is flexible text format allowing the interchange and the representation of complex data finding an appropriate model for an xml data warehouse tends to become complicated as more and more solutions appear hence in this survey paper we present an overview of the different proposals that use xml within data warehousing technology these proposals range from using xml data sources for regular warehouses to those using full xml warehousing solutions some researches merely focus on document storage facilities while others present adaptations of xml technology for olap even though there are growing number of researches on the subject many issues still remain unsolved
in this paper we propose an extension algorithm to closet one of the most efficient algorithms for mining frequent closed itemsets in static transaction databases to allow it to mine frequent closed itemsets in dynamic transaction databases in dynamic transaction database transactions may be added deleted and modified with time based on two variant tree structures our algorithm retains the previous mined frequent closed itemsets and updates them by considering the changes in the transaction databases only hence the frequent closed itemsets in the current transaction database can be obtained without rescanning the entire changed transaction database the performance of the proposed algorithm is compared with closet showing performance improvements for dynamic transaction databases compared to using mining algorithms designed for static transaction databases
this paper explores the suitability of dense circulant graphs of degree four for the design of on chip interconnection networks networks based on these graphs reduce the torus diameter in factor which translates into significant performance gains for unicast traffic in addition they are clearly superior to tori when managing collective communications this paper introduces new two dimensional node's labeling of the networks explored which simplifies their analysis and exploitation in particular it provides simple and optimal solutions to two important architectural issues routing and broadcasting other implementation issues such as network folding and scalability by using hierarchical networks are also explored in this work
wireless sensors are very small computers and understanding the timing and behavior of software written for them is crucial to ensuring that they perform correctly this paper outlines lightweight method for gathering behavioral and timing information from simulated executions of software written in the nesc tinyos environment the resulting data is used to generate both behavioral and timing profiles of the software using uml sequence diagrams to visualize the behavior and to present the timing information
repeated elements are ubiquitous and abundant in both manmade and natural scenes editing such images while preserving the repetitions and their relations is nontrivial due to overlap missing parts deformation across instances illumination variation etc manually enforcing such relations is laborious and error prone we propose novel framework where user scribbles are used to guide detection and extraction of such repeated elements our detection process which is based on novel boundary band method robustly extracts the repetitions along with their deformations the algorithm only considers the shape of the elements and ignores similarity based on color texture etc we then use topological sorting to establish partial depth ordering of overlapping repeated instances missing parts on occluded instances are completed using information from other instances the extracted repeated instances can then be seamlessly edited and manipulated for variety of high level tasks that are otherwise difficult to perform we demonstrate the versatility of our framework on large set of inputs of varying complexity showing applications to image rearrangement edit transfer deformation propagation and instance replacement
we investigate the problem of optimizing the routing performance of virtual network by adding extra random links our asynchronous and distributed algorithm ensures by adding single extra link per node that the resulting network is navigable small world ie in which greedy routing using the distance in the original network computes paths of polylogarithmic length between any pair of nodes with probability previously known small world augmentation processes require the global knowledge of the network and centralized computations which is unrealistic for large decentralized networks our algorithm based on careful multi layer sampling of the nodes and the construction of light overlay network bypasses these limitations for bounded growth graphs ie graphs where for any node and any radius the number of nodes within distance from is at most constant times the number of nodes within distance our augmentation process proceeds with high probability in log log communication rounds with log log messages of size log bits sent per node and requiring only log log bit space in each node where is the number of nodes and the diameter in particular with the only knowledge of original distances greedy routing computes between any pair of nodes in the augmented network path of length at most log log with probability and of expected length log log hence we provide distributed scheme to augment any bounded growth graph into small world with high probability in polylogarithmic time while requiring polylogarithmic memory we consider that the existence of such lightweight process might be first step towards the definition of more general construction process that would validate kleinberg's model as plausible explanation for the small world phenomenon in large real interaction networks
to achieve interoperability modern information systems and commerce applications use mappings to translate data from one representation to another in dynamic environments like the web data sources may change not only their data but also their schemas their semantics and their query capabilities such changes must be reflected in the mappings mappings left inconsistent by schema change have to be detected and updated as large complicated schemas become more prevalent and as data is reused in more applications manually maintaining mappings even simple mappings like view definitions is becoming impractical we present novel framework and tool tomas for automatically adapting mappings as schemas evolve our approach considers not only local changes to schema but also changes that may affect and transform many components of schema we consider comprehensive class of mappings for relational and xml schemas with choice types and nested constraints our algorithm detects mappings affected by structural or constraint change and generates all the rewritings that are consistent with the semantics of the mapped schemas our approach explicitly models mapping choices made by user and maintains these choices whenever possible as the schemas and mappings evolve we describe an implementation of mapping management and adaptation tool based on these ideas and compare it with mapping generation tool
in this paper we discuss the energy efficient multicast problem in ad hoc wireless networks each node in the network is assumed to have fixed level of transmission power the problem of our concern is given an ad hoc wireless network and multicast request how to find multicast tree such that the total energy cost of the multicast tree is minimized we first prove this problem is np hard and it is unlikely to have an approximation algorithm with constant performance ratio of the number of nodes in the network we then propose an algorithm based on the directed steiner tree method that has theoretically guaranteed approximation performance ratio we also propose two efficient heuristics node join tree njt and tree join tree tjt algorithms the njt algorithm can be easily implemented in distributed fashion extensive simulations have been conducted to compare with other methods and the results have shown significant improvement on energy efficiency of the proposed algorithms
the key notion in service oriented architecture is decoupling clients and providers of service based on an abstract service description which is used by the service broker to point clients to suitable service implementation client then sends service requests directly to the service implementation problem with the current architecture is that it does not provide trustworthy means for clients to specify service brokers to verify and service implementations to prove that certain desired non functional properties are satisfied during service request processing an example of such non functional property is access and persistence restrictions on the data received as part of the service requests in this work we propose an extension of the service oriented architecture that provides these facilities we also discuss prototype implementation of this architecture and report preliminary results that demonstrate the potential practical value of the proposed architecture in real world software applications
virtual humans are being increasingly used in different domains virtual human modeling requires to consider aspects belonging to different levels of abstractions for example at lower levels one has to consider aspects concerning the geometric definition of the virtual human model and appearance while at higher levels one should be able to define how the virtual human behaves into an environment anim the standard for representing humanoids in xd vrml worlds is mainly concerned with low level modeling aspects as result the developer has to face the problem of defining the virtual human behavior and translating it into lower levels eg geometrical and kinematic aspects in this paper we propose vha virtual human architecture software architecture that allows one to easily manage an interactive anim virtual human into xd vrml worlds the proposed solution allows the developer to focus mainly on high level aspects of the modeling process such as the definition of the virtual human behavior
robotic tape libraries are popular for applications with very high storage requirements such as video servers here we study the throughput of tape library system we design new scheduling algorithm the so called relief and compare it against some older straightforward ones like fcfs maximum queue length mql and an unfair one bypass roughly equivalent to shortest job first the proposed algorithm incorporates an aging mechanism in order to attain fairness and we prove that under certain assumptions it minimizes the average start up latency extensive simulation experiments show that relief outperforms its competitors fair and unfair alike with up to improvement in throughput for the same rejection ratio
this paper demonstrates the advantages of using controlled mobility in wireless sensor networks wsns for increasing their lifetime ie the period of time the network is able to provide its intended functionalities more specifically for wsns that comprise large number of statically placed sensor nodes transmitting data to collection point the sink we show that by controlling the sink movements we can obtain remarkable lifetime improvements in order to determine sink movements we first define mixed integer linear programming milp analytical model whose solution determines those sink routes that maximize network lifetime our contribution expands further by defining the first heuristics for controlled sink movements that are fully distributed and localized our greedy maximum residual energy gmre heuristic moves the sink from its current location to new site as if drawn toward the area where nodes have the highest residual energy we also introduce simple distributed mobility scheme random movement or rm according to which the sink moves uncontrolled and randomly throughout the network the different mobility schemes are compared through extensive ns based simulations in networks with different nodes deployment data routing protocols and constraints on the sink movements in all considered scenarios we observe that moving the sink always increases network lifetime in particular our experiments show that controlling the mobility of the sink leads to remarkable improvements which are as high as sixfold compared to having the sink statically and optimally placed and as high as twofold compared to uncontrolled mobility
the method described in this article evaluates case similarity in the retrieval stage of case based reasoning cbr it thus plays key role in deciding which case to select and therefore in deciding which solution will be eventually applied in cbr there are many retrieval techniques one feature shared by most is that case retrieval is based on attribute similarity and importance however there are other crucial factors that should be considered such as the possible consequences of given solution in other words its potential loss and gain as their name clearly implies these concepts are defined as functions measuring loss and gain when given retrieval case solution is applied moreover these functions help the user to choose the best solution so that when mistake is made the resulting loss is minimal in this way the highest benefit is always obtained
activity centric computing acc systems seek to address the fragmentation of office work across tools and documents by allowing users to organize work around the computational construct of an activity defining and structuring appropriate activities within system poses challenge for users that must be overcome in order to benefit from acc support we know little about how knowledge workers appropriate the activity construct to address this we studied users appropriation of production quality acc system lotus activities for everyday work by employees in large corporation we contribute to better understanding of how users articulate their individual and collaborative work in the system by providing empirical evidence of their patterns of appropriation we conclude by discussing how our findings can inform the design of other acc systems for the workplace
we present an interactive system for synthesizing urban layouts by example our method simultaneously performs both structure based synthesis and an image based synthesis to generate complete urban layout with plausible street network and with aerial view imagery our approach uses the structure and image data of real world urban areas and synthesis algorithm to provide several high level operations to easily and interactively generate complex layouts by example the user can create new urban layouts by sequence of operations such as join expand and blend without being concerned about low level structural details further the ability to blend example urban layout fragments provides powerful way to generate new synthetic content we demonstrate our system by creating urban layouts using example fragments from several real world cities each ranging from hundreds to thousands of city blocks and parcels
dimensionality reduction is an essential data preprocessing technique for large scale and streaming data classification tasks it can be used to improve both the efficiency and the effectiveness of classifiers traditional dimensionality reduction approaches fall into two categories feature extraction and feature selection techniques in the feature extraction category are typically more effective than those in feature selection category however they may break down when processing large scale data sets or data streams due to their high computational complexities similarly the solutions provided by the feature selection approaches are mostly solved by greedy strategies and hence are not ensured to be optimal according to optimized criteria in this paper we give an overview of the popularly used feature extraction and selection algorithms under unified framework moreover we propose two novel dimensionality reduction algorithms based on the orthogonal centroid algorithm oc the first is an incremental oc ioc algorithm for feature extraction the second algorithm is an orthogonal centroid feature selection ocfs method which can provide optimal solutions according to the oc criterion both are designed under the same optimization criterion experiments on reuters corpus volume data set and some public large scale text data sets indicate that the two algorithms are favorable in terms of their effectiveness and efficiency when compared with other state of the art algorithms
most large public displays have been used for providing information to passers by with the primary purpose of acting as one way information channels to individual users we have developed large public display to which users can send their own media content using mobile devices the display supports multi touch interaction thus enabling collaborative use of the display this display called citywall was set up in city center with the goal of showing information of events happening in the city we observed two user groups who used mobile phones with upload capability during two large scale events happening in the city our findings are that this kind of combined use of personal mobile devices and large public display as publishing forum used collaboratively with other users creates unique setting that extends the group's feeling of participation in the events we substantiate this claim with examples from user data
abstract almost all semantics for logic programs with negation identify set sem of models of program as the intended semantics of and any model in this class is considered possible meaning of with regard to the semantics the user has in mind thus for example in the case of stable models check end of sentence choice models check end of sentence answer sets check end of sentence etc different possible models correspond to different ways of completing the incomplete information in the logic program however different end users may have different ideas on which of these different models in sem is reasonable one from their point of view for instance given sem user may prefer model in sem to model in sem based on some evaluation criterion that she has in this paper we develop logic program semantics based on optimal models this semantics does not add yet another semantics to the logic programming arena it takes as input an existing semantics sem and user specified objective function obj and yields new semantics underline rm opt subseteq sem that realizes the objective function within the framework of preferred models identified already by sem thus the user who may or may not know anything about logic programming has considerable flexibility in making the system reflect her own objectives by building on top of existing semantics known to the system in addition to the declarative semantics we provide complete complexity analysis and algorithms to compute optimal models under varied conditions when sem is the stable model semantics the minimal models semantics and the all models semantics
the growing nature of databases and the flexibility inherent in the sql query language that allows arbitrarily complex formulations can result in queries that take inordinate amount of time to complete to mitigate this problem strategies that are optimized to return the first few rows or top rows in case of sorted results are usually employed however both these strategies can lead to unpredictable query processing times thus in this paper we propose supporting time constrained sql queries specifically user issues sql query as before but additionally provides nature of constraint soft or hard an upper bound for query processing time and acceptable nature of results partial or approximate the dbms takes the criteria constraint type time limit quality of result into account in generating the query execution plan which is expected guaranteed to complete in the allocated time for soft hard time constraint if partial results are acceptable then the technique of reducing result set cardinality ie returning first few or top rows is used whereas if approximate results are acceptable then sampling is used to compute query results within the specified time limit for the latter case we argue that trading off quality of results for predictable response time is quite useful however for this case we provide additional aggregate functions to estimate the aggregate values and to compute the associated confidence interval this paper presents the notion of time constrained sql queries discusses the challenges in supporting such construct describes framework for supporting such queries and outlines its implementation in oracle database by exploiting oracle's cost based optimizer and extensibility capabilities
scenarios have been advocated as means of improving requirements engineering yet few methods or tools exist to support scenario based re the paper reports method and software assistant tool for scenario based re that integrates with use case approaches to object oriented development the method and operation of the tool are illustrated with financial system case study scenarios are used to represent paths of possible behavior through use case and these are investigated to elaborate requirements the method commences by acquisition and modeling of use case the use case is then compared with library of abstract models that represent different application classes each model is associated with set of generic requirements for its class hence by identifying the class es to which the use case belongs generic requirements can be reused scenario paths are automatically generated from use cases then exception types are applied to normal event sequences to suggest possible abnormal events resulting from human error generic requirements are also attached to exceptions to suggest possible ways of dealing with human error and other types of system failure scenarios are validated by rule based frames which detect problematic event patterns the tool suggests appropriate generic requirements to deal with the problems encountered the paper concludes with review of related work and discussion of the prospects for scenario based re methods and tools
the new hybrid clone detection tool nicad combines the strengths and overcomes the limitations of both text based and ast based clone detection techniques and exploits novel applications of source transformation system to yield highly accurate identification of cloned code in software systems in this paper we present an in depth study of near miss function clones in open source software using nicad we examine more than open source java and num systems including the entire linux kernel apache httpd jsdk swing and dbo and compare their use of cloned code in several different dimensions including language clone size clone similarity clone location and clone density both by proportion of cloned functions and lines of cloned code we manually verify all detected clones and provide complete catalogue of different clones in an online repository in variety of formats these validated results can be used as cloning reference for these systems and as benchmark for evaluating other clone detection tools copyright copy john wiley sons ltd in this paper we provide an empirical study of function clones in more than open source java and num systems of varying kinds and sizes including the entire linux kernel using the new hybrid clone detection method nicad we manually verify all the detected clones and provide complete catalogue of the different clones in an online repository in variety of formats our studies show that there are large number of near miss function clones in those systems copyright copy john wiley sons ltd
mobile agents are becoming increasingly important in the highly distributed applications frameworks seen today their routing dispatching from node to node is very important issue as we need to safeguard application efficiency achieve better load balancing and resource utilization throughout the underlying network selecting the best target server for dispatching mobile agent is therefore multi faceted problem that needs to be carefully tackled in this paper we propose distributed adaptive routing schemes next node selection for mobile agents the proposed schemes overcome risks like load oscillations ie agents simultaneously abandoning congested node in search for other less saturated node we try to induce different routing decisions taken by agents to achieve load balancing and better utilization of network resources we consider five different algorithms and evaluate them through simulations our findings are quite promising both from the user application and the network infrastructure perspective
the evolution of geographic phenomena has been one of the concerns of spatiotemporal database research however in large spectrum of geographical applications users need more than mere representation of data evolution for instance in urban management applications mdash eg cadastral evolution mdash users often need to know why how and by whom certain changes have been performed as well as their possible impact on the environment answers to such queries are not possible unless supplementary information concerning real world events is associated with the corresponding changes in the database and is managed efficiently this paper proposes solution to this problem which is based on extending spatiotemporal database with mechanism for managing documentation on the evolution of geographic information this solution has been implemented in gis based prototype which is also discussed in the paper
we describe an efficient top down strategy for overlap removal and floorplan repair which repairs overlaps in floorplans produced by placement algorithms or rough floorplanning methodologies the algorithmic framework that we propose incorporates novel geometric shifting technique within top down flow the effect of our algorithm is quantified across broad range of floorplans produced by multiple tools our method succeeds in producing valid placements in almost all cases moreover compared to leading methods it requires only one fifth the run time and produces placements with to less hpwl and up to less cell movement
in this paper we describe conceptual framework and address the related issues and solutions in the identification of three major challenges for the development and evaluation of immersive digital educational games idegs these challenges are advancing adaptive educational technologies to shape learning experience ensuring the individualization of learning experiences adaptation to personal aims needs abilities and prerequisites ii providing technological approaches to reduce the development costs for idegs by enabling the creation of entirely different stories and games for variety of different learning domains each based more or less on the same pool of story units patterns and structures iii developing robust evaluation methodologies for idegs by the extension of iso to include user satisfaction motivation and learning progress and other user experience ux attributes while our research and development is by no means concluded we believe that we have arrived at stage where conclusions may be drawn which will be of considerable use to other researchers in this domain
recently lot of work has been done on formalization of business process specification in particular using petri nets and process algebra however these efforts usually do not explicitly address complex business process development which necessitates the specification coordination and synchronization of large number of business steps it is imperative that these atomic tasks are associated correctly and monitored for countless dependencies moreover as these business processes grow they become critically reliant on large number of split and merge points which additionally increases modeling complexity therefore one of the central challenges in complex business process modeling is the composition of dependent business steps we address this challenge and introduce formally correct method for automated composition of algebraic expressions in complex business process modeling based on acyclic directed graph reductions we show that our method generates an equivalent algebraic expression from an appropriate acyclic directed graph if the graph is well formed and series parallel additionally we encapsulate the reductions in an algorithm that transforms business step dependencies described by users into digraphs recognizes structural conflicts identifies wheatstone bridges and finally generates algebraic expressions
to narrow the semantic gap in content based image retrieval cbir relevance feedback is utilized to explore knowledge about the user's intention in finding target image or image category users provide feedback by marking images returned in response to query image as relevant or irrelevant existing research explores such feedback to refine querying process select features or learn image classifier however the vast amount of unlabeled images is ignored and often substantially limited examples are engaged into learning in this paper we address the two issues and propose novel effective method called relevance aggregation projections rap for learning potent subspace projections in semi supervised way given relevances and irrelevances specified in the feedback rap produces subspace within which the relevant examples are aggregated into single point and the irrelevant examples are simultaneously separated by large margin regarding the query plus its feedback samples as labeled data and the remainder as unlabeled data rap falls in special paradigm of imbalanced semi supervised learning through coupling the idea of relevance aggregation with semi supervised learning we formulate constrained quadratic optimization problem to learn the subspace projections which entail semantic mining and therefore make the underlying cbir system respond to the user's interest accurately and promptly experiments conducted over large generic image database show that our subspace approach outperforms existing subspace methods for cbir even with few iterations of user feedback
applications that analyze mine and visualize large datasets are considered an important class of applications in many areas of science engineering and business queries commonly executed in data analysis applications often involve user defined processing of data and application specific data structures if data analysis is employed in collaborative environment the data server should execute multiple such queries simultaneously to minimize the response time to clients in this paper we present the design of runtime system for executing multiple query workloads on shared memory machine we describe experimental results using an application for browsing digitized microscopy images
in recent years classification learning for data streams has become an important and active research topic major challenge posed by data streams is that their underlying concepts can change over time which requires current classifiers to be revised accordingly and timely to detect concept change common methodology is to observe the online classification accuracy if accuracy drops below some threshold value concept change is deemed to have taken place an implicit assumption behind this methodology is that any drop in classification accuracy can be interpreted as symptom of concept change unfortunately however this assumption is often violated in the real world where data streams carry noise that can also introduce significant reduction in classification accuracy to compound this problem traditional noise cleansing methods are incompetent for data streams those methods normally need to scan data multiple times whereas learning for data streams can only afford one pass scan because of data's high speed and huge volume another open problem in data stream classification is how to deal with missing values when new instances containing missing values arrive how learning model classifies them and how the learning model updates itself according to them is an issue whose solution is far from being explored to solve these problems this paper proposes novel classification algorithm flexible decision tree flexdt which extends fuzzy logic to data stream classification the advantages are three fold first flexdt offers flexible structure to effectively and efficiently handle concept change second flexdt is robust to noise hence it can prevent noise from interfering with classification accuracy and accuracy drop can be safely attributed to concept change third it deals with missing values in an elegant way extensive evaluations are conducted to compare flexdt with representative existing data stream classification algorithms using large suite of data streams and various statistical tests experimental results suggest that flexdt offers significant benefit to data stream classification in real world scenarios where concept change noise and missing values coexist
as more and more human motion data are becoming widely used to animate computer graphics figures in many applications the growing need for compact storage and fast transmission makes it imperative to compress motion data we propose data driven method for efficient compression of human motion sequences by exploiting both spatial and temporal coherences of the data we first segment motion sequence into subsequences such that the poses within subsequence lie near low dimensional linear space we then compress each segment using principal component analysis our method achieves further compression by storing only the key frames projections to the principal component space and interpolating the other frames in between via spline functions the experimental results show that our method can achieve significant compression rate with low reconstruction errors
wireless mesh networks wmns can provide seamless broadband connectivity to network users with low setup and maintenance costs to support next generation applications with real time requirements however these networks must provide improved quality of service guarantees current mesh protocols use techniques that fail to accurately predict the performance of end to end paths and do not optimize performance based on knowledge of mesh network structures in this paper we propose quorum routing protocol optimized for wmns that provides accurate qos properties by correctly predicting delay and loss characteristics of data traffic quorum integrates novel end to end packet delay estimation mechanism with stability aware routing policies allowing it to more accurately follow qos requirements while minimizing misbehavior of selfish nodes
this paper presents method for acquiring concession strategy of an agent in multi issue negotiation this method learns how to make concession to an opponent for realizing win win negotiation to learn the concession strategy we adopt reinforcement learning first an agent receives proposal from an opponent the agent recognizes negotiation state using the difference between their proposals and the difference between their concessions according to the state the agent makes proposal by reinforcement learning reward of the learning is profit of an agreement and punishment of negotiation breakdown the experimental results showed that the agents could acquire the negotiation strategy that avoids negotiation breakdown and increases profits of an agreement as result agents can acquire the action policy that strikes balance between cooperation and competition
the capabilities of current mobile devices especially pdas are making it possible to design and develop mobile applications that employ visual techniques for using geographic data in the field these applications can be extremely useful in areas as diverse as tourism business natural resources management and homeland security in this paper we present system aimed at supporting users in the exploratory analysis of geographic data on pdas through highly interactive interface based on visual dynamic queries we propose alternative visualizations to display query results and present an experimental evaluation aimed at comparing their effectiveness on pda in tourist scenario our findings provide an experimental confirmation of the unsuitability of the typical visualization employed by classic dynamic query systems which displays only those results that fully satisfy query in those cases where only sub optimal results are obtainable for such cases the results of our study highlight the usefulness of visualizations that display all results and their degree of satisfaction of the query
the problem confronted in the content based image retrieval research is the semantic gap between the low level feature representing and high level semantics in the images this paper describes way to bridge such gap by learning the similar images given from the user the system extracts the similar region pairs and classifies those similar region pairs either as object or non object semantics and either as object relation or non object relation semantics automatically which are obtained from comparing the distances and spatial relationships in the similar region pairs by themselves the system also extracts interesting parts of the features from the similar region pair and then adjusts each interesting feature and region pair weight dynamically using those objects and object relation semantics as well as the dynamic weights adjustment from the similar images the semantics of those similar images can be mined and used for searching the similar images the experiments show that the proposed system can retrieve the similar images well and efficient
in this paper some studies have been made on the essence of fuzzy linear discriminant analysis lda algorithm and fuzzy support vector machine fsvm classifier respectively as kernel based learning machine fsvm is represented with the fuzzy membership function while realizing the same classification results with that of the conventional pair wise classification it outperforms other learning machines especially when unclassifiable regions still remain in those conventional classifiers however serious drawback of fsvm is that the computation requirement increases rapidly with the increase of the number of classes and training sample size to address this problem an improved fsvm method that combines the advantages of fsvm and decision tree called dt fsvm is proposed firstly furthermore in the process of feature extraction reformative lda algorithm based on the fuzzy nearest neighbors fknn is implemented to achieve the distribution information of each original sample represented with fuzzy membership grade which is incorporated into the redefinition of the scatter matrices in particular considering the fact that the outlier samples in the patterns may have some adverse influence on the classification result we developed novel lda algorithm using relaxed normalized condition in the definition of fuzzy membership function thus the classification limitation from the outlier samples is effectively alleviated finally by making full use of the fuzzy set theory complete lda cf lda framework is developed by combining the reformative lda rf lda feature extraction method and dt fsvm classifier this hybrid fuzzy algorithm is applied to the face recognition problem extensive experimental studies conducted on the orl and nust face images databases demonstrate the effectiveness of the proposed algorithm
the region analysis of tofte and talpin is an attempt to determine statically the life span of dynamically allocated objects but the calculus is at once intuitively simple yet deceptively subtle and previous theoretical analyses have been frustratingly complex no analysis has revealed and explained in simple terms the connection between the subleties of the calculus and the imperative features it builds on we present novel approach for proving safety and correctness of simplified version of the region calculus we give stratified operational semantics composed of highlevel semantics dealing with the conceptual difficulties of effect annotations and low level one with explicit operations on region indexed store the main results of the paper are proof simpler than previous ones and modular approach to type safety and correctness the flexibility of this approach is demonstrated by the simplicity of the extension to the full calculus with type and region polymorphism
ontologies enable to directly encode domain knowledge in software applications so ontology based systems can exploit the meaning of information for providing advanced and intelligent functionalities one of the most interesting and promising application of ontologies is information extraction from unstructured documents in this area the extraction of meaningful information from pdf documents has been recently recognized as an important and challenging problem this paper proposes an ontology based information extraction system for pdf documents founded on well suited knowledge representation approach named self populating ontology spo the spo approach combines object oriented logic based features with formal grammar capabilities and allows expressing knowledge in term of ontology schemas instances and extraction rules called descriptors aimed at extracting information having also tabular form the novel aspect of the spo approach is that it allows to represent ontologies enriched by rules that enable them to populate them self with instances extracted from unstructured pdf documents in the paper the tractability of the spo approach is proven moreover features and behavior of the prototypical implementation of the spo system are illustrated by means of running example
using wireless peer to peer interactions between portable devices it is possible to locally share information and maintain spatial temporal knowledge emanating from the surroundings we consider the prospects for unleashing ambient data from the surrounding environment for information provision using two biological phenomena human mobility and human social interaction this leads to analogies with epidemiology and is highly relevant to future technology rich environments here embedded devices in the physical environment such as sensors and wireless enabled appliances represent information sources that can provide extensive situated information in this paper we address candidate scenario where isolated sensors in the environment provide real time data from fixed locations using simulation we examine what happens when information is greedily acquired and shared by mobile participants through peer to peer interaction this is assessed taking into account availability of source nodes and the effects of mobility with respect to temporal accuracy of information the results reaffirm the need to consider range of mobility models in testing and validating protocols
we study adding aggregate operators such as summing up elements of column of relation to logics with counting mechanisms the primary motivation comes from database applications where aggregate operators are present in all real life query languages unlike other features of query languages aggregates are not adequately captured by the existing logical formalisms consequently all previous approaches to analyzing the expressive power of aggregation were only capable of producing partial results depending on the allowed class of aggregate and arithmetic operationswe consider powerful counting logic and extend it with the set of all aggregate operators we show that the resulting logic satisfies analogs of hanf's and gaifman's theorems meaning that it can only express local properties we consider database query language that expresses all the standard aggregates found in commercial query languages and show how it can be translated into the aggregate logic thereby providing number of expressivity bounds that do not depend on particular class of arithmetic functions and that subsume all those previously known we consider restricted aggregate logic that gives us tighter capture of database languages and also use it to show that some questions on expressivity of aggregation cannot be answered without resolving some deep problems in complexity theory
while there have been advances in visualization systems particularly in multi view visualizations and visual exploration the process of building visualizations remains major bottleneck in data exploration we show that provenance metadata collected during the creation of pipelines can be reused to suggest similar content in related visualizations and guide semi automated changes we introduce the idea of query by example in the context of an ensemble of visualizations and the use of analogies as first class operations in system to guide scalable interactions we describe an implementation of these techniques in vistrails publicly available open source system
since the first results published in by liu and layland on the rate monotonic rm and earliest deadline first edf algorithms lot of progress has been made in the schedulability analysis of periodic task sets unfortunately many misconceptions still exist about the properties of these two scheduling methods which usually tend to favor rm more than edf typical wrong statements often heard in technical conferences and even in research papers claim that rm is easier to analyze than edf it introduces less runtime overhead it is more predictable in overload conditions and causes less jitter in task executionsince the above statements are either wrong or not precise it is time to clarify these issues in systematic fashion because the use of edf allows better exploitation of the available resources and significantly improves system's performancethis paper compares rm against edf under several aspects using existing theoretical results specific simulation experiments or simple counterexamples to show that many common beliefs are either false or only restricted to specific situations
we present methodology for data warehouse design and its application within the telecom italia information system the methodology is based on conceptual representation of the enterprise which is exploited both in the integration phase of the warehouse information sources and during the knowledge discovery activity on the information stored in the warehouse the application of the methodology in the telecom italia framework has been supported by prototype software tools both for conceptual modeling and for data integration and reconciliation
sprint is middleware infrastructure for high performance and high availability data management it extends the functionality of standalone in memory database imdb server to cluster of commodity shared nothing servers applications accessing an imdb are typically limited by the memory capacity of the machine running the imdb sprint partitions and replicates the database into segments and stores them in several data servers applications are then limited by the aggregated memory of the machines in the cluster transaction synchronization and commitment rely on total order multicast differently from previous approaches sprint does not require accurate failure detection to ensure strong consistency allowing fast reaction to failures experiments conducted on cluster with data servers using tpc and micro benchmark showed that sprint can provide very good performance and scalability
the effort in software process support has focused so far on modeling and enacting processes certain amount of work has been done but little has reached satisfactory level of maturity and acceptance in our opinion this is due to the difficulty for system to accommodate the very numerous aspects involved in software processes complete process support should cover topics ranging from low level tasks like compiling to organizational and strategic tasks this includes process enhancement resource management and control cooperative work etc the environment must also be convenient for software engineers team leaders managers and so on it must be able to describe details for efficient execution and be high level for capturing understanding etc as matter of fact the few tools that have reached sufficient maturity have focussed on single topic and addressed single class of usersit is our claim that no single system can provide satisfactory solution except in clearly defined subdomain thus we shifted our attention from finding the universal system to finding ways to make many different systems cooperate with their associated formalisms and process enginesthis paper presents novel approach for software process support environments based on federation of heterogeneous and autonomous components the approach has been implemented and experimented in the apel environment it is shown which architecture and technology is involved how it works which interoperability paradigms have been used which problems we have solved and which issues are still under study
as the internet grows in size it becomes crucial to understand how the speeds of links in the network must improve in order to sustain the pressure of new end nodes being added each day although the speeds of links in the core and at the edges improve roughly according to moore's law this improvement alone might not be enough indeed the structure of the internet graph and routing in the network might necessitate much faster improvements in the speeds of key links in the network in this paper using combination of analysis and extensive simulations we show that the worst congestion in the internet as level graph in fact scales poorly with the network size sup omega sup where is the number of nodes when shortest path routing is used to route traffic between ases we also show somewhat surprisingly that policy based routing does not exacerbate the maximum congestion when compared to shortest path routing our results show that it is crucial to identify ways to alleviate this congestion to avoid some links from being perpetually congested to this end we show that the congestion scaling properties of internet like graphs can be improved dramatically by introducing moderate amounts of redundancy in the graph in terms of parallel edges between pairs of adjacent nodes
challenge involved in applying density based clustering to categorical biomedical data is that the cube of attribute values has no ordering defined making the search for dense subspaces slow we propose the hierdenc algorithm for hierarchical density based clustering of categorical data and complementary index for searching for dense subspaces efficiently the hierdenc index is updated when new objects are introduced such that clustering does not need to be repeated on all objects the updating and cluster retrieval are efficient comparisons with several other clustering algorithms showed that on large datasets hierdenc achieved better runtime scalability on the number of objects as well as cluster quality by fast collapsing the bicliques in large networks we achieved an edge reduction of as much as hierdenc is suitable for large and quickly growing datasets since it is independent of object ordering does not require re clustering when new data emerges and requires no user specified input parameters
this paper surveys how the maximum adjacency ma ordering of the vertices in graph can be used to solve various graph problems we first explain that the minimum cut problem can be solved efficiently by utilizing the ma ordering the idea is then extended to fundamental operation of graph edge splitting based on this the edge connectivity augmentation problem for given and also for the entire range of can be solved efficiently by making use of the ma ordering where it is asked to add the smallest number of new edges to given graph so that its edge connectivity is increased to other related topics are also surveyed
this paper examines algorithmic aspects of searching for approximate functional dependencies in database relation the goal is to avoid exploration of large parts of the space of potential rules this is accomplished by leveraging found rules to make finding other rules more efficient the overall strategy is an attribute at time iteration which uses local breadth first searches on lattices that increase in width and height in each iteration the resulting algorithm provides many opportunities to apply heuristics to tune the search for particular data sets and or search objectives the search can be tuned at both the global iteration level and the local search level number of heuristics are developed and compared experimentally
current recommender systems attempt to identify appealing items for user by applying syntactic matching techniques which suffer from significant limitations that reduce the quality of the offered suggestions to overcome this drawback we have developed domain independent personalization strategy that borrows reasoning techniques from the semantic web elaborating recommendations based on the semantic relationships inferred between the user's preferences and the available items our reasoning based approach improves the quality of the suggestions offered by the current personalization approaches and greatly reduces their most severe limitations to validate these claims we have carried out case study in the digital tv field in which our strategy selects tv programs interesting for the viewers from among the myriad of contents available in the digital streams our experimental evaluation compares the traditional approaches with our proposal in terms of both the number of tv programs suggested and the users perception of the recommendations finally we discuss concerns related to computational feasibility and scalability of our approach
despite the effectiveness of search engines the persistently increasing amount of web data continuously obscures the search task efforts have thus concentrated on personalized search that takes account of user preferences new concept is introduced towards this direction search based on ranking of local set of categories that comprise user search profile new algorithms are presented that utilize web page categories to personalize search results series of user based experiments show that the proposed solutions are efficient finally we extend the application of our techniques in the design of topic focused crawlers which can be considered an alternative personalized search
we discuss some basic issues of interactive computations in the framework of rough granular computing among these issues are hierarchical modeling of granule structures and interactions between granules of different complexity interactions between granules on which computations are performed are among the fundamental concepts of wisdom technology wistech wistech is encompassing such areas as interactive computations multiagent systems cognitive computation natural computing complex adaptive and autonomous systems or knowledge representation and reasoning about knowledge
the tremendous growth of system memories has increased the capacities and capabilities of memory resident embedded databases yet current embedded databases need to be tuned in order to take advantage of new memory technologies in this paper we study the implications of hosting memory resident databases and propose hardware and software query driven techniques to improve their performance and energy consumption we exploit the structured organization of memories which enables selective mode of operation in which banks are accessed selectively unused banks are placed in lower power mode based on access pattern information we propose hardware techniques that dynamically control the memory by making the system adapt to the access patterns that arise from queries we also propose software query directed scheme that directly modifies the queries to reduce the energy consumption by ensuring uniform bank accesses our results show that these optimizations could lead to at the least reduction in memory energy we also show that query directed schemes better utilize the low power modes achieving up to improvement
modern database systems provide not only powerful data models but also complex query languages supporting powerful features such as the ability to create new database objects and invocation of arbitrary methods possibly written in third party programming language in this sense query languages have evolved into powerful programming languages surprisingly little work exists utilizing techniques from programming language research to specify and analyse these query languages this paper provides formal high level operational semantics for complex value oql like query language that can create fresh database objects and invoke external methods we define type system for our query language and prove an important soundness propertywe define simple effect typing discipline to delimit the computational effects within our queries we prove that this effect system is correct and show how it can be used to detect cases of non determinism and to define correct query optimizations
we present practical technique for pointing and selection using combination of eye gaze and keyboard triggers eyepoint uses two step progressive refinement process fluidly stitched together in look press look release action which makes it possible to compensate for the accuracy limitations of the current state of the art eye gaze trackers while research in gaze based pointing has traditionally focused on disabled users eyepoint makes gaze based pointing effective and simple enough for even able bodied users to use for their everyday computing tasks as the cost of eye gaze tracking devices decreases it will become possible for such gaze based techniques to be used as viable alternative for users who choose not to use mouse depending on their abilities tasks and preferences
there has been tremendous growth in the amount and range of information available on the internet the users requests for online information can be captured by long tail model few popular websites enjoy high number of visitations while the majority of the rest are less frequently requested in this study we use real world data to investigate this phenomenon and show that both users physical location and time of access affect the heterogeneity of website requests the effect can partially be explained by differences in demographic characteristics at locations and diverse user browsing behavior in weekdays and weekends these results can be used to design better online marketing strategies affiliate advertising models and internet caching algorithms with sensitivities to user location and time of access differences
leakage energy reduction for caches has been the target of many recent research efforts in this work we propose novel compiler directed approach to reduce the data cache leakage energy by exploiting the program behavior the proposed approach is based on the observation that only small portion of the data are active at runtime and the program spends lot of time in loops so large portion of data cache lines which are not accessed by the loop can be placed into the leakage control mode to reduce leakage energy consumption the compiler directed approach does not require hardware counters to monitor the access patterns of the cache lines and it is adaptive to the program behavior the experimental results show that the compiler directed approach is very competitive in terms of energy consumption and energy delay product compared to the recently proposed pure hardware based approach we also show that the utilization of loop transformations can increase the effectiveness of our strategy
with an increasing use of data mining tools and techniques we envision that knowledge discovery and data mining system kddms will have to support and optimize for the following scenarios sequence of queries user may analyze one or more datasets by issuing sequence of related complex mining queries and multiple simultaneous queries several users may be analyzing set of datasets concurrently and may issue related complex queriesthis paper presents systematic mechanism to optimize for the above cases targeting the class of mining queries involving frequent pattern mining on one or multiple datasets we present system architecture and propose new algorithms to simultaneously optimize multiple such queries and use knowledgeable cache to store and utilize the past query results we have implemented and evaluated our system with both real and synthetic datasets our experimental results show that our techniques can achieve speedup of up to factor of compared with the systems which do not support caching or optimize for multiple queries
collaborative brainstorming can be challenging but important part of creative group problem solving mind mapping has the potential to enhance the brainstorming process but has its own challenges when used in group we introduce groupmind collaborative mind mapping tool that addresses these challenges and opens new opportunities for creative teamwork including brainstorming we present semi controlled evaluation of groupmind and its impact on teamwork problem solving and collaboration for brainstorming activities groupmind performs better than using traditional whiteboard in both interaction group and nominal group settings for the task involving memory recall the hierarchical mind map structure also imposes important framing effects on group dynamics and idea organization during the brainstorming process we also present design ideas to assist in the development of future tools to support creative problem solving in groups
new paradigm for mobile service chain's competitive and collaborative mechanism is proposed in this study the main idea of the proposed approach is based on multi agent system with optimal profit of the pull push and collaborative models among the portal access service provider pasp the product service provider psp and the mobile service provider msp to address the running mechanism for the multi agent system an integrated system framework is proposed based on the agent evolution algorithm aea which could resolve all these modes to examine the feasibility of the framework prototype system based on java repast is implemented the simulation experiments show that this system can help decision makers take the appropriate strategies with higher profits by analyzing the expectations and variances or risks of each player's profit the interaction between and among entities in the chain is well understood it is found that in the situation where collaborative mechanism is applied the performance of players is better as compared to the other two situations where competitive mechanism is implemented if some constraints are applied the risk will be kept at low level
the common abstraction of xml schema by unranked regular tree languages is not entirely accurate to shed some light on the actual expressive power of xml schema intuitive semantical characterizations of the element declarations consistent edc rule are provided in particular it is obtained that schemas satisfying edc can only reason about regular properties of ancestors of nodes hence with respect to expressive power xml schema is closer to dtds than to tree automata these theoretical results are complemented with an investigation of the xml schema definitions xsds occurring in practice revealing that the extra expressiveness of xsds over dtds is only used to very limited extent as this might be due to the complexity of the xml schema specification and the difficulty of understanding the effect of constraints on typing and validation of schemas simpler formalism equivalent to xsds is proposed it is based on contextual patterns rather than on recursive types and it might serve as light weight front end for xml schema next the effect of edc on the way xml documents can be typed is discussed it is argued that cleaner more robust larger but equally feasible class is obtained by replacing edc with the notion of pass preorder typing ppt schemas that allow one to determine the type of an element of streaming document when its opening tag is met this notion can be defined in terms of grammars with restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns finally algorithms for recognition simplification and inclusion of schemas for the various classes are given
we propose method to handle approximate searching by image content in medical image databases image content is represented by attributed relational graphs holding features of objects and relationships between objects the method relies on the assumption that fixed number of labeled or expected objects eg heart lungs etc are common in all images of given application domain in addition to variable number of unexpected or unlabeled objects eg tumor hematoma etc the method can answer queries by example such as find all rays that are similar to smith's ray the stored images are mapped to points in multidimensional space and are indexed using state of the art database methods trees the proposed method has several desirable propertiesdatabase search is approximate so that all images up to prespecified degree of similarity tolerance are retrievedit has no false dismissals ie all images qualifying query selection criteria are retrieved it is much faster than sequential scanning for searching in the main memory and on the disk ie by up to an order of magnitude thus scaling up well for large databases
this paper proposes novel method using constant inter frame motion for self calibration from an image sequence of an object rotating around single axis with varying camera internal parameters our approach makes use of the facts that in many commercial systems rotation angles are often controlled by an electromechanical system and that the inter frame essential matrices are invariant if the rotation angles are constant but not necessary known therefore recovering camera internal parameters is possible by making use of the equivalence of essential matrices which relate the unknown calibration matrices to the fundamental matrices computed from the point correspondences we also describe linear method that works under restrictive conditions on camera internal parameters the solution of which can be used as the starting point of the iterative non linear method with looser constraints the results are refined by enforcing the global constraints that the projected trajectory of any point should be conic after compensating for the focusing and zooming effects finally using the bundle adjustment method tailored to the special case ie static camera and constant object rotation the structure of the object is recovered and the camera parameters are further refined simultaneously to determine the accuracy and the robustness of the proposed algorithm we present the results on both synthetic and real sequences
discriminative reranking is one method for constructing high performance statistical parsers collins discriminative reranker requires source of candidate parses for each sentence this paper describes simple yet novel method for constructing sets of best parses based on coarse to fine generative parser charniak this method generates best lists that are of substantially higher quality than previously obtainable we used these parses as the input to maxent reranker johnson et al riezler et al that selects the best parse from the set of parses for each sentence obtaining an score of on sentences of length or less
we have proposed the extent system for automated photograph annotation using image content and context analysis key component of extent is landmark recognition system called landmarker in this paper we present the architecture of landmarker the content of query photograph is analyzed and compared against database of sample landmark images to recognize any landmarks it contains an algorithm is presented for comparing query image with sample image context information may be used to assist landmark recognition also we show how landmarker deals with scalability to allow recognition of large number of landmarks we have implemented prototype of the system and present empirical results on large dataset
this article describes our research on spoken language translation aimed toward the application of computer aids for second language acquisition the translation framework is incorporated into multilingual dialogue system in which student is able to engage in natural spoken interaction with the system in the foreign language while speaking query in their native tongue at any time to obtain spoken translation for language assistance thus the quality of the translation must be extremely high but the domain is restricted experiments were conducted in the weather information domain with the scenario of native english speaker learning mandarin chinese we were able to utilize large corpus of english weather domain queries to explore and compare variety of translation strategies formal example based and statistical translation quality was manually evaluated on test set of spontaneous utterances the best speech translation performance percnt correct percnt incorrect and percnt rejected is achieved by system which combines the formal and example based methods using parsability by domain specific chinese grammar as rejection criterion
preventive measures sometimes fail to defect malicious attacks with cyber attacks on data intensive applications becoming an ever more serious threat intrusion tolerant database systems are significant concern the main objective of intrusion tolerant database systems is to detect attacks and to assess and repair the damage caused by the attacks in timely manner such that the database will not be damaged to such degree that is unacceptable or useless this paper focuses on efficient damage assessment and repair in resilient distributed database systems the complexity of distributed database systems caused by data partition distributed transaction processing and failures makes damage assessment and repair much more challenging than in centralized database systems this paper identifies the key challenges and presents an efficient algorithm for distributed damage assessment and repair
this paper presents new algorithm that detects set of dominant points on the boundary of an eight connected shape to obtain polygonal approximation of the shape itself the set of dominant points is obtained from the original break points of the initial boundary where the integral square is zero for this goal most of the original break points are deleted by suppressing those whose perpendicular distance to an approximating straight line is lower than variable threshold value the proposed algorithm iteratively deletes redundant break points until the required approximation which relies on decrease in the length of the contour and the highest error is achieved comparative experiment with another commonly used algorithm showed that the proposed method produced efficient and effective polygonal approximations for digital planar curves with features of several sizes
multisource data flow problems involve information which may enter nodes independently through different classes of edges in some cases dissimilar meet operations appear to be used for different types of nodes these problems include bidirectional and flow sensitive problems as well as many static analyses of concurrent programs with synchronization tuple frameworks type of standard data flow framework provide natural encoding for multisource problems using single meet operator previously the solution of these problems has been described as the fixed point of set of data flow equations using our tuple representation we can access the general results of standard data flow frameworks concerning convergence time and solution precision for these problems we demonstrate this for the bidirectional component of partial redundancy suppression and two problems on the program summary graph an interesting subclass of tuple frameworks the join of meets frameworks is useful for reachability problems especially those stemming from analyses of explicitly parallel programs we give results on function space properties for join of meets frameworks that indicate precise solutions for most of them will be difficult to obtain
in this paper we analyze the node spatial distribution of mobile wireless ad hoc networks characterizing this distribution is of fundamental importance in the analysis of many relevant properties of mobile ad hoc networks such as connectivity average route length and network capacity in particular we have investigated under what conditions the node spatial distribution resulting after large number of mobility steps resembles the uniform distribution this is motivated by the fact that the existing theoretical results concerning mobile ad hoc networks are based on this assumption in order to test this hypothesis we performed extensive simulations using two well known mobility models the random waypoint model which resembles intentional movement and brownian like model which resembles nonintentional movement our analysis has shown that in brownian like motion the uniformity assumption does hold and that the intensity of the concentration of nodes in the center of the deployment region that occurs in the random waypoint model heavily depends on the choice of some mobility parameters for extreme values of these parameters the uniformity assumption is impaired
we present tool that predicts whether the software under development inside an ide has bug an ide plugin performs this prediction using the change classification technique to classify source code changes as buggy or clean during the editing session change classification uses support vector machines svm machine learning classifier algorithm to classify changes to projects mined from their configuration management repository this technique besides being language independent and relatively accurate can classify change immediately upon its completion and use features extracted solely from the change delta added deleted and the source code to predict buggy changes thus integrating change classification within an ide can predict potential bugs in the software as the developer edits the source code ideally reducing the amount of time spent on fixing bugs later to this end we have developed change classification plugin for eclipse based on client server architecture described in this paper
weak pseudorandom function wprf is cryptographic primitive similar to but weaker than pseudorandom function for wprfs one only requires that the output is pseudorandom when queried on random inputs we show that unlike normal prfs wprfs are seed incompressible in the sense that the output of wprf is pseudorandom even if bounded amount of information about the key is leakedas an application of this result we construct simple mode of operation which when instantiated with any wprf gives leakage resilient stream cipher the implementation of such cipher is secure against every side channel attack as long as the amount of information leaked per round is bounded but overall can be arbitrary large the construction is simpler than the previous one dziembowski pietrzak focs as it only uses single primitive wprf in straight forward manner
we describe framework for finding and tracking trails for autonomous outdoor robot navigation through combination of visual cues and ladar derived structural information the algorithm is able to follow paths which pass through multiple zones of terrain smoothness border vegetation tread material and illumination conditions our shape based visual trail tracker assumes that the approaching trail region is approximately triangular under perspective it generates region hypotheses from learned distribution of expected trail width and curvature variation and scores them using robust measure of color and brightness contrast with flanking regions the structural component analogously rewards hypotheses which correspond to empty or low density regions in groundstrike filtered ladar obstacle map our system's performance is analyzed on several long sequences with diverse appearance and structural characteristics ground truth segmentations are used to quantify performance where available and several alternative algorithms are compared on the same data
after reviewing number of internet tools and technologies originating in the field of logic programming and discussing promissing directions of ongoing research we describe logic programming based networking infrastructure which combines reasoning and knowledge processing with flexible coordination of dynamic state changes and computation mobility as well as and its use for the design of intelligent mobile agent programs lightweight logic programming language jinni implemented in java is introduced as flexible scripting tool for gluing together knowledge processing components and java objects in networked client server applications and thin client environments as well as through applets over the web mobile threads implemented by capturing first order continuations in compact data structure sent over the network allow jinni to interoperate with remote high performance binprolog servers for cpu intensive knowledge processing controlled natural language to prolog translator with support of third party speech recognition and text to speech translation allows interaction with users not familiar with logic programming
science projects of various disciplines face fundamental challenge thousands of users want to obtain new scientific results by application specific and dynamic correlation of data from globally distributed sources considering the involved enormous and exponentially growing data volumes centralized data management reaches its limits since scientific data are often highly skewed and exploration tasks exhibit large degree of spatial locality we propose the locality aware allocation of data objects onto distributed network of interoperating databases hisbase is an approach to data management in scientific federated data grids that addresses the scalability issue by combining established techniques of database research in the field of spatial data structures quadtrees histograms and parallel databases with the scalable resource sharing and load balancing capabilities of decentralized peer to peer pp networks the proposed combination constitutes complementary science infrastructure enabling load balancing and increased query throughput
the web and especially major web search engines are essential tools in the quest to locate online information for many people this paper reports results from research that examines characteristics and changes in web searching from nine studies of five web search engines based in the us and europe we compare interactions occurring between users and web search engines from the perspectives of session length query length query complexity and content viewed among the web search engines the results of our research shows users are viewing fewer result pages searchers on us based web search engines use more query operators than searchers on european based search engines there are statistically significant differences in the use of boolean operators and result pages viewed and one cannot necessary apply results from studies of one particular web search engine to another web search engine the wide spread use of web search engines employment of simple queries and decreased viewing of result pages may have resulted from algorithmic enhancements by web search engine companies we discuss the implications of the findings for the development of web search engines and design of online content
in this paper we formulate two classes of problems the colored range query problems and the colored point enclosure query problems to model multi dimensional range and point enclosure queries in the presence of categorical information many of these problems are difficult to solve using traditional data structural techniques based on new framework of combining sketching techniques and traditional data structures we obtain two sets of results in solving the problems approximately and efficiently in addition the framework can be employed to attack other related problems by finding the appropriate summary structures
in this paper we show how pattern matching can be seen to arise from proof term assignment for the focused sequent calculus this use of the curry howard correspondence allows us to give novel coverage checking algorithm and makes it possible to give rigorous correctness proof for the classical pattern compilation strategy of building decision trees via matrices of patterns
in this paper we propose complete model handling the physical simulation of deformable objects we formulate continuous expressions for stretching bending and twisting energies these expressions are mechanically rigorous and geometrically exact both elastic and plastic deformations are handled to simulate wide range of materials we validate the proposed model in several classical test configurations the use of geometrical exact energies with dynamic splines provides very accurate results as well as interactive simulation times which shows the suitability of the proposed model for constrained cad applications we illustrate the application potential of the proposed model by describing virtual system for cable positioning which can be used to test compatibility between planned fixing clip positions and mechanical cable properties
emerging applications in the area of emergency response and disaster management are increasingly demanding interactive capabilities to allow for the quick understanding of critical situation in particular in urban environments key component of these interactive simulations is how to recreate the behavior of crowd in real time while supporting individual behaviors crowds can often be unpredictable and present mixed behaviors such as panic or aggression that can very rapidly change based on unexpected new elements introduced into the environment we present preliminary research specifically oriented towards the simulation of large crowds for emergency response and rescue planning situations our approach uses highly scalable architecture integrated with an efficient rendering architecture and an immersive visualization environment for interaction in this environment users can specify complex scenarios plug in crowd behavior algorithms and interactively steer the simulation to analyze and evaluate multiple what if situations
we introduce an algorithm for space variant filtering of video based on spatio temporal laplacian pyramid and use this algorithm to render videos in order to visualize prerecorded eye movements spatio temporal contrast and colour saturation are reduced as function of distance to the nearest gaze point of regard ie non fixated distracting regions are filtered out whereas fixated image regions remain unchanged results of an experiment in which the eye movements of an expert on instructional videos are visualized with this algorithm so that the gaze of novices is guided to relevant image locations show that this visualization technique facilitates the novices perceptual learning
this paper presents an investigation into dynamic self adjustment of task deployment and other aspects of self management through the embedding of multiple policiesnon dedicated loosely coupled computing environments such as clusters and grids are increasingly popular platforms for parallel processing these abundant systems are highly dynamic environments in which many sources of variability affect the run time efficiency of tasks the dynamism is exacerbated by the incorporation of mobile devices and wireless communicationthis paper proposes an adaptive strategy for the flexible run time deployment of tasks to continuously maintain efficiency despite the environmental variability the strategy centres on policy based scheduling which is informed by contextual and environmental inputs such as variance in the round trip communication time between client and its workers and the effective processing performance of each workera self management framework has been implemented for evaluation purposes the framework integrates several policy controlled adaptive services with the application code enabling the run time behaviour to be adapted to contextual and environmental conditions using this framework an exemplar self managing parallel application is implemented and used to investigate the extent of the benefits of the strategy
in the area of health care and sports in recent years variety of mobile applications have been established mobile devices are of emerging interest due to their high availability and increasing computing power in many different health scenarios in this paper we present scalable secure sensor monitoring platform ssmp which collects vital data of users vital parameters can be collected by just one single sensor or in multi sensor configuration nowadays wide spectrum of sensors is available which provide wireless connectivity eg bluetooth vital data can then easily be transmitted to mobile device which subsequently transmits these data to an ehealth portal there are already solutions implementing these capabilities however privacy aspects of users are very often neglected since health data may enable people to draw potentially compromising conclusions eg insurance companies it is absolutely necessary to design an enhanced security concept in this context to complicate matters further the trustworthiness of providers which are operating with user's health data can not be determined by users priori this means that the security concept implemented by the provider may bear security flaws additionally there is no guarantee that the provider preserves the users privacy claims in this work we propose security concept incorporating privacy aspects using mobile devices for transferring and storing health data at portal in addition the concept guarantees anonymity in the transfer process as well as for stored data at service provider hence insider attacks based on stored data can be prevented
next generation system designs are challenged by multiple walls among them the inter related impediments offered by power dissipation limits and reliability are particularly difficult ones that all current chip system design teams are grappling with in this paper we first describe the attendant challenges in integrated multi dimensional pre silicon modeling and the solution approaches being pursued later we focus on leading edge solutions for power thermal and failure rate mitigation that have been proposed in our work over the past decade
the main goal of this paper is to provide routing table free online algorithms for wireless sensor networks wsns to select cost eg node residual energies and delay efficient paths as basic information to drive the routing process both node costs and hop count distances are considered particular emphasis is given to greedy routing schemes due to their suitability for resource constrained and highly dynamic networks for what concerns greedy forwarding we present the statistically assisted routing algorithm sara where forwarding decisions are driven by statistical information on the costs of the nodes within coverage and in the second order neighborhood by analysis we prove that an optimal online policy exists we derive its form and we exploit it as the core of sara besides greedy techniques sub optimal algorithms where node costs can be partially propagated through the network are also presented these techniques are based on real time learning lrta algorithms which through an initial exploratory phase converge to quasi globally optimal paths all the proposed schemes are then compared by simulation against globally optimal solutions discussing the involved trade offs and possible performance gains the results show that the exploitation of second order cost information in sara substantially increases the goodness of the selected paths with respect to fully localized greedy routing finally the path quality can be further increased by lrta schemes whose convergence can be considerably enhanced by properly setting real time search parameters however these solutions fail in highly dynamic scenarios as they are unable to adapt the search process to time varying costs
peer to peer pp networks are beginning to form the infrastructure of future applications computers are organized in pp overlay networks to facilitate search queries with reasonable cost so scalability is major aim in design of pp networks in this paper to obtain high factor of scalability we partition network search space using consistent static shared upper ontology we name our approach semantic partition tree spt all resources and queries are annotated using the upper ontology and queries are semantically routed in the overlay network also each node indexes addresses of other nodes that possess contents expressible by the concept it maintains so our approach can be conceived as an ontology based distributed hash table dht also we introduce lookup service for the network which is very scalable and independent of the network size and just depends on depth of the ontology tree further we introduce broadcast algorithm on the network we present worst case analysis of both lookup and broadcast algorithms and measure their performance using simulation the results show that our scheme is highly scalable and can be used in real pp applications
stacked wafer integration has the potential to improve multiprocessor system on chip mpsoc integration density performance and power efficiency however the power density of mpsocs increases with the number of active layers resulting in high chip temperatures this can reduce system reliability reduce performance and increase cooling cost thermal optimization for mpsocs imposes numerous challenges it is difficult to manage assignment and scheduling of heterogeneous workloads to maintain thermal safety in addition the thermal characteristics of mpsocs differ from those of mpsocs because each stacked layer has different thermal resistance to the ambient and vertically adjacent processors have strong temperature correlation we propose mpsoc thermal optimization algorithm that conducts task assignment scheduling and voltage scaling power balancing algorithm is initially used to distribute tasks among cores and active layers detailed thermal analysis is used to guide hotspot mitigation algorithm that incrementally reduces the peak mpsoc temperature by appropriately adjusting task execution times and voltage levels the proposed algorithm considers leakage power consumption and adapts to inter layer thermal heterogeneity performance evaluation on set of multiprogrammed and multithreaded benchmarks indicates that the proposed techniques can optimize dmpsoc power consumption power profile and chip peak temperature
the test scheduling problem is one of the major issues in the test integration of system on chip soc and test schedule is usually influenced by the test access mechanism tam in this paper we propose graph based approach to power constrained test scheduling with tam assignment and test conflicts also considered by mapping test schedule to subgraph of the test compatibility graph an interval graph recognition method can be used to determine the order of the core tests we then present heuristic algorithm that can effectively assign tam wires to the cores given the test order with the help of the tabu search method and the test compatibility graph the proposed algorithm allows rapid exploration of the solution space experimental results for the itc benchmarks show that short test length is achieved within reasonable computation time
commercial workloads form an important class of applications and have performance characteristics that are distinct from scientific and technical benchmarks such as spec cpu however due to the prohibitive simulation time of commercial workloads it is extremely difficult to use them in computer architecture research in this paper we study the efficacy of using statistical sampling based simulation methodology for two classes of commercial workloads java server benchmark specjbb and an online transaction processing oltp benchmark dbt our results show that although specjbb shows distinct garbage collection phases there are no large scale phases in the oltp benchmark we take advantage of this stationary behavior in steady phase and propose statistical sampling based simulation technique dynasim with two dynamic stopping rules in this approach the simulation terminates once the target accuracy has been met we apply dynasim to simulate commercial workloads and show that with the simulation of only few million total instructions the error can be within at confidence level of dynasim compares favorably with random sampling and representative sampling in terms of the total number of instructions simulated time cost and with representative sampling in terms of the number of checkpoints storage cost dynasim increases the usability of sampling based simulation approach for commercial workloads and will encourage the use of commercial workloads in computer architecture research
we describe new data format for storing triangular symmetric and hermitian matrices called rectangular full packed format rfpf the standard two dimensional arrays of fortran and also known as full format that are used to represent triangular and symmetric matrices waste nearly half of the storage space but provide high performance via the use of level blas standard packed format arrays fully utilize storage array space but provide low performance as there is no level packed blas we combine the good features of packed and full storage using rfpf to obtain high performance via using level blas as rfpf is standard full format representation also rfpf requires exactly the same minimal storage as packed the format each lapack full and or packed triangular symmetric and hermitian routine becomes single new rfpf routine based on eight possible data layouts of rfpf this new rfpf routine usually consists of two calls to the corresponding lapack full format routine and two calls to level blas routines this means no new software is required as examples we present lapack routines for cholesky factorization cholesky solution and cholesky inverse computation in rfpf to illustrate this new work and to describe its performance on several commonly used computer platforms performance of lapack full routines using rfpf versus lapack full routines using the standard format for both serial and smp parallel processing is about the same while using half the storage performance gains are roughly one to factor of for serial and one to factor of for smp parallel times faster using vendor lapack full routines with rfpf than with using vendor and or reference packed routines
application resource usage models can be used in the decision making process for ensuring quality of service as well as for capacity planning apart from their general use in performance modeling optimization and systems management current solutions for modeling application resource usage tend to address parts of the problem by either focusing on specific application or specific platform or on small subset of system resources we propose simple and flexible approach for modeling application resource usage in platform independent manner that enables the prediction of application resource usage on unseen platforms the technique proposed is application agnostic requiring no modification to the application binary or source and no knowledge of application semantics we implement linux based prototype and evaluate it using four different workloads including real world applications and benchmarks our experiments reveal prediction errors that are bound within of the observed for these workloads when using the proposed approach
similarity search is core module of many data analysis tasks including search by example classification and clustering for time series data dynamic time warping dtw has been proven very effective similarity measure since it minimizes the effects of shifting and distortion in time however the quadratic cost of dtw computation to the length of the matched sequences makes its direct application on databases of long time series very expensive we propose technique that decomposes the sequences into number of segments and uses cheap approximations thereof to compute fast lower bounds for their warping distances we present several progressively tighter bounds relying on the existence or not of warping constraints finally we develop an index and multi step technique that uses the proposed bounds and performs two levels of filtering to efficiently process similarity queries thorough experimental study suggests that our method consistently outperforms state of the art methods for dtw similarity search
various known models of probabilistic xml can be represented as instantiations of the abstract notion of documents in addition to ordinary nodes documents have distributional nodes that specify the possible worlds and their probabilistic distribution particular families of documents are determined by the types of distributional nodes that can be used as well as by the structural constraints on the placement of those nodes in document some of the resulting families provide natural extensions and combinations of previously studied probabilistic xml models the focus of the paper is on the expressive power of families of documents in particular two main issues are studied the first is the ability to efficiently translate given document of one family into another family the second is closure under updates namely the ability to efficiently represent the result of updating the instances of document of given family as another document of that family for both issues we distinguish two variants corresponding to value based and object based semantics of documents
we discuss the parallelization of algorithms for solving poly nomial systems symbolically by way of triangular decompositions we introduce component level parallelism for which the number of processors in use depends on the geometry of the solution set of the input system our long term goal is to achieve an efficient multi level parallelism coarse grained component level for tasks computing geometric objects in the solution sets and medium fine grained level for polynomial arithmetic such as gcd resultant computation within each task component level parallelization of triangular decompositions belongs to the class of dynamic irregular parallel applications which leads us to address the following question how to exploit geometrical information at an early stage of the solving process that would be favorable to parallelization we report on the effectiveness of the approaches that we have applied including modular methods solving by decreasing order of dimension task pool with dimension and rank guided scheduling we have extended the aldor programming language to support multiprocessed parallelism on smps and realized preliminary implementation our experimentation shows promising speedups for some well known problems and proves that our component level parallelization is practically efficient we expect that this speedup would add multiplicative factor to the speedup of medium fine grained level parallelization as parallel gcd and resultant computations
transactional coherence and consistency tcc offers way to simplify parallel programming by executing all code within transactions in tcc systems transactions serve as the fundamental unit of parallel work communication and coherence as each transaction completes it writes all of its newly produced state to shared memory atomically while restarting other processors that have speculatively read stale data with this mechanism tcc based system automatically handles data synchronization correctly without programmer intervention to gain the benefits of tcc programs must be decomposed into transactions we describe two basic programming language constructs for decomposing programs into transactions loop conversion syntax and general transaction forking mechanism with these constructs writing correct parallel programs requires only small incremental changes to correct sequential programs the performance of these programs may then easily be optimized based on feedback from real program execution using few simple techniques
modern stream applications such as sensor monitoring systems and publish subscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams efficient sharing of computations among multiple continuous queries especially for the memory and cpu intensive window based operations is critical novel challenge in this scenario is to allow resource sharing among similar queries even if they employ windows of different lengths this paper first reviews the existing sharing methods in the literature and then illustrates the significant performance shortcomings of these methodsthis paper then presents novel paradigm for the sharing of window join queries namely we slice window states of join operator into fine grained window slices and form chain of sliced window joins by using an elaborate pipelining methodology the number of joins after state slicing is reduced from quadratic to linear this novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes based on the state slice sharing paradigm two algorithms are proposed for the chain buildup one minimizes the memory consumption while the other minimizes the cpu usage the algorithms are proven to find the optimal chain with respect to memory or cpu usage for given query workload we have implemented the slice share paradigm within the data stream management system cape the experimental results show that our strategy provides the best performance over diverse range of workload settings among all alternate solutions in the literature
in ad hoc networks the performance is significantly degraded as the size of the network grows the network clustering by which the nodes are hierarchically organized on the basis of the proximity relieves this performance degradation finding the weakly connected dominating set wcds is promising approach for clustering the wireless ad hoc networks finding the minimum wcds in the unit disk graph is an np hard problem and host of approximation algorithms has been proposed in this article we first proposed centralized approximation algorithm called dla cc based on distributed learning automata dla for finding near optimal solution to the minimum wcds problem then we propose dla based clustering algorithm called dla dc for clustering the wireless ad hoc networks the proposed cluster formation algorithm is distributed implementation of dla cc in which the dominator nodes and their closed neighbors assume the role of the cluster heads and cluster members respectively in this article we compute the worst case running time and message complexity of the clustering algorithm for finding near optimal cluster head set we argue that by proper choice of the learning rate of the clustering algorithm trade off between the running time and message complexity of algorithm with the cluster head set size clustering optimality can be made the simulation results show the superiority of the proposed algorithms over the existing methods
traditional compilers compile and optimize files separately making worst case assumptions about the program context in which file is to be linked more aggressive compilation architectures perform cross file interprocedural or whole program analyses potentially producing much faster programs but substantially increasing the cost of compilation even more radical are systems that perform all compilation and optimization at run time such systems can optimize programs based on run time program and system properties as well as static whole program properties however run time compilers also called dynamic compilers or just in time compilers suffer under severe constraints on allowable compilation time since any time spent compiling steals from time spent running the program none of these compilation models dominates the others each has unique strengths and weaknesses not present in the other modelswe are developing new staged compilation model which strives to combine high run time code quality with low compilation overhead compilation is organized as series of stages with stages corresponding to for example separate compilation library linking program linking and run time execution any given optimization can be performed at any of these stages to reduce compilation time while maintaining high effectiveness an optimization should be performed at the earliest stage that provides the necessary program context information to carry out the optimization effectively moreover single optimization can itself be spread across multiple stages with earlier stages performing preplanning work that enables the final stage to complete the optimization quickly in this way we hope to produce highly optimized programs nearly as good as what could be done with purely run time compiler that had an unconstrained compilation time budget but at much more practical compile time costwe are building the whirlwind optimizing compiler as the concrete embodiment of this staged compilation model initially targeting object oriented languages key component of whirlwind is set of techniques for automatically constructing staged compilers from traditional unstaged compilers including aggressive applications of specialization and other partial evaluation technology
many cache management schemes designed for mobile environments are based on invalidation reports irs however ir based approach suffers from long query latency and it cannot efficiently utilize the broadcast bandwidth in this paper we propose techniques to address these problems first by replicating small fraction of the essential information related to cache invalidation the query latency can be reduced then we propose techniques to efficiently utilize the broadcast bandwidth based on counters associated with each data item novel techniques are designed to maintain the accuracy of the counter in case of server failures client failures and disconnections extensive simulations are provided and used to evaluate the proposed methodology compared to previous ir based algorithms the proposed solution can significantly reduce the query latency improve the bandwidth utilization and effectively deal with disconnections and failures
recently many natural language processing nlp applications have improved the quality of their output by using various machine learning techniques to mine information extraction ie patterns for capturing information from the input text currently to mine ie patterns one should know in advance the type of the information that should be captured by these patterns in this work we propose novel methodology for corpus analysis based on cross examination of several document collections representing different instances of the same domain we show that this methodology can be used for automatic domain template creation as the problem of automatic domain template creation is rather new there is no well defined procedure for the evaluation of the domain template quality thus we propose methodology for identifying what information should be present in the template using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates
model transformations provide powerful capability to automate model refinements however the use of model transformation languages may present challenges to those who are unfamiliar with specific transformation language this paper presents an approach called model transformation by demonstration mtbd which allows an end user to demonstrate the exact transformation desired by actually editing source model and demonstrating the changes that evolve to target model an inference engine built into the underlying modeling tool records all editing operations and infers transformation pattern which can be reused in other models the paper motivates the need for the approach and discusses the technical contributions of mtbd case study with several sample inferred transformations serves as concrete example of the benefits of mtbd
spectral clustering refers to flexible class of clustering procedures that can produce high quality clusterings on small data sets but which has limited applicability to large scale problems due to its computational complexity of in general with the number of data points we extend the range of spectral clustering by developing general framework for fast approximate spectral clustering in which distortion minimizing local transformation is first applied to the data this framework is based on theoretical analysis that provides statistical characterization of the effect of local distortion on the mis clustering rate we develop two concrete instances of our general framework one based on local means clustering kasp and one based on random projection trees rasp extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy specifically our algorithms outperform means by large margin in terms of accuracy and run several times faster than approximate spectral clustering based on the nystrom method with comparable accuracy and significantly smaller memory footprint remarkably our algorithms make it possible for single machine to spectral cluster data sets with million observations within several minutes
many software systems suffer from missing support for behavioral runtime composition and configuration of software components the concern behavioral composition and configuration is not treated as first class entity but instead it is hard coded in different programming styles leading to tangled composition and configuration code that is hard to understand and maintain we propose to embed dynamic language with tailorable object and class concept into the host language in which the components are written and use the tailorable language for behavioral composition and configuration tasks using this approach we can separate the concerns behavioral composition and configuration from the rest of the software system leading to more reusable understandable and maintainable composition and configuration of software components
deep packet inspection dpi has been widely adopted in detecting network threats such as intrusion viruses and spam it is challenging however to achieve high speed dpi due to the expanding rule sets and ever increasing line rates key issue is that the size of the finite automata falls beyond the capacity of on chip memory thus incurring expensive off chip accesses in this paper we present dpico hardware based dpi engine that utilizes novel techniques to minimize the storage requirements for finite automata the techniques proposed are modified content addressable memory mcam interleaved memory banks and data packing the experiment results show the scalable performance of dpico can achieve up to gbps throughput using contemporary fpga chip experiment data also show that dpico based accelerator can improve the pattern matching performance of dpi server by up to times
an accurate tractable analytic cache model for time shared systems is presented which estimates the overall cache miss rate of multiprocessing system with any cache size and time quanta the input to the model consists of the isolated miss rate curves for each process the time quanta for each of the executing processes and the total cache size the output is the overall miss rate trace driven simulations demonstrate that the estimated miss rate is very accurate since the model provides fast and accurate way to estimate the effect of context switching it is useful for both understanding the effect of context switching on caches and optimizing cache performance for time shared systems cache partitioning mechanism is also presented and is shown to improve the cache miss rate up to over the normal lru replacement policy
in this paper we investigate technique for fusing approximate knowledge obtained from distributed heterogeneous information sources this issue is substantial eg in modeling multiagent systems where group of loosely coupled heterogeneous agents cooperate in achieving common goal information exchange leading ultimately to knowledge fusion is natural and vital ingredient of this process we use generalization of rough sets and relations which depends on allowing arbitrary similarity relations the starting point of this research is where framework for knowledge fusion in multiagent systems is introduced agents individual perceptual capabilities are represented by similarity relations further aggregated to express joint capabilities of teams this aggregation expressing shift from individual to social level of agents activity has been formalized by means of dynamic logic the approach of doherty et al uses the full propositional dynamic logic which does not guarantee tractability of reasoning our idea is to adapt the techniques of nguyen to provide an engine for tractable approximate database querying restricted to horn fragment of serial dynamic logic we also show that the obtained formalism is quite powerful in applications
the problem of modeling memory locality of applications to guide compiler optimizations in systematic manner is an important unsolved problem made even more significant with the advent of multi core and many core architectures we describe an approach based on novel source level metric called static reuse distance to model the memory behavior of applications written in matlab we use matlab as representative language that lets end users express their algorithms precisely but at relatively high level matlab's high level characteristics allow the static analysis to focus on large objects such as arrays without losing accuracy due to processor specific layout of scalar values in memory we present an efficient algorithm to compute static reuse distances using an extended version of dependence graphs our approach differs from earlier similar attempts in three important aspects it targets high level programming systems characterized by heavy use of libraries it works on full programs instead of being confined to loops and it integrates practical mechanisms to handle separately compiled procedures as well as pre compiled library procedures that are only available in binary form we study matlab code taken from real programs to demonstrate the effectiveness of our model finally we present some applications of our approach to program transformations that are known to be important in matlab but are expected to be relevant to other similar high level languages as well
the problem of frequent item discovery in streaming data has attracted lot of attention lately while the above problem has been studied extensively and several techniques have been proposed for its solution these approaches treat all the values of the data stream equally nevertheless not all values are of equal importance in several situations we are interested more in the new values that have appeared in the stream rather than in the older onesin this paper we address the problem of finding recent frequent items in data stream given small bounded memory and present novel algorithms to this direction we propose basic algorithm that extends the functionality of existing approaches by monitoring item frequencies in recent windows subsequently we present an improved version of the algorithm with significantly improved performance in terms of accuracy at no extra memory cost finally we perform an extensive experimental evaluation and show that the proposed algorithms can efficiently identify the frequent items in ad hoc recent windows of data stream
the past two decades have presented significant technological developments of mobile information and communication technology ict such as portable technologies eg mobile phones notebook computers personal digital assistants and associated wireless infrastructures eg wireless local area networks mobile telecommunications infrastructures bluetooth personal area networks mobile ict offers range of technical opportunities for organisations and their members to implement enterprise mobility however the challenges of unlocking the opportunities of enterprise mobility are not well understood one of the key issues is to establish systems and associated working practices that are deemed usable by both individuals and the organisation the aim of this paper is to show that the concept of organisational usability can enrich the understanding of mobile ict in organisations as an addition to the traditional understanding of individual usability organisational usability emphasises the role of mobile ict beyond individual support large scale study of four different ways of organising foreign exchange trading in middle eastern bank serves as the concrete foundation for the discussion the empirical study showed how the final of the four attempts at establishing trading deployed mobile ict to enable mobile trading and by providing solution which was deemed usable for both the organisation and the traders the paper contributes to the understanding of how usability of mobile ict critically depends on carefully balancing individual and organisational requirements it also demonstrates the need for research in enterprise mobility to embrace both individual and organisational concerns in order to grasp the complexity of the phenomena
high performance clusters have been growing rapidly in scale most of these clusters deploy high speed interconnect such as infini band to achieve higher performance most scientific applications executing on these clusters use the message passing interface mpi as the parallel programming model thus the mpi library has key role in achieving application performance by consuming as few resources as possible and enabling scalable performance state of the art mpi implementations over infiniband primarily use the reliable connection rc transport due to its good performance and attractive features however the rc transport requires connection between every pair of communicating processes with each requiring several kb of memory as clusters continue to scale memory requirements in rc based implementations increase the connection less unreliable datagram ud transport is an attractive alternative which eliminates the need to dedicate memory for each pair of processes in this paper we present high performance ud based mpi design we implement our design and compare the performance and resource usage with the rc based mvapich we evaluate npb smg sweepd and sppm up to processes on an core infiniband cluster for smg our prototype shows speedup and seven fold reduction in memory for processes additionally based on our model our design has an estimated times reduction in memory over mvapich at processes when all connections are created to the best of our knowledge this is the first research work that presents high performance mpi design over infiniband that is completely based on ud and can achieve near identical or better application performance than rc
outlier detection has been used for centuries to detect and where appropriate remove anomalous observations from data outliers arise due to mechanical faults changes in system behaviour fraudulent behaviour human error instrument error or simply through natural deviations in populations their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences it can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing the original outlier detection methods were arbitrary but now principled and systematic techniques are used drawn from the full gamut of computer science and statistics in this paper we introduce survey of contemporary techniques for outlier detection we identify their respective motivations and distinguish their advantages and disadvantages in comparative review
this paper presents novel image feature representation method called multi texton histogram mth for image retrieval mth integrates the advantages of co occurrence matrix and histogram by representing the attribute of co occurrence matrix using histogram it can be considered as generalized visual attribute descriptor but without any image segmentation or model training the proposed mth method is based on julesz's textons theory and it works directly on natural images as shape descriptor meanwhile it can be used as color texture descriptor and leads to good performance the proposed mth method is extensively tested on the corel dataset with natural images the results demonstrate that it is much more efficient than representative image feature descriptors such as the edge orientation auto correlogram and the texton co occurrence matrix it has good discrimination power of color texture and shape features
it is widely believed that distributed software development is riskier and more challenging than collocated development prior literature on distributed development in software engineering and other fields discuss various challenges including cultural barriers expertise transfer difficulties and communication and coordination overhead we evaluate this conventional belief by examining the overall development of windows vista and comparing the post release failures of components that were developed in distributed fashion with those that were developed by collocated teams we found negligible difference in failures this difference becomes even less significant when controlling for the number of developers working on binary we also examine component characteristics such as code churn complexity dependency information and test code coverage and find very little difference between distributed and collocated components to investigate if less complex components are more distributed further we examine the software process and phenomena that occurred during the vista development cycle and present ways in which the development process utilized may be insensitive to geography by mitigating the difficulties introduced in prior work in this area
seed based framework for textual information extraction allows for weakly supervised extraction of named entities from anonymized web search queries the extraction is guided by small set of seed named entities without any need for handcrafted extraction patterns or domain specific knowledge allowing for the acquisition of named entities pertaining to various classes of interest to web search users inherently noisy search queries are shown to be highly valuable albeit little explored resource for web based named entity discovery
making the structure of software visible during system development helps build shared understanding of the context for each piece of work ii identify progress with implementation and iii highlight any conflict between individual development activities finding an adequate representation for such information is not straightforward especially for large applications this paper describes an implementation of such visualization system designed to explore some of the issues involved the approach is based on war room command console metaphor and uses bank of eight linked consoles to present information the tool was applied to several industrial software systems written in mixture of java and one of which was over million lines of code in size
this paper is concerned with the parallel evaluation of datalog rule programs mainly by processors that are interconnected by communication network we introduce paradigm called data reduction for the parallel evaluation of general datalog program several parallelization strategies discussed previously in cw gst ws are special cases of this paradigm the paradigm parallelizes the evaluation by partitioning among the processors the instantiations of the rules after presenting the paradigm we discuss the following issues that we see fundamental for parallelization strategies derived from the paradigm properties of the strategies that enable reduction in the communication overhead decomposability load balancing and application to programs with negation we prove that decomposability concept introduced previously in ws cw is undecidable
dynamic load balancing is key factor in achieving high performance for large scale distributed simulations on grid infrastructures in grid environment the available resources and the simulation's computation and communication behavior may experience run time critical imbalances consequently an initial static partitioning should be combined with dynamic load balancing scheme to ensure the high performance of the distributed simulation many improved or novel dynamic load balancing designs have been proposed in recent years which aim to improve the distributed simulation performance such designs are in general non formalized and the realizations of the designs are highly time consuming and error prone practices in this paper we propose formal dynamic load balancing design approach using discrete event system specification devs we discuss the feasibility of using devs and as an additional step we consider studying recently proposed design through formalized devs model system our focus is how devs component based formalized design approach can predict some of the key design factors before the design is realized or can further validate and consolidate realized dynamic load balancing designs
the glasgow haskell compiler ghc has quite sophisticated support for concurrency in its runtime system which is written in low level code as ghc evolves the runtime system becomes increasingly complex error prone difficult to maintain and difficult to add new concurrency features this paper presents an alternative approach to implement concurrency in ghc rather than hard wiring all kinds of concurrency features the runtime system is thin substrate providing only small set of concurrency primitives and the remaining concurrency features are implemented in software libraries written in haskell this design improves the safety of concurrency support it also provides more customizability of concurrency features which can be developed as haskell library packages and deployed modularly
for large scale and residual software like network service reliability is critical requirement recent research has shown that most of network software still contains number of bugs methods for automated detection of bugs in software can be classified into static analysis based on formal verification and runtime checking based on fault injection in this paper framework for checking software security vulnerability is proposed the framework is based on automated bug detection technologies ie static analysis and fault injection which are complementary each other the proposed framework provides new direction in which various kinds of software can be checked its vulnerability by making use of static analysis and fault injection technology in experiment on proposed framework we find unknown vulnerability as well as known vulnerability in windows network module
we present the design implementation and evaluation of promise novel peer to peer media streaming system encompassing the key functions of peer lookup peer based aggregated streaming and dynamic adaptations to network and peer conditions particularly promise is based on new application level pp service called collectcast collectcast performs three main functions inferring and leveraging the underlying network topology and performance information for the selection of senders monitoring the status of peers and connections and reacting to peer connection failure or degradation with low overhead dynamically switching active senders and standby senders so that the collective network performance out of the active senders remains satisfactory based on both real world measurement and simulation we evaluate the performance of promise and discuss lessons learned from our experience with respect to the practicality and further optimization of promise
several mesh like coarse grained reconfigurable architectures have been devised in the last few years accompanied with their corresponding mapping flows one of the major bottlenecks in mapping algorithms on these architectures is the limited memory access bandwidth only few mapping methodologies encountered the problem of the limited bandwidth while none has explored how the performance improvements are affected from the architectural characteristics we study in this paper the impact that the architectural parameters have on performance speedups achieved when the pes local rams are used for storing the variables with data reuse opportunities the data reuse values are transferred in the internal interconnection network instead of being fetched from external memories in order to reduce the data transfer burden on the bus network novel mapping algorithm is also proposed that uses list scheduling technique the experimental results quantified the trade offs that exist between the performance improvements and the memory access latency the interconnection network and the processing element's local ram size for this reason our mapping methodology targets on flexible architecture template which permits such an exploration more specifically the experiments showed that the improvements increase with the memory access latency while richer interconnection topology can improve the operation parallelism by factor of on average finally for the considered set of benchmarks the operation parallelism has been improved from to from the application of our methodology and by having each pe's local ram size of words
operational transformation ot is technique originally invented for supporting consistency maintenance in collaborative text editors word processors have much richer data types and more comprehensive operations than plain text editors among others the capability of updating attributes of any types of object is an essential feature of all word processors in this paper we report an extension of ot for supporting generic update operation in addition to insert and delete operations for collaborative word processing we focus on technical issues and solutions involved in transforming updates for both consistency maintenance and group undo novel technique called multi version single display mvsd has been devised to resolve conflict between concurrent updates and integrated into the framework of ot this work has been motivated by and conducted in the coword project which aims to convert ms word into real time collaborative word processor without changing its source code this ot extension is relevant not only to word processors but also to range of interactive applications that can be modelled as editors
we present fully automatic method for content selection evaluation in summarization that does not require the creation of human model summaries our work capitalizes on the assumption that the distribution of words in the input and an informative summary of that input should be similar to each other results on large scale evaluation from the text analysis conference show that input summary comparisons are very effective for the evaluation of content selection our automatic methods rank participating systems similarly to manual model based pyramid evaluation and to manual human judgments of responsiveness the best feature jensen shannon divergence leads to correlation as high as with manual pyramid and with responsiveness evaluations
software comprehension understanding software structure and behavior is essential for developing maintaining and improving software this is particularly true of agent based systems in which the actions of autonomous agents are affected by numerous factors such as events in dynamic environment local uncertain beliefs and intentions of other agents existing comprehension tools are not suited to such large concurrent software and do not leverage concepts of the agent oriented paradigm to aid the user in understanding the software's behavior to address the software comprehension of agent based systems this research proposes method and accompanying tool that automates some of the manual tasks performed by the human user during software comprehension such as explanation generation and knowledge verification
this paper focuses on the realizability problem of framework for modeling and specifying the global behavior of reactive electronic services services in this framework web accessible programs peers communicate by asynchronous message passing and virtual global watcher listens silently to the network the global behavior is characterized by conversation which is the infinite sequence of messages observed by the watcher we show that given b√ºchi automaton specifying the desired set of conversations called conversation protocol it is possible to implement it using set of finite state peers if three realizability conditions are satisfied in particular the synthesized peers will conform to the protocol by generating only those conversations specified by the protocol our results enable top down verification strategy where conversation protocol is specified by realizable b√ºchi automaton the properties of the protocol are verified on the b√ºchi automaton specification the peer implementations are synthesized from the protocol via projection
traditional duplicate elimination techniques are not applicable to many data stream applications in general precisely eliminating duplicates in an unbounded data stream is not feasible in many streaming scenarios therefore we target at approximately eliminating duplicates in streaming environments given limited space based on well known bitmap sketch we introduce data structure stable bloom filter and novel and simple algorithm the basic idea is as follows since there is no way to store the whole history of the stream sbf continuously evicts the stale information so that sbf has room for those more recent elements after finding some properties of sbf analytically we show that tight upper bound of false positive rates is guaranteed in our empirical study we compare sbf to alternative methods the results show that our method is superior in terms of both accuracy and time effciency when fixed small space and an acceptable false positive rate are given
modern techniques for distributed information retrieval use set of documents sampled from each server but these samples have been underutilised in server selection we describe new server selection algorithm sushi which unlike earlier algorithms can make full use of the text of each sampled document and which does not need training data sushi can directly optimise for many common cases including high precision retrieval and by including simple stopping condition can do so while reducing network traffic our experiments compare sushi with alternatives and show it achieves the same effectiveness as the best current methods while being substantially more efficient selecting as few as as many servers
design and control of vector fields is critical for many visualization and graphics tasks such as vector field visualization fluid simulation and texture synthesis the fundamental qualitative structures associated with vector fields are fixed points periodic orbits and separatrices in this paper we provide new technique that allows for the systematic creation and cancellation of fixed points and periodic orbits this technique enables vector field design and editing on the plane and surfaces with desired qualitative properties the technique is based on conley theory which provides unified framework that supports the cancellation of fixed points and periodic orbits we also introduce novel periodic orbit extraction and visualization algorithm that detects for the first time periodic orbits on surfaces furthermore we describe the application of our periodic orbit detection and vector field simplification algorithms to engine simulation data demonstrating the utility of the approach we apply our design system to vector field visualization by creating data sets containing periodic orbits this helps us understand the effectiveness of existing visualization techniques finally we propose new streamline based technique that allows vector field topology to be easily identified
sensor networks are very specific type of wireless networks where both security and performance issues need to be solved efficiently in order to avoid manipulations of the sensed data and at the same time minimize the battery energy consumption this paper proposes an efficient way to perform data collection by grouping the sensors in aggregation zones allowing the aggregators to process the sensed data inside the aggregation zone in order to minimize the amount of transmissions to the sink moreover the paper provides security mechanism based on hash chains to secure data transmissions in networks with low capability sensors and without the requirements of an instantaneous source authentication
grid resources are non dedicated and thus grid users are forced to compete with resource owners for idle cpu cycles as result the turnaround times of both the grid jobs and the owners jobs are invariably delayed to resolve this problem the current study proposes progressive multi layer resource reconfiguration framework designated as pmr in which intra and inter site reconfiguration strategies are employed to adapt grid users jobs dynamically to changes in the available cpu resources at each node the experimental results show that pmr enables the idle cpu cycles of resource to be fully exploited by grid users with minimum interference to the resource owner's jobs
there exist emerging applications of data streams that require association rule mining such as network traffic monitoring and web click streams analysis different from data in traditional static databases data streams typically arrive continuously in high speed with huge amount and changing data distribution this raises new issues that need to be considered when developing association rule mining techniques for stream data this paper discusses those issues and how they are addressed in the existing literature
classification is an important problem in data mining given an example and class classifier usually works by estimating the probability of being member of ie membership probability well calibrated classifiers are those able to provide accurate estimates of class membership probabilities that is the estimated probability is close to which is the true empirical probability of being member of given that the probability estimated by the classifier is calibration is not necessary property for producing accurate classifiers and thus most of the research has focused on direct accuracy maximization strategies ie maximum margin rather than on calibration however non calibrated classifiers are problematic in applications where the reliability associated with prediction must be taken into account ie cost sensitive classification cautious classification etc in these applications sensible use of the classifier must be based on the reliability of its predictions and thus the classifier must be well calibrated in this paper we show that lazy associative classifiers lac are accurate and well calibrated using well known sound entropy minimization method we explore important applications where such characteristics ie accuracy and calibration are relevant and we demonstrate empirically that lac drastically outperforms other classifiers such as svms naive bayes and decision trees even after these classifiers are calibrated by specific methods additional highlights of lac include the ability to incorporate reliable predictions for improving training and the ability to refrain from doubtful predictions
in recent years wireless sensor networking has shown great promise in applications ranging from industrial control environmental monitoring and inventory tracking given the resource constrained nature of sensor devices and the dynamic wireless channel used for communication sensor networking protocol needs to be compact energy efficient and highly adaptable in this paper we present sampl simple aggregation and message passing layer aimed at flexible aggregation of sensor information over long period of time and supporting sporadic messages from mobile devices sampl is compact network layer that operates on top of low power csma ca based mac protocol the protocol has been designed with extensibility in mind to support new transducer devices and unforeseen applications without requiring reprogramming of the entire network sampl uses highly adaptive tree based routing scheme to achieve highly robust operation in time varying environment the protocol supports peer to peer data transactions local storage of data similar to what many rfid systems provide as well as secure gateway to infrastructure communication sampl is built on top of the nano rk operating system that runs on the firefly sensor networking platform nano rk's resource management primitives are used to create virtual energy budgets within sampl that enforce application lifetimes as of october sampl has been operating as part of the sensor andrew project at carnegie mellon university with battery powered sensor nodes for over seven months and continues to be actively used as research testbed we describe our deployment tools and network health monitoring strategies necessary for configuring and maintaining long term operation of sensor network our approach has led to sustainable average packet success rate of across the entire network
to execute mpi applications reliably fault tolerance mechanisms are needed message logging is well known solution to provide fault tolerance for mpi applications it as been proved that it can tolerate higher failure rate than coordinated checkpointing however pessimistic and causal message logging can induce high overhead on failure free execution in this paper we present op new optimistic message logging protocol based on active optimistic message logging contrary to existing optimistic message logging protocols that saves dependency information on reliable storage periodically op logs dependency information as soon as possible to reduce the amount of data piggybacked on application messages thus it reduces the overhead of the protocol on failure free execution making it more scalable and simplifying recovery op is implemented as module of the open mpi library experiments show that active message logging is promising to improve scalability and performance of optimistic message logging
we introduce sensordcsp naturally distributed benchmark based on real world application that arises in the context of networked distributed systems in order to study the performance of distributed csp discsp algorithms in truly distributed setting we use discrete event network simulator which allows us to model the impact of different network traffic conditions on the performance of the algorithms we consider two complete discsp algorithms asynchronous backtracking abt and asynchronous weak commitment search awc and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of sensordcsp we found that random delays due to network traffic or in some cases actively introduced by the agents combined with dynamic decentralized restart strategy can improve the performance of discsp algorithms in addition we introduce gsensordcsp plain embedded version of sensordcsp that is closely related to various real life dynamic tracking systems we perform both analytical and empirical study of this benchmark domain in particular this benchmark allows us to study the attractiveness of solution repairing for solving sequence of discsps that represent the dynamic tracking of set of moving objects
we analyze theoretically the subspace best approximating images of convex lambertian object taken from the same viewpoint but under different distant illumination conditions since the lighting is an arbitrary function the space of all possible images is formally infinite dimensional however previous empirical work has shown that images of largely diffuse objects actually lie very close to five dimensional subspace in this paper we analytically construct the principal component analysis for images of convex lambertian object explicitly taking attached shadows into account and find the principal eigenmodes and eigenvalues with respect to lighting variability our analysis makes use of an analytic formula for the irradiance in terms of spherical harmonic coefficients of the illumination and shows under appropriate assumptions that the principal components or eigenvectors are identical to the spherical harmonic basis functions evaluated at the surface normal vectors our main contribution is in extending these results to the single viewpoint case showing how the principal eigenmodes and eigenvalues are affected when only limited subset the upper hemisphere of normals is available and the spherical harmonics are no longer orthonormal over the restricted domain our results are very close both qualitatively and quantitatively to previous empirical observations and represent the first essentially complete theoretical explanation of these observations our analysis is also likely to be of interest in other areas of computer vision and image based rendering in particular our results indicate that using complex illumination for photometric problems in computer vision is not significantly more difficult than using directional sources
various programming languages allow the construction of structure shy programs such programs are defined generically for many different datatypes and only specify specific behavior for few relevant subtypes typical examples are xml query languages that allow selection of subdocuments without exhaustively specifying intermediate element tags other examples are languages and libraries for polytypic or strategic functional programming and for adaptive object oriented programming in this paper we present an algebraic approach to transformation of declarative structure shy programs in particular for strategic functions and xml queries we formulate rich set of algebraic laws not just for transformation of structure shy programs but also for their conversion into structure sensitive programs and vice versa we show how subsets of these laws can be used to construct effective rewrite systems for specialization generalization and optimization of structure shy programs we present type safe encoding of these rewrite systems in haskell which itself uses strategic functional programming techniques
programs which manipulate pointers are hard to debug pointer analysis algorithms originally aimed at optimizing compilers may provide some remedy by identifying potential errors such as dereferencing null pointers by statically analyzing the behavior of programs on all their input dataour goal is to identify the core program analysis techniques that can be used when developing realistic tools which detect memory errors at compile time without generating too many false alarms our preliminary experience indicates that the following techniques are necessary finding aliases between pointers ii flow sensitive techniques that account for the program control flow constructs iii partial interpretation of conditional statements iv analysis of the relationships between pointers and sometimes analysis of the underlying data structures manipulated by the programwe show that combination of these techniques can yield better results than those achieved by state of the art tools yet it is not clear to us whether our ideas are applicable to large programs
in this paper we introduce new approach for the embedding of linear elastic deformable models our technique results in significant improvements in the efficient physically based simulation of highly detailed objects first our embedding takes into account topological details that is disconnected parts that fall into the same coarse element are simulated independently second we account for the varying material properties by computing stiffness and interpolation functions for coarse elements which accurately approximate the behaviour of the embedded material finally we also take into account empty space in the coarse embeddings which provides better simulation of the boundary the result is straightforward approach to simulating complex deformable models with the ease and speed associated with coarse regular embedding and with quality of detail that would only be possible at much finer resolution
recently the increasing use of time series data has initiated various research and development attempts in the field of data and knowledge management time series data is characterized as large in data size high dimensionality and update continuously moreover the time series data is always considered as whole instead of individual numerical fields indeed large set of time series data is from stock market stock time series has its own characteristics over other time series moreover dimensionality reduction is an essential step before many time series analysis and mining tasks for these reasons research is prompted to augment existing technologies and build new representation to manage financial time series data in this paper financial time series is represented according to the importance of the data points with the concept of data point importance tree data structure which supports incremental updating is proposed to represent the time series and an access method for retrieving the time series data point from the tree which is according to their order of importance is introduced this technique is capable to present the time series in different levels of detail and facilitate multi resolution dimensionality reduction of the time series data in this paper different data point importance evaluation methods new updating method and two dimensionality reduction approaches are proposed and evaluated by series of experiments finally the application of the proposed representation on mobile environment is demonstrated
through analysis and experiments this paper investigates two phase waiting algorithms to minimize the cost of waiting for synchronization in large scale multiprocessors in two phase algorithm thread first waits by polling synchronization variable if the cost of polling reaches limit lpoll and further waiting is necessary the thread is blocked incurring an additional fixed cost the choice of lpoll is critical determinant of the performance of two phase algorithms we focus on methods for statically determining lpoll because the run time overhead of dynamically determining lpoll can be comparable to the cost of blocking in large scale multiprocessor systems with lightweight threads our experiments show that always block lpoll is good waiting algorithm with performance that is usually close to the best of the algorithms compared we show that even better performance can be achieved with static choice of lpoll based on knowledge of likely wait time distributions motivated by the observation that different synchronization types exhibit different wait time distributions we prove that static choice of lpoll can yield close to optimal on line performance against an adversary that is restricted to choosing wait times from fixed family of probability distributions this result allows us to make an optimal static choice of lpoll based on synchronization type for exponentially distributed wait times we prove that setting lpoll results in waiting cost that is no more than times the cost of an optimal off line algorithm for uniformly distributed wait times we prove that setting lpoll square root of results in waiting cost that is no more than square root of the golden ratio times the cost of an optimal off line algorithm experimental measurements of several parallel applications on the alewife multiprocessor simulator corroborate our theoretical findings
code sandboxing is useful for many purposes but most sandboxing techniques require kernel modifications do not completely isolate guest code or incur substantial performance costs vx is multipurpose user level sandbox that enables any application to load and safely execute one or more guest plug ins confining each guest to system call api controlled by the host application and to restricted memory region within the host's address space vx runs guest code efficiently on several widespread operating systems without kernel extensions or special privileges it protects the host program from both reads and writes by its guests and it allows the host to restrict the instruction set available to guests the key to vx combination of portability flexibility and efficiency is its use of segmentation hardware to sandbox the guest's data accesses along with lightweight instruction translator to sandbox guest instructions we evaluate vx using microbenchmarks and whole system benchmarks and we examine four applications based on vx an archival storage system an extensible public key infrastructure an experimental user level operating system running atop another host os and linux system call jail the first three applications export custom apis independent of the host os to their guests making their plug ins binary portable across host systems compute intensive workloads for the first two applications exhibit between slowdown and speedup on vx relative to native execution speedups result from vx instruction translator improving the cache locality of guest code the experimental user level operating system allows the use of the guest os's applications alongside the host's native applications and runs faster than whole system virtual machine monitors such as vmware and qemu the linux system call jail incurs up to overhead but requires no kernel modifications and is delegation based avoiding concurrency vulnerabilities present in other interposition mechanisms
little is known about the strategies end user programmers use in debugging their programs and even less is known about gender differences that may exist in these strategies without this type of information designers of end user programming systems cannot know the target at which to aim if they are to support male and female end user programmers we present study investigating this issue we asked end user programmers to debug spreadsheets and to describe their debugging strategies using mixed methods we analyzed their strategies and looked for relationships among participants strategy choices gender and debugging success our results indicate that males and females debug in quite different ways that opportunities for improving support for end user debugging strategies for both genders are abundant and that tools currently available to end user debuggers may be especially deficient in supporting debugging strategies used by females
visual complexity is an apparent feature in website design yet its effects on cognitive and emotional processing are not well understood the current study examined website complexity within the framework of aesthetic theory and psychophysiological research on cognition and emotion we hypothesized that increasing the complexity of websites would have detrimental cognitive and emotional impact on users in passive viewing task pvt website screenshots differing in their degree of complexity operationalized by jpeg file size correlation with complexity ratings in preliminary study were presented to participants in randomized order additionally standardized visual search task vst assessing reaction times and one week delayed recognition task on these websites were conducted and participants rated all websites for arousal and valence psychophysiological responses were assessed during the pvt and vst visual complexity was related to increased experienced arousal more negative valence appraisal decreased heart rate and increased facial muscle tension musculus corrugator visual complexity resulted in increased reaction times in the vst and decreased recognition rates reaction times in the vst were related to increases in heart rate and electrodermal activity these findings demonstrate that visual complexity of websites has multiple effects on human cognition and emotion including experienced pleasure and arousal facial expression autonomic nervous system activation task performance and memory it should thus be considered an important factor in website design
the vast expansion of interconnectivity with the internet and the rapid evolution of highly capable but largely insecure mobile devices threatens cellular networks in this paper we characterize the impact of the large scale compromise and coordination of mobile phones in attacks against the core of these networks through combination of measurement simulation and analysis we demonstrate the ability of botnet composed of as few as compromised mobile phones to degrade service to area code sized regions by as such attacks are accomplished through the execution of network service requests and not constant stream of phone calls users are unlikely to be aware of their occurrence we then investigate number of significant network bottlenecks their impact on the density of compromised nodes per base station and how they can be avoided we conclude by discussing number of countermeasures that may help to partially mitigate the threats posed by such attacks
many enterprise applications require the use of object oriented middleware and message oriented middleware in combination middleware mediated transacdons have been proposed as transaction model to address reliability of such applications they extend distributed object transactions to include message oriented transactions in this paper we present three message queuing patterns that we have found useful for implementing middleware mediated transactions we discuss and show how the patterns can be applied to support guaranteed compensation in the engineering of transactional enterprise applications
join is fundamental operator in data stream management system dsms it is more efficient to share execution of multiple windowed joins than separate execution of everyone because the former saves part of cost in common windows therefore shared window join is adopted widely in multi queries dsms when all tasks of queries exceed maximum system capacity the overloaded dsms fails to process all of its input data and keep up with the rates of data arrival especially in time critical environment queries should be completed not just timely but within certain deadlines in this paper we address load shedding approach for shared window join over real time data streams load shedding algorithm ls sjrt cw is proposed to handle queries shared window join in overloaded real time system effectively it would reduce load shedding overhead by adjusting sliding window size experiment results show that our algorithm would decrease average deadline miss ratio over some ranges of workloads
as interactive multimedia communications are developing rapidly on the internet they present stringent challenges on end to end ee performance on the other hand however the internet's architecture ipv remains almost the same as it was originally designed for only data transmission purpose and has experienced big hurdle to actualize qos universally this paper designs cooperatively overlay routing service cors aiming to overcome the performance limit inherent in the internet's ip layer routing service the key idea of cors is to efficiently compose number of eligible application layer paths with suitable relays in the overlay network besides the direct ip path cors can transfer data simultaneously through one or more application layer paths to adaptively satisfy the data's application specific requirements on ee performance simulation results indicate the proposed schemes are scalable and effective practical experiments based on prototype implemented on planetlab show that cors is feasible to enhance the transmission reliability and the quality of multimedia communications
sensors have been increasingly used for many ubiquitous computing applications such as asset location monitoring visual surveillance and human motion tracking in such applications it is important to place sensors such that every point of the target area can be sensed by more than one sensor especially many practical applications require coverage for triangulation hull building and etc also in order to extract meaningful information from the data sensed by multiple sensors those sensors need to be placed not too close to each other minimum separation requirement to address the coverage problem with the minimum separation requirement our recent work kim et al proposes two heuristic methods so called overlaying method and tre based method which complement each other depending on the minimum separation requirement for these two methods we also provide mathematical analysis that can clearly guide us when to use the tre based method and when to use the overlaying method and also how many sensors are required to make it self contained in this paper we first revisit the two heuristic methods then as an extension we present an ilp based optimal solution targeting for grid coverage with this ilp based optimal solution we investigate how much close the two heuristic methods are to the optimal solution finally this paper discusses the impacts of the proposed methods on real deployed systems using two example sensor systems to the best of our knowledge this is the first work that systematically addresses the coverage problem with the minimum separation requirement
mobile location aware applications have become quite popular across range of new areas such as pervasive games and mobile edutainment applications however it is only recently that approaches have been presented which combine gaming and education with mobile augmented reality systems however they typically lack close crossmedia integration of the surroundings and often annotate or extend the environment rather than modifying and altering it in this paper we present mobile outdoor mixed reality game for exploring the history of city in the spatial and the temporal dimension we introduce the design and concept of the game and present universal mechanism to define and setup multi modal user interfaces for the game challenges finally we discuss the results of the user tests
surrogate is an object that stands for document and enables navigation to that document hypermedia is often represented with textual surrogates even though studies have shown that image and text surrogates facilitate the formation of mental models and overall understanding surrogates may be formed by breaking document down into set of smaller elements each of which is surrogate candidate while processing these surrogate candidates from an html document relevant information may appear together with less useful junk material such as navigation bars and advertisements this paper develops pattern recognition based approach for eliminating junk while building the set of surrogate candidates the approach defines features on candidate elements and uses classification algorithms to make selection decisions based on these features for the purpose of defining features in surrogate candidates we introduce the document surrogate model dsm streamlined document object model dom like representation of semantic structure using quadratic classifier we were able to eliminate junk surrogate candidates with an average classification rate of by using this technique semiautonomous agents can be developed to more effectively generate surrogate collections for users we end by describing new approach for hypermedia and the semantic web which uses the dsm to define value added surrogates for document
we consider generic garbled circuit gc based techniques for secure function evaluation sfe in the semi honest modelwe describe efficient gc constructions for addition subtraction multiplication and comparison functions our circuits for subtraction and comparison are approximately two times smaller in terms of garbled tables than previous constructions this implies corresponding computation and communication improvements in sfe of functions using our efficient building blocks the techniques rely on recently proposed free xor gc techniquefurther we present concrete and detailed improved gc protocols for the problem of secure integer comparison and related problems of auctions minimum selection and minimal distance performance improvement comes both from building on our efficient basic blocks and several problem specific gc optimizations we provide precise cost evaluation of our constructions which serves as baseline for future protocols
the restricted correspondence problem is the task of solving the classical stereo correspondence problem when the surface being observed is known to belong to family of surfaces that vary in known way with one or more parameters under this constraint the surface can be extracted far more robustly than by classical stereo applied to an arbitrary surface since the problem is solved semi globally rather than locally for each epipolar line here the restricted correspondence problem is solved for two examples the first being the extraction of the parameters of an ellipsoid from calibrated stereo pair the second example is the estimation of the osculating paraboloid at the frontier points of convex object
recently research on text mining has attracted lots of attention from both industrial and academic fields text mining concerns of discovering unknown patterns or knowledge from large text repository the problem is not easy to tackle due to the semi structured or even unstructured nature of those texts under consideration many approaches have been devised for mining various kinds of knowledge from texts one important aspect of text mining is on automatic text categorization which assigns text document to some predefined category if the document falls into the theme of the category traditionally the categories are arranged in hierarchical manner to achieve effective searching and indexing as well as easy comprehension for human beings the determination of category themes and their hierarchical structures were most done by human experts in this work we developed an approach to automatically generate category themes and reveal the hierarchical structure among them we also used the generated structure to categorize text documents the document collection was trained by self organizing map to form two feature maps these maps were then analyzed to obtain the category themes and their structure although the test corpus contains documents written in chinese the proposed approach can be applied to documents written in any language and such documents can be transformed into list of separated terms
unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints ie pairs of instances labeled as belonging to same or different clusters in recent years number of algorithms have been proposed for enhancing clustering quality by employing such supervision such methods use the constraints to either modify the objective function or to learn the distance measure we propose probabilistic model for semi supervised clustering based on hidden markov random fields hmrfs that provides principled framework for incorporating supervision into prototype based clustering the model generalizes previous approach that combines constraints and euclidean distance learning and allows the use of broad range of clustering distortion measures including bregman divergences eg euclidean distance and divergence and directional similarity measures eg cosine similarity we present an algorithm that performs partitional semi supervised clustering of data by minimizing an objective function derived from the posterior energy of the hmrf model experimental results on several text data sets demonstrate the advantages of the proposed framework
evaluative texts on the web have become valuable source of opinions on products services events individuals etc recently many researchers have studied such opinion sources as product reviews forum posts and blogs however existing research has been focused on classification and summarization of opinions using natural language processing and data mining techniques an important issue that has been neglected so far is opinion spam or trustworthiness of online opinions in this paper we study this issue in the context of product reviews which are opinion rich and are widely used by consumers and product manufacturers in the past two years several startup companies also appeared which aggregate opinions from product reviews it is thus high time to study spam in reviews to the best of our knowledge there is still no published study on this topic although web spam and email spam have been investigated extensively we will see that opinion spam is quite different from web spam and email spam and thus requires different detection techniques based on the analysis of million reviews and million reviewers from amazoncom we show that opinion spam in reviews is widespread this paper analyzes such spam activities and presents some novel techniques to detect them
given string and language the hamming distance of to is the minimum hamming distance of to any string in the edit distance of string to language is analogously definedfirst we prove that there is language in ac such that both hamming and edit distance to this language are hard to approximate they cannot be approximated with factor for any unless np denotes the length of the input string second we show the parameterized intractability of computing the hamming distance we prove that for every there exists language in ac for which computing the hamming distance is hard moreover there is language in for which computing the hamming distance is wp hardthen we show that the problems of computing the hamming distance and of computing the edit distance are in some sense equivalent by presenting approximation ratio preserving reductions from the former to the latter and vice versafinally we define hamp to be the class of languages to which the hamming distance can efficiently ie in polynomial time be computed we show some properties of the class hamp on the other hand we give evidence that characterization in terms of automata or formal languages might be difficult
in this paper new color space called the rgb color ratio space is proposed and defined according to reference color such that an image can be transformed from conventional color space to the rgb color ratio space because color in the rgb color ratio space is represented as three color ratios and intensity the chrominance can be completely reserved three color ratios and the luminance can be de correlated with the chrominance different from traditional distance measurement road color model is determined by an ellipse area in the rgb ratio space enclosed by the estimated boundaries proposed adaptive fuzzy logic in which fuzzy membership functions are defined according to estimated boundaries is introduced to implement clustering rules therefore each pixel will have its own fuzzy membership function corresponding to its intensity basic neural network is trained and used to achieve parameters optimization the low computation cost of the proposed segmentation method shows the feasibility for real time application experimental results for road detection demonstrate the robustness to intensity variation of the proposed approach
we show the decidability of model checking pa processes against several first order logics based upon the reachability predicate the main tool for this result is the recognizability by tree automata of the reachability relation the tree automata approach and the transition logics we use allow smooth and general treatment of parameterized model checking for pa this approach is extended to handle quite general notion of costs of pa steps in particular when costs are parikh images of traces we show decidability of transition logic extended by some form of first order reasoning over costs
hierarchical agent framework is proposed to construct monitoring layer towards self aware parallel systems on chip socs with monitoring services as new design dimension systems are capable of observing and reconfiguring themselves dynamically at all levels of granularity based on application requirements and platform conditions agents with hierarchical priorities work adaptively and cooperatively to maintain and improve system performance in the presence of variations and faults function partitioning of agents and hierarchical monitoring operations on parallel socs are analyzed applying the design approach on the network on chip noc platform demonstrates the design process and benefits using the novel approach
proliferation of portable wireless enabled laptop computers and pdas cost effective deployment of access points and availability of the license exempt bands and appropriate networking standards contribute to the conspicuous success of ieee wlans in the article we provide comprehensive overview of techniques for capacity improvement and qos provisioning in the ieee protocol family these techniques represent the efforts both in the research community and the ieee working groups specifically we summarize the operations of ieee legacy as well as its extension introduce several protocol modeling techniques and categorize the various approaches to improve protocol capacity to provide qos by either devising new mac protocol components or fine tuning protocol parameters in ieee and to judiciously arbitrate radio resources eg transmission rate and power to demonstrate how to adapt qos provisioning in newly emerging areas we use the wireless mesh network as an example discuss the role ieee plays in such network and outline research issues that arise
digital audio video data have become an integral part of multimedia information systems to reduce storage and bandwidth requirements they are commonly stored in compressed format such as mpeg increasing amounts of mpeg encoded audio and video documents are available online and in proprietary collections in order to effectively utilise them we need tools and techniques to automatically analyse segment and classify mpeg video content several techniques have been developed both in the audio and visual domain to analyse videos this paper presents survey of audio and visual analysis techniques on mpeg encoded media that are useful in supporting variety of video applications although audio and visual feature analyses have been carried out extensively they become useful to applications only when they convey semantic meaning of the video content therefore we also present survey of works that provide semantic analysis on mpeg encoded videos
bottom sketch is summary of set of items with nonnegative weights each such summary allows us to compute approximate aggregates over the set of items bottom sketches are obtained by associating with each item in ground set an independent random rank drawn from probability distribution that depends on the weight of the item for each subset of interest the bottom sketch is the set of the minimum ranked items and their ranks bottom sketches have numerous applications we develop and analyze data structures and estimators for bottom sketches to facilitate their deployment we develop novel estimators and algorithms that show that they are superior alternative to other sketching methods in both efficiency of obtaining the sketches and the accuracy of the estimates derived from the sketches
novel touch based interaction method by use of orientation information of touch region is proposed to capture higher dimensional information of touch including position and an orientation as well we develop robust algorithms to detect contact shape and to estimate its orientation angle also we suggest practical guidelines to use our method through experiments considering various conditions and show possible service scenarios of aligning documents and controlling media player
this paper offers theoretical study of constraint simplification fundamental issue for the designer of practical type inference system with subtyping in the simpler case where constraints are equations simple isomorphism between constrained type schemes and finite state automata yields complete constraint simplification method using it as guide for the intuition we move on to the case of subtyping and describe several simplification algorithms although no longer complete they are conceptually simple efficient and very effective in practice overall this paper gives concise theoretical account of the techniques found at the core of our type inference system our study is restricted to the case where constraints are interpreted in non structural lattice of regular terms nevertheless we highlight small number of general ideas which explain our algorithms at high level and may be applicable to variety of other systems copyright academic press
personalization of learning has become prominent issue in the educational field at various levels this article elaborates different view on personalisation than what usually occurs in this area its baseline is that personalisation occurs when learning turns out to become personal in the learner's mind through literature survey we analyze constitutive dimensions of this inner sense of personalisation here we devote special attention to confronting learners with tracked information making their personal interaction footprints visible contrasts with the back office usage of this data by researchers instructors or adaptive systems we contribute prototype designed for the moodle platform according to the conceptual approach presented here
the fluid documents project has developed various research prototypes that show that powerful annotation techniques based on animated typographical changes can help readers utilize annotations more effectively our recently developed fluid open hypermedia prototype supports the authoring and browsing of fluid annotations on third party web pages this prototype is an extension of the arakne environment an open hypermedia application that can augment web pages with externally stored hypermedia structures this paper describes how various web standards including dom css xlink xpointer and rdf can be used and extended to support fluid annotations
wireless sensor networks have created new opportunities for data collection in variety of scenarios such as environmental and industrial where we expect data to be temporally and spatially correlated researchers may want to continuously collect all sensor data from the network for later analysis suppression both temporal and spatial provides opportunities for reducing the energy cost of sensor data collection we demonstrate how both types can be combined for maximal benefit we frame the problem as one of monitoring node and edge constraints monitored node triggers report if its value changes monitored edge triggers report if the difference between its nodes values changes the set of reports collected at the base station is used to derive all node values we fully exploit the potential of this global inference in our algorithm conch short for constraint chaining constraint chaining builds network of constraints that are maintained locally but allow global view of values to be maintained with minimal cost network failure complicates the use of suppression since either causes an absence of reports we add enhancements to conch to build in redundant constraints and provide method to interpret the resulting reports in case of uncertainty using simulation we experimentally evaluate conch's effectiveness against competing schemes in number of interesting scenarios
abstract researchers have recently discovered several interesting self organized regularities from the world wide web ranging from the structure and growth of the web to the access patterns in web surfing what remains to be great challenge in web log mining is how to explain user behavior underlying observed web usage regularities in this paper we will address the issue of how to characterize the strong regularities in web surfing in terms of user navigation strategies and present an information foraging agent based approach to describing user behavior by experimenting with the agent based decision models of web surfing we aim to explain how some web design factors as well as user cognitive factors may affect the overall behavioral patterns in web usage
most video retrieval systems are multimodal commonly relying on textual information low and high level semantic features extracted from query visual examples in this work we study the impact of exploiting different knowledge sources in order to automatically retrieve query visual examples relevant to video retrieval task our hypothesis is that the exploitation of external knowledge sources can help on the identification of query semantics as well as on improving the understanding of video contents we propose set of techniques to automatically obtain additional query visual examples from different external knowledge sources such as dbpedia flickr and google images which have different coverage and structure characteristics the proposed strategies attempt to exploit the semantics underlying the above knowledge sources to reduce the ambiguity of the query and to focus the scope of the image searches in the repositories we assess and compare the quality of the images obtained from the different external knowledge sources when used as input of number of video retrieval tasks we also study how much they complement manually provided sets of examples such as those given by trecvid tasks based on our experimental results we report which external knowledge source is more likely to be suitable for the evaluated retrieval tasks results also demonstrate that the use of external knowledge can be good complement to manually provided examples and when lacking of visual examples provided by user our proposed approaches can retrieve visual examples to improve the user's query
we introduce light weight scalable truthful routing protocol lstop for selfish nodes problem in mobile ad hoc networks where node may use different cost to send packets to different neighbours lstop encourages nodes cooperation by rewarding nodes for their forwarding service according to their cost it incurs low overhead of in the worst case and only on the average we show the truthfulness of lstop and present the result of an extensive simulation study to show that lstop approaches optimal cost routing and achieves significant better network performance compared to ad hoc vcg
dynamic binary translators dbts provide powerful platforms for building dynamic program monitoring and adaptation tools dbts however have high memory demands because they cache translated code and auxiliary code to software code cache and must also maintain data structures to support the code cache the high memory demands make it difficult for memory constrained embedded systems to take advantage of dbt based tools previous research on dbt memory management focused on the translated code and auxiliary code only however we found that data structures are comparable to the code cache in size we show that the translated code size auxiliary code size and the data structure size interact in complex manner depending on the path selection trace selection and link formation strategy therefore holistic memory efficiency comprising translated code auxiliary code and data structures cannot be improved by focusing on the code cache only in this paper we use path selection for improving holistic memory efficiency which in turn impacts performance in memory constrained environments although there has been previous research on path selection such research only considered performance in memory unconstrained environments the challenge for holistic memory efficiency is that the path selection strategy results in complex interactions between the memory demand components also individual aspects of path selection and the holistic memory efficiency may impact performance in complex ways we explore these interactions to motivate path selection targeting holistic memory demand we enumerate all the aspects involved in path selection design and evaluate comprehensive set of approaches for each aspect finally we propose path selection strategy that reduces memory demands by and at the same time improves performance by compared to an industrial strength dbt
effective identification of coexpressed genes and coherent patterns in gene expression data is an important task in bioinformatics research and biomedical applications several clustering methods have recently been proposed to identify coexpressed genes that share similar coherent patterns however there is no objective standard for groups of coexpressed genes the interpretation of co expression heavily depends on domain knowledge furthermore groups of coexpressed genes in gene expression data are often highly connected through large number of intermediate genes there may be no clear boundaries to separate clusters clustering gene expression data also faces the challenges of satisfying biological domain requirements and addressing the high connectivity of the data sets in this paper we propose an interactive framework for exploring coherent patterns in gene expression data novel coherent pattern index is proposed to give users highly confident indications of the existence of coherent patterns to derive coherent pattern index and facilitate clustering we devise an attraction tree structure that summarizes the coherence information among genes in the data set we present efficient and scalable algorithms for constructing attraction trees and coherent pattern indices from gene expression data sets our experimental results show that our approach is effective in mining gene expression data and is scalable for mining large data sets
we address specific enterprise document search scenario where the information need is expressed in an elaborate manner in our scenario information needs are expressed using short query of few keywords together with examples of key reference pages given this setup we investigate how the examples can be utilized to improve the end to end performance on the document retrieval task our approach is based on language modeling framework where the query model is modified to resemble the example pages we compare several methods for sampling expansion terms from the example pages to support query dependent and query independent query expansion the latter is motivated by the wish to increase aspect recall and attempts to uncover aspects of the information need not captured by the query for evaluation purposes we use the csiro data set created for the trec enterprise track the best performance is achieved by query models based on query independent sampling of expansion terms from the example documents
the view update problem is concerned with indirectly modifying those tuples that satisfy view or derived table by an appropriate update against the corresponding base tables the notion of deduction tree is defined and the relationship between such trees and the view update problem for indefinite deductive databases is considered it is shown that traversal of an appropriate deduction tree yields sufficient information to perform view updates at the propositional level to obtain similar result at the first order level it is necessary for theoretical and computational reasons to impose some weak stratification and definiteness constraints on the database
this paper describes new method for contouring signed grid whose edges are tagged by hermite data ie exact intersection points and normals this method avoids the need to explicitly identify and process features as required in previous hermite contouring methods using new numerically stable representation for quadratic error functions we develop an octree based method for simplifying contours produced by this method we next extend our contouring method to these simpli pound ed octrees this new method imposes no constraints on the octree such as being restricted octree and requires no crack patching we conclude with simple test for preserving the topology of the contour during simplification
benchmarking is critical when evaluating performance but is especially difficult for file and storage systems complex interactions between devices caches kernel daemons and other os components result in behavior that is rather difficult to analyze moreover systems have different features and optimizations so no single benchmark is always suitable the large variety of workloads that these systems experience in the real world also adds to this difficulty in this article we survey file system and storage benchmarks from recent papers we found that most popular benchmarks are flawed and many research papers do not provide clear indication of true performance we provide guidelines that we hope will improve future performance evaluations to show how some widely used benchmarks can conceal or overemphasize overheads we conducted set of experiments as specific example slowing down read operations on ext by factor of resulted in only percnt wall clock slowdown in popular compile benchmark finally we discuss future work to improve file system and storage benchmarking
vector and matrix clocks are extensively used in asynchronous distributed systems this paper asks how does the clock abstraction generalize to address this problem the paper motivates and proposes logical clocks of arbitrary dimensions it then identifies and explores the conceptual link between such clocks and knowledge it establishes the necessary and sufficient conditions on the size and dimension of clocks required to attain any specified level of knowledge about the timestamp of the most recent system state for which this is possible without using any messages in the clock protocol the paper then gives algorithms to determine the time stamp of the latest system state about which specified level of knowledge is attainable in given system state and to compute the timestamp of the earliest system state in which specified level of knowledge about given system state is attainable the results are applicable to applications that deal with certain class of properties identified as monotonic properties
check if we can apply woodruff's method to our protocol we show an efficient secure two party protocol based on yao's construction which provides security against malicious adversaries yao's original protocol is only secure in the presence of semi honest adversaries security against malicious adversaries can be obtained by applying the compiler of goldreich micali and wigderson the gmw compiler however this approach does not seem to be very practical as it requires using generic zero knowledge proofsour construction is based on applying cut and choose techniques to the original circuit and inputs security is proved according to the ideal real simulation paradigm and the proof is in the standard model with no random oracle model or common reference string assumptions the resulting protocol is computationally efficient the only usage of asymmetric cryptography is for running oblivious transfers for each input bit or for each bit of statistical security parameter whichever is larger our protocol combines techniques from folklore like cut and choose along with new techniques for efficiently proving consistency of inputs we remark that naive implementation of the cut and choose technique with yao's protocol does not yield secure protocol this is the first paper to show how to properly implement these techniques and to provide full proof of securityour protocol can also be interpreted as constant round black box reduction of secure two party computation to oblivious transfer and perfectly hiding commitments or black box reduction of secure two party computation to oblivious transfer alone with number of rounds which is linear in statistical security parameter these two reductions are comparable to kilian's reduction which uses ot alone but incurs number of rounds which is linear in the depth of the circuit
reuse signature or reuse distance pattern is an accurate model for program memory accessing behaviors it has been studied and shown to be effective in program analysis and optimizations by many recent works however the high overhead associated with reuse distance measurement restricts the scope of its application this paper explores applying sampling in reuse signature collection to reduce the time overhead we compare different sampling strategies and show that an enhanced systematic sampling with uniform coverage of all distance ranges can be used to extrapolate the reuse distance distribution based on that analysis we present novel sampling method with measurement accuracy of more than our average speedup of reuse signature collection is while the best improvement observed is this is the first attempt to utilize sampling in measuring reuse signatures experiments with varied programs and instrumentation tools show that sampling has great potential in promoting the practical uses of reuse signatures and enabling more optimization opportunities
embedded system designers face unique set of challenges in making their systems more secure as these systems often have stringent resource constraints or must operate in harsh or physically insecure environments one of the security issues that have recently drawn attention is software integrity which ensures that the programs in the system have not been changed either by an accident or an attack in this paper we propose an efficient hardware mechanism for runtime verification of software integrity using encrypted instruction block signatures we introduce several variations of the basic mechanism and give details of three techniques that are most suitable for embedded systems performance evaluation using selected mibench mediabench and basicrypt benchmarks indicates that the considered techniques impose relatively small performance overhead the best overall technique has performance overhead in the range when protecting byte instruction blocks with byte signatures with byte instruction blocks the overhead is in the range the average overhead with kb cache is with additional investment in signature cache this overhead can be almost completely eliminated
clustered microarchitectures are an effective organization to deal with the problem of wire delays and complexity by partitioning some of the processor resources the organization of the data cache is key factor in these processors due to its effect on cache miss rate and inter cluster communications this paper investigates alternative designs of the data cache centralized distributed replicated and physically distributed cache architectures are analyzed results show similar average performance but significant performance variations depending on the application features specially cache miss ratio and communications in addition we also propose novel instruction steering scheme in order to reduce communications this scheme conditionally stalls the dispatch of instructions depending on the occupancy of the clusters whenever the current instruction cannot be steered to the cluster holding most of the inputs this new steering outperforms traditional schemes results show an average speedup of and up to for some applications
age specific human computer interaction ashci has vast potential applications in daily life however automatic age estimation technique is still underdeveloped one of the main reasons is that the aging effects on human faces present several unique characteristics which make age estimation challenging task that requires non standard classification approaches according to the speciality of the facial aging effects this paper proposes the ages aging pattern subspace method for automatic age estimation the basic idea is to model the aging pattern which is defined as sequence of personal aging face images by learning representative subspace the proper aging pattern for an unseen face image is then determined by the projection in the subspace that can best reconstruct the face image while the position of the face image in that aging pattern will indicate its age the ages method has shown encouraging performance in the comparative experiments either as an age estimator or as an age range estimator
this paper presents an empirical study that evaluates oo method function points oomfp functional size measurement procedure for object oriented systems that are specified using the oo method approach laboratory experiment with students was conducted to compare oomfp with the ifpug function point analysis fpa procedure on range of variables including efficiency reproducibility accuracy perceived ease of use perceived usefulness and intention to use the results show that oomfp is more time consuming than fpa but the measurement results are more reproducible and accurate the results also indicate that oomfp is perceived to be more useful and more likely to be adopted in practice than fpa in the context of oo method systems development we also report lessons learned and suggest improvements to the experimental procedure employed and replications of this study using samples of industry practitioners
as an approach that applies not only to support user navigation on the web recommender systems have been built to assist and augment the natural social process of asking for recommendations from other people in typical recommender system people provide suggestions as inputs which the system aggregates and directs to appropriate recipients in some cases the primary computation is in the aggregation in others the value of the system lies in its ability to make good matches between the recommenders and those seeking recommendationsin this paper we discuss the architectural and design features of webmemex system that provides recommended information based on the captured history of navigation from list of people well known to the users including the users themselves allows users to have access from any networked machine demands user authentication to access the repository of recommendations and allows users to specify when the capture of their history should be performed
as the world uses more digital video that requires greater storage space grid computing is becoming indispensable for urgent problems in multimedia content analysis parallel horus support tool for applications in multimedia grid computing lets users implement multimedia applications as sequential programs for efficient execution on clusters and grids based on wide area multimedia services
traditionally software pipelining is applied either to theinnermost loop of given loop nest or from the innermostloop to outer loops in this paper we propose three stepapproach called single dimension software pipelining ssp to software pipeline loop nest at an arbitraryloop levelthe first step identifies the most profitable loop level forsoftware pipelining in terms of initiation rate or data reusepotential the second step simplifies the multi dimensionaldata dependence graph ddg into dimensional ddgand constructs dimensional schedule for the selectedloop level the third step derives simple mapping functionwhich specifies the schedule time for the operations of themulti dimensional loop based on the dimensional schedulewe prove that the ssp method is correct and at least asefficient as other modulo scheduling methodswe establish the feasibility and correctness of our approachby implementing it on the ia architecture experimentalresults on small number of loops show significantperformance improvements over existing modulo schedulingmethods that software pipeline loop nest from the innermostloop
the problem of performing tasks on asynchronous or undependable processors is basic problem in distributed computing this paper considers an abstraction of this problem called write all using processors write into all locations of an array of size in this problem writing abstracts the notion of performing simple task despite substantial research there is dearth of efficient deterministic asynchronous algorithms for write all efficiency of algorithms is measured in terms of work that accounts for all local steps performed by the processors in solving the problem thus an optimal algorithm would have work however it is known that optimality cannot be achieved when the quest then is to obtain work optimal solutions for this problem using non trivial compared to number of processors recently it was shown that optimality can be achieved using non trivial number of processors where log the new result in this paper significantly extends the range of processors for which optimality is achieved the result shows that optimality can be achieved using close to processors more precisely using log processors for any additionally the new result uses only the atomic read write memory without resorting to using the test and set primitive that was necessary in the previous solution this paper presents the algorithm and gives its analysis showing that the work complexity of the algorithm is which is optimal when while all prior deterministic algorithms require super linear work when
given sequence of symbols over some alphabet sigma of size sigma we develop new compression methods that are very simple to implement ii provide time random access to any symbol or short substring of the original sequence our simplest solution uses at most bits of space where and is the zeroth order empirical entropy of we discuss number of improvements and trade offs over the basic method for example we can achieve bits of space for log sigma several applications are discussed including text compression compressed full text indexing and string matching
most search engines display some document metadata such as title snippet and url in conjunction with the returned hits to aid users in determining documents however metadata is usually fragmented pieces of information that even when combined does not provide an overview of returned document in this paper we propose mechanism of enriching metadata of the returned results by incorporating automatically extracted document keyphrases with each returned hit we hypothesize that keyphrases of document can better represent the major theme in that document therefore by examining the keyphrases in each returned hit users can better predict the content of documents and the time spent on downloading and examining the irrelevant documents will be reduced substantially
when the mobile environment consists of light weight devices the energy consumption of location based services lbss and the limited bandwidth of the wireless network become important issues motivated by this we propose new spatial query processing algorithms to support mobile continuous nearest neighbor query mcnnq in wireless broadcast environments our solution provides general client server architecture for answering mcnnq on objects with unknown and possibly variable movement types our solution enables the application of spatio temporal access methods specifically designed for particular type to arbitrary movements without any false misses our algorithm does not require any conventional spatial index for mcnnq processing it can be adapted to static or moving objects and does not require additional knowledge eg direction of moving objects beyond the maximum speed and the location of each object extensive experiments demonstrate that our location based data dissemination algorithm significantly outperforms index based solutions
creating maintaining or using digital library requires the manipulation of digital documents information workspaces provide visual representation allowing users to collect organize annotate and author information the visual knowledge builder vkb helps users access collect annotate and combine materials from digital libraries and other sources into personal information workspace vkb has been enhanced to include direct search interfaces for nsdl and google users create visualization of search results while selecting and organizing materials for their current activity additionally metadata applicators have been added to vkb this interface allows the rapid addition of metadata to documents and aids the user in the extraction of existing metadata for application to other documents study was performed to compare the selection and organization of documents in vkb to the commonly used tools ofa web browser and word processor this study shows the value of visual workspaces for such effort but points to the need for subdocument level objects ephemeral visualizations and support for moving from visual representations to metadata
growing attention is being paid to application security at requirements engineering time confidentiality is particular subclass of security concerns that requires sensitive information to never be disclosed to unauthorized agents disclosure refers to undesired knowledge states of such agents in previous work we have extended our requirements specification framework with epistemic constructs for capturing what agents may or may not know about the application roughly an agent knows some property if the latter is found in the agent's memorythis paper makes the semantics of such constructs further precise through formal model of how sensitive information may appear or disappear in an agent's memory based on this extended framework catalog of specification patterns is proposed to codify families of confidentiality requirements proof of concept tool is presented for early checking of requirements models against such confidentiality patterns in case of violation the counterexample scenarios generated by the tool show how an unauthorized agent may acquire confidential knowledge counter measures should then be devised to produce further confidentiality requirements
constructing multipath routes in manets is important for providing reliable delivery load balancing and bandwidth aggregation however popular multipath routing approaches fail to produce spatially disjoint routes in simple and cost effective manner and existing single path approaches cannot be easily modified to produce multiple disjoint routes in this paper we propose electric field based routing efr as reliable framework for routing in manets by applying the concept of electric field lines our location based protocol naturally provides spatially disjoint routes based on the shapes of these lines the computation is highly localized and requires no explicit coordination among routes efr can also be easily extended to offer load balancing bandwidth aggregation and power management through simulation efr shows higher delivery ratio and lower overhead under high mobility high network loads and network failures compared to popular multipath and location based schemes efr also demonstrates high resiliency to dos attacks
simulation is widely used for developing evaluating and analyzing sensor network applications especially when deploying large scale sensor network remains expensive and labor intensive however due to its computation intensive nature existent simulation tools have to make trade offs between fidelity and scalability and thus offer limited capabilities as design and analysis tools in this paper we introduce disens distributed sensor network simulation highly scalable distributed simulation system for sensor networks disens does not only faithfully emulates an extensive set of sensor hardware and supports extensible radio power models so that sensor network applications can be simulated transparently with high fidelity but also employs distributed memory parallel cluster system to attack the complex simulation problem combining an efficient distributed synchronization protocol and sophisticated node partitioning algorithm based on existent research disens achieves greater scalability than even many discrete event simulators on small to medium size cluster nodes disens is able to simulate hundreds of motes in realtime speed and scale to thousands in sub realtime speed to our knowledge disens is the first full system sensor network simulator with such scalability
this paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment and specifically contributes to the research environment by developing comprehensive framework for classification of these research endeavors the framework presented classifies security literature into system solutions behavioral solutions hybrid solutions and related technologies each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security the types of grid and security situations they apply best to and the pros and cons for each type of solution further several areas of research were identified in the course of the literature survey where more study is warranted these avenues for future research are also discussed in this paper several types of grid systems exist currently and the security needs and solutions to address those needs for each type vary as much as the types of systems themselves this research framework will aid in future research efforts to define analyze and address grid security problems for the many varied types of grid setups as well as the many security situations that each grid may face
in this paper we extend the scope of mining association rules from traditional single dimensional intratransaction associations to multidimensional intertransaction associations intratransaction associations are the associations among items with the same transaction where the notion of the transaction could be the items bought by the same customer the events happened on the same day and so on however an intertransaction association describes the association relationships among different transactions such as ldquo if company a's stock goes up on day b's stock will go down on day but go up on day rdquo in this case whether we treat company or day as the unit of transaction the associated items belong to different transactions moreover such an intertransaction association can be extended to associate multiple contextual properties in the same rule so that multidimensional intertransaction associations can be defined and discovered two dimensional intertransaction association rule example is ldquo after mcdonald and burger king open branches kfc will open branch two months later and one mile away rdquo which involves two dimensions time and space mining intertransaction associations poses more challenges on efficient processing than mining intratransaction associations interestingly intratransaction association can be treated as special case of intertransaction association from both conceptual and algorithmic point of view in this study we introduce the notion of multidimensional intertransaction association rules study their measurements mdash support and confidence mdash and develop algorithms for mining intertransaction associations by extension of apriori we overview our experience using the algorithms on both real life and synthetic data sets further extensions of multidimensional intertransaction association rules and potential applications are also discussed
modern data mining settings involve combination of attribute valued descriptors over entities as well as specified relationships between these entities we present an approach to cluster such non homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints unlike prior work that views constraints as boolean criteria we present formulation that allows constraints to be satisfied or violated in smooth manner this enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function we present results on both synthetic data as well as several real world datasets
one of the main challenges faced by content based publish subscribe systems is handling large amount of dynamic subscriptions and publications in multidimensional content space to reduce subscription forwarding load and speed up content matching subscription covering subsumption and merging techniques have been proposed in this paper we propose mics multidimensional indexing for content space that provides an efficient representation and processing model for large number of subscriptions and publications mics creates one dimensional representation for publications and subscriptions using hilbert space filling curve based on this representation we propose novel content matching and subscription management covering subsumption and merging algorithms our experimental evaluation indicates that the proposed approach significantly speeds up subscription management operations compared to the naive linear approach
an overall sensornet architecture would help tame the increasingly complex structure of wireless sensornet software and help foster greater interoperability between different codebases previous step in this direction is the sensornet protocol sp unifying link abstraction layer this paper takes the natural next step by proposing modular network layer for sensornets that sits atop sp this modularity eases implementation of new protocols by increasing code reuse and enables co existing protocols to share and reduce code and resources consumed at run time we demonstrate how current protocols can be decomposed into this modular structure and show that the costs in performance and code footprint are minimal relative to their monolithic counterparts
die stacking is an exciting new technology that increases transistor density by vertically integrating two or more die with dense high speed interface the result of die stacking is significant reduction of interconnect both within die and across dies in system for instance blocks within microprocessor can be placed vertically on multiple die to reduce block to block wire distance latency and power disparate si technologies can also be combined in die stack such as dram stacked on cpu resulting in lower power higher bw and lower latency interfaces without concern for technology integration into single process flow has the potential to change processor design constraints by providing substantial power and performance benefits despite the promising advantages of there is significant concern for thermal impact in this research we study the performance advantages and thermal challenges of two forms of die stacking stacking large dram or sram cache on microprocessor and dividing traditional microarchitecture between two die in stack results it is shown that mb stacked dram cache can reduce the cycles per memory access of twothreaded rms benchmark on average by and as much as while increasing the peak temperature by negligible ¬∫c off die bw and power are also reduced by on average it is also shown that floorplan of high performance microprocessor can simultaneously reduce power and increase performance with small ¬∫c increase in peak temperature voltage scaling can reach neutral thermals with simultaneous power reduction and performance improvement
as gml is becoming the de facto standard for geographic data storage transmission and exchange more and more geographic data exists in gml format in applications gml documents are usually very large in size because they contain large number of verbose markup tags and large amount of spatial coordinate data in order to speedup data transmission and reduce network cost it is essential to develop effective and efficient gml compression tools although gml is special case of xml current xml compressors are not effective if directly applied to gml because these compressors have been designed for general xml data in this paper we propose gpress compressor for effectively compressing gml documents to the best of our knowledge gpress is the first compressor specifically for gml documents compression gpress exploits the unique characteristics of gml documents to achieve good performance extensive experiments over real world gml documents show that gpress evidently outperforms xmill one of the best existing xml compressors in compression ratio while its compression efficiency is comparable to the existing xml compressors
aggregation and cube are important operations for online analytical processing olap many efficient algorithms to compute aggregation and cube for relational olap have been developed some work has been done on efficiently computing cube for multidimensional data warehouses that store data sets in multidimensional arrays rather than in tables however to our knowledge there is nothing to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional olap this paper presents set of aggregation algorithms on compressed data warehouses for multidimensional olap these algorithms operate directly on compressed data sets which are compressed by the mapping complete compression methods without the need to first decompress them the algorithms have different performance behaviors as function of the data set parameters sizes of outputs and main memory availability the algorithms are described and the and cpu cost functions are presented in this paper decision procedure to select the most efficient algorithm for given aggregation request is also proposed the analysis and experimental results show that the algorithms have better performance on sparse data than the previous aggregation algorithms
emerging grid computing infrastructures such as cloud computing can only become viable alternatives for the enterprise if they can provide stable service levels for business processes and sla based costing in this paper we describe and apply three step approach to map sla and qos requirements of business processes to such infrastructures we start with formalization of service capabilities and business process requirements we compare them and if we detect performance or reliability gap we dynamically improve performance of individual services deployed in grid and cloud computing environments here we employ translucent replication of services an experimental evaluation in amazon ec verified our approach
in this paper we present new end to end protocol namely scalable streaming video protocol ssvp which operates on top of udp and is optimized for unicast video streaming applications ssvp employs additive increase multiplicative decrease aimd based congestion control and adapts the sending rate by properly adjusting the inter packet gap ipg the smoothness oriented modulation of aimd parameters and ipg adjustments reduce the magnitude of aimd oscillation and allow for smooth transmission patterns while tcp friendliness is maintained our experimental results demonstrate that ssvp eventually adapts to the vagaries of the network and achieves remarkable performance on real time video delivery in the event where awkward network conditions impair the perceptual video quality we investigate the potential improvement via layered adaptation mechanism that utilizes receiver buffering and adapts video quality along with long term variations in the available bandwidth the adaptation mechanism sends new layer based on explicit criteria that consider both the available bandwidth and the amount of buffering at the receiver preventing wasteful layer changes that have an adverse effect on user perceived quality quantifying the interactions of ssvp with the specific adaptation scheme we identify notable gains in terms of video delivery especially in the presence of limited bandwidth
this work examines how awareness systems class of technologies that support sustained and effortless communication between individuals and groups can support family communication going beyond the evaluation of specific design concepts this paper reports on three studies that aimed to answer the following research questions do families want to be aware of each other through the day or would they perhaps rather not know more about each other's activities and whereabouts than they already do if they do wish to have some awareness what should they be aware of the research involved in depth interviews with participants field trial of an awareness system connecting five busy parents with their children and survey of participants conducted over the web triangulation of the results of the three studies leads to the following conclusions some busy parents want to automatically exchange awareness information during the day while others do not availability of partner for coordinating family activities daily activities in new family situations activity and location information of dependent children are salient awareness information needs for this group awareness information needs to vary with contexts suggesting the need for flexible mechanisms to manage the sharing of such information
we present novel framework based on continuous fluid simulator for general simulation of realistic bubbles with which we can handle as many significant dynamic bubble effects as possible to capture very thin liquid film of bubbles we have developed regional level set method allowing multi manifold interface tracking based on the definitions of regional distance and its five operators the implementation of the regional level set method is very easy an implicit surface of liquid film with arbitrary thickness can be reconstructed from the regional level set function to overcome the numerical instability problem we exploit new semi implicit surface tension model which is unconditionally stable and makes the simulation of surface tension dominated phenomena much more efficient an approximated film thickness evolution model is proposed to control the bubble's lifecycle all these new techniques combine into general framework that can produce various realistic dynamic effects of bubbles
shortest path queries spq are essential in many graph analysis and mining tasks however answering shortest path queries on the fly on large graphs is costly to online answer shortest path queries we may materialize and index shortest paths however straightforward index of all shortest paths in graph of vertices takes space in this paper we tackle the problem of indexing shortest paths and online answering shortest path queries as many large real graphs are shown richly symmetric the central idea of our approach is to use graph symmetry to reduce the index size while retaining the correctness and the efficiency of shortest path query answering technically we develop framework to index large graph at the orbit level instead of the vertex level so that the number of breadth first search trees materialized is reduced from to delta where delta le is the number of orbits in the graph we explore orbit adjacency and local symmetry to obtain compact breadth first search trees compact bfs trees an extensive empirical study using both synthetic data and real data shows that compact bfs trees can be built efficiently and the space cost can be reduced substantially moreover online shortest path query answering can be achieved using compact bfs trees
many large organizations have multiple large databases as they transact from multiple branches most of the previous pieces of work are based on single database thus it is necessary to study data mining on multiple databases in this paper we propose two measures of similarity between pair of databases also we propose an algorithm for clustering set of databases efficiency of the clustering process has been improved using the following strategies reducing execution time of clustering algorithm using more appropriate similarity measure and storing frequent itemsets space efficiently
knowing that two numerical variables always hold different values at some point of program can be very useful especially for analyzing aliases if then and are not aliased and this knowledge is of great help for many other program analyses surprisingly disequalities are seldom considered in abstract interpretation most of the proposed numerical domains being restricted to convex sets in this paper we propose to combine simple ordering properties with disequalities difference bound matrices or dbms is domain proposed by david dill for expressing relations of the form or we define ddbms disequalities dbms as conjunctions of dbms with simple disequalities of the form or we give algorithms on ddbms for deciding the emptiness computing normal form and performing the usual operations of an abstract domain these algorithms have the same complexity where is the number of variables than those for classical dbms if the variables are considered to be valued in dense set or in the arithmetic case the emptiness decision is np complete and other operations run in
as organizations implement information strategies that call for sharing access to resources in the networked environment mechanisms must be provided to protect the resources from adversaries the proposed delegation framework addresses the issue of how to advocate selective information sharing in role based systems while minimizing the risks of unauthorized access we introduce systematic approach to specify delegation and revocation policies using set of rules we demonstrate the feasibility of our framework through policy specification enforcement and proof of concept implementation on specific domains eg the healthcare environment we believe that our work can be applied to organizations that rely heavily on collaborative tasks
as new communications media foster international collaborations we would be remiss in overlooking cultural differences when assessing them in this study pairs in three cultural groupings american american aa chinese chinese cc and american chinese ac worked on two decision making tasks one face to face and the other via im drawing upon prior research we predicted differences in conversational efficiency conversational content interaction quality persuasion and performance the quantitative results combined with conversation analysis suggest that the groups viewed the task differently aa pairs as an exercise in situation specific compromise cc as consensus reaching cultural differences were reduced but not eliminated in the im condition
automatic annotation of medical images is an increasingly important tool for physicians in their daily activity hospitals nowadays produce an increasing amount of data manual annotation is very costly and prone to human mistakes this paper proposes multi cue approach to automatic medical image annotation we represent images using global and local features these cues are then combined using three alternative approaches all based on the support vector machine algorithm we tested our methods on the irma database and with two of the three approaches proposed here we participated in the imageclefmed benchmark evaluation in the medical image annotation track these algorithms ranked first and fifth respectively among all submission experiments using the third approach also confirm the power of cue integration for this task
we evaluate various heuristics for hierarchical spectral clustering in large telephone call and web graphs spectral clustering without additional heuristics often produces very uneven cluster sizes or low quality clusters that may consist of several disconnected components fact that appears to be common for several data sources but to our knowledge no general solution provided so far divide and merge recently described postfiltering procedure may be used to eliminate bad quality branches in binary tree hierarchy we propose an alternate solution that enables way cuts in each step by immediately filtering unbalanced or low quality clusters before splitting them furtherour experiments are performed on graphs with various weight and normalization built based on call detail records and web crawls we measure clustering quality both by modularity as well as by the geographic and topical homogeneity of the clusters compared to divide and merge we give more homogeneous clusters with more desirable distribution of the cluster sizes
enforcing strong replica consistency among set of replicas of service deployed across an asynchronous distributed system in the presence of crash failures is real practical challenge if each replica runs the consistency protocol bundled with the actual service implementation this target cannot be achieved as replicas need to be located over partially synchronous distributed system to solve the distributed agreement problems underlying strong replica consistencya three tier architecture for software replication enables the separation of the replication logic ie protocols and mechanisms necessary for managing software replication from both clients and server replicas the replication logic is embedded in middle tier that confines the need of partial synchrony and thus frees replica deploymentin this paper we first introduce the basic concepts underlying three tier replication then we present the interoperable replication logic irl architecture fault tolerant corba compliant infrastructure irl exploits three tier approach to replicate stateful deterministic corba objects and allows object replicas to run on object request brokers from different vendors description of an irl prototype developed in our department is proposed along with an extensive performance analysis
this paper presents cognitive vision system capable of autonomously learning protocols from perceptual observations of dynamic scenes the work is motivated by the aim of creating synthetic agent that can observe scene containing interactions between unknown objects and agents and learn models of these sufficient to act in accordance with the implicit protocols present in the scene discrete concepts utterances and object properties and temporal protocols involving these concepts are learned in an unsupervised manner from continuous sensor input alone crucial to this learning process are methods for spatio temporal attention applied to the audio and visual sensor data these identify subsets of the sensor data relating to discrete concepts clustering within continuous feature spaces is used to learn object property and utterance models from processed sensor data forming symbolic description the progol inductive logic programming system is subsequently used to learn symbolic models of the temporal protocols presented in the presence of noise and over representation in the symbolic data input to it the models learned are used to drive synthetic agent that can interact with the world in semi natural way the system has been evaluated in the domain of table top game playing and has been shown to be successful at learning protocol behaviours in such real world audio visual environments
the modeling analysis and design of systems is generally based on many formalisms to describe discrete and or continuous behaviors and to map these descriptions into specific platform in this context the article proposes the concept of functional metamodeling to capture then to integrate modeling languages the concept offers an alternative to standard model driven engineering mde and is well adapted to mathematical descriptions such as the ones found in system modeling as an application set of functional metamodels is proposed for dataflows usable to model continuous behaviors state transition systems usable to model discrete behaviors and metamodel for actions to model interactions with target platform and concurrent execution model of control architecture for legged robot is proposed as an application of these modeling languages
we propose simple obstacle model to be used while simulating wireless sensor networks to the best of our knowledge this is the first time such an integrated and systematic obstacle model for these networks has been proposed we define several types of obstacles that can be found inside the deployment area of wireless sensor network and provide categorization of these obstacles based on their nature physical and communication obstacles ie obstacles that are formed out of node distribution patterns or have physical presence respectively their shape and their change of nature over time we make an extension to custom made sensor network simulator simdust and conduct number of simulations in order to study the effect of obstacles on the performance of some representative in terms of their logic data propagation protocols for wireless sensor networks our findings confirm that obstacle presence has significant impact on protocol performance and also that different obstacle shapes and sizes may affect each protocol in different ways this provides an insight into how routing protocol will perform in the presence of obstacles and highlights possible protocol shortcomings moreover our results show that the effect of obstacles is not directly related to the density of sensor network and cannot be emulated only by changing the network density
technological achievements have made it possible to fabricate cmos circuits with over billion transistors implement boolean operations using quantum devices and or the spin of an electron implement transformations using bio and molecular based cells problems with many of these technologies are due to such factors as process variations defects and impurities in materials and solutions and noise consequently many systems built from these technologies operate imperfectly luckily there are many complex and large market systems applications that tolerate acceptable though not always correct results in addition there is emerging body of mathematical analysis related to imperfect computation in this paper we first introduce the concepts of acceptable error tolerance and acceptable performance degradation and demonstrate how important attributes of these concepts can be quantified we interlace this discussion with several examples of systems that can effectively employ these two concepts next we mention several immerging technologies that motivate the need to study these concepts as well as related mathematical paradigms finally we will list few cad issues that are needed to support this new form of technological revolution
in mainstream oo languages inheritance can be used to add new methods or to override existing methods virtual classes and feature oriented programming are techniques which extend the mechanism of inheritance so that it is possible to refine nested classes as well these techniques are attractive for programming in the large because inheritance becomes tool for manipulating whole class hierarchies rather than individual classes nevertheless it has proved difficult to design static type systems for virtual classes because virtual classes introduce dependent types the compile time type of an expression may depend on the run time values of objects in that expressionwe present formal object calculus which implements virtual classes in type safe manner our type system uses novel technique based on prototypes which blur the distinction between compile time and run time at run time prototypes act as objects and they can be used in ordinary computations at compile time they act as types prototypes are similar in power to dependent types and subtyping is shown to be form of partial evaluation we prove that prototypes are type safe but undecidable and briefly outline decidable semi algorithm for dealing with them
distributed sparing is method to improve the performance of raid disk arrays with respect to dedicated sparing system with disks including the spare disk since it utilizes the bandwidth of all disks we analyze the performance of raid with distributed sparing in normal mode degraded mode and rebuild mode in an oltp environment which implies small reads and writes the analysis in normal mode uses an queuing model which takes into account the components of disk service time in degraded mode low cost approximate method is developed to estimate the mean response time of fork join requests resulting from accesses to recreate lost data on the failed disk rebuild mode performance is analyzed by considering an vacationing server model with multiple vacations of different types to take into account differences in processing requirements for reading the first and subsequent tracks an iterative solution method is used to estimate the mean response time of disk requests as well as the time to read each disk which is shown to be quite accurate through validation against simulation results we next compare raid performance in system without cache with cache and with nonvolatile storage nvs cache the last configuration in addition to improved read response time due to cache hits provides fast write capability such that dirty blocks can be destaged asynchronously and at lower priority than read requests resulting in an improvement in read response time the small write penalty is also reduced due to the possibility of repeated writes to dirty blocks in the cache and by taking advantage of disk geometry to efficiently destage multiple blocks at time
this work investigates the problem of privacy preserving mining of frequent itemsets we propose procedure to protect the privacy of data by adding noisy items to each transaction then an algorithm is proposed to reconstruct frequent itemsets from these noise added transactions the experimental results indicate that this method can achieve rather high level of accuracy our method utilizes existing algorithms for frequent itemset mining and thereby takes full advantage of their progress to mine frequent itemset efficiently
in mobile computing environments as result of the reduced capacity of local storage it is commonly not feasible to replicate entire datasets on each mobile unit in addition reliable secure and economical access to central servers is not always possible moreover since mobile computers are designed to be portable they are also physically small and thus often unable to hold or process the large amounts of data held in centralised databases as many systems are only as useful as the data they can process the support provided by database and system management middleware for applications in mobile environments is an important driver for the uptake of this technology by application providers and thus also for the wider use of the technologyone of the approaches to maximize the available storage is through the use of database summarisation to date most strategies for reducing data volumes have used compression techniques that ignore the semantics of the data those that do not use data compression techniques adopt structural ie data and use independent methods in this paper we outline the special constraints imposed on storing information in mobile databases and provide flexible data summarisation policy the method works by assigning level of priority to each data item through the setting of number of parameters the paper discusses some policies for setting these parameters and some implementation strategies
with concurrent and garbage collected languages like java and becoming popular the need for suitable non intrusive efficient and concurrent multiprocessor garbage collector has become acute we propose novel mark and sweep on the fly algorithm based on the sliding views mechanism of levanoni and petrank we have implemented our collector on the jikes java virtual machine running on netfinity multiprocessor and compared it to the concurrent algorithm and to the stop the world collector supplied with jikes jvm the maximum pause time that we measured with our benchmarks over all runs was ms in all runs the pause times were smaller than those of the stop the world collector by two orders of magnitude and they were also always shorter than the pauses of the jikes concurrent collector throughput measurements of the new garbage collector show that it outperforms the jikes concurrent collector by up to as expected the stop the world does better than the on the fly collectors with results showing about differenceon top of being an effective mark and sweep on the fly collector standing on its own our collector may also be used as backup collector collecting cyclic data structures for the levanoni petrank reference counting collector these two algorithms perfectly fit sharing the same allocator similar data structure and similar jvm interface
dynamic energy performance scaling deps framework is proposed to save energy in fixed priority hard real time embedded systems in this generalized framework two existing technologies ie dynamic hardware resource configuration dhrc and dynamic voltage frequency scaling dvfs can be combined for energy performance tradeoff the problem of selecting the optimal hardware configuration and voltage frequency parameters is formulated to achieve maximal energy savings and meet the deadline constraint simultaneously through case study the effectiveness of deps has been validated
phrase based statistical machine translation approach mdash the alignment template approach mdash is described this translation approach allows for general many to many relations between words thereby the context of words is taken into account in the translation model and local changes in word order from source to target language can be learned explicitly the model is described using log linear modeling approach which is generalization of the often used source ndash channel approach thereby the model is easier to extend than classical statistical machine translation systems we describe in detail the process for learning phrasal translations the feature functions used and the search algorithm the evaluation of this approach is performed on three different tasks for the german ndash english speech verbmobil task we analyze the effect of various system components on the french ndash english canadian hansards task the alignment template system obtains significantly better results than single word based translation model in the chinese ndash english national institute of standards and technology nist machine translation evaluation it yields statistically significantly better nist scores than all competing research and commercial translation systems
we introduce the concept of administrative scope in role hierarchy and demonstrate that it can be used as basis for role based administration we then develop family of models for role hierarchy administration rha employing administrative scope as the central concept we then extend rha the most complex model in the family to complete decentralized model for role based administration we show that sarbac the resulting role based administrative model has significant practical and theoretical advantages over arbac we also discuss how administrative scope might be applied to the administration of general hierarchical structures how our model can be used to reduce inheritance in the role hierarchy and how it can be configured to support discretionary access control features
broadcast data dissemination is well suited for mobile wireless environments where bandwidth is scarce and mutual interference must be minimised however broadcasting monopolises the medium precluding clients from performing any other communication we address this problem in two ways firstly we segment the server broadcast with intervening periods of silence during which the wireless devices may communicate secondly we reduce the average access delay for clients using novel cooperative caching scheme our scheme is fully decentralised and uses information available locally at the client our results show that our model prevents the server from monopolising the medium and that our caching strategy reduces client access delays significantly
recently planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems in this paper we present the language Œ∫c which extends the declarative planning language by action costs Œ∫c provides the notion of admissible and optimal plans which are plans whose overall action costs are within given limit resp minimum over all plans ie cheapest plans as we demonstrate this novel language allows for expressing some nontrivial planning tasks in declarative way furthermore it can be utilized for representing planning problems under other optimality criteria such as computing shortest plans with the least number of steps and refinement combinations of cheapest and fastest plans we study complexity aspects of the language Œ∫c and provide transformation to logic programs such that planning problems are solved via answer set programming furthermore we report experimental results on selected problems our experience is encouraging that answer set planning may be valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved
in this contribution we present new paradigm and methodology for the network on chip noc based design of complex hardware software systems while classical industrial design platforms represent dedicated fixed architectures for specific applications flexible noc architectures open new degrees of system reconfigurability after giving an overview on required demands for noc hyper platforms we describe the realisation of these prerequisites within the hinoc platform we introduce new dynamic hardware software co design methodology for pre and post manufacturing design finally we will summarize the concept combined with an outlook on further investigations
mobile services operate on hosts with diverse capabilities in heterogeneous networks where the usage of resources such as processor memory and network is constantly changing in order to maintain efficiency in terms of performance and resource utilization such services should be able to adapt to changes in their environmentthis paper proposes and empirically evaluates an application transparent adaptation strategy for service oriented systems the strategy is based upon the solution of an optimization model derived from an existing suite of metrics for services which maps system services to network nodesthe strategy is evaluated empirically using number of distinct scenarios involving runtime changes in processor memory and network utilization in order to maintain execution efficiency in response to these changing operating conditions the strategy rearranges the service topology of the system dynamically by moving services between network nodes the results show that the negative impact of environmental changes on runtime efficiency can be reduced after adaptation from to depending on the selected parameters
we present an optimization framework for exploring gradient domain solutions for image and video processing the proposed framework unifies many of the key ideas in the gradient domain literature under single optimization formulation our hope is that this generalized framework will allow the reader to quickly gain general understanding of the field and contribute new ideas of their own we propose novel metric for measuring local gradient saliency that identifies salient gradients that give rise to long coherent edges even when the individual gradients are faint we present general weighting scheme for gradient constraints that improves the visual appearance of results we also provide solution for applying gradient domain filters to videos and video streams in coherent manner finally we demonstrate the utility of our formulation in creating effective yet simple to implement solutions for various image processing tasks to exercise our formulation we have created new saliency based sharpen filter and pseudo image relighting application we also revisit and improve upon previously defined filters such as nonphotorealistic rendering image deblocking and sparse data interpolation over images eg colorization using optimization
upcoming multi media compression applications will require high memory bandwidth in this paper we estimate that software reference implementation of an mpeg video decoder typically requires mtransfers to memory to decode cif times video object plane vop at frames this imposes high penalty in terms of power but also performancehowever we also show that we can heavily improve on the memory transfers without sacrificing speed even gaining about on cache misses and cycles for dec alpha by aggressive code transformations for this purpose we have manually applied an extended version of our data transfer and storage exploration dtse methodology which was originally developed for custom hardware implementations
although hierarchical pp systems have been found to outperform flat systems in many respects current pp research does not focus on strategies to build and maintain such systems available solutions assume either no or little coordination between peers that could lead the system toward satisfying globally defined goal eg minimizing traffic in this paper we focus on hierarchical dhts and provide full set of algorithms to build and maintain such systems that mitigate this problem in particular given the goal state of minimizing the total traffic without overloading any peer our algorithms dynamically adjust the system state as to keep the goal met at any time the algorithms are fully decentralized and probabilistic all decisions taken by the peers are based on their partial view on set of system wide parameters thus they demonstrate the main principle of self organization the system behavior emerges from local interactions our simulations run in range of realistic settings confirm good performance of the algorithms
programmers have traditionally used locks to synchronize concurrent access to shared data lock based synchronization however has well known pitfalls using locks for fine grain synchronization and composing code that already uses locks are both difficult and prone to deadlock transactional memory provides an alternate concurrency control mechanism that avoids these pitfalls and significantly eases concurrent programming transactional memory language constructs have recently been proposed as extensions to existing languages or included in new concurrent language specifications opening the door for new compiler optimizations that target the overheads of transactional memorythis paper presents compiler and runtime optimizations for transactional memory language constructs we present high performance software transactional memory system stm integrated into managed runtime environment our system efficiently implements nested transactions that support both composition of transactions and partial roll back our jit compiler is the first to optimize the overheads of stm and we show novel techniques for enabling jit optimizations on stm operations we measure the performance of our optimizations on way smp running multi threaded transactional workloads our results show that these techniques enable transactional memory's performance to compete with that of well tuned synchronization
this paper proposes neural network based approach for solving the resource discovery problem in peer to peer pp networks and an adaptive global local memetic algorithm aglma for performing the training of the neural network this training is very challenging due to the large number of weights and noise caused by the dynamic neural network testing the aglma is memetic algorithm consisting of an evolutionary framework which adaptively employs two local searchers having different exploration logic and pivot rules furthermore the aglma makes an adaptive noise compensation by means of explicit averaging on the fitness values and dynamic population sizing which aims to follow the necessity of the optimization process the numerical results demonstrate that the proposed computational intelligence approach leads to an efficient resource discovery strategy and that the aglma outperforms two classical resource discovery strategies as well as popular neural network training algorithm
this paper introduces the prophet critic hybrid conditionalbranch predictor which has two component predictorsthat play the role of either prophet or critictheprophet is conventional predictor that uses branch historyto predict the direction of the current branchfurther accessesof the prophet yield predictions for the branches followingthe current onepredictions for the current branchand the ones that follow are collectively known as thebranch's futurethey are actually prophecy or predictedbranch futurethe critic uses both the branch's history andfuture to give critique of the prophet's prediction fo thecurrent branchthe critique either agree or disagree isused to generate the final prediction for the branchour results show an byte prophet critic hybridhas fewer mispredicts than byte bc gskewpredictor predictor similar to that of the proposed compaq alpha ev processor across wide range of applicationsthe distance between pipeline flushes due to mispredictsincreases from one flush per micro operations uops to one per uopsfor gcc the percentage of mispredictedbranches drops from to on machinebased on the intel pentium processor this improvesupc uops per cycle by for gcc andreduces the number of uops fetched along both correct andincorrect paths by
in this paper we propose new dynamic and efficient bounding volume hierarchy for breakable objects undergoing structured and or unstructured motion our object space method is based on different ways to incrementally update the hierarchy during simulation by exploiting temporal coherence and lazy evaluation techniques this leads to significant advantages in terms of execution speed furthermore we also show how our method lends itself naturally for an adaptive low memory cost implementation which may be of critical importance in some applications finally we propose two different techniques for detecting self intersections one using our hierarchical data structure and the other is an improved sorting based method
in this article we present an experimental study of the properties of webgraphs we study large crawl from of pages and about billion edges made available by the webbase project at stanford as well as several synthetic ones generated according to various models proposed recently we investigate several topological properties of such graphs including the number of bipartite cores and strongly connected components the distribution of degrees and pagerank values and some correlations we present comparison study of the models against these measuresour findings are that the webbase sample differs slightly from the older samples studied in the literature and ii despite the fact that these models do not catch all of its properties they do exhibit some peculiar behaviors not found for example in the models from classical random graph theorymoreover we developed software library able to generate and measure massive graphs in secondary memory this library is publicy available under the gpl licence we discuss its implementation and some computational issues related to secondary memory graph algorithms
process algebraic techniques for distributed systems are increasingly being targeted at identifying abstractions that are adequate for both high level programming and specification and security analysis and verification drawing on our earlier work in bugliesi and focardi we investigate the expressive power of core set of security and network abstractions that provide high level primitives for specifying the honest principals in network while at the same time enabling an analysis of the network level adversarial attacks that may be mounted by an intruder we analyse various bisimulation equivalences for security that arise from endowing the intruder with label label different adversarial capabilities and label ii label increasingly powerful control over the interaction among the distributed principals of network by comparing the relative strength of the bisimulation equivalences we obtain direct measure of the intruder's discriminating power and hence of the expressiveness of the corresponding intruder model
we present aspect oriented programming in jiazzi jiazzi enhances java with separately compiled externally linked code modules called units units can act as effective aspect constructs with the ability to separate crosscutting concern code in non invasive and safe way unit linking provides convenient way for programmers to explicitly control the inclusion and configuration of code that implements concern while separate compilation of units enhances the independent development and deployment of the concern the expressiveness of concern separation is enhanced by units in two ways first classes can be made open to the addition of new behavior fields and methods after they are initially defined which enables the direct modularization of concerns whose code crosscut object boundaries second the signatures of methods and classes can also be made open to refinement which permits more aggressive modularization by isolating the naming and calling requirements of concern implementation
end user programming has become ubiquitous so much so that there are more end user programmers today than there are professional programmers end user programming empowers but to do what make really bad decisions based on really bad programs enter software engineering's focus on quality considering software quality is necessary because there is ample evidence that the programs end users create are filled with expensive errors in this paper consider what happens when we add to end user programming environments considerations of software quality going beyond the create program aspect of end user programming describe philosophy to software engineering for end users and then survey several projects in this area basic premise is that end user software engineering can only succeed to the extent that it respects the fact that the user probably has little expertise or even interest in software engineering
motivated by the optimality of shortest remaining processing time srpt for mean response time in recent years many computer systems have used the heuristic of favoring small jobs in order to dramatically reduce user response times however rarely do computer systems have knowledge of exact remaining sizes in this paper we introduce the class of smart policies which formalizes the heuristic of favoring small jobs in way that includes wide range of policies that schedule using inexact job size information examples of smart policies include policies that use exact size information eg srpt and psjf ii policies that use job size estimates and iii policies that use finite number of size based priority levels for many smart policies eg srpt with inexact job size information there are no analytic results available in the literature in this work we prove four main results we derive upper and lower bounds on the mean response time the mean slowdown the response time tail and the conditional response time of smart policies in each case the results explicitly characterize the tradeoff between the accuracy of the job size information used to prioritize and the performance of the resulting policy thus the results provide designers insight into how accurate job size information must be in order to achieve desired performance guarantees
in this paper we show how to augment object oriented application interfaces with enhanced specifications that include sequencing constraints called protocols protocols make explicit the relationship between messages methods supported by the application these relationships are usually only given implicitly either in the code or in textual comments we define notions of interface compatibility based upon protocols and show how compatibility can be checked discovering class of errors that cannot be discovered via the type system alone we then define software adaptors that can be used to bridge the difference between object oriented applications that have functionally compatible but type incompatible interfaces we discuss what it means for an adaptor to be well formed leveraging the information provided by protocols we show how adaptors can be automatically generated from high level description called an interface mapping
location based and personalized services are the key factors for promoting user satisfaction however most service providers did not consider the needs of mobile user in terms of their location and event participation consequently the service provider may lose the chance for better service and profit in this paper we present multi stage collaborative filtering mscf process to provide event recommendation based on mobile user's location to achieve this purpose the collaborative filtering cf technique is employed and the adaptive resonance theory art network is applied to cluster mobile users according to their personal profile sequential pattern mining is then used to discover the correlations between events for recommendation the mscf is designed not only to recommend for the old registered mobile user ormu but also to handle the cold start problem for new registered mobile user nrmu this research is designed to achieve the followings to present personalized event recommendation system for mobile users to discover mobile users moving patterns to provide recommendations based on mobile users preferences to overcome the cold start problem for new registered mobile user the experimental results of this research show that the mscf is able to accomplish the above purposes and shows better outcome for cold start problem when comparing with user based cf and item based cf
in this research we aim to identify factors that significantly affect the clickthrough of web searchers our underlying goal is determine more efficient methods to optimize the clickthrough rate we devise clickthrough metric for measuring customer satisfaction of search engine results using the number of links visited number of queries user submits and rank of clicked links we use neural network to detect the significant influence of searching characteristics on future user clickthrough our results show that high occurrences of query reformulation lengthy searching duration longer query length and the higher ranking of prior clicked links correlate positively with future clickthrough we provide recommendations for leveraging these findings for improving the performance of search engine retrieval and result ranking along with implications for search engine marketing copy wiley periodicals inc
various code certification systems allow the certification and static verification of important safety properties such as memory and control flow safety these systems are valuable tools for verifying that untrusted and potentially malicious code is safe before execution however one important safety property that is not usually included is that programs adhere to specific bounds on resource consumption such as running time we present decidable type system capable of specifying and certifying bounds on resource consumption our system makes two advances over previous resource bound certification systems both of which are necessary for practical system we allow the execution time of programs and their subroutines to vary depending on their arguments and we provide fully automatic compiler generating certified executables from source level programs the principal device in our approach is strategy for simulating dependent types using sum and inductive kinds
assessing mobility in thorough fashion is crucial step toward more efficient mobile network design recent research on mobility has focused on two main points analyzing models and studying their impact on data transport these works investigate the consequences of mobility in this paper instead we focus on the causes of mobility starting from established research in sociology we propose simps mobility model of human crowds with pedestrian motion this model defines process called sociostation rendered by two complimentary behaviors namely socialize and isolate that regulate an individual with regard to her his own sociability level simps leads to results that agree with scaling laws observed both in small scale and large scale human motion although our model defines only two simple individual behaviors we observe many emerging collective behaviors group formation splitting path formation and evolution
data domain description techniques aim at deriving concise descriptions of objects belonging to category of interest for instance the support vector domain description svdd learns hypersphere enclosing the bulk of provided unlabeled data such that points lying outside of the ball are considered anomalous however relevant information such as expert and background knowledge remain unused in the unsupervised setting in this paper we rephrase data domain description as semi supervised learning task that is we propose semi supervised generalization of data domain description sssvdd to process unlabeled and labeled examples the corresponding optimization problem is non convex we translate it into an unconstraint continuous problem that can be optimized accurately by gradient based techniques furthermore we devise an effective active learning strategy to query low confidence observations our empirical evaluation on network intrusion detection and object recognition tasks shows that our sssvdds consistently outperform baseline methods in relevant learning settings
hair simulation remains one of the most challenging aspects of creating virtual characters most research focuses on handling the massive geometric complexity of hundreds of thousands of interacting hairs this is accomplished either by using brute force simulation or by reducing degrees of freedom with guide hairs this paper presents hybrid eulerian lagrangian approach to handling both self and body collisions with hair efficiently while still maintaining detail bulk interactions and hair volume preservation is handled efficiently and effectively with flip based fluid solver while intricate hair hair interaction is handled with lagrangian self collisions thus the method has the efficiency of continuum guide based hair models with the high detail of lagrangian self collision approaches
we present an adaptive work stealing thread scheduler steal for fork join multithreaded jobs like those written using the cilk multithreaded language or the hood work stealing library the steal algorithm is appropriate for large parallel servers where many jobs share common multiprocessor resource and in which the number of processors available to particular job may vary during the job's execution steal provides continual parallelism feedback to job scheduler in the form of processor requests and the job must adaptits execution to the processors allotted to it assuming that the job scheduler never allots any job more processors than requested by thejob's thread scheduler steal guarantees that the job completes in near optimal time while utilizing at least constant fraction of the allotted processors our analysis models the job scheduler as the thread scheduler's adversary challenging the thread scheduler to be robust to the system environment and the job scheduler's administrative policies we analyze the performance of steal using trim analysis which allows us to prove that our thread scheduler performs poorly on at most small number of time steps while exhibiting near optimal behavior on the vast majority to be precise suppose that job has work and span critical path length on machine with processors steal completes the job in expected lg time steps where is the length of scheduling quantum and denotes the lg trimmed availability this quantity is the average of the processor availability over all but the lg time steps having the highest processor availability when the job's parallelism dominates the trimmed availability that is the job achieves nearly perfect linear speedup conversely when the trimmed mean dominates the parallelism the asymptotic running time of the job is nearly its span
mobile storage devices such as usb flash drives offer flexible solution for the transport and exchange of data nevertheless in order to prevent unauthorized access to sensitive data many enterprises require strict security policies for the use of such devices with the effect of rendering their advantages rather unfruitful trusted virtual domains tvds provide secure it infrastructure offering homogeneous and transparent enforcement of access control policies on data and network resources however the current model does not specifically deal with mobile storage devices in this paper we present an extension of the tvd architecture to incorporate the usage of mobile storage devices our proposal addresses three major issues coherent extension of tvd policy enforcement by introducing architectural components that feature identification and management of transitory devices transparent mandatory encryption of sensitive data stored on mobile devices and highly dynamic centralized key management service in particular we address offline scenarios allowing users to access and modify data while being temporarily disconnected from the domain we also present prototype implementation based on the turaya security kernel
we consider the problem of clustering web image search results generally the image search results returned by an image search engine contain multiple topics organizing the results into different semantic clusters facilitates users browsing in this paper we propose hierarchical clustering method using visual textual and link analysis by using vision based page segmentation algorithm web page is partitioned into blocks and the textual and link information of an image can be accurately extracted from the block containing that image by using block level link analysis techniques an image graph can be constructed we then apply spectral techniques to find euclidean embedding of the images which respects the graph structure thus for each image we have three kinds of representations ie visual feature based representation textual feature based representation and graph based representation using spectral clustering techniques we can cluster the search results into different semantic clusters an image search example illustrates the potential of these techniques
several supervised learning algorithms are suited to classify instances into multiclass value space multinomial logit mnl is recognized as robust classifier and is commonly applied within the crm customer relationship management domain unfortunately to date it is unable to handle huge feature spaces typical of crm applications hence the analyst is forced to immerse himself into feature selection surprisingly in sharp contrast with binary logit current software packages lack any feature selection algorithm for multinomial logit conversely random forests another algorithm learning multiclass problems is just like mnl robust but unlike mnl it easily handles high dimensional feature spaces this paper investigates the potential of applying the random forests principles to the mnl framework we propose the random multinomial logit rmnl ie random forest of mnls and compare its predictive performance to that of mnl with expert feature selection random forests of classification trees we illustrate the random multinomial logit on cross sell crm problem within the home appliances industry the results indicate substantial increase in model accuracy of the rmnl model to that of the mnl model with expert feature selection
dynamic information flow tracking dift is an important tool for detecting common security attacks and memory bugs dift tool tracks the flow of information through monitored program's registers and memory locations as the program executes detecting and containing fixing problems on the fly unfortunately sequential dift tools are quite slow and dift is quite challenging to parallelize in this paper we present new approach to parallelizing dift like functionality extending our recent work on accelerating sequential dift we consider variant of dift that tracks the information flow only through unary operations relaxed dift and yet makes sense for detecting security attacks and memory bugs we present parallel algorithm for relaxed dift based on symbolic inheritance tracking which achieves linear speed up asymptotically moreover we describe techniques for reducing the constant factors so that speed ups can be obtained even with just few processors we implemented the algorithm in the context of log based architectures lba system which provides hardware support for logging program trace and delivering it to other monitoring processors our simulation results on spec benchmarks and video player show that our parallel relaxed dift reduces the overhead to as low as using monitoring cores on core chip multiprocessor
this article presents resolution matched shadow maps rmsm modified adaptive shadow map asm algorithm that is practical for interactive rendering of dynamic scenes adaptive shadow maps which build quadtree of shadow samples to match the projected resolution of each shadow texel in eye space offer robust solution to projective and perspective aliasing in shadow maps however their use for interactive dynamic scenes is plagued by an expensive iterative edge finding algorithm that takes highly variable amount of time per frame and is not guaranteed to converge to correct solution this article introduces simplified algorithm that is up to ten times faster than asms has more predictable performance and delivers more accurate shadows our main contribution is the observation that it is more efficient to forgo the iterative refinement analysis in favor of generating all shadow texels requested by the pixels in the eye space image the practicality of this approach is based on the insight that for surfaces continuously visible from the eye adjacent eye space pixels map to adjacent shadow texels in quadtree shadow space this means that the number of contiguous regions of shadow texels which can be efficiently generated with rasterizer is proportional to the number of continuously visible surfaces in the scene moreover these regions can be coalesced to further reduce the number of render passes required to shadow an image the secondary contribution of this paper is demonstrating the design and use of data parallel algorithms inseparably mixed with traditional graphics programming to implement novel interactive rendering algorithm for the scenes described in this paper we achieve frames per second on static scenes and frames per second on dynamic scenes for and images with maximum effective shadow resolution of texels
escape analysis is static analysis that determines whether the lifetime of data may exceed its static scopethis paper first presents the design and correctness proof of an escape analysis for javatm this analysis is interprocedural context sensitive and as flow sensitive as the static single assignment form so assignments to object fields are analyzed in flow insensitive manner since java is an imperative language the effect of assignments must be precisely determined this goal is achieved thanks to our technique using two interdependent analyses one forward one backward we introduce new method to prove the correctness of this analysis using aliases as an intermediate step we use integers to represent the escaping parts of values which leads to fast and precise analysisour implementation blanchet which applies to the whole java language is then presented escape analysis is applied to stack allocation and synchronization elimination in our benchmarks we stack allocate percnt to percnt of data eliminate more than percnt of synchronizations on most programs percnt and percnt on two examples and get up to percnt runtime decrease percnt on average our detailed experimental study on large programs shows that the improvement comes more from the decrease of the garbage collection and allocation times than from improvements on data locality contrary to what happened for ml this comes from the difference in the garbage collectors
regular path queries are way of declaratively expressing queries on graphs as regular expression like patterns that are matched against paths in the graph there are two kinds of queries existential queries which specify properties about individual paths and universal queries which specify properties about all paths they provide simple and convenient framework for expressing program analyses as queries on graph representations of programs for expressing verification model checking problems as queries on transition systems for querying semi structured data etc parametric regular path queries extend the patterns with variables called parameters which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relatethis paper shows how variety of program analysis and model checking problems can be expressed easily and succinctly using parametric regular path queries the paper describes the specification design analysis and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries detailed complexity analysis of the algorithms detailed analytical and experimental performance comparison of variations of the algorithms and data structures and investigation of efficiency tradeoffs between different formulations of queries
mobile agent technology has emerged as promising programming paradigm for developing highly dynamic and large scale service oriented computing middlewares due to its desirable features for this purpose first of all scalable location transparent agent communication issue should be addressed in mobile agent systems despite agent mobility although there were proposed several directory service and message delivery mechanisms their disadvantages force them not to be appropriate to both low overhead location management and fast delivery of messages to agents migrating frequently to mitigate their limitations this paper presents scalable distributed directory service and message delivery mechanism the proposed mechanism enables each mobile agent to autonomously leave tails of forwarding pointers on some few of its visiting nodes depending on its preferences this feature results in low message forwarding overhead and low storage and maintenance cost of increasing chains of pointers per host also keeping mobile agent location information in the effective binding cache of each sending agent the sending agent can communicate with mobile agents much faster compared with the existing ones
idle resources can be exploited not only to run important local tasks such as data replication and virus checking but also to make contributions to society by participating in open computing projects like seti home when executing background processes to utilize such valuable idle resources we need to explicitly control them so that the user will not be discouraged from exploiting idle resources by foreground performance degradation unfortunately common priority based schedulers lack such explicit execution control in addition to encourage active use of idle resources mechanism for controlling background processes should not require modifications to the underlying operating system or user applications if such modifications are required the user may be reluctant to employ the mechanism in this paper we argue that we can reasonably detect resource contention between foreground and background processes and properly control background process execution at the user level we infer the existence of resource contention from the approximated resource shares of background processes our approach takes advantage of dynamically instrumented probes which are becoming increasingly popular in estimating the resource shares also it considers different resource types in combination and can handle varied workloads including multiple background processes we show that our system effectively avoids the performance degradation of foreground activities by suspending background processes in an appropriate fashion our system keeps the increase in foreground execution time due to background processes below or much lower in most of our experiments also we extend our approach to address undesirable resource allocations to cpu intensive processes that can occur in multiprocessor environments
we practices lack an impact on industry partly due to we field that is not quality aware in fact it is difficult to find we methodologies that pay explicit attention to quality aspects however the use of systematic process that includes quality concerns from the earliest stages of development can contribute to easing the building up of quality guaranteed web applications without drastically increasing development costs and time to market in this kind of process quality issues should be taken into account while developing each outgoing artifact from the requirements model to the final application also quality models should be defined to evaluate the quality of intermediate we artifacts and how it contributes to improving the quality of the deployed application in order to tackle its construction while avoiding some of the most common problems that existing quality models suffer from in this paper we propose number of we quality models to address the idiosyncrasies of the different stakeholders and we software artifacts involved additionally we propose that these we quality models are supported by an ontology based we measurement meta model that provides set of concepts with clear semantics and relationships this we quality metamodel is one of the main contributions of this paper furthermore we provide an example that illustrates how such metamodel may drive the definition of particular we quality model
in this work we present rendering method with guaranteed interactive frame rates in complex scenes the algorithm is based on an new data structure determined in preprocessing to avoid frozen displays in large simulative visualizations like industrial plants typically described as cad models within preprocessing polygons are grouped by size and within these groups core clusters are calculated based on similarity and locality the clusters and polygons are building up hierarchy including weights ascertained within repetitive stages of re grouping and re clustering this additional information allows to choose subset over all primitives to reduce scene complexity depending on the viewer's position sight and the determined weights within the hierarchy to guarantee specific frame rate the number of rendered primitives is limited by constant and typically constrained by hardware this reduction is controlled by the pre calculated weights and the viewer's position and is not done arbitrarily at least the rendered section is suitable scene approximation that includes the viewer's interests combining all this constant frame rate including million polygons at fps is obtainable practical results indicate that our approach leads to good scene approximations and realtime rendering of very large environments at the same time
because of the high volume and unpredictable arrival rate stream processing systems may not always be able to keep up with the input data streams resulting in buffer overflow and uncontrolled loss of data load shedding the prevalent strategy for solving this overflow problem has so far only been considered for relational stream processing but not for xml shedding applied to xml stream processing brings new opportunities and challenges due to complex nested nature of xml structures in this paper we tackle this unsolved xml shedding problem using three pronged approach first we develop an xquery preference model that enables users to specify the relative importance of preserving different subpatterns in the xml result structure this transforms shedding into the problem of rewriting the user query into shed queries that return approximate query answers with utility as measured by the given user preference model second we develop cost model to compare the performance of alternate shed queries third we develop two shedding algorithms optshed and fastshed optshed guarantees to find an optimal solution however at the cost of exponential complexity fastshed as confirmed by our experiments achieves close to optimal result in wide range of test cases finally we describe the in automaton shedding mechanism for xquery stream engines the experiments show that our proposed utility driven shedding solutions consistently achieve higher utility results compared to the existing relational shedding techniques
in the recent years the web has been rapidly deepened with the prevalence of databases online on this deep web many sources are structured by providing structured query interfaces and results organizing such structured sources into domain hierarchy is one of the critical steps toward the integration of heterogeneous web sources we observe that for structured web sources query schemas ie attributes in query interfaces are discriminative representatives of the sources and thus can be exploited for source characterization in particular by viewing query schemas as type of categorical data we abstract the problem of source organization into the clustering of categorical data our approach hypothesizes that homogeneous sources are characterized by the same hidden generative models for their schemas to find clusters governed by such statistical distributions we propose new objective function model differentiation which employs principled hypothesis testing to maximize statistical heterogeneity among clusters our evaluation over hundreds of real sources indicates that the schema based clustering accurately organizes sources by object domains eg books movies and on clustering web query schemas the model differentiation function outperforms existing ones such as likelihood entropy and context linkages with the hierarchical agglomerative clustering algorithm
nearest neighbor search is an important and widely used technique in number of important application domains in many of these domains the dimensionality of the data representation is often very high recent theoretical results have shown that the concept of proximity or nearest neighbors may not be very meaningful for the high dimensional case therefore it is often complex problem to find good quality nearest neighbors in such data sets furthermore it is also difficult to judge the value and relevance of the returned results in fact it is hard for any fully automated system to satisfy user about the quality of the nearest neighbors found unless he is directly involved in the process this is especially the case for high dimensional data in which the meaningfulness of the nearest neighbors found is questionable in this paper we address the complex problem of high dimensional nearest neighbor search from the user perspective by designing system which uses effective cooperation between the human and the computer the system provides the user with visual representations of carefully chosen subspaces of the data in order to repeatedly elicit his preferences about the data patterns which are most closely related to the query point these preferences are used in order to determine and quantify the meaningfulness of the nearest neighbors our system is not only able to find and quantify the meaningfulness of the nearest neighbors but is also able to diagnose situations in which the nearest neighbors found are truly not meaningful
we propose finite element simulation method that addresses the full range of material behavior from purely elastic to highly plastic for physical domains that are substantially reshaped by plastic flow fracture or large elastic deformations to mitigate artificial plasticity we maintain simulation mesh in both the current state and the rest shape and store plastic offsets only to represent the non embeddable portion of the plastic deformation to maintain high element quality in tetrahedral mesh undergoing gross changes we use dynamic meshing algorithm that attempts to replace as few tetrahedra as possible and thereby limits the visual artifacts and artificial diffusion that would otherwise be introduced by repeatedly remeshing the domain from scratch our dynamic mesher also locally refines and coarsens mesh and even creates anisotropic tetrahedra wherever simulation requests it we illustrate these features with animations of elastic and plastic behavior extreme deformations and fracture
shopping lists play central role in grocery shopping among other things shopping lists serve as memory aids and as tool for budgeting more interestingly shopping lists serve as an expression and indication of customer needs and interests accordingly shopping lists can be used as an input for recommendation techniques in this paper we describe methodology for making recommendations about additional products to purchase using items on the user's shopping list as shopping list entries seldom correspond to products we first use information retrieval techniques to map the shopping list entries into candidate products association rules are used to generate recommendations based on the candidate products we evaluate the usefulness and interestingness of the recommendations in user study
the emergence of wireless and mobile networks has made possible the introduction of new research area commerce or mobile commerce mobile payment is natural successor to web centric payments which has emerged as one of the sub domains of mobile commerce applications study reveals that there are wide ranges of mobile payment solutions and models which are available with the aid of various services such as short message service sms but there is no specific mobile payment system for educational institutions to collect the fees as well as for student community to pay the fees without huge investment this paper proposes secured framework for mobile payment consortia system mpcs to carry out the transactions from the bank to the academic institutions for the payment of fees by students through mobile phone mobile payment consortia system provides an end to end security using public key infrastructure pki through mobile information device profile midp enabled mobile device this framework provides an efficient reliable and secured system to perform mobile payment transactions and reduces transactional cost for both students and educational institutions mobile payment consortia system is designed with strong authentication and non repudiation by employing digital signatures confidentiality and message integrity are also provided by encrypting the messages at application level and by using public key certificates and digital signature envelops
the advance of multi core architectures provides significant benefits for parallel and throughput oriented computing but the performance of individual computation threads does not improve and may even suffer penalty because of the increased contention for shared resources this paper explores the idea of using available general purpose cores in cmp as helper engines for individual threads running on the active cores we propose lightweight architectural framework for efficient event driven software emulation of complex hardware accelerators and describe how this framework can be applied to implement variety of prefetching techniques we demonstrate the viability and effectiveness of our framework on wide range of applications from the spec cpu and olden benchmark suites on average our mechanism provides performance benefits within of pure hardware implementations furthermore we demonstrate that running event driven prefetching threads on top of baseline with hardware stride prefetcher yields significant speedups for many programs finally we show that our approach provides competitive performance improvements over other hardware approaches for multi core execution while executing fewer instructions and requiring considerably less hardware support
texture is an essential component of computer generated models for texture mapping procedure to be effective it has to generate continuous textures and cause only small mapping distortion the angle based flattening abf parameterization method is guaranteed to provide continuous no foldovers mapping it also minimizes the angular distortion of the parameterization including locating the optimal planar domain boundary however since it concentrates on minimizing the angular distortion of the mapping it can introduce relatively large linear distortionin this paper we introduce new procedure for reducing length distortion of an existing parameterization and apply it to abf results the linear distortion reduction is added as second step in texture mapping computation the new method is based on computing mapping from the plane to itself which has length distortion very similar to that of the abf parameterization by applying the inverse mapping to the result of the initial parameterization we obtain new parameterization with low length distortion we notice that the procedure for computing the inverse mapping can be applied to any other convenient mapping from the three dimensional surface to the plane in order to improve itthe mapping in the plane is computed by applying weighted laplacian smoothing to cartesian grid covering the planar domain of the initial mapping both the mapping and its inverse are provably continuous since angle preserving conformal mappings such as abf locally preserve distances as well the planar mapping has small local deformation as result the inverse mapping does not significantly increase the angular distortionthe combined texture mapping procedure provides mapping with low distance and angular distortion which is guaranteed to be continuous
retrieval speed and precision ultimately determine the success of any database system this article outlines the challenges posed by distributed and heterogeneous database systems including those that store unstructured data and surveys recent work much work remains to help users retrieve information with ease and efficiency from heterogeneous environment in which relational object oriented textual and pictorial databases coexist the article outlines the progress that has been made in query processing in distributed relational database systems and heterogeneous and multidatabase systems
the ultimate challenges of system modeling concern designing accurate yet highly transparent and user centric models we have witnessed plethora of neurofuzzy architectures which are aimed at addressing these two highly conflicting requirements this study is concerned with the design and the development of transparent logic networks realized with the aid of fuzzy neurons and fuzzy unineurons the construction of networks of this form requires formation of efficient interfaces that constitute conceptually appealing bridge between the model and the real world experimental environment in which the model is to be used in general the interfaces are constructed by invoking some form of granulation of information and binary boolean discretization in particular we introduce new discretization environment that is realized by means of particle swarm optimization pso and data clustering implemented by the means algorithm the underlying structure of the network is optimized by invoking combination of the pso and the mechanisms of conventional gradient based learning we discuss various optimization strategies by considering boolean as well as fuzzy data coming as the result of discretization of original experimental data and then involving several learning strategies we elaborate on the interpretation aspects of the network and show how those could be strengthened through efficient pruning we also show how the interpreted network leads to simpler and more accurate logic description of the experimental data number of experimental studies are included
this paper shows that even very small data caches when split to serve data streams exhibiting temporal and spatial localities can improve performance of embedded applications without consuming excessive silicon real estate or power it also shows that large block sizes or higher set associativities are unnecessary with split cache organizations we use benchmark programs from mibench to show that our cache organization outperforms other organizations in terms of miss rates access times energy consumption and silicon area
in this paper we address the task of crosslingual semantic relatedness we introduce method that relies on the information extracted from wikipedia by exploiting the interlanguage links available between wikipedia versions in multiple languages through experiments performed on several language pairs we show that the method performs well with performance comparable to monolingual measures of relatedness
the fair evaluation and comparison of side channel attacks and countermeasures has been long standing open question limiting further developments in the field motivated by this challenge this work makes step in this direction and proposes framework for the analysis of cryptographic implementations that includes theoretical model and an application methodology the model is based on commonly accepted hypotheses about side channels that computations give rise to it allows quantifying the effect of practically relevant leakage functions with combination of information theoretic and security metrics measuring the quality of an implementation and the strength of an adversary respectively from theoretical point of view we demonstrate formal connections between these metrics and discuss their intuitive meaning from practical point of view the model implies unified methodology for the analysis of side channel key recovery attacks the proposed solution allows getting rid of most of the subjective parameters that were limiting previous specialized and often ad hoc approaches in the evaluation of physically observable devices it typically determines the extent to which basic but practically essential questions such as how to compare two implementations or how to compare two side channel adversaries can be answered in sound fashion
with the significant advances in mobile computing technology there is an increasing demand for various mobile applications to process transactions in real time fashion when remote data access is considered in mobile environment data access delay becomes one of the most serious problems in meeting transaction deadlines in this paper we propose multi version data model and adopt the relative consistency as the correctness criterion for processing of real time transactions in mobile environment the purpose is to reduce the impacts of unpredictable and unreliable mobile network on processing of the real time transactions under the proposed model the overheads for concurrency control can be significantly reduced and the data availability is much enhanced even under network failures real time transaction may access stale data provided that they are relatively consistent with the data accessed by the transaction and the staleness of the data is within the requirements an image transaction model which pre fetches multiple data versions at fixed hosts is proposed to reduce the data access delay and to simplify the management of the real time transactions in mobile environment the image transaction model also helps in reducing the transaction restart overheads and minimizing the impacts of the unpredictable performance of mobile network on transaction executions
bisimulation semantics are very pleasant way to define the semantics of systems mainly because the simplicity of their definitions and their nice coalgebraic properties however they also have some disadvantages they are based on sequential operational semantics defined by means of an ordinary transition system and in order to be bisimilar two systems have to be too similar in this work we will present several natural proposals to define weaker bisimulation semantics that we think properly capture the desired behaviour of distributed systems the main virtue of all these semantics is that they are real bisimulation semantics thus inheriting most of the good properties of bisimulation semantics this is so because they can be defined as particular instances of jacobs and hughes categorical definition of simulation which they have already proved to satisfy all those properties
new ldquo range space rdquo approach is described for synergistic resolution of both stereovision and reflectance visual modeling problems simultaneously this synergistic approach can be applied to arbitrary camera arrangements with different intrinsic and extrinsic parameters image types image resolutions and image number these images are analyzed in step wise manner to extract range measurements and also to render customized perspective view the entire process is fully automatic an extensive and detailed experimental validation phase supports the basic feasibility and generality of the range space approach
category level object recognition segmentation and tracking in videos becomes highly challenging when applied to sequences from hand held camera that features extensive motion and zooming an additional challenge is then to develop fully automatic video analysis system that works without manual initialization of tracker or other human intervention both during training and during recognition despite background clutter and other distracting objects moreover our working hypothesis states that category level recognition is possible based only on an erratic flickering pattern of interest point locations without extracting additional features compositions of these points are then tracked individually by estimating parametric motion model groups of compositions segment video frame into the various objects that are present and into background clutter objects can then be recognized and tracked based on the motion of their compositions and on the shape they form finally the combination of this flow based representation with an appearance based one is investigated besides evaluating the approach on challenging video categorization database with significant camera motion and clutter we also demonstrate that it generalizes to action recognition in natural way
development of new embedded systems requires tuning of the software applications to specific hardware blocks and platforms as well as to the relevant input data instances the behaviour of these applications heavily relies on the nature of the input data samples thus making them strongly data dependent for this reason it is necessary to extensively profile them with representative samples of the actual input data an important aspect of this profiling is done at the dynamic data type level which actually steers the designers choice of implementation of these data types the behaviour of the applications is then characterized through an analysis phase as collection of software metadata that can be used to optimize the system as whole in this paper we propose to represent the behaviour of data dependent applications to enable optimizations rather than to analyze their structure or to define the engineering process behind them moreover we specifically limit ourselves to the scope of applications dominated by dynamically allocated data types running on embedded systems we characterize the software metadata that these optimizations require and we present methodology as well as appropriate techniques to obtain this information from the original application the optimizations performed on complete case study utilizing the extracted software metadata achieve overall improvements of up to in the number of cycles spent accessing memory when compared to code optimized only with the static techniques applied by gnu
this work addresses the issue of answering spatio temporal range queries when there is uncertainty associated with the model of the moving objects uncertainty is inherent in moving objects database mod applications and capturing it in the data model has twofold impact the number of updates when the actual trajectory deviates from its mod representation the linguistic constructs and the processing algorithms for querying the mod the paper presents both spatial and temporal uncertainty aspects which are combined into one model of uncertain trajectories given the model the methodology is presented which enables processing of queries such as what is the probability that given moving object was will be inside given region sometimes always during given time interval where the regions are bounded by arbitrary polygons
in recent years grid systems and peer to peer networks are the most commonly used solutions to achieve the same goal the sharing of resources and services in heterogeneous dynamic distributed environments many studies have proposed hybrid approaches that try to conjugate the advantages of the two models this paper proposes an architecture that integrates the pp interaction model in grid environments so as to build an open cooperative model wherein grid entities are composed in decentralized way in particular this paper focuses on qos aware discovery algorithm for pp grid systems analyzing protocol and explaining techniques used to improve its performance
technological success has ushered in massive amounts of data for scientific analysis to enable effective utilization of these data sets for all classes of users supporting intuitive data access and manipulation interfaces is crucial this paper describes an autonomous scientific workflow system that enables high level natural language based queries over low level data sets our technique involves combination of natural language processing metadata indexing and semantically aware workflow composition engine which dynamically constructs workflows for answering queries based on service and data availability specific contribution of this work is metadata registration scheme that allows for unified index of heterogeneous metadata formats and service annotations our approach thus avoids standardized format for storing all data sets or the implementation of federated mediator based querying framework we have evaluated our system using case study from the geospatial domain to show functional results our evaluation supports the potential benefits which our approach can offer to scientific workflow systems and other domain specific data intensive applications
this paper evaluates pointer tainting an incarnation of dynamic information flow tracking dift which has recently become an important technique in system security pointer tainting has been used for two main purposes detection of privacy breaching malware eg trojan keyloggers obtaining the characters typed by user and detection of memory corruption attacks against non control data eg buffer overflow that modifies user's privilege level in both of these cases the attacker does not modify control data such as stored branch targets so the control flow of the target program does not change phrased differently in terms of instructions executed the program behaves normally as result these attacks are exceedingly difficult to detect pointer tainting is considered one of the onlymethods for detecting them in unmodified binaries unfortunately almost all of the incarnations of pointer tainting are flawed in particular we demonstrate that the application of pointer tainting to the detection of keyloggers and other privacybreaching malware is problematic we also discuss whether pointer tainting is able to reliably detect memory corruption attacks against non control data pointer tainting generates itself the conditions for false positives we analyse the problems in detail and investigate various ways to improve the technique most have serious drawbacks in that they are either impractical and incur many false positives still and or cripple the technique's ability to detect attacks in conclusion we argue that depending on architecture and operating system pointer tainting may have some value in detecting memory orruption attacks albeit with false negatives and not on the popular architecture but it is fundamentally not suitable for automated detecting of privacy breaching malware such as keyloggers
common belief in the scientific community is that traffic classifiers based on deep packet inspection dpi are far more expensive in terms of computational complexity compared to statistical classifiers in this paper we counter this notion by defining accurate models for deep packet inspection classifier and statistical one based on support vector machines and by evaluating their actual processing costs through experimental analysis the results suggest that contrary to the common belief dpi classifier and an svm based one can have comparable computational costs although much work is left to prove that our results apply in more general cases this preliminary analysis is first indication of how dpi classifiers might not be as computationally complex compared to other approaches as we previously thought
in recent years both performance and power have become key factors in efficient memory design in this paper we propose systematic approach to reduce the energy consumption of the entire memory hierarchy we first evaluate an existing power aware memory system where memory modules can exist in different power modes and then propose on chip memory module buffers called energy saver buffers esb which reside in between the cache and main memory esbs reduce the additional overhead incurred due to frequent resynchronization of the memory modules in low power state an additional improvement is attained by using model that dynamically resizes the active cache based on the varying needs of program our experimental results demonstrate that an integrated approach can reduce the energy delay product by as much as when compared to traditional non power aware memory hierarchy
we show how properties of an interesting class of imperative programs can be calculated by means of relational modeling and symbolic computation the ideas of are implemented using symbolic computations based on maple
focus on organizing and implementing workflows in government from standpoint of data awareness or even data centricity might provide opportunities to address several of the challenges facing governments efforts to improve government effectiveness and efficiency as well as interoperation between government entities the notion of data aware as opposed to data unaware or just process centric workflows is based on taking into account and using the particular enactments and instance specific data in the workflow itself and beyond its single instance in other words on the one hand workflows process data however through their enactments and instances on the other hand workflows are data ready as inputs for other workflows including cross process enactment mining and analyses to be useful as strategy in government we need to explore how data centric approaches can tackle the specific challenges of government such as the ill structured or semi structured workflows found in emergency and disaster response management and we need to better understand the specific constraints under which intra and cross agency data centric workflows can be designed and implemented this paper lays out research agenda that will inform questions about the issues with and the potential of the data centric approach in the context of government
many electronic cash systems have been proposed with the proliferation of the internet and the activation of electronic commerce cash enables the exchange of digital coins with value assured by the bank's signature and with concealed user identity in an electronic cash system user can withdraw coins from the bank and then spends each coin anonymously and unlinkably in this paper we design an efficient anonymous mobile payment system based on bilinear pairings in which the anonymity of coins is revocable by trustee in case of dispute the message transfer from the customer to the merchant occurs only once during the payment protocol also the amount of communication between customer and merchant is about bits therefore our mobile payment system can be used in the wireless networks with the limited bandwidth the security of the new system is under the computational diffie hellman problem in the random oracle model
in this paper we present formal model named pobsam policy based self adaptive model for developing and modeling self adaptive systems in this model policies are used as mechanism to direct and adapt the behavior of self adaptive systems pobsam model consists of set of self managed modules smm an smm is collection of autonomous managers and managed actors managed actors are dedicated to functional behavior while autonomous managers govern the behavior of managed actors by enforcing suitable policies to adapt smm behavior in response to changes policies governing an smm are adjusted ie dynamic policies are used to govern and adapt system behavior we employ the combination of an algebraic formalism and an actor based model to specify this model formally managers are modeled as meta actors whose policies are described using an algebra managed actors are expressed by an actor model furthermore we provide an operational semantics for pobsam described using labeled transition systems
embedded systems designers are free to choose the most suitable configuration of cache in modern processor based socs choosing the appropriate cache configuration necessitates the simulation of long memory access traces to accurately obtain hit miss rates the long execution time taken to simulate these traces particularly separate simulation for each configuration is major drawback researchers have proposed techniques to speed up the simulation of caches with lru replacement policy these techniques are of little use in the majority of embedded processors as these processors utilize round robin policy based caches in this paper we propose fast cache simulation approach called scud sorted collection of unique data for caches with the round robin policy scud is single pass cache simulator that can simulate multiple cache configurations with varying set sizes and associativities by reading the application trace once utilizing fast binary searches in novel data structure scud simulates an application trace significantly faster than widely used single configuration cache simulator dinero iv we show scud can simulate set of cache configurations up to times faster than dinero iv scud shows an average speed up of times over dinero iv for mediabench applications and an average speed up of over times for spec cpu applications
the frequent items problem is to process stream of items and find all items occurring more than given fraction of the time it is one of the most heavily studied problems in data stream mining dating back to the many applications rely directly or indirectly on finding the frequent items and implementations are in use in large scale industrial systems however there has not been much comparison of the different methods under uniform experimental conditions it is common to find papers touching on this topic in which important related work is mischaracterized overlooked or reinvented in this paper we aim to present the most important algorithms for this problem in common framework we have created baseline implementations of the algorithms and used these to perform thorough experimental study of their properties we give empirical evidence that there is considerable variation in the performance of frequent items algorithms the best methods can be implemented to find frequent items with high accuracy using only tens of kilobytes of memory at rates of millions of items per second on cheap modern hardware
constructing correct concurrent garbage collection algorithms is notoriously hard numerous such algorithms have been proposed implemented and deployed and yet the relationship among them in terms of speed and precision is poorly understood and the validation of one algorithm does not carry over to othersas programs with low latency requirements written in garbagecollected languages become part of society's mission critical infrastructure it is imperative that we raise the level of confidence in the correctness of the underlying system and that we understand the trade offs inherent in our algorithmic choicein this paper we present correctness preserving transformations that can be applied to an initial abstract concurrent garbage collection algorithm which is simpler more precise and easier to prove correct than algorithms used in practice but also more expensive and with less concurrency we then show how both pre existing and new algorithms can be synthesized from the abstract algorithm by series of our transformations we relate the algorithms formally using new definition of precision and informally with respect to overhead and concurrencythis provides many insights about the nature of concurrent collection allows the direct synthesis of new and useful algorithms reduces the burden of proof to single simple algorithm and lays the groundwork for the automated synthesis of correct concurrent collectors
moving and resizing desktop windows are frequently performed but largely unexplored interaction tasks the standard title bar and border dragging techniques used for window manipulation have not changed much over the years we studied three new methods to move and resize windows the new methods are based on proxy and goal crossing techniques to eliminate the need of long cursor movements and acquiring narrow window borders instead moving and resizing actions are performed by manipulating proxy objects close to the cursor and by sweeping cursor motions across window borders we compared these techniques with the standard techniques the results indicate that further investigations and redesigns of window manipulation techniques are worthwhile all new techniques were faster than the standard techniques with task completion times improving more than in some cases also the new resizing techniques were found to be less error prone than the traditional click and drag method
permissive nominal logic pnl is an extension of first order logic where term formers can bind names in their arguments this allows for direct axiomatisations with binders such as the quantifier of first order logic itself and the binder of the lambda calculus this also allows us to finitely axiomatise arithmetic like first and higher order logic and unlike other nominal logics equality reasoning is not necessary to alpha rename all this gives pnl much of the expressive power of higher order logic but terms derivations and models of pnl are first order in character and the logic seems to strike good balance between expressivity and simplicity
computing environments on cellphones especially smartphones are becoming more open and general purpose thus they also become attractive targets of malware cellphone malware not only causes privacy leakage extra charges and depletion of battery power but also generates malicious traffic and drains down mobile network and service capacity in this work we devise novel behavior based malware detection system named pbmds which adopts probabilistic approach through correlating user inputs with system calls to detect anomalous activities in cellphones pbmds observes unique behaviors of the mobile phone applications and the operating users on input and output constrained devices and leverages hidden markov model hmm to learn application and user behaviors from two major aspects process state transitions and user operational patterns built on these pbdms identifies behavioral differences between malware and human users through extensive experiments on major smartphone platforms we show that pbmds can be easily deployed to existing smartphone hardware and it achieves high detection accuracy and low false positive rates in protecting major applications in smartphones
context the technology acceptance model tam was proposed in as means of predicting technology usage however it is usually validated by using measure of behavioural intention to use bi rather than actual usage objective this review examines the evidence that the tam predicts actual usage using both subjective and objective measures of actual usage method we performed systematic literature review based on search of six digital libraries along with vote counting meta analysis to analyse the overall results results the search identified relevant empirical studies in articles the results show that bi is likely to be correlated with actual usage however the tam variables perceived ease of use peu and perceived usefulness pu are less likely to be correlated with actual usage conclusion care should be taken using the tam outside the context in which it has been validated
recent years have transformed the web from web of content to web of applications and social content thus it has become crucial to be able to tap on this social aspect of the web whenever possible in addition to its content particularly for focused crawling in this paper we present novel profile based focused crawling system for dealing with the increasingly popular social media sharing web sites without assuming any privileged access to the internal private databases of such websites nor any requirement for the existence of apis for the extraction of social data our experiments prove the robustness of our profile based focused crawler as well as significant improvement in harvest ratio compared to breadth first and opic crawlers when crawling the flickr web site for two different topics
the combination of sgml and database technology allows to refine both declarative and navigational access mechanisms for structured document collection with regard to declarative access the user can formulate complex information needs without knowing query language the respective document type definition dtd or the underlying modelling navigational access is eased by hyperlink rendition mechanisms going beyond plain link integrity checking with our approach the database internal representation of documents is configurable it allows for an efficient implementation of operations because dtd knowledge is not needed for document structure recognition we show how the number of method invocations and the cost of parsing can be significantly reduced
information shown on tabletop display can appear distorted when viewed by seated user even worse the impact of this distortion is different depending on the location of the information on the display in this paper we examine how this distortion affects the perception of the basic graphical elements of information visualization shown on displays at various angles we first examine perception of these elements on single display and then compare this to perception across displays in order to evaluate the effectiveness of various elements for use in tabletop and multi display environment we found that the perception of some graphical elements is more robust to distortion than others we then develop recommendations for building data visualizations for these environments
the paper proposes new knowledge representation language called dlp which extends disjunctive logic programming with strong negation by inheritance the addition of inheritance enhances the knowledge modeling features of the language providing natural representation of default reasoning with exceptions declarative model theoretic semantics of dlp is provided which is shown to generalize the answer set semantics of disjunctive logic programs the knowledge modeling features of the language are illustrated by encoding classical nonmonotonic problems in dlp the complexity of dlp is analyzed proving that inheritance does not cause any computational overhead as reasoning in dlp has exactly the same complexity as reasoning in disjunctive logic programming this is confirmed by the existence of an efficient translation from dlp to plain disjunctive logic programming using this translation an advanced kr system supporting the dlp language has been implemented on top of the dlv system and has subsequently been integrated into dlv
the event service of the common object request broker architecture corba is useful in supporting decoupled and asynchronous communication between distributed object components however the specification of the event service standard does not require implementation to provide facilities to guarantee efficient event data delivery consequently applications in which large number of objects need to communicate via an event service channel may suffer from poor performance in this paper generic corba based framework is proposed to tackle this scalability problem two techniques are applied namely event channel federation and load balancing the solution is transparent in the sense that it exports the same idl interface as the original event service we explore three critical dimensions underlying the design of the load balancing algorithm and conduct experiments to evaluate their impact on the overall performance of the framework the results provide some useful insights into the improvement of the scalability of the event service
the development of efficient techniques for transforming massive volumes of remotely sensed hyperspectral data into scientific understanding is critical for space based earth science and planetary exploration although most available parallel processing strategies for information extraction and mining from hyperspectral imagery assume homogeneity in the underlying computing platform heterogeneous networks of computers hnocs have become promising cost effective solution expected to play major role in many on going and planned remote sensing missions in this paper we develop new morphological parallel algorithm for hyperspectral image classification using heterompi an extension of mpi for programming high performance computations on hnocs the main idea of heterompi is to automate and optimize the selection of group of processes that executes heterogeneous algorithm faster than any other possible group in heterogeneous environment in order to analyze the impact of many to one gather communication operations introduced by our proposed algorithm we resort to recently proposed collective communication model the parallel algorithm is validated using two heterogeneous clusters at university college dublin and massively parallel beowulf cluster at nasa's goddard space flight center
this paper presents many core heterogeneous computational platform that employs gals compatible circuit switched on chip network the platform targets streaming dsp and embedded applications that have high degree of task level parallelism among computational kernels the test chip was fabricated in nm cmos consisting of simple small programmable cores three dedicated purpose accelerators and three shared memory modules all processors are clocked by their own local oscillators and communication is achieved through simple yet effective source synchronous communication technique that allows each interconnection link between any two processors to sustain peak throughput of one data word per cycle complete wlan baseband receiver was implemented on this platform it has real time throughput of mbps with all processors running at mhz and and consumes an average mw with mw or dissipated by its interconnection links we can fully utilize the benefit of the gals architecture and by adjusting each processor's oscillator to run at workload based optimal clock frequency with the chip's dual supply voltages set at and the receiver consumes only mw in power reduction measured results of its power consumption on the real chip come within the difference of only compared with the estimated results showing our design to be highly reliable and efficient
despite the fact that global software development gsd is steadily becoming the standard engineering mode in the software industry commercial projects still struggle with how to effectively manage it recent research and our own experiences from numerous gsd projects at capgemini sd indicate that staging the development process with handover checkpoints is promising practice in order to tackle many of the encountered problems in practice in this paper we discuss typical management problems in gsd we describe how handover checkpoints are used at capgemini sd to control and safely manage large gsd projects we show how these handover checkpoints and the use of cohesive and self contained work packages effectively mitigate the discussed management problems we are continuously refining and improving our handover checkpoint approach by applying it within large scale commercial gsd projects we thus believe that the presented results can serve the practitioner as fundament for implementing and customizing handover checkpoints within his own organisation
play on demand is usually regarded as feasible access mode for web content including streaming video web pages and so on web services and some software as service saas applications but not for common desktop applications this paper presents such solution for windows desktop applications based on lightweight virtualization and network transportation technologies which allows user to run her personalized software on any compatible computer across the internet even though they do not exist on local disks of the host in our approach the user's data and their configurations are stored on portable usb device at run time the desktop applications are downloaded from the internet and run in lightweight virtualization environment in which some resource accessing apis such as registry files directories environment variables and the like are intercepted and redirected to the portable device or network as needed because applications are played without installation like streaming media they can be called streaming software moreover to protect software vendors rights access control technologies are used to block any illegal access in the current implementation pp transportation is used as the transport method however our design actually does not rely on pp and another data delivery mechanism like dedicated file server could be employed instead to make the system more predictable this paper describes the design and technical details for this system presents demo application and evaluates it performance the proposed solution is shown to be more efficient in performance and storage capacity than some of the existing solutions based on vm techniques
in an application where sparse matching of feature points is used towards fast scene reconstruction the choice of the type of features to be matched has an important impact on the quality of the resulting model in this work method is presented for quickly and reliably selecting and matching points from three views of scene the selected points are based on epipolar gradients and consist of stable image features relevant to reconstruction then the selected points are matched using edge transfer measure of geometric consistency for point triplets and the edges on which they lie this matching scheme is tolerant to image deformations due to changes in viewpoint models drawn from matches obtained by the proposed technique are shown to demonstrate its usefulness
several approaches to collaborative filtering have been studied but seldom have studies been reported for large several millionusers and items and dynamic the underlying item set is continually changing settings in this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of google news we generate recommendations using three approaches collaborative filtering using minhash clustering probabilistic latent semantic indexing plsi and covisitation counts we combine recommendations from different algorithms using linear model our approach is content agnostic and consequently domain independent making it easily adaptable for other applications and languages with minimal effort this paper will describe our algorithms and system setup in detail and report results of running the recommendations engine on google news
this article presents an approach to identify abstract data types adt and abstract state encapsulations ase also called abstract objects in source code this approach named similarity clustering groups together functions types and variables into adt and ase candidates according to the proportion of features they share the set of features considered includes the context of these elements the relationships to their environment and informal information prototype tool has been implemented to support this approach it has been applied to three systems each between ndash kloc the adts and ases identified by the approach are compared to those identified by software engineers who did not know the proposed approach or other automatic approaches within this case study this approach has been shown to have higher detection quality and to identify in most of the cases more adts and ases than the other techniques in all other cases its detection quality is second best nb this article reports on work in progress on this approach which has evolved since it was presented in the original ase conference paper
competitive native solvers for answer set programming asp perform backtracking search by assuming the truth of literals the choice of literals the heuristic is fundamental for the performance of these systems most of the efficient asp systems employ heuristic based on look ahead that is literal is tentatively assumed and its heuristic value is based on its deterministic consequences however looking ahead is costly operation and indeed look ahead often accounts for the majority of time taken by asp solvers for satisfiability sat radically different approach called look back heuristic proved to be quite successful instead of looking ahead one uses information gathered during the computation performed so far thus looking back in this approach atoms which have been frequently involved in inconsistencies are preferred in this paper we carry over this approach to the framework of disjunctive asp we design number of look back heuristics exploiting peculiarities of asp and implement them in the asp system dlv we compare their performance on collection of hard asp programs both structured and randomly generated these experiments indicate that very basic approach works well outperforming all of the prominent disjunctive asp systems dlv with its traditional heuristic gnt and cmodels on many of the instances considered
the widespread presence of simd devices in today's microprocessors has made compiler techniques for these devices tremendously important one of the most important and difficult issues that must be addressed by these techniques is the generation of the data permutation instructions needed for non contiguous and misaligned memory references these instructions are expensive and therefore it is of crucial importance to minimize their number to improve performance and in many cases enable speedups over scalar codealthough it is often difficult to optimize an isolated data reorganization operation collection of related data permutations can often be manipulated to reduce the number of operations this paper presents strategy to optimize all forms of data permutations the strategy is organized into three steps first all data permutations in the source program are converted into generic representation these permutations can originate from vector accesses to non contiguous and misaligned memory locations or result from compiler transformations second an optimization algorithm is applied to reduce the number of data permutations in basic block by propagating permutations across statements and merging consecutive permutations whenever possible the algorithm can significantly reduce the number of data permutations finally code generation algorithm translates generic permutation operations into native permutation instructions for the target platform experiments were conducted on various kinds of applications the results show that up to of the permutation instructions are eliminated and as result the average performance improvement is on vmx and on sse for several applications near perfect speedups have been achieved on both platforms
this paper presents an approach that uses special purpose rbac constraints to base certain access control decisions on context information in our approach context constraint is defined as dynamic rbac constraint that checks the actual values of one or more contextual attributes for predefined conditions if these conditions are satisfied the corresponding access request can be permitted accordingly conditional permission is an rbac permission which is constrained by one or more context constraints we present an engineering process for context constraints that is based on goal oriented requirements engineering techniques and describe how we extended the design and implementation of an existing rbac service to enable the enforcement of context constraints with our approach we aim to preserve the advantages of rbac and offer an additional means for the definition and enforcement of fine grained context dependent access control policies
publish subscribe systems are used increasingly often as communication mechanism in loosely coupled distributed applications with their gradual adoption in mission critical areas it is essential that systems are subjected to rigorous performance analysis before they are put into production however existing approaches to performance modeling and analysis of publish subscribe systems suffer from many limitations that seriously constrain their practical applicability in this paper we present set of generalized and comprehensive analytical models of publish subscribe systems employing different peer to peer and hierarchical routing schemes the proposed analytical models address the major limitations underlying existing work in this area and are the first to consider all major performance relevant system metrics including the expected broker and link utilization the expected notification delay the expected time required for new subscriptions to become fully active as well as the expected routing table sizes and message rates to illustrate our approach and demonstrate its effectiveness and practicality we present case study showing how our models can be exploited for capacity planning and performance prediction in realistic scenario
speed up techniques that exploit given node coordinates have proven useful for shortest path computations in transportation networks and geographic information systems to facilitate the use of such techniques when coordinates are missing from some or even all of the nodes in network we generate artificial coordinates using methods from graph drawing experiments on large set of german train timetables indicate that the speed up achieved with coordinates from our drawings is close to that achieved with the true coordinates and in some special cases even better
this paper proposes novel method for phrase based statistical machine translation based on the use of pivot language to translate between languages and with limited bilingual resources we bring in third language called the pivot language for the language pairs and there exist large bilingual corpora using only and bilingual corpora we can build translation model for the advantage of this method lies in the fact that we can perform translation between and even if there is no bilingual corpus available for this language pair using bleu as metric our pivot language approach significantly outperforms the standard model trained on small bilingual corpus moreover with small bilingual corpus available our method can further improve translation quality by using the additional and bilingual corpora
media spaces and videoconference systems are beneficial for connecting separated co workers and providing rich contextual information however image sharing communication tools may also touch on sensitive spots of the human psyche related to personal perceived image issues eg appearance self image self presentation and vanity we conducted two user studies to examine the impact of self image concerns on the use of media spaces and videoconference systems our results suggest that personal perceived image concerns have considerable impact on the comfort level of users and may hinder effective communication we also found that image filtering techniques can help users feel more comfortable our results revealed that distortion filters which are frequently cited to help preserve privacy do not tend to be the ones preferred by users instead users seemed to favor filters that make subtle changes to their appearance or in some instances they preferred to use surrogate instead
xml documents have recently become ubiquitous because of their varied applicability in number of applications classification is an important problem in the data mining domain but current classification methods for xml documents use ir based methods in which each document is treated as bag of words such techniques ignore significant amount of information hidden inside the documents in this paper we discuss the problem of rule based classification of xml data by using frequent discriminatory substructures within xml documents such technique is more capable of finding the classification characteristics of documents in addition the technique can also be extended to cost sensitive classification we show the effectiveness of the method with respect to other classifiers we note that the methodology discussed in this paper is applicable to any kind of semi structured data
in this paper we present new algorithm named turbosyn for fpga synthesis with retiming and pipelining to minimizethe clock period for sequential circuitsfor target clockperiod since pipelining can eliminate all critical paths but not critical loops we concentrate on fpga synthesis toeliminate the critical loopswe combine the combinationalfunctional decomposition technique with retiming to performthe sequential functional decomposition and incorporate itin the label computation of turbomap to eliminate allcritical loopsthe results show significant improvementover the state of the art fpga mapping and resynthesis algorithms times reduction on the clock period moreover we develop novel approach for positive loop detectionwhich leads to over times speedup of the algorithmas result turbosyn can optimize sequential circuits ofover gates and flipflops in reasonable time
real time database management systems rt dbms have the necessary characteristics for providing efficient support to develop applications in which both data and transactions have temporal constraints however in the last decade new applications were identified and are characterized by large geographic distribution high heterogeneity lack of global control partial failures and lack of safety besides they need to manage large data volumes with real time constraints scheduling algorithms should consider transactions with soft deadlines and the concurrency control protocols should allow conflicting transactions to execute in parallel the last ones should be based in their requirements which are specified through both quality of services functions and performance metrics in this work method to model and develop applications that execute in open and unpredictable environments is proposed based on this model it is possible to perform analysis and simulations of systems to guide the decision making process and to identify solutions for improving it for validating the model case study considering the application domain of sensors network is discussed
in this paper we describe new via configurable routing architecture which shows much better throughput and performance than the previous structures we demonstrate how to construct single via mask fabric to reduce the mask cost further and we analyze the penalties which it incurs to solve the routability problem commonly existing in fabric based designs an efficient white space allocation and an incremental cell movement scheme are suggested which help to provide fast design convergence and early prediction of circuit's mappability to given fabric
array redistribution is usually needed for more efficiently executing data parallel program on distributed memory multicomputers to minimize the redistribution data transfer cost processor mapping techniques were proposed to reduce the amount of redistributed data elements theses techniques demand that the beginning data elements on processor not be redistributed in the redistribution on the other hand for satisfying practical computation needs programmer may require other data elements to be un redistributed localized in the redistribution in this paper we propose flexible processor mapping technique for the block cyclic redistribution to allow the programmer to localize the required data elements in the redistribution we also present an efficient redistribution method for the redistribution employing our proposed technique the data transfer cost reduction and system performance improvement for the redistributions with data localization are analyzed and presented in our experimental results
consider data warehouses as large data repositories queried for analysis and data mining in variety of application contexts query over such data may take large amount of time to be processed in regular pc consider partitioning the data into set of pcs nodes with either parallel database server or any database server at each node and an engine independent middleware nodes and network may even not be fully dedicated to the data warehouse in such scenario care must be taken for handling processing heterogeneity and availability so we study and propose efficient solutions for this we concentrate on three main contributions performance wise index measuring relative performance replication degree flexible chunk wise organization with on demand processing these contributions extend the previous work on de clustering and replication and are generic in the sense that they can be applied in very different contexts and with different data partitioning approaches we evaluate their merits with prototype implementation of the system
some of the most difficult questions to answer when designing distributed application are related to mobility what information to transfer between sites and when and how to transfer it network transparent distribution the property that program's behavior is independent of how it is partitioned among sites does not directly address these questions therefore we propose to extend all language entities with network behavior that enables efficient distributed programming by giving the programmer simple and predictable control over network communication patterns in particular we show how to give objects an arbitrary mobility behavior that is independent of the objects definition in this way the syntax and semantics of objects are the same regardless of whether they are used as stationary servers mobile agents or simply as caches these ideas have been implemented in distributed oz concurrent object oriented language that is state aware and has dataflow synchronization we prove that the implementation of objects in distributed oz is network transparent to satisfy the predictability condition the implementation avoids forwarding chains through intermediate sites the implementation is an extension to the publicly available dfki oz system
many spatiotemporal applications store moving object data in the form of trajectories various recent works have addressed interesting queries on trajectorial data mainly focusing on range queries and nearest neighbor queries here we examine another interesting query the time relaxed spatiotemporal trajectory join trstj which effectively finds groups of moving objects that have followed similar movements in different times we first attempt to address the trstj problem using symbolic representation algorithm which we have recently proposed for trajectory joins however we show experimentally that this solution produces false positives that grow rapidly with the increase of the problem size as result it is inefficient for trstj queries as it leads to large query time overhead in order to improve query performance we propose two important heuristics that turn the symbolic represenation approach effective for trstj queries our first improvement allows the use of multiple origins when processing strings representing trajectories the experimental evaluation shows that the multiple origin approach drastically reduces query performance we then present divide and conquer approach to further reduce false positives through symbolic class separation the proposed solutions can be combined together which leads to even better query performance we present an experimental study revealing the advantages of using these approaches for solving time relaxed spatiotemporal trajectory join queries
divide and conquer programs are easily parallelized by letting the programmer annotate potential parallelism in the form of spawn and sync constructs to achieve efficient program execution the generated work load has to be balanced evenly among the available cpus for single cluster systems random stealing rs is known to achieve optimal load balancing however rs is inefficient when applied to hierarchical wide area systems where multiple clusters are connected via wide area networks wans with high latency and low bandwidthin this paper we experimentally compare rs with existing load balancing strategies that are believed to be efficient for multi cluster systems random pushing and two variants of hierarchical stealing we demonstrate that in practice they obtain less than optimal results we introduce novel load balancing algorithm cluster aware random stealing crs which is highly efficient and easy to implement crs adapts itself to network conditions and job granularities and does not require manually tuned parameters although crs sends more data across the wans it is faster than its competitors for out of test applications with various wan configurations it has at most overhead in run time compared to rs on single large cluster even with high wide area latencies and low wide area bandwidths these strong results suggest that divide and conquer parallelism is useful model for writing distributed supercomputing applications on hierarchical wide area systems
counterexample guided abstraction refinement cegar is key technique for the verification of computer programs grumberg et al developed cegar based algorithm for the modal calculus there every abstract state is split in refinement step in this paper the work of grumberg et al is generalized by presenting new cegar based algorithm for the calculus it is based on more expressive abstract model and applies refinement only locally at single abstract state ie the lazy abstraction technique for safety properties is adapted to the calculus furthermore it separates refinement determination from the valued based model checking three different heuristics for refinement determination are presented and illustrated
trust management systems are frameworks for authorization in modern distributed systems allowing remotely accessible resources to be protected by providers by allowing providers to specify policy and access requesters to possess certain access rights trust management automates the process of determining whether access should be allowed on the basis of policy rights and an authorization semantics in this paper we survey modern state of the art in trust management authorization focusing on features of policy and rights languages that provide the necessary expressiveness for modern practice we characterize systems in light of generic structure that takes into account components of practical implementations we emphasize systems that have formal foundation since security properties of them can be rigorously guaranteed underlying formalisms are reviewed to provide necessary background
we show how range of role based access control rbac models may be usefully represented as constraint logic programs executable logical specifications the rbac models that we define extend the standard rbac models that are described by sandhu et al and enable security administrators to define range of access policies that may include features like denials of access and temporal authorizations that are often useful in practice but which are not widely supported in existing access control models representing access policies as constraint logic programs makes it possible to support certain policy options constraint checks and administrator queries that cannot be represented by using related methods like logic programs representing an access control policy as constraint logic program also enables access requests and constraint checks to be efficiently evaluated
this paper contributes to growing body of design patterns in interaction design for cooperative work while also describing how to go from field studies to design patterns it focuses on sociable face to face situations the patterns are based on field studies and design work in three sociable settings where desirable use qualities were identified and translated into forces in three design patterns for controlling information visibility on the basis of the patterns the design of multiple device multimedia platform is described it is shown that desirable qualities of systems in use can be utilised as forces in patterns which means that traditional qualitative research is highly valuable when documenting design knowledge in patterns three classes of interaction design patterns are identified environments for interactions means for interaction and interfaces for interaction these classes describe types of patterns within hierarchical model of interaction design
the wireless network community has become increasingly aware of the benefits of data driven link estimation and routing as compared with beacon based approaches but the issue of biased link sampling bls has not been well studied even though it affects routing convergence in the presence of network and environment dynamics focusing on traffic induced dynamics we examine the open unexplored question of how serious the bls issue is and how to effectively address it when the routing metric etx is used for wide range of traffic patterns and network topologies and using both node oriented and network wide analysis and experimentation we discover that the optimal routing structure remains quite stable even though the properties of individual links and routes vary significantly as traffic pattern changes in cases where the optimal routing structure does change data driven link estimation and routing is either guaranteed to converge to the optimal structure or empirically shown to converge to close to optimal structure these findings provide the foundation for addressing the bls issue in the presence of traffic induced dynamics and suggest approaches other than existing ones these findings also demonstrate that it is possible to maintain an optimal stable routing structure despite the fact that the properties of individual links and paths vary in response to network dynamics
mobile computation in which executing computations can move from one physical computing device to another is recurring theme from os process migration to language level mobility to virtual machine migration this article reports on the design implementation and verification of overlay networks to support reliable communication between migrating computations in the nomadic pict project we define two levels of abstraction as calculi with precise semantics low level nomadic pi calculus with migration and location dependent communication and high level calculus that adds location independent communication implementations of location independent communication as overlay networks that track migrations and forward messages can be expressed as translations of the high level calculus into the low we discuss the design space of such overlay network algorithms and define three precisely as such translations based on the calculi we design and implement the nomadic pict distributed programming language to let such algorithms and simple applications above them to be quickly prototyped we go on to develop the semantic theory of the nomadic pi calculi proving correctness of one example overlay network this requires novel equivalences and congruence results that take migration into account and reasoning principles for agents that are temporarily immobile eg waiting on lock elsewhere in the system the whole stands as demonstration of the use of principled semantics to address challenging system design problems
choosing good variable order is crucial for making symbolic state space generation algorithms truly efficient one such algorithm is the mdd based saturation algorithm for petri nets implemented in smart whose efficiency relies on exploiting event locality this paper presents novel static ordering heuristic that considers place invariants of petri nets in contrast to related work we use the functional dependencies encoded by invariants to merge decision diagram variables rather than to eliminate them we prove that merging variables always yields smaller mdds and improves event locality while eliminating variables may increase mdd sizes and break locality combining this idea of merging with heuristics for maximizing event locality we obtain an algorithm for static variable order which outperforms competing approaches regarding both time efficiency and memory efficiency as we demonstrate by extensive benchmarking
in this paper we consider the problem of processor allocation on mesh based multiprocessor systems we employ the idea of using migration to minimize fragmentation and the overall processing time of the tasks in our schemes we consider the use of task migration whenever required to improve the problem of fragmentation to this end we propose three efficient schemes to improve the performance of first fit allocation strategies commonly used in practice the first scheme called the first fit mesh bifurcation ffmb scheme attempts to start the search for free submesh from either the bottom left corner or the top left corner of the mesh so as to reduce the amount of fragmentation in the mesh the next two schemes called the online dynamic compaction single corner odc sc and online dynamic compaction four corners odc fc schemes use task migration to improve the performance of existing submesh allocation strategies we perform rigorous simulation experiments based on practical workloads as reported in the literature to quantify all our proposed schemes and compare them against standard schemes existing in the literature based on the results we make clear recommendations on the choice of the strategies
data exchange and virtual data integration have been the subject of several investigations in the recent literature at the same time the notion of peer data management has emerged as powerful abstraction of many forms of flexible and dynamic data centere ddistributed systems although research on the above issues has progressed considerably in the last years clear understanding on how to combine data exchange and data integration in peer data management is still missing this is the subject of the present paper we start our investigation by first proposing novel framework for peer data exchange showing that it is generalization of the classical data exchange setting we also present algorithms for all the relevant data exchange tasks and show that they can all be done in polynomial time with respect to data complexity based on the motivation that typical mappings and integrity constraints found in data integration are not captured by peer data exchange we extend the framework to incorporate these features one of the main difficulties is that the constraints of this new class are not amenable to materialization we address this issue by resorting to suitable combination of virtual and materialized data exchange showing that the resulting framework is generalization of both classical data exchange and classical data integration and that the new setting incorporates the most expressive types of mapping and constraints considered in the two contexts finally we present algorithms for all the relevant data management tasks also in the new setting and show that again their data complexity is polynomial
radiance transfer represents how generic source lighting is shadowed and scattered by an object to produce view dependent appearance we generalize by rendering transfer at two scales macro scale is coarsely sampled over an object's surface providing global effects like shadows cast from an arm onto body meso scale is finely sampled over small patch to provide local texture low order spherical harmonics represent low frequency lighting dependence for both scales to render coefficient vector representing distant source lighting is first transformed at the macro scale by matrix at each vertex of coarse mesh the resulting vectors represent spatially varying hemisphere of lighting incident to the meso scale function called radiance transfer texture rtt then specifies the surface's meso scale response to each lighting basis component as function of spatial index and view direction finally dot product of the macro scale result vector with the vector looked up from the rtt performs the correct shading integral we use an id map to place rtt samples from small patch over the entire object only two scalars are specified at high spatial resolution results show that bi scale decomposition makes preprocessing practical and efficiently renders self shadowing and interreflection effects from dynamic low frequency light sources at both scales
we present multiple pass streaming algorithms for basic clustering problem for massive data sets if our algorithm is allotted passes it will produce an approximation with error at most epsilon using otilde epsilon bits of memory the most critical resource for streaming computation we demonstrate that this tradeoff between passes and memory allotted is intrinsic to the problem and model of computation by proving lower bounds on the memory requirements of any pass randomized algorithm that are nearly matched by our upper bounds to the best of our knowledge this is the first time nearly matching bounds have been proved for such an exponential tradeoff for randomized computationin this problem we are given set of points drawn randomly according to mixture of uniform distributions and wish to approximate the density function of the mixture the points are placed in datastream possibly in adversarial order which may only be read sequentially by the algorithm we argue that this models among others the datastream produced by national census of the incomes of all citizens
we present unified feature representation of pointclouds and apply it to face recognition the representation integrates local and global geometrical cues in single compact representation which makes matching probe to large database computationally efficient the global cues provide geometrical coherence for the local cues resulting in better descriptiveness of the unified representation multiple rank tensors scalar features are computed at each point from its local neighborhood and from the global structure of the pointcloud forming multiple rank tensor fields the pointcloud is then represented by the multiple rank tensor fields which are invariant to rigid transformations each local tensor field is integrated with every global field in histogram which is indexed by local field in one dimension and global field in the other dimension finally pca coefficients of the histograms are concatenated into single feature vector the representation was tested on frgc data set and achieved identification and verification rate at far
we present method for learning model of human body shape variation from corpus of range scans our model is the first to capture both identity dependent and pose dependent shape variation in correlated fashion enabling creation of variety of virtual human characters with realistic and non linear body deformations that are customized to the individual our learning method is robust to irregular sampling in pose space and identity space and also to missing surface data in the examples our synthesized character models are based on standard skinning techniques and can be rendered in real time
unconstrained consumer photos pose great challenge for content based image retrieval unlike professional images or domain specific images consumer photos vary significantly more often than not the objects in the photos are ill posed occluded and cluttered with poor lighting focus and exposure in this paper we propose cascading framework for combining intra image and inter class similarities in image retrieval motivated from probabilistic bayesian principles support vector machines are employed to learn local view based semantics based on just in time fusion of color and texture features new detection driven block based segmentation algorithm is designed to extract semantic features from images the detection based indexes also serve as input for support vector learning of image classifiers to generate class relative indexes during image retrieval both intra image and inter class similarities are combined to rank images experiments using query by example on genuine heterogeneous consumer photos with semantic queries show that the combined matching approach is better than matching with single index it also outperformed the method of combining color and texture features by in average precision
compiler based auto parallelization is much studied area yet has still not found wide spread application this is largely due to the poor exploitation of application parallelism subsequently resulting in performance levels far below those which skilled expert programmer could achieve we have identified two weaknesses in traditional parallelizing compilers and propose novel integrated approach resulting in significant performance improvements of the generated parallel code using profile driven parallelism detection we overcome the limitations of static analysis enabling us to identify more application parallelism and only rely on the user for final approval in addition we replace the traditional target specific and inflexible mapping heuristics with machine learning based prediction mechanism resulting in better mapping decisions while providing more scope for adaptation to different target architectures we have evaluated our parallelization strategy against the nas and spec omp benchmarks and two different multi core platforms dual quad core intel xeon smp and dual socket qs cell blade we demonstrate that our approach not only yields significant improvements when compared with state of the art parallelizing compilers but comes close to and sometimes exceeds the performance of manually parallelized codes on average our methodology achieves of the performance of the hand tuned openmp nas and spec parallel benchmarks on the intel xeon platform and gains significant speedup for the ibm cell platform demonstrating the potential of profile guided and machine learning based parallelization for complex multi core platforms
we present new variational method for multi view stereovision and non rigid three dimensional motion estimation from multiple video sequences our method minimizes the prediction error of the shape and motion estimates both problems then translate into generic image registration task the latter is entrusted to global measure of image similarity chosen depending on imaging conditions and scene properties rather than integrating matching measure computed independently at each surface point our approach computes global image based matching score between the input images and the predicted images the matching process fully handles projective distortion and partial occlusions neighborhood as well as global intensity information can be exploited to improve the robustness to appearance changes due to non lambertian materials and illumination changes without any approximation of shape motion or visibility moreover our approach results in simpler more flexible and more efficient implementation than in existing methods the computation time on large datasets does not exceed thirty minutes on standard workstation finally our method is compliant with hardware implementation with graphics processor units our stereovision algorithm yields very good results on variety of datasets including specularities and translucency we have successfully tested our motion estimation algorithm on very challenging multi view video sequence of non rigid scene
interfaces based on recognition technologies are used extensively in both the commercial and research worlds but recognizers are still error prone and this results in human performance problems brittle dialogues and other barriers to acceptance and utility of recognition systems interface techniques specialized to recognition systems can help reduce the burden of recognition errors but building these interfaces depends on knowledge about the ambiguity inherent in recognition we have extended user interface toolkit in order to model and to provide structured support for ambiguity at the input event level this makes it possible to build re usable interface components for resolving ambiguity and dealing with recognition errors these interfaces can help to reduce the negative effects of recognition errors by providing these components at toolkit level we make it easier for application writers to provide good support for error handling further with this robust support we are able to explore new types of interfaces for resolving more varied range of ambiguity
we present an efficient approach for end to end out of core construction and interactive inspection of very large arbitrary surface models the method tightly integrates visibility culling and out of core data management with level of detail framework at preprocessing time we generate coarse volume hierarchy by binary space partitioning the input triangle soup leaf nodes partition the original data into chunks of fixed maximum number of triangles while inner nodes are discretized into fixed number of cubical voxels each voxel contains compact direction dependent approximation of the appearance of the associated volumetric sub part of the model when viewed from distance the approximation is constructed by visibility aware algorithm that fits parametric shaders to samples obtained by casting rays against the full resolution dataset at rendering time the volumetric structure maintained off core is refined and rendered in front to back order exploiting vertex programs for gpu evaluation of view dependent voxel representations hardware occlusion queries for culling occluded subtrees and asynchronous for detecting and avoiding data access latencies since the granularity of the multiresolution structure is coarse data management traversal and occlusion culling cost is amortized over many graphics primitives the efficiency and generality of the approach is demonstrated with the interactive rendering of extremely complex heterogeneous surface models on current commodity graphics platforms
paraphrasing van rijsbergen the time is ripe for another attempt at using natural language processing nlp for information retrieval ir this paper introduces my dissertation study which will explore methods for integrating modern nlp with state of the art ir techniques in addition to text will also apply retrieval to conversational speech data which poses unique set of considerations in comparison to text greater use of nlp has potential to improve both text and speech retrieval
on line analytical processing olap is technology basically created to provide users with tools in order to explore and navigate into data cubes unfortunately in huge and sparse data exploration becomes tedious task and the simple user's intuition or experience does not lead to efficient results in this paper we propose to exploit the results of the multiple correspondence analysis mca in order to enhance data cube representations and make them more suitable for visualization and thus easier to analyze our approach addresses the issues of organizing data in an interesting way and detects relevant facts our purpose is to help the interpretation of multidimensional data by efficient and simple visual effects to validate our approach we compute its efficiency by measuring the quality of resulting multidimensional data representations in order to do so we propose an homogeneity criterion to measure the visual relevance of data representations this criterion is based on the concept of geometric neighborhood and similarity between cells experimental results on real data have shown the interest of using our approach on sparse data cubes
there is growing demand for provisioning of different levels of quality of service qos on scalable web servers to meet changing resource availability and to satisfy different client requirements in this paper we investigate the problem of providing proportional qos differentiation with respect to response time on web servers we first present processing rate allocation scheme based on the foundations of queueing theory it provides different processing rates to requests of different client classes so as to achieve the differentiation objective at application level process is used as the resource allocation principal for achieving processing rates on apache web servers we design and implement an adaptive process allocation approach guided by the queueing theoretical rate allocation scheme on an apache server this application level implementation however shows weak qos predictability because it does not have fine grained control over the consumption of resources that the kernel consumes and hence the processing rate is not strictly proportional to the number of processes allocated we then design feedback controller and integrate it with the queueing theoretical approach it adjusts process allocations according to the difference between the target response time and the achieved response time using proportional integral derivative controller experimental results demonstrate that this integrated approach can enable web servers to provide robust proportional response time differentiation
we present hybridpointing technique that lets users easily switch between absolute and relative pointing with direct input device such as pen our design includes new graphical element the trailing widget which remains close at hand but does not interfere with normal cursor operation the use of visual feedback to aid the user's understanding of input state is discussed and several novel visual aids are presented an experiment conducted on large wall sized display validates the benefits of hybridpointing under certain conditions we also discuss other situations in which hybridpointing may be useful finally we present an extension to our technique that allows for switching between absolute and relative input in the middle of single drag operation
many adaptive routing algorithms have been proposed for wormhole routed interconnection networks comparatively little work however has been done on determining how the selection function routing policy affects the performance of an adaptive routing algorithm in this paper we present detailed simulation study of various selection functions for fully adaptive wormhole routing on two dimensional meshes the simulation results show that the choice of selection function has significant effect on the average message latency in addition it is possible to find single selection function that exhibits excellent performance across wide range of traffic patterns network sizes and number of virtual channels thus well chosen selection function for an adaptive routing algorithm can lead to consistently better performance than an arbitrary selection function one of the selection functions considered is theoretically optimal selection function ieee trans comput october we show that although theoretically optimal the actual performance of the optimal selection function is not always best an explanation and interpretation of the results is provided
imperative and object oriented programs make ubiquitous use of shared mutable objects updating shared object can and often does transgress boundary that was supposed to be established using static constructs such as class with private fields this paper shows how auxiliary fields can be used to express two state dependent encapsulation disciplines ownership kind of separation and friendship kind of sharing methodology is given for specification and modular verification of encapsulated object invariants and shown sound for class based language as an example the methodology is used to specify iterators which are problematic for previous ownership systems
energy consumption is of significant concern in battery operated embedded systems in the processors of such systems the instruction cache consumes significant fraction of the total energy one of the most popular methods to reduce the energy consumption is to shut down idle cache banks however we observe that operating idle cache banks at reduced voltage frequency level along with the active banks in pipelined manner can potentially achieve even better energy savings in this paper we propose novel dvs based pipelined reconfigurable instruction memory hierarchy called prim canonical example of our proposed prim consists of four cache banks two of these cache banks can be configured at runtime to operate at lower voltage and frequency levels than that of the normal cache instruction fetch throughput is maintained by pipelining the accesses to the low voltage banks we developed profile driven compilation framework that analyzes applications and inserts the appropriate cache reconfiguration points our experimental results show that prim can significant reduce the energy consumption for popular embedded benchmarks with minimal performance overhead we obtained and energy savings for aggressive and conservative vdd settings respectively at the expense of performance overhead
software defect prediction is important for reducing test times by allocating testing resources effectively in terms of predicting the defects in software naive bayes outperforms wide range of other methods however naive bayes assumes the independence and equal importance of attributes in this work we analyze these assumptions of naive bayes using public software defect data from nasa our analysis shows that independence assumption is not harmful for software defect data with pca pre processing our results also indicate that assigning weights to static code attributes may increase the prediction performance significantly while removing the need for feature subset selection
this paper presents fundamentally new approach to integrating local decisions from various nodes and efficiently routing data in sensor networks by classifying the nodes in the sensor field as hot or cold in accordance with whether or not they sense the target we are able to concentrate on smaller set of nodes and gear the routing of data to and from the sink to fraction of the nodes that exist in the network the introduction of this intermediary step is fundamentally new and allows for efficient and meaningful fusion and routing this is made possible through the use of novel markov random field mrf approach which to the best of our knowledge has never been applied to sensor networks in combination with maximum posteriori probability map stochastic relaxation tools to flag out the hot nodes in the network and to optimally combine their data and decisions towards an integrated and collaborative global decision fusion this global decision supersedes all local decisions and provides the basis for efficient use of the sensed data because of the mrf local nature nodes need not see or interact with other nodes in the sensor network beyond their immediate neighborhood which can either be defined in terms of distance between nodes or communication connectivity hence adding to the flexibility of dealing with irregular and varying sensor topologies and also minimizing node power usage and providing for easy scalability the routing of the hot nodes data is confined to cone of nodes and power constraints are taken into account we also use the found location of the centroid of the hot nodes over time to track the movement of the target this is achieved by using the segmentation at time as an initial state in the stochastic map relaxation at time dt
pen gesture interfaces have difficulty supporting arbitrary multiple stroke selections because lifting the pen introduces ambiguity as to whether the next stroke should add to the existing selection or begin new one we explore and evaluate techniques that use non preferred hand button or touchpad to phrase together one or more independent pen strokes into unitary multi stroke gesture we then illustrate how such phrasing techniques can support multiple stroke selection gestures with tapping crossing lassoing disjoint selection circles of exclusion selection decorations and implicit grouping operations these capabilities extend the expressiveness of pen gesture interfaces and suggest new directions for multiple stroke pen input techniques
in this paper we propose an image completion algorithm which takes advantage of the countless number of images available on internet photo sharing sites to replace occlusions in an input image the algorithm automatically selects the most suitable images from database of downloaded images and seamlessly completes the input image using the selected images with minimal user intervention experimental results on input images captured at various locations and scene conditions demonstrate the effectiveness of the proposed technique in seamlessly reconstructing user defined occlusions
we extend distributed database query optimization techniques to support database programming language language much richer than relational query languages with the richness comes difficulties eg how to recognize joins and how to handle aliases in this paper we describe our techniques dataflow analysis abstract evaluation partial evaluation and rewriting also we overview the algorithm that uses these techniques
this paper considers new security protocol paradigm whereby principals negotiate and on the fly generate security protocols according to their needs when principals wish to interact then rather than offering each other fixed menu of known protocols they negotiate and possibly with the collaboration of other principles synthesise new protocol that is tailored specifically to their current security environment and requirements this approach provides basis for autonomic security protocols such protocols are self configuring since only principal assumptions and protocol goals need to be priori configured the approach has the potential to survive security compromises that can be modelled as changes in the beliefs of the principals compromise of key or change in the trust relationships between principals can result in principal self healing and synthesising new protocol to survive the event
bilingual documentation has become common phenomenon in official institutions and private companies in this scenario the categorization of bilingual text is useful tool in this paper different approaches will be proposed to tackle this bilingual classification task on the one hand three finite state transducer algorithms from the grammatical inference framework will be presented on the other hand naive combination of smoothed gram models will be introduced to evaluate the performance of bilingual classifiers two categorized bilingual corpora of different complexity were considered experiments in limited domain task show that all the models obtain similar results however results on more open domain task denote the supremacy of the naive approach
code generation for embedded processors opens up the possibility for several performance optimization techniques that have been ignored by traditional compilers due to compilation time constraints we present techniques that take into account the parameters of the data caches for organizing scalar and array variables declared in embedded code into memory with the objective of improving data cache performance we present techniques for clustering variables to minimize compulsory cache misses and for solving the memory assignment problem to minimize conflict cache misses our experiments with benchmark code kernels from dsp and other domains on the cw embedded processor from lsi logic indicate significant improvements in data cache performance by the application of our memory organization technique
one purpose of software metrics is to measure the quality of programs the results can be for example used to predict maintenance costs or improve code quality an emerging view is that if software metrics are going to be used to improve quality they must help in finding code that should be refactored often refactoring or applying design pattern is related to the role of the class to be refactored in client based metrics project gives the class context these metrics measure how class is used by other classes in the context we present new client based metric lcic lack of coherence in clients which analyses if the class being measured has coherent set of roles in the program interfaces represent the roles of classes if class does not have coherent set of roles it should be refactored or new interface should be defined for the class we have implemented tool for measuring the metric lcic for java projects in the eclipse environment we calculated lcic values for classes of several open source projects we compare these results with results of other related metrics and inspect the measured classes to find out what kind of refactorings are needed we also analyse the relation of different design patterns and refactorings to our metric our experiments reveal the usefulness of client based metrics to improve the quality of code
ranked queries return the top objects of database according to preference function we present and evaluate experimentally and theoretically core algorithm that answers ranked queries in an efficient pipelined manner using materialized ranked views we use and extend the core algorithm in the described prefer and merge systems prefer precomputes set of materialized views that provide guaranteed query performance we present an algorithm that selects near optimal set of views under space constraints we also describe multiple optimizations and implementation aspects of the downloadable version of prefer then we discuss merge which operates at metabroker and answers ranked queries by retrieving minimal number of objects from sources that offer ranked queries speculative version of the pipelining algorithm is described
the geographic routing is an ideal approach to realize pointto point routing in wireless sensor networks because packets can be delivered by only maintaining small set of neighbors physical positions the geographic routing assumes that packet can be moved closer to the destination in the network topology if it is moved geographically closer to the destination in the physical space this assumption however only holds in an ideal model where uniformly distributed nodes communicate with neighbors through wireless channels with perfect reception because this model oversimplifies the spatial complexity of wireless sensor network the geographic routing may often lead packet to the local minimum or low quality route unlike the geographic forwarding the etx embedding proposed in this paper can accurately encode both network's topological structure and channel quality to small size nodes virtual coordinates which makes it possible for greedy forwarding to guide packet along an optimal routing path our performance evaluation based on both the mica sensor platform and tossim simulator shows that the greedy forwarding based on etx embedding outperforms previous geographic routing approaches
soft real time systems can tolerate some occasional deadline misses this feature provides unique opportunity to reduce system's energy consumption in this paper we study the system with firm deadline popular model for soft real time systems it basically requires at least successful completions in any consecutive executions our goal is to design such system with dual supply voltages for energy efficiency to reach this goal we first propose an on line greedy deterministic scheduler that provides the firm guarantee with the provably minimum energy consumption we then develop novel exact method to compute the scheduler's average energy consumption per iteration this leads us to the numerical solution to the voltage set up problem which seeks for the values of the two supply voltages to achieve the most energy efficiency with firm guarantee simulation shows that dual voltage system can reduce significant amount of energy over single voltage system our numerical method finds the best voltage set ups in seconds while it takes hours to obtain almost identical solutions by simulation
in this paper we describe how the memory management mechanisms of the intel iapx are used to implement the visibility rules of ada at any point in the execution of an ada reg program on the the program has protected address space that corresponds exactly to the program's accessibility at the corresponding point in the program's source this close match of architecture and language did not occur because the was designed to execute ada mdash it was not rather both ada and the are the result of very similar design goals to illustrate this point we compare in their support for ada the memory management mechanisms of the to those of traditional computers the most notable differences occur in heap space management and multitasking with respect to the former we describe degree of hardware software cooperation that is not typical of other systems in the latter area we show how ada's view of sharing is the same as the but differs totally from the sharing permitted by traditional systems description of these differences provide some insight into the problems of implementing an ada compiler for traditional architecture
the diffusion of mobile devices in the working landscape is promoting collaboration across time and space following through this development we investigate opportunities for improving awareness in mobile environments with view to enable collaboration under power constraints and transitory network disconnections we elaborate in particular on synchronous cscw and expose with it significant details of group awareness while we contribute protocol for awareness support over large areas that strikes balance between energy consumption and notification time to avoid user disruption this protocol notifies awareness information in multicast fashion while the bandwidth is allocated dynamically among notifications and data requests thus minimizing the time needed by each one of them and ensuring the isochronous delivery of information to all clients the efficiency and scalability of our protocol are evaluated with simulation experiments whereby we compare various notification schemes and finally choose one that changes dynamically over time
the advances in wireless and mobile computing allow mobile user to perform wide range of aplications once limited to non mobile hard wired computing environments as the geographical position of mobile user is becoming more trackable users need to pull data which are related to their location perhaps seeking information about unfamiliar places or local lifestyle data in these requests location attribute has to be identified in order to provide more efficient access to location dependent data whose value is determined by the location to which it is related local yellow pages local events and weather information are some of the examples of these data in this paper we give formalization of location relatedness in queries we differentiate location dependence and location awareness and provide thorough examples to support our approach
power consumption is major factor that limits the performance of computers we survey the ldquo state of the art rdquo in techniques that reduce the total power consumed by microprocessor system over time these techniques are applied at various levels ranging from circuits to architectures architectures to system software and system software to applications they also include holistic approaches that will become more important over the next decade we conclude that power management is multifaceted discipline that is continually expanding with new techniques being developed at every level these techniques may eventually allow computers to break through the ldquo power wall rdquo and achieve unprecedented levels of performance versatility and reliability yet it remains too early to tell which techniques will ultimately solve the power problem
automatic localisation of correspondences for the construction of statistical shape models from examples has been the focus of intense research during the last decade several algorithms are available and benchmarking is needed to rank the different algorithms prior work has argued that the quality of the models produced by the algorithms can be evaluated by measuring compactness generality and specificity in this paper severe problems with these standard measures are analysed both theoretically and experimentally both on natural and synthetic datasets we also propose that ground truth correspondence measure gcm is used for benchmarking and in this paper benchmarking is performed on several state of the art algorithms using seven real and one synthetic dataset
we present core calculus with two of key constructs for parallelism namely async and finish our calculus forms convenient basis for type systems and static analyses for languages with async finish parallelism and for tractable proofs of correctness for example we give short proof of the deadlock freedom theorem of saraswat and jagadeesan our main contribution is type system that solves the open problem of context sensitive may happen in parallel analysis for languages with async finish parallelism we prove the correctness of our type system and we report experimental results of performing type inference on lines of code our analysis runs in polynomial time takes total of seconds on our benchmarks and produces low number of false positives which suggests that our analysis is good basis for other analyses such as race detectors
we present efficient fixed parameter algorithms for the np complete edge modification problems cluster editing and cluster deletion here the goal is to make the fewest changes to the edge set of an input graph such that the new graph is vertex disjoint union of cliques allowing up to edge additions and deletions cluster editing we solve this problem in time allowing only up to edge deletions cluster deletion we solve this problem in time the key ingredients of our algorithms are two easy to implement bounded search tree algorithms and reduction to problem kernel of size this improves and complements previous work
data integration is the problem of combining data residing at different autonomous heterogeneous sources and providing the client with unified reconciled global view of the data we discuss data integration systems taking the abstract viewpoint that the global view is an ontology expressed in class based formalism we resort to an expressive description logic alcqi that fully captures class based representation formalisms and we show that query answering in data integration as well as all other relevant reasoning tasks is decidable however when we have to deal with large amounts of data the high computational complexity in the size of the data makes the use of full fledged expressive description logic infeasible in practice this leads us to consider dl lite specifically tailored restriction of alcqi that ensures tractability of query answering in data integration while keeping enough expressive power to capture the most relevant features of class based formalisms
novel technique is proposed for the management of reconfigurable device in order to get true hardware multitasking we use vertex list set to keep track of the free area boundary this structure contains the best candidate locations for the task and several heuristics are proposed to select one of them based in fragmentation and adjacency look ahead heuristic that anticipates the next known event is also proposed metric is used to estimate the fragmentation status of the fpga based on the number of holes and their shape defragmentation measures are taken when needed
labeling text data is quite time consuming but essential for automatic text classification especially manually creating multiple labels for each document may become impractical when very large amount of data is needed for training multi label text classifiers to minimize the human labeling efforts we propose novel multi label active learning approach which can reduce the required labeled data without sacrificing the classification accuracy traditional active learning algorithms can only handle single label problems that is each data is restricted to have one label our approach takes into account the multi label information and select the unlabeled data which can lead to the largest reduction of the expected model loss specifically the model loss is approximated by the size of version space and the reduction rate of the size of version space is optimized with support vector machines svm an effective label prediction method is designed to predict possible labels for each unlabeled data point and the expected loss for multi label data is approximated by summing up losses on all labels according to the most confident result of label prediction experiments on several real world data sets all are publicly available demonstrate that our approach can obtain promising classification result with much fewer labeled data than state of the art methods
this paper introduces novel technique for joint surface reconstruction and registration given set of roughly aligned noisy point clouds it outputs noise free and watertight solid model the basic idea of the new technique is to reconstruct prototype surface at increasing resolution levels according to the registration accuracy obtained so far and to register all parts with this surface we derive non linear optimization problem from bayesian formulation of the joint estimation problem the prototype surface is represented as partition of unity implicit surface which is constructed from piecewise quadratic functions defined on octree cells and blended together using spline basis functions allowing the representation of objects with arbitrary topology with high accuracy we apply the new technique to set of standard data sets as well as especially challenging real world cases in practice the novel prototype surface based joint reconstruction registration algorithm avoids typical convergence problems in registering noisy range scans and substantially improves the accuracy of the final output
we study traffic measurement issue for active queue management and dynamic bandwidth allocation at single network node under the constraint of cell loss probability clp or buffer overflow probability using the concept of measurement based virtual queue vq and frequency domain traffic filtering we propose an online algorithm to estimate the real time bandwidth demand under both short and long term cell loss constraint the algorithm is adaptive and robust to the piece wise stationary traffic dynamics the vq runs in parallel to the real queueing system and monitors the latter in non intrusive way it captures proper traffic sampling interval tc simulation and analysis show its critical role in achieving higher utilization of bandwidth and buffer resource without violating qos requirement given an appropriate tc we argue that network controls such as the call admission control cac and dynamic bandwidth allocation are facilitated with accurate information about traffic loading and qos status at the smallest timescale
two methods have been used extensively to model resting contact for rigid body simulation the first approach the penalty method applies virtual springs to surfaces in contact to minimize interpenetration this method as typically implemented results in oscillatory behavior and considerable penetration the second approach based on formulating resting contact as linear complementarity problem determines the resting contact forces analytically to prevent interpenetration the analytical method exhibits expected case polynomial complexity in the number of contact points and may fail to find solution in polynomial time when friction is modeled we present fast penalty method that minimizes oscillatory behavior and leads to little penetration during resting contact our method compares favorably to the analytical method with regard to these two measures while exhibiting much faster performance both asymptotically and empirically
this paper proposes cluster based peer to peer system called peercluster for sharing data over the internet in peercluster all participant computers are grouped into various interest clusters each of which contains computers that have the same interests the intuition behind the system design is that by logically grouping users interested in similar topics together we can improve query efficiency to efficiently route and broadcast messages across within interest clusters hypercube topology is employed in addition to ensure that the structure of the interest clusters is not altered by arbitrary node insertions deletions we have devised corresponding join and leave protocols the complexities of these protocols are analyzed moreover we augment peercluster with system recovery mechanism to make it robust against unpredictable computer network failures using an event driven simulation we evaluate the performance of our approach by varying several system parameters the experimental results show that peercluster outperforms previous approaches in terms of query efficiency while still providing the desired functionality of keyword based search
grid resource management has been traditionally limited to just two levels of hierarchy namely local resource managers and metaschedulers this results in non manageable and thus not scalable architecture where each metascheduler has to be able to access thousands of resources which also implies having detailed knowledge about their interfaces and configuration this paper presents recursive architecture allowing an arbitrary number of levels in the hierarchy this way resources can be arranged in different ways for example following organizational boundaries or aggregating them by similarity while hiding the access details an implementation of this architecture is shown as well as its benefits in terms of autonomy scalability deployment and security the proposed implementation is based on existing interfaces allowing for standardization
tail calls are expected not to consume stack space in most functional languages however there is no support for tail calls in some environments even in such environments proper tail calls can be implemented with technique called trampoline to reduce the overhead of trampolining while preserving stack space asymptotically we propose selective tail call elimination based on an effect system the effect system infers the number of successive tail calls generated by the execution of an expression and trampolines are introduced only when they are necessary
automatic processing of medical dictations poses significant challenge we approach the problem by introducing statistical framework capable of identifying types and boundaries of sections lists and other structures occurring in dictation thereby gaining explicit knowledge about the function of such elements training data is created semi automatically by aligning parallel corpus of corrected medical reports and corresponding transcripts generated via automatic speech recognition we highlight the properties of our statistical framework which is based on conditional random fields crfs and implemented as an efficient publicly available toolkit finally we show that our approach is effective both under ideal conditions and for real life dictation involving speech recognition errors and speech related phenomena such as hesitation and repetitions
this paper concerns construction of additive stretched spanners with few edges for vertex graphs having tree decomposition into bags of diameter at most ie the tree length graphs for such graphs we construct additive spanners with dn nlogn edges and additive spanners with dn edges this provides new upper bounds for chordal graphs for which we also show lower bound and prove that there are graphs of tree length for which every multiplicative spanner and thus every additive spanner requires edges
sequential sat solver satori was recently proposed as an alternative to combinational sat in verification applications this paper describes the design of seq sat an efficient sequential sat solver with improved search strategies over satori the major improvements include new and better heuristic for minimizing the set of assignments to state variables new priority based search strategy and flexible sequential search framework which integrates different search strategies and decision variable selection heuristic more suitable for solving the sequential problems we present experimental results to demonstrate that our sequential sat solver can achieve orders of magnitude speedup over satori we plan to release the source code of seq sat along with this paper
finding frequent patterns in continuous stream of transactions is critical for many applications such as retail market data analysis network monitoring web usage mining and stock market prediction even though numerous frequent pattern mining algorithms have been developed over the past decade new solutions for handling stream data are still required due to the continuous unbounded and ordered sequence of data elements generated at rapid rate in data stream therefore extracting frequent patterns from more recent data can enhance the analysis of stream data in this paper we propose an efficient technique to discover the complete set of recent frequent patterns from high speed data stream over sliding window we develop compact pattern stream tree cps tree to capture the recent stream data content and efficiently remove the obsolete old stream data content we also introduce the concept of dynamic tree restructuring in our cps tree to produce highly compact frequency descending tree structure at runtime the complete set of recent frequent patterns is obtained from the cps tree of the current window using an fp growth mining technique extensive experimental analyses show that our cps tree is highly efficient in terms of memory and time complexity when finding recent frequent patterns from high speed data stream
initial algebra semantics is cornerstone of the theory of modern functional programming languages for each inductive data type it provides fold combinator encapsulating structured recursion over data of that type church encoding build combinator which constructs data of that type and fold build rule which optimises modular programs by eliminating intermediate data of that type it has long been thought that initial algebra semantics is not expressive enough to provide similar foundation for programming with nested types specifically the folds have been considered too weak to capture commonly occurring patterns of recursion and no church encodings build combinators or fold build rules have been given for nested types this paper overturns this conventional wisdom by solving all of these problems
existing test suite reduction techniques employed for testing web applications have either used traditional program coverage based requirements or usage based requirements in this paper we explore three different strategies to integrate the use of program coverage based requirements and usage based requirements in relation to test suite reduction for web applications we investigate the use of usage based test requirements for comparison of test suites that have been reduced based on program coverage based test requirements we examine the effectiveness of test suite reduction process based on combination of both usage based and program coverage based requirements finally we modify popular test suite reduction algorithm to replace part of its test selection process with selection based on usage based test requirements our case study suggests that integrating program coverage based and usage based test requirements has positive impact on the effectiveness of the resulting test suites
this paper demonstrates the use of model based evaluation approach for instrumentation systems iss the overall objective of this study is to provide early feedback to tool developers regarding is overhead and performance such feedback helps developers make appropriate design decisions about alternative system configurations and task scheduling policies we consider three types of system architectures network of workstations now symmetric multiprocessors smp and massively parallel processing mpp systems we develop resource occupancy rocc model for an on line is for an existing tool and parameterize it for an ibm sp platform this model is simulated to answer several what if questions regarding two policies to schedule instrumentation data forwarding collect and forward cf and batch and forward bf in addition this study investigates two alternatives for forwarding the instrumentation data direct and binary tree forwarding for an mpp system simulation results indicate that the bf policy can significantly reduce the overhead and that the tree forwarding configuration exhibits desirable scalability characteristics for mpp systems initial measurement based testing results indicate more than percent reduction in the direct is overhead when the bf policy was added to paradyn parallel performance measurement tool
database optimizers require statistical information about data distributions in order to evaluate result sizes and access plan costs for processing user queries in this context we consider the problem of estimating the size of the projections of database relation when measures on attribute domain cardinalities are maintained in the system our main theoretical contribution is new formal model ad valid under the hypotheses of attribute independence and uniform distribution of attribute values derived considering the difference between time invariant domain the set of values that an attribute can assume and time dependent active domain the set of values that are actually assumed at certain time early models developed under the same assumptions are shown to be formally incorrect since the ad model is computationally high demanding we also introduce an approximate easy to compute model ad that unlike previous approximations yields low errors on all the parameter space of the active domain cardinalities finally we extend the ad model to the case of nonuniform distributions and present experimental results confirming the good behavior of the model
in the past years research on inductive inference has developed along different lines eg in the formalizations used and in the classes of target concepts considered one common root of many of these formalizations is gold's model of identification in the limit this model has been studied for learning recursive functions recursively enumerable languages and recursive languages reflecting different aspects of machine learning artificial intelligence complexity theory and recursion theory one line of research focuses on indexed families of recursive languages classes of recursive languages described in representation scheme for which the question of membership for any string in any of the given languages is effectively decidable with uniform procedure such language classes are of interest because of their naturalness the survey at hand picks out important studies on learning indexed families including basic as well as recent research summarizes and illustrates the corresponding results and points out links to related fields such as grammatical inference machine learning and artificial intelligence in general
two complications frequently arise in real world applications motion and the contamination of data by outliers we consider fundamental clustering problem the center problem within the context of these two issues we are given finite point set of size and an integer in the standard center problem the objective is to compute set of center points to minimize the maximum distance from any point of to its closest center or equivalently the smallest radius such that can be covered by disks of this radius in the discrete center problem the disk centers are drawn from the points of and in the absolute center problem the disk centers are unrestricted we generalize this problem in two ways first we assume that points are in continuous motion and the objective is to maintain solution over time second we assume that some given robustness parameter
existing works on processing of extensible markup language xml documents have been concentrated on query optimisation storage problems documents transformation compressing methods and normalisation there are only few papers on concurrency control in accessing and modifying xml documents which are stored in xml database systems the aim of this paper is to analyse and compare the quantity of concurrency control methods for xml database systems based on dom api
we compare the performance of three usual allocations namely max min fairness proportional fairness and balanced fairness in communication network whose resources are shared by random number of data flows the model consists of network of processor sharing queues the vector of service rates which is constrained by some compact convex capacity set representing the network resources is function of the number of customers in each queue this function determines the way network resources are allocated we show that this model is representative of rich class of wired and wireless networks we give in this general framework the stability condition of max min fairness proportional fairness and balanced fairness and compare their performance on number of toy networks
context uncertainty is an unavoidable issue in software engineering and an important area of investigation this paper studies the impact of uncertainty on total duration ie make span for implementing all features in operational release planning objective the uncertainty factors under investigation are the number of new features arriving during release construction the estimated effort needed to implement features the availability of developers and the productivity of developers method an integrated method is presented combining monte carlo simulation to model uncertainty in the operational release planning orp process with process simulation to model the orp process steps and their dependencies as well as an associated optimization heuristic representing an organization specific staffing policy for make span minimization the method allows for evaluating the impact of uncertainty on make span the impact of uncertainty factors both in isolation and in combination are studied in three different pessimism levels through comparison with baseline plan initial evaluation of the method is done by an explorative case study at chartwell technology inc to demonstrate its applicability and its usefulness results the impact of uncertainty on release make span increases both in terms of magnitude and variance with an increase of pessimism level as well as with an increase of the number of uncertainty factors among the four uncertainty factors we found that the strongest impact stems from the number of new features arriving during release construction we have also demonstrated that for any combination of uncertainty factors their combined ie simultaneous impact is bigger than the addition of their individual impacts conclusion the added value of the presented method is that managers are able to study the impact of uncertainty on existing ie baseline operational release plans pro actively
in this paper an approach for the implementation of quality based web search engine is proposed quality retrieval is introduced and an overview on previous efforts to implement such service is given machine learning approaches are identified as the most promising methods to determine the quality of web pages features for the most appropriate characterization of web pages are determined quality model is developed based on human judgments this model is integrated into meta search engine which assesses the quality of all results at run time the evaluation results show that quality based ranking does lead to better results concerning the perceived quality of web pages presented in the result set the quality models are exploited to identify potentially important features and characteristics for the quality of web pages
in this work we present new in network techniques for communication efficient approximate query processing in wireless sensornets we use model based approach that constructs and maintains spanning tree within the network rooted at the basestation the tree maintains compressed summary information for each link that is used to stub out traversal during query processing our work is based on formal model of the in network tree construction task framed as an optimization problemwe demonstrate hardness results for that problem and develop efficient approximation algorithms for subtasks that are too expensive to compute exactly we also propose efficient heuristics to accommodate wider set of workloads and empirically evaluate their performance and sensitivity to model changes
in practice any database management system sometimes needs reorganization that is change in some aspect of the logical and or physical arrangement of database in traditional practice many types of reorganization have required denying access to database taking the database offline during reorganization taking database offline can be unacceptable for highly available hour database for example database serving electronic commerce or armed forces or for very large database solution is to reorganize online concurrently with usage of the database incrementally during users activities or interpretively this article is tutorial and survey on requirements issues and strategies for online reorganization it analyzes the issues and then presents the strategies which use the issues the issues most of which involve design trade offs include use of partitions the locus of control for the process that reorganizes background process or users activities reorganization by copying to newly allocated storage as opposed to reorganizing in place use of differential files references to data that has moved performance and activation of reorganization the article surveys online strategies in three categories of reorganization the first category maintenance involves restoring the physical arrangement of data instances without changing the database definition this category includes restoration of clustering reorganization of an index rebalancing of parallel or distributed data garbage collection for persistent storage and cleaning reclamation of space in log structured file system the second category involves changing the physical database definition topics include construction of indexes conversion between trees and linear hash files and redefinition eg splitting of partitions the third category involves changing the logical database definition some examples are changing column's data type changing the inheritance hierarchy of object classes and changing relationship from one to many to many to many the survey encompasses both research and commercial implementations and this article points out several open research topics as highly available or very large databases continue to become more common and more important in the world economy the importance of online reorganization is likely to continue growing
we have devised an algorithm for minimal placement of bank selections in partitioned memory architectures this algorithm is parameterizable for chosen metric such as speed space or energy bank switching is technique that increases the code and data memory in microcontrollers without extending the address buses given program in which variables have been assigned to data banks we present novel optimization technique that minimizes the overhead of bank switching through cost effective placement of bank selection instructions the placement is controlled by number of different objectives such as runtime low power small code size or combination of these parameters we have formulated the minimal placement of bank selection instructions as discrete optimization problem that is mapped to partitioned boolean quadratic programming pbqp problem we implemented the optimization as part of pic microchip backend and evaluated the approach for several optimization objectives our benchmark suite comprises programs from mibench and dspstone plus microcontroller real time kernel and drivers for microcontroller hardware devices our optimization achieved reduction in program memory space of between and percent and an overall improvement with respect to instruction cycles between and percent our optimization achieved the minimal solution for all benchmark programs we investigated the scalability of our approach toward the requirements of future generations of microcontrollers this study was conducted as worst case analysis on the entire mibench suite our results show that our optimization scales well to larger numbers of memory banks scales well to the larger problem sizes that will become feasible with future microcontrollers and achieves minimal placement for more than percent of all functions from mibench
we present new metric for routing in multi radio multi hop wireless networks we focus on wireless networks with stationary nodes such as community wireless networksthe goal of the metric is to choose high throughput path between source and destination our metric assigns weights to individual links based on the expected transmission time ett of packet over the link the ett is function of the loss rate and the bandwidth of the link the individual link weights are combined into path metric called weighted cumulative ett wcett that explicitly accounts for the interference among links that use the same channel the wcett metric is incorporated into routing protocol that we call multi radio link quality source routingwe studied the performance of our metric by implementing it in wireless testbed consisting of nodes each equipped with two wireless cards we find that in multi radio environment our metric significantly outperforms previously proposed routing metrics by making judicious use of the second radio
semantic annotations of web services can support the effective and efficient discovery of services and guide their composition into workflows at present however the practical utility of such annotations is limited by the small number of service annotations available for general use manual annotation of services is time consuming and thus expensive task so some means are required by which services can be automatically or semi automatically annotated in this paper we show how information can be inferred about the semantics of operation parameters based on their connections to other annotated operation parameters within tried and tested workflows because the data links in the workflows do not necessarily contain every possible connection of compatible parameters we can infer only constraints on the semantics of parameters we show that despite their imprecise nature these so called loose annotations are still of value in supporting the manual annotation task inspecting workflows and discovering services we also show that derived annotations for already annotated parameters are useful by comparing existing and newly derived annotations of operation parameters we can support the detection of errors in existing annotations the ontology used for annotation and in workflows the derivation mechanism has been implemented and its practical applicability for inferring new annotations has been established through an experimental evaluation the usefulness of the derived annotations is also demonstrated
we describe an ethnographic study that explores how low tech and new tech surfaces support participation and collaboration during workshop breakout session the low tech surfaces were post it notes and large sheets of paper the new tech surfaces were writeable walls and multi touch tabletop four groups used the different surfaces during three phases brief presentation of position papers and discussion of themes ii the creation of group presentation and iii report back session participation and collaboration varied depending on the physical technological and social factors at play when using the different surfaces we discuss why this is the case noting how new shareable surfaces may need to be constrained to invite participation in ways that are simply taken for granted because of their familiarity when using low tech materials
this paper addresses the pragmatics of web information systems wis by analysing their usage starting from classification of intentions we first present life cases which capture observations of user behaviour in reality we discuss the facets of life cases and present semi formal way for their documentation life cases can be used in pragmatic way to specify story space which is an important component of storyboard in second step we complement life cases by user models that are specified by various facets of actor profiles that are needed for them we analyse actor profiles and present semi formal way for their documentation we outline how these profiles can be used to specify actors which are an important component of storyboard finally we analyse contexts and the way they impact on life cases user models and the storyboard
we present method to align words in bitext that combines elements of traditional statistical approach with linguistic knowledge we demonstrate this approach for arabic english using an alignment lexicon produced by statistical word aligner as well as linguistic resources ranging from an english parser to heuristic alignment rules for function words these linguistic heuristics have been generalized from development corpus of parallel sentences our aligner ualign outperforms both the commonly used giza aligner and the state of the art leaf aligner on measure and produces superior scores in end to end statistical machine translation bleu points over giza and over leaf
this paper investigates efficient evaluation of database updates and presents procedural semantics for stratified update programs that extend stratified logic programs with bulk updates and hypothetical reasoning bulk rules with universal quantification in the body allow an arbitrary update to be applied simultaneously for every answer of an arbitrary query hypothetical reasoning is supported by testing the success or failure of an update the procedural semantics offers efficient goal dash oriented tabled evaluation of database updates it guarantees termination for function dash free stratified update programs and avoids repeated computation of identical subgoals
we consider computationally efficient incentive compatiblemechanisms that use the vcg payment scheme and study how well theycan approximate the social welfare in auction settings we present anovel technique for setting lower bounds on the approximation ratioof this type of mechanisms specifically for combinatorial auctionsamong submodular and thus also subadditive bidders we prove an lower bound which is close to the knownupper bound of and qualitatively higher than theconstant factor approximation possible from purely computationalpoint of view
embedded systems have been traditional area of strength in the research agenda of the university of california at berkeley in parallel to this effort pattern of graduate and undergraduate classes has emerged that is the result of distillation process of the research results in this paper we present the considerations that are driving our curriculum development and we review our undergraduate and graduate program in particular we describe in detail graduate class eecs design of embedded systems modeling validation and synthesis that has been taught for six years common feature of our education agenda is the search for fundamentals of embedded system science rather than embedded system design techniques an approach that today is rather unique
pointer analysis classic problem in software program analysis has emerged as an important problem to solve in design automation at time when complex designs specified in the form of code need to be synthesized or verified however precise pointer analysis algorithms that are both context and flow sensitive fscs have not been shown to scale in this paper we report new solution for fscs analysis which can evaluate the program states of all program points under billions of different calling paths our solution extends the recently proposed symbolic pointer analysis spa technology which exploits the efficiency of binary decision diagrams bdds with our new strategy of problem solving called superposed symbolic computation and its application on our generic pointer analysis framework we are able to report the first result on all spec benchmarks that completes context sensitive flow insensitive analysis in seconds and context sensitive flow sensitive analysis in minutes
we present novel technique called radiance scaling for the depiction of surface shape through shading it adjusts reflected light intensities in way dependent on both surface curvature and material characteristics as result diffuse shading or highlight variations become correlated to surface feature variations enhancing surface concavities and convexities this approach is more versatile compared to previous methods first it produces satisfying results with any kind of material we demonstrate results obtained with phong and ashikmin brdfs cartoon shading sub lambertian materials and perfectly reflective or refractive objects second it imposes no restriction on lighting environment it does not require dense sampling of lighting directions and works even with single light third it makes it possible to enhance surface shape through the use of precomputed radiance data such as ambient occlusion prefiltered environment maps or lit spheres our novel approach works in real time on modern graphics hardware
mobile nodes in some challenging network scenarios eg battlefield and disaster recovery scenarios suffer from intermittent connectivity and frequent partitions disruption tolerant network dtn technologies are designed to enable communications in such environments several dtn routing schemes have been proposed however not much work has been done on designing schemes that provide efficient information access in such challenging network scenarios in this paper we explore how content based information retrieval system can be designed for dtns there are three important design issues namely how data should be replicated and stored at multiple nodes how query is disseminated in sparsely connected networks and how query response is routed back to the issuing node we first describe how to select nodes for storing the replicated copies of data items we consider the random and the intelligent caching schemes in the random caching scheme nodes that are encountered first by data generating node are selected to cache the extra copies while in the intelligent caching scheme nodes that can potentially meet more nodes eg faster nodes are selected to cache the extra data copies the number of replicated data copies can be the same for all data items or varied depending on the access frequencies of the data items in this work we consider fixed proportional and square root replication schemes then we describe two query dissemination schemes copy selective query spraying wss scheme and hop neighborhood spraying lns scheme in the wss scheme nodes that can move faster are selected to cache the queries while in the lns scheme nodes that are within hops of querying node will cache the queries for message routing we use an enhanced prophet scheme where next hop node is selected only if its predicted delivery probability to the destination is higher than certain threshold we conduct extensive simulation studies to evaluate different combinations of the replication and query dissemination algorithms our results reveal that the scheme that performs the best is the one that uses the wss scheme combined with binary spread of replicated data copies the wss scheme can achieve higher query success ratio when compared to scheme that does not use any data and query replication furthermore the square root and proportional replication schemes provide higher query success ratio than the fixed copy approach with varying node density in addition the intelligent caching approach can further improve the query success ratio by with varying node density our results using different mobility models reveal that the query success ratio degrades at most when the community based model is used compared to the random waypoint rwp model broch et al performance comparison of multihop wireless ad hoc network routing protocols acm mobicom pp compared to the rwp and the community based mobility models the umassbusnet model from the dieselnet project zhang et al modeling of bus based disruption tolerant network trace proceedings of acm mobihoc achieves much lower query success ratio because of the longer inter node encounter time
we propose an automatic instrumentation method for embedded software annotation to enable performance modeling in high level hardware software co simulation environments the proposed cross annotation technique consists of extending retargetable compiler infrastructure to allow the automatic instrumentation of embedded software at the basic block level thus target and annotated native binaries are guaranteed to have isomorphic control flow graphs cfg the proposed method takes into account the processor specific optimizations at the compiler level and proves to be accurate with low simulation overhead
one approach to prolong the lifetime of wireless sensor network wsn is to deploy some relay nodes to communicate with the sensor nodes other relay nodes and the base stations the relay node placement problem for wireless sensor networks is concerned with placing minimum number of relay nodes into wireless sensor network to meet certain connectivity or survivability requirements previous studies have concentrated on the unconstrained version of the problem in the sense that relay nodes can be placed anywhere in practice there may be some physical constraints on the placement of relay nodes to address this issue we study constrained versions of the relay node placement problem where relay nodes can only be placed at set of candidate locations in the connected relay node placement problem we want to place minimum number of relay nodes to ensure that each sensor node is connected with base station through bidirectional path in the survivable relay node placement problem we want to place minimum number of relay nodes to ensure that each sensor node is connected with two base stations or the only base station in case there is only one base station through two node disjoint bidirectional paths for each of the two problems we discuss its computational complexity and present framework of polynomial time approximation algorithms with small approximation ratios extensive numerical results showthat our approximation algorithms can produce solutions very close to optimal solutions
we propose low leakage cache architecture based on the observation of the spatio temporal properties of data caches in particular we exploit the fact that during the program lifetime few data values tend to exhibit both spatial and temporal locality in cache ie values that are simultaneously stored by several lines at the same time leakage energy can be reduced by turning off those lines and storing these values in smaller separate memory in this work we introduce an architecture that implements such scheme as well as an algorithm to detect these special values we show that by using as few as four values we can achieve leakage energy savings with an additional reduction of dynamic energy as consequence of reduced average cache access cost
in november the fcc ruled that the digital tv whitespaces be used for unlicensed access this is an exciting development because dtv whitespaces are in the low frequency range mhz compared to typical cellular and ism bands thus resulting in much better propagation characteristics and much higher spectral efficiencies the fcc has also mandated certain guidelines for short range unlicensed access so as to avoid any interference to dtv receivers we consider the problem of wifi like access popularly referred to as wifi for enterprizes we assume that the access points and client devices are equipped with cognitive radios ie they can adaptively choose the center frequency bandwidth and ower of operation the access points can be equipped with one or more radios our goal is to design complete system which does not violate the fcc mandate ii dynamically assigns center frequency and bandwidth to each access point based on their demands and iii squeezes the maximum efficiency from the available spectrum this problem is far more general than prior work that investigated dynamic spectrum allocation in cellular and ism bands due to the non homogenous nature of the whitespaces ie different whitespace widths in different parts of the spectrum and the large range of frequency bands with different propagation characteristics this calls for more holistic approach to system design that also accounts for frequency dependent propagation characteristics and radio frontend characteristics in this paper we first propose design rules for holistic system design we then describe an architecture derived from our design rules finally we propose demand based dynamic spectrum allocation algorithms with provable worst case guarantees we provide extensive simulation results showing that the performance of our algorithm is within of the optimal in typical settings and ii and the dtv whitespaces can provide significantly higher data rates compared to the ghz ism band our approach is general enough for designing any system with access to wide range of spectrum
opinion mining is the task of extracting from set of documents opinions expressed by source on specified target this article presents comparative study on the methods and resources that can be employed for mining opinions from quotations reported speech in newspaper articles we show the difficulty of this task motivated by the presence of different possible targets and the large variety of affect phenomena that quotes contain we evaluate our approaches using annotated quotations extracted from news provided by the emm news gathering engine we conclude that generic opinion mining system requires both the use of large lexicons as well as specialised training and testing data
we consider the setting of multiprocessor where the speeds of the processors can be individually scaled jobs arrive over time and have varying degrees of parallelizability nonclairvoyant scheduler must assign the jobs to processors and scale the speeds of the processors we consider the objective of energy plus flow time for jobs that may have side effects or that are not checkpointable we show an bound on the competitive ratio of any deterministic algorithm here is the number of processors and is the exponent of the power function for checkpointable jobs without side effects we give an log competitive algorithm thus for jobs that may have side effects or that are not checkpointable the achievable competitive ratio grows quickly with the number of processors but for checkpointable jobs without side effects the achievable competitive ratio grows slowly with the number of processors we then show lower bound of log on the competitive ratio of any algorithm for checkpointable jobs without side effects finally we slightly improve the upper bound on the competitive ratio for the single processor case which is equivalent to the case that all jobs are fully parallelizable by giving an improved analysis of previously proposed algorithm
model to query document databases by both their content and structure is presented the goal is to obtain query language that is expressive in practice while being efficiently implementable features not present at the same time in previous work the key ideas of the model are set oriented query language based on operations on nearby structure elements of one or more hierarchies together with content and structural indexing and bottom up evaluation the model is evaluated in regard to expressiveness and efficiency showing that it provides good trade off between both goals finally it is shown how to include in the model other media different from text
the state explosion problem of formal verification has obstructed its application to large scale software systems in this article we introduce set of new condensation theories iot failure equivalence iot state equivalence and firing dependence theory to cope with this problem our condensation theories are much weaker than current theories used for the compositional verification of petri nets more significantly our new condensation theories can eliminate the interleaved behaviors caused by asynchronously sending actions therefore our technique provides much more powerful means for the compositional verification of asynchronous processes our technique can efficiently analyze several state based properties boundedness reachable markings reachable submarkings and deadlock states based on the notion of our new theories we develop set of condensation rules for efficient verification of large scale software systems the experimental results show significant improvement in the analysis large scale concurrent systems
we consider the distribution of channels of live multimedia content eg radio or tv broadcasts via multiple content aggregators in our work an aggregator receives channels from content sources and redistributes them to potentially large number of mobile hosts each aggregator can offer channel in various configurations to cater for different wireless links mobile hosts and user preferences as result mobile host can generally choose from different configurations of the same channel offered by multiple alternative aggregators which may be available through different interfaces eg in hotspot mobile host may need to handoff to another aggregator once it receives channel to prevent service disruption mobile host may for instance need to handoff to another aggregator when it leaves the subnets that make up its current aggregator's service area eg hotspot or cellular network in this paper we present the design of system that enables multi homed mobile hosts to seamlessly handoff from one aggregator to another so that they can continue to receive channel wherever they go we concentrate on handoffs between aggregators as result of mobile host crossing subnet boundary as part of the system we discuss lightweight application level protocol that enables mobile hosts to select the aggregator that provides the best configuration of channel the protocol comes into play when mobile host begins to receive channel and when it crosses subnet boundary while receiving the channel we show how our protocol can be implemented using the standard ietf session control and description protocols sip and sdp the implementation combines sip and sdp's offer answer model in novel way
in this paper we describe the design implementation and evaluation of software framework that supports the development of mobile context aware trails based applications trail is contextually scheduled collection of activities and represents generic model that can be used to satisfy the activity management requirements of wide range of context based time management applications trails overcome limitations with traditional time management techniques based on static to do lists by dynamically reordering activities based on emergent context
in multiuser multimedia information systems eg movie on demand digital editing scheduling the retrievals of continuous media objects becomes challenging task this is because of both intra and inter iobject time dependencies intraobject time dependency refers to the real time display requirement of continuous media object interobject time dependency is the temporal relationships defined among multiple continuous media objects in order to compose tailored multimedia presentations user might define complex time dependencies among multiple continuous media objects with various lengths and display bandwidths scheduling the retrieval tasks corresponding to the components of such presentation in order to respect both inter and intra task time dependencies is the focus of this study to tackle this task scheduling problem crs we start with simpler scheduling problem ars where there is no inter task time dependency eg movie on demand next we investigate an augmented version of ars termed ars where requests reserve displays in advance eg reservation based movie on demand finally we extend our techniques proposed for ars and ars to address the crs problem we also provide formal definition of these scheduling problems and proof of their np hardness
in automatic software verification we have observed theoretical convergence of model checking and program analysis in practice however model checkers are still mostly concerned with precision eg the removal of spurious counterexamples for this purpose they build and refine reachability trees lattice based program analyzers on the other hand are primarily concerned with efficiency we designed an algorithm and built tool that can be configured to perform not only purely tree based or purely lattice based analysis but offers many intermediate settings that have not been evaluated before the algorithm and tool take one or more abstract interpreters such as predicate abstraction and shape analysis and configure their execution and interaction using several parameters our experiments show that such customization may lead to dramatic improvements in the precision efficiency spectrum
in web database that dynamically provides information in response to user queries two distinct schemas interface schema the schema users can query and result schema the schema users can browse are presented to users each partially reflects the actual schema of the web database most previous work only studied the problem of schema matching across query interfaces of web databases in this paper we propose novel schema model that distinguishes the interface and the result schema of web database in specific domain in this model we address two significant web database schema matching problems intra site and inter site the first problem is crucial in automatically extracting data from web databases while the second problem plays significant role in meta retrieving and integrating data from different web databases we also investigate unified solution to the two problems based on query probing and instance based schema matching techniques using the model cross validation technique is also proposed to improve the accuracy of the schema matching our experiments on real web databases demonstrate that the two problems can be solved simultaneously with high precision and recall
in this paper we propose run time strategy for allocating application tasks to embedded multiprocessor systems on chip platforms where communication happens via the network on chip approach as novel contribution we incorporate the user behavior information in the resource allocation process this allows the system to better respond to real time changes and to adapt dynamically to different user needs several algorithms are proposed for solving the task allocation problem while minimizing the communication energy consumption and network contention when the user behavior is taken into consideration we observe more than communication energy savings with negligible energy and run time overhead compared to an arbitrary contiguous task allocation strategy
automatically recognising which html documents on the web contain items of interest for user is non trivial as step toward solving this problem we propose an approach based on information extraction ontologies given html documents tables and forms our document recognition system extracts expected ontological vocabulary keywords and keyword phrases and expected ontological instance data particular values for ontological concepts we then use machine learned rules over this extracted information to determine whether an html document contains items of interest experimental results show that our ontological approach to categorisation works well having achieved measures above for all applications we tried
we present new approach to accelerate collision detection for deformable models our formulation applies to all triangulated models and significantly reduces the number of elementary tests between features of the mesh ie vertices edges and faces we introduce the notion of representative triangles standard geometric triangles augmented with mesh feature information and use this representation to achieve better collision query performance the resulting approach can be combined with bounding volume hierarchies and works well for both inter object and self collision detection we demonstrate the benefit of representative triangles on continuous collision detection for cloth simulation and body collision scenarios we observe up to one order of magnitude reduction in feature pair tests and up to improvement in query time
graph theory has been shown to provide powerful tool for representing and tackling machine learning problems such as clustering semi supervised learning and feature ranking this paper proposes graph based discrete differential operator for detecting and eliminating competence critical instances and class label noise from training set in order to improve classification performance results of extensive experiments on artificial and real life classification problems substantiate the effectiveness of the proposed approach
there has been little research into how end users might be able to communicate advice to machine learning systems if this resource the users themselves could somehow work hand in hand with machine learning systems the accuracy of learning systems could be improved and the users understanding and trust of the system could improve as well we conducted think aloud study to see how willing users were to provide feedback and to understand what kinds of feedback users could give users were shown explanations of machine learning predictions and asked to provide feedback to improve the predictions we found that users had no difficulty providing generous amounts of feedback the kinds of feedback ranged from suggestions for reweighting of features to proposals for new features feature combinations relational features and wholesale changes to the learning algorithm the results show that user feedback has the potential to significantly improve machine learning systems but that learning algorithms need to be extended in several ways to be able to assimilate this feedback
information visualisation has become increasingly important in science engineering and commerce as tool to convey and explore complex sets of information this paper introduces visualisation schema which uses visual attributes as the principle components of visualisation we present new classification of visual attributes according to information accuracy information dimension and spatial requirements and obtain values for the information content and information density of each attribute the classification applies only to the perception of quantitative information and initial results of experiments suggest that it can not be extended to other visual processing tasks such as preattentive target detectionthe classification in combination with additional guidelines given in this paper provide the reader with useful tool for creating visualisations which convey complex sets of information more effectively
in this article we examine how clausal resolution can be applied to specific but widely used nonclassical logic namely discrete linear temporal logic thus we first define normal form for temporal formulae and show how arbitrary temporal formulae can be translated into the normal form while preserving satisfiability we then introduce novel resolution rules that can be applied to formulae in this normal form provide range of examples and examine the correctness and complexity of this approach finally we describe related work and future developments concerning this work
bitwise operations are commonly used in low level systems code to access multiple data fields that have been packed into single word program analysis tools that reason about such programs must model the semantics of bitwise operations precisely in order to capture program control and data flow through these operations we present type system for subword data structures that explitictly tracks the flow of bit values in the program and identifies consecutive sections of bits as logical entities manipulated atomically by the programmer our type inference algorithm tags each integer value of the program with bitvector type that identifies the data layout at the subword level these types are used in translation phase to remove bitwise operations from the program thereby allowing verification engines to avoid the expensive low level reasoning required for analyzing bitvector operations we have used software model checker to check properties of translated versions of linux device driver and memory protection system the resulting verification runs could prove many more properties than the naive model checker that did not reason about bitvectors and could prove properties much faster than model checker that did reason about bitvectors we have also applied our bitvector type inference algorithm to generate program documentation for virtual memory subsystem of an os kernel while we have applied the type system mainly for program understanding and verification bitvector types also have applications to better variable ordering heuristics in boolean model checking and memory optimizations in compilers for embedded software
in art grouping plays major role to convey relationships of objects and the organization of scenes it is separated from style which only determines how groups are rendered to achieve visual abstraction of the depicted scene we present an approach to interactively derive grouping information in dynamic scene our solution is simple and general the resulting grouping information can be used as an input to any rendering style we provide an efficient solution based on an extended mean shift algorithm customized by user defined criteria the resulting system is temporally coherent and real time the computational cost is largely determined by the scene's structure rather than by its geometric complexity
this paper describes an approach spy to recovering the specification of software component from the observation of its run time behavior it focuses on components that behave as data abstractions components are assumed to be black boxes that do not allow any implementation inspection the inferred description may help understand what the component does when no formal specification is available spy works in two main stages first it builds deterministic finite state machine that models the partial behavior of instances of the data abstraction this is then generalized via graph transformation rules the rules can generate possibly infinite number of behavior models which generalize the description of the data abstraction under an assumption of regularity with respect to the observed behavior the rules can be viewed as likely specification of the data abstraction we illustrate how spy works on relevant examples and we compare it with competing methods
this paper presents concept hierarchy based approach to privacy preserving data collection for data mining called the level model the level model allows data providers to divulge information at any chosen privacy level level on any attribute data collected at high level signifies divulgence at higher conceptual level and thus ensures more privacy providing guarantees prior to release such as satisfying anonymity samarati sweeney can further protect the collected data set from privacy breaches due to linking the released data set with external data sets however the data mining process which involves the integration of various data values can constitute privacy breach if combinations of attributes at certain levels result in the inference of knowledge that exists at lower level this paper describes the level reduction phenomenon and proposes methods to identify and control the occurrence of this privacy breach
replay is an important technique in program analysis allowing to reproduce bugs to track changes and to repeat executions for better understanding of the results unfortunately since re executing concurrent program does not necessarily produce the same ordering of events replay of such programs becomes difficult task the most common approach to replay of concurrent programs is based on analyzing the logical dependencies among concurrent events and requires complete recording of the execution we are trying to replay as well as complete control over the program's scheduler in realistic settings we usually have only partial recording of the execution and only partial control over the scheduling decisions thus such an analysis is often impossible in this paper we present an approach for replay in the presence of partial information and partial control our approach is based on novel application of the cross entropy method and it does not require any logical analysis of dependencies among concurrent events roughly speaking given partial recording of an execution we define performance function on executions which reaches its maximum on or any other execution that coincides with on the recorded events then the program is executed many times in iterations on each iteration adjusting the probabilistic scheduling decisions so that the performance function is maximized our method is also applicable to debugging of concurrent programs in which the program is changed before it replayed in order to increase the information from its execution we implemented our replay method on concurrent java programs and we show that it consistently achieves close replay in presence of incomplete information and incomplete control as well as when the program is changed before it is replayed
the work in this paper is motivated by the real world problems such as mining frequent traversal path patterns from very large web logs generalized suffix trees over very large alphabet can be used to solve such problems however traditional algorithms such as the weiner ukkonen and mccreight algorithms are not sufficient assurance of practicality because of large magnitudes of the alphabet and the set of strings in those real world problems two new algorithms are designed for fast construction of generalized suffix trees over very large alphabet and their performance is analyzed in comparison with the well known ukkonen algorithm it is shown that these two algorithms have better performance and can deal with large alphabets and large string sets well
we present texture synthesis scheme based on neighborhood matching with contributions in two areas parallelism and control our scheme defines an infinite deterministic aperiodic texture from which windows can be computed in real time on gpu we attain high quality synthesis using new analysis structure called the gaussian stack together with coordinate upsampling step and subpass correction approach texture variation is achieved by multiresolution jittering of exemplar coordinates combined with the local support of parallel synthesis the jitter enables intuitive user controls including multiscale randomness spatial modulation over both exemplar and output feature drag and drop and periodicity constraints we also introduce synthesis magnification fast method for amplifying coarse synthesis results to higher resolution
traditional behavior based worm detection can't eliminate the influence of the worm like pp traffic effectively as well as detect slow worms to try to address these problems this paper first presents user habit model to describe the factors which influent the generation of network traffic then design of hpbrwd host packet behavior ranking based worm detection and some key issues about it are introduced this paper has three contributions to the worm detection presenting hierarchical user habit model using normal software and time profile to eliminate the worm like pp traffic and accelerate the detection of worms presenting hpbrwd to effectively detect worms experiments results show that hpbrwd is effective to detect worms
we present the design and implementation of new garbage collection framework that significantly generalizes existing copying collectors the beltway framework exploits and separates object age and incrementality it groups objects in one or more increments on queues called belts collects belts independently and collects increments on belt in first in first out order we show that beltway configurations selected by command line options act and perform the same as semi space generational and older first collectors and encompass all previous copying collectors of which we are aware the increasing reliance on garbage collected languages such as java requires that the collector perform well we show that the generality of beltway enables us to design and implement new collectors that are robust to variations in heap size and improve total execution time over the best generational copying collectors of which we are aware by up to and on average by to for small to moderate heap sizes new garbage collection algorithms are rare and yet we define not just one but new family of collectors that subsumes previous work this generality enables us to explore larger design space and build better collectors
comprehending and modifying software is at the heart of many software engineering tasks and this explains the growing interest that software reverse engineering has gained in the last years broadly speaking reverse engineering is the process of analyzing subject system to create representations of the system at higher level of abstraction this paper briefly presents an overview of the field of reverse engineering reviews main achievements and areas of application and highlights key open research issues for the future
we present shape retrieval methodology based on the theory of spherical harmonics using properties of spherical harmonics scaling and axial flipping invariance is achieved rotation normalization is performed by employing the continuous principal component analysis along with novel approach which applies pca on the face normals of the model the model is decomposed into set of spherical functions which represents not only the intersections of the corresponding surface with rays emanating from the origin but also points in the direction of each ray which are closer to the origin than the furthest intersection point the superior performance of the proposed methodology is demonstrated through comparison against state of the art approaches on standard databases
how to allocate computing and communication resources in way that maximizes the effectiveness of control and signal processing has been an important area of research the characteristic of multi hop real time wireless sensor network raises new challenges first the constraints are more complicated and new solution method is needed second distributed solution is needed to achieve scalability this article presents solutions to both of the new challenges the first solution to the optimal rate allocation is centralized solution that can handle the more general form of constraints as compared with prior research the second solution is distributed version for large sensor networks using pricing scheme it is capable of incremental adjustment when utility functions change this article also presents new sensor device network backbone architecture real time independent channels rich which can easily realize multi hop real time wireless sensor networking
data mining mechanisms have widely been applied in various businesses and manufacturing companies across many industry sectors sharing data or sharing mined rules has become trend among business partnerships as it is perceived to be mutually benefit way of increasing productivity for all parties involved nevertheless this has also increased the risk of unexpected information leaks when releasing data to conceal restrictive itemsets patterns contained in the source database sanitization process transforms the source database into released database that the counterpart cannot extract sensitive rules from the transformed result also conceals non restrictive information as an unwanted event called side effect or the misses cost the problem of finding an optimal sanitization method which conceals all restrictive itemsets but minimizes the misses cost is np hard to address this challenging problem this study proposes the maximum item conflict first micf algorithm experimental results demonstrate that the proposed method is effective has low sanitization rate and can generally achieve significantly lower misses cost than those achieved by the minfia maxfia iga and algob methods in several real and artificial datasets
networked games can provide groupware developers with important lessons in how to deal with real world networking issues such as latency limited bandwidth and packet loss games have similar demands and characteristics to groupware but unlike the applications studied by academics games have provided production quality real time interaction for many years the techniques used by games have not traditionally been made public but several game networking libraries have recently been released as open source providing the opportunity to learn how games achieve network performance we examined five game libraries to find networking techniques that could benefit groupware this paper presents the concepts most valuable to groupware developers including techniques to deal with limited bandwidth reliability and latency some of the techniques have been previously reported in the networking literature therefore the contribution of this paper is to survey which techniques have been shown to work over several years and then to link these techniques to quality requirements specific to groupware by adopting these techniques groupware designers can dramatically improve network performance on the real world internet
recent trend in interface design for classrooms in developing regions has many students interacting on the same display using mice text entry has emerged as an important problem preventing such mouse based singledisplay groupware systems from offering compelling interactive activities we explore the design space of mouse based text entry and develop techniques with novel characteristics suited to the multiple mouse scenario we evaluated these in phase study over days with students in developing region schools the results show that one technique effectively balanced all of our design dimensions another was most preferred by students and both could benefit from augmentation to support collaborative interaction our results also provide insights into the factors that create an optimal text entry technique for single display groupware systems
search engines need to evaluate queries extremely fast challenging task given the quantities of data being indexed significant proportion of the queries posed to search engines involve phrases in this article we consider how phrase queries can be efficiently supported with low disk overheads our previous research has shown that phrase queries can be rapidly evaluated using nextword indexes but these indexes are twice as large as conventional inverted files alternatively special purpose phrase indexes can be used but it is not feasible to index all phrases we propose combinations of nextword indexes and phrase indexes with inverted files as solution to this problem our experiments show that combined use of partial nextword partial phrase and conventional inverted index allows evaluation of phrase queries in quarter the time required to evaluate such queries with an inverted file alone the additional space overhead is only percnt of the size of the inverted file
the concept of privacy preserving has recently been proposed in response to the concerns of preserving personal or sensible information derived from data mining algorithms for example through data mining sensible information such as private information or patterns may be inferred from non sensible information or unclassified data there have been two types of privacy concerning data mining output privacy tries to hide the mining results by minimally altering the data input privacy tries to manipulate the data so that the mining result is not affected or minimally affectedfor output privacy in hiding association rules current approaches require hidden rules or patterns to be given in advance this selection of rules would require data mining process to be executed first based on the discovered rules and privacy requirements hidden rules or patterns are then selected manually however for some applications we are interested in hiding certain constrained classes of association rules such as collaborative recommendation association rules to hide such rules the pre process of finding these hidden rules can be integrated into the hiding process as long as the recommended items are given in this work we propose two algorithms dcis decrease confidence by increase support and dcds decrease confidence by decrease support to automatically hiding collaborative recommendation association rules without pre mining and selection of hidden rules examples illustrating the proposed algorithms are given numerical simulations are performed to show the various effects of the algorithms recommendations of appropriate usage of the proposed algorithms based on the characteristics of databases are reported
in recent years peer to peer pp file sharing systems have evolved to accommodate growing numbers of participating peers in particular new features have changed the properties of the unstructured overlay topologies formed by these peers little is known about the characteristics of these topologies and their dynamics in modern file sharing applications despite their importance this paper presents detailed characterization of pp overlay topologies and their dynamics focusing on the modern gnutella network we present cruiser fast and accurate pp crawler which can capture complete snapshot of the gnutella network of more than one million peers in just few minutes and show how inaccuracy in snapshots can lead to erroneous conclusions such as power law degree distribution leveraging recent overlay snapshots captured with cruiser we characterize the graph related properties of individual overlay snapshots and overlay dynamics across slices of back to back snapshots our results reveal that while the gnutella network has dramatically grown and changed in many ways it still exhibits the clustering and short path lengths of small world network furthermore its overlay topology is highly resilient to random peer departure and even systematic attacks more interestingly overlay dynamics lead to an onion like biased connectivity among peers where each peer is more likely connected to peers with higher uptime therefore long lived peers form stable core that ensures reachability among peers despite overlay dynamics
while soft processor cores provided by fpga vendors offer designers with increased flexibility such processors typically incur penalties in performance and energy consumption compared to hard processor core alternatives the recently developed technology of warp processing can help reduce those penalties warp processing is the dynamic and transparent transformation of critical software regions from microprocessor execution to much faster circuit execution on an fpga in this article we describe an implementation of warp processor on xilinx virtex ii pro and spartan fpgas incorporating one or more microblaze soft processor cores we further provide detailed analysis of the energy overhead of dynamically partitioning an application's kernels to hardware executing within an fpga considering an implementation that periodically partitions the executing application once every minute microblaze based warp processor implemented on spartan fpga achieves average speedups of times and energy reductions of percnt compared to the microblaze soft processor core alone mdash providing competitive performance and energy consumption compared to existing hard processor cores
this paper proposes and evaluates single isa heterogeneousmulti core architectures as mechanism to reduceprocessor power dissipation our design incorporatesheterogeneous cores representing different points inthe power performance design space during an application'sexecution system software dynamically chooses themost appropriate core to meet specific performance andpower requirementsour evaluation of this architecture shows significant energybenefits for an objective function that optimizes forenergy efficiency with tight performance threshold for spec benchmarks our results indicate average energyreduction while only sacrificing in performancean objective function that optimizes for energy delay withlooser performance bounds achieves on average nearly afactor of three improvement in energy delay product whilesacrificing only in performance energy savings aresubstantially more than chip wide voltage frequency scaling
skip graphs are novel distributed data structure based on skip lists that provide the full functionality of balanced tree in distributed system where elements are stored in separate nodes that may fail at any time they are designed for use in searching peer to peer networks and by providing the ability to perform queries based on key ordering they improve on existing search tools that provide only hash table functionality unlike skip lists or other tree data structures skip graphs are highly resilient tolerating large fraction of failed nodes without losing connectivity in addition constructing inserting new elements into searching skip graph and detecting and repairing errors in the data structure introduced by node failures can be done using simple and straight forward algorithms
algorithmic solutions can help reduce energy consumption in computing environs
collaborative filtering has become an established method to measure users similarity and to make predictions about their interests however prediction accuracy comes at the cost of user's privacy in order to derive accurate similarity measures users are required to share their rating history with each other in this work we propose new measure of similarity which achieves comparable prediction accuracy to the pearson correlation coefficient and that can successfully be estimated without breaking users privacy this novel method works by estimating the number of concordant discordant and tied pairs of ratings between two users with respect to shared random set of ratings in doing so neither the items rated nor the ratings themselves are disclosed thus achieving strictly private collaborative filtering the technique has been evaluated using the recently released netflix prize dataset
this article introduces novel representation for three dimensional objects in terms of local affine invariant descriptors of their images and the spatial relationships between the corresponding surface patches geometric constraints associated with different views of the same patches under affine projection are combined with normalized representation of their appearance to guide matching and reconstruction allowing the acquisition of true affine and euclidean models from multiple unregistered images as well as their recognition in photographs taken from arbitrary viewpoints the proposed approach does not require separate segmentation stage and it is applicable to highly cluttered scenes modeling and recognition results are presented
we study an approach to text categorization that combines distributional clustering of words and support vector machine svm classifier this word cluster representation is computed using the recently introduced information bottleneck method which generates compact and efficient representation of documents when combined with the classification power of the svm this method yields high performance in text categorization this novel combination of svm with word cluster representation is compared with svm based categorization using the simpler bag of words bow representation the comparison is performed over three known datasets on one of these datasets the newsgroups the method based on word clusters significantly outperforms the word based representation in terms of categorization accuracy or representation efficiency on the two other sets reuters and webkb the word based representation slightly outperforms the word cluster representation we investigate the potential reasons for this behavior and relate it to structural differences between the datasets
we consider the problem of finding highly correlated pairs in large data set that is given threshold not too small we wish to report all the pairs of items or binary attributes whose pearson correlation coefficients are greater than the threshold correlation analysis is an important step in many statistical and knowledge discovery tasks normally the number of highly correlated pairs is quite small compared to the total number of pairs identifying highly correlated pairs in naive way by computing the correlation coefficients for all the pairs is wasteful with massive data sets where the total number of pairs may exceed the main memory capacity the computational cost of the naive method is prohibitive in their kdd paper hui xiong et al address this problem by proposing the taper algorithm the algorithm goes through the data set in two passes it uses the first pass to generate set of candidate pairs whose correlation coefficients are then computed directly in the second pass the efficiency of the algorithm depends greatly on the selectivity pruning power of its candidate generating stagein this work we adopt the general framework of the taper algorithm but propose different candidate generation method for pair of items taper's candidate generation method considers only the frequencies supports of individual items our method also considers the frequency support of the pair but does not explicitly count this frequency support we give simple randomized algorithm whose false negative probability is negligible the space and time complexities of generating the candidate set in our algorithm are asymptotically the same as taper's we conduct experiments on synthesized and real data the results show that our algorithm produces greatly reduced candidate set one that can be several orders of magnitude smaller than that generated by taper because of this our algorithm uses much less memory and can be faster the former is critical for dealing with massive data
this study uses neural field model to investigate computational aspects of population coding and decoding when the stimulus is single variable general prototype model for the encoding process is proposed in which neural responses are correlated with strength specified by gaussian function of their difference in preferred stimuli based on the model we study the effect of correlation on the fisher information compare the performances of three decoding methods that differ in the amount of encoding information being used and investigate the implementation of the three methods by using recurrent network this study not only rediscovers main results in existing literatures in unified way but also reveals important new features especially when the neural correlation is strong as the neural correlation of firing becomes larger the fisher information decreases drastically we confirm that as the width of correlation increases the fisher information saturates and no longer increases in proportion to the number of neurons however we prove that as the width increases further wider than times the effective width of the turning function the fisher information increases again and it increases without limit in proportion to the number of neurons furthermore we clarify the asymptotic efficiency of the maximum likelihood inference mli type of decoding methods for correlated neural signals it shows that when the correlation covers nonlocal range of population excepting the uniform correlation and when the noise is extremely small the mli type of method whose decoding error satisfies the cauchy type distribution is not asymptotically efficient this implies that the variance is no longer adequate to measure decoding accuracy
recently there has been growing interest in gossip based protocols that employ randomized communication to ensure robust information dissemination in this paper we present novel gossip based scheme using which all the nodes in an node overlay network can compute the common aggregates of min max sum average and rank of their values using log log messages within log log log rounds of communication to the best of our knowledge ours is the first result that shows how to compute these aggregates with high probability using only log log messages in contrast the best known gossip based algorithm for computing these aggregates requires nlog messages and log rounds thus our algorithm allows system designers to trade off small increase in round complexity with significant reduction in message complexity this can lead to dramatically lower network congestion and longer node lifetimes in wireless and sensor networks where channel bandwidth and battery life are severely constrained
partitioning an application among software running on microprocessor and hardware co processors in on chip configurable logic has been shown to improve performance and energy consumption in embedded systems meanwhile dynamic software optimization methods have shown the usefulness and feasibility of runtime program optimization but those optimizations do not achieve as much as partitioning we introduce first approach to dynamic hardware software partitioning we describe our system architecture and initial on chip tools including profiler decompiler synthesis and placement and routing tools for simplified configurable logic fabric able to perform dynamic partitioning of real benchmarks we show speedups averaging for five benchmarks taken from powerstone netbench and our own benchmarks
developing desirable framework for handling inconsistencies in software requirements specifications is challenging problem it has been widely recognized that the relative priority of requirements can help developers to make some necessary trade off decisions for resolving con flicts however for most distributed development such as viewpoints based approaches different stakeholders may assign different levels of priority to the same shared requirements statement from their own perspectives the disagreement in the local levels of priority assigned to the same shared requirements statement often puts developers into dilemma during the inconsistency handling process the main contribution of this paper is to present prioritized merging based framework for handling inconsistency in distributed software requirements specifications given set of distributed inconsistent requirements collections with the local prioritization we first construct requirements specification with prioritization from an overall perspective we provide two approaches to constructing requirements specification with the global prioritization including merging based construction and priority vector based construction following this we derive proposals for handling inconsistencies from the globally prioritized requirements specification in terms of prioritized merging moreover from the overall perspective these proposals may be viewed as the most appropriate to modifying the given inconsistent requirements specification in the sense of the ordering relation over all the consistent subsets of the requirements specification finally we consider applying negotiation based techniques to viewpoints so as to identify an acceptable common proposal from these proposals
even the best laid plans can fail and robot plans executed in real world domains tend to do so often the ability of robot to reliably monitor the execution of plans and detect failures is essential to its performance and its autonomy in this paper we propose technique to increase the reliability of monitoring symbolic robot plans we use semantic domain knowledge to derive implicit expectations of the execution of actions in the plan and then match these expectations against observations we present two realizations of this approach crisp one which assumes deterministic actions and reliable sensing and uses standard knowledge representation system loom and probabilistic one which takes into account uncertainty in action effects in sensing and in world states we perform an extensive validation of these realizations through experiments performed both in simulation and on real robots
database queries are often exploratory and users often find their queries return too many answers many of them irrelevant existing work either categorizes or ranks the results to help users locate interesting results the success of both approaches depends on the utilization of user preferences however most existing work assumes that all users have the same user preferences but in real life different users often have different preferences this paper proposes two step solution to address the diversity issue of user preferences for the categorization approach the proposed solution does not require explicit user involvement the first step analyzes query history of all users in the system offline and generates set of clusters over the data each corresponding to one type of user preferences when user asks query the second step presents to the user navigational tree over clusters generated in the first step such that the user can easily select the subset of clusters matching his needs the user then can browse rank or categorize the results in selected clusters the navigational tree is automatically constructed using cost based algorithm which considers the cost of visiting both intermediate nodes and leaf nodes in the tree an empirical study demonstrates the benefits of our approach
database applications often require to evaluate queries containing quantifiers or disjunctions eg for handling general integrity constraints existing efficient methods for processing quantifiers depart from the relational model as they rely on non algebraic procedures looking at quantified query evaluation from new angle we propose an approach to process quantifiers that makes use of relational algebra operators only our approach performs in two phases the first phase normalizes the queries producing canonical form this form permits to improve the translation into relational algebra performed during the second phase the improved translation relies on new operator the complement join that generalizes the set difference on algebraic expressions of universal quantifiers that avoid the expensive division operator in many cases and on special processing of disjunctions by means of constrained outer joins our method achieves an efficiency at least comparable with that of previous proposals better in most cases furthermore it is considerably simpler to implement as it completely relies on relational data structures and operators
peer to peer pp model being widely adopted in todays internet computing suffers from the problem of topology mismatch between the overlay networks and the underlying physical network traditional topology optimization techniques identify physical closer nodes to connect as overlay neighbors but could significantly shrink the search scope recent efforts have been made to address the mismatch problem without sacrificing search scope but they either need time synchronization among peers or have low convergent speed in this paper we propose scalable bipartite overlay sbo scheme to optimize the overlay topology by identifying and replacing the mismatched connections in sbo we employ an efficient strategy for distributing optimization tasks in peers with different colors we conducted comprehensive simulations to evaluate this design the results show that sbo achieves approximately reduction on traffic cost and about reduction on query response time our comparisons with previous approaches to address the topology mismatch problem have shown that sbo can achieve fast convergent speed without the need of time synchronization among peers
bytecodes and virtual machines vm are prevailing programming facilities in contemporary software industry due to their ease of portability across various platforms thus it is critical to improve their trustworthiness this paper addresses the interesting and challenging problem of certifying bytecode programs over certified vms our solutions to this problem include logical systems cbp for bytecode machine is built to modularly certify bytecode programs with abstract control stacks and unstructured control flows and the corresponding stack based virtual machine is implemented and certified simulation relation between bytecode program and vm implementation is developed and proved to achieve the objective that once some safety property of bytecode program is certified in cbp system the property will be preserved on any certified vm we prove the soundness and demonstrate its power by certifying some example programs with the coq proof assistant this work not only provides solid theoretical foundation for reasoning about bytecode programs but also gains insight into building proof preserving compilers
multiple clock domain mcd processor addresses the challenges of clock distribution and power dissipation by dividing chip into several coarse grained clock domains allowing frequency and voltage to be reduced in domains that are not currently on the application's critical path given reconfiguration mechanism capable of choosing appropriate times and values for voltage frequency scaling an mcd processor has the potential to achieve significant energy savings with low performance degradationearly work on mcd processors evaluated the potential for energy savings by manually inserting reconfiguration instructions into applications or by employing an oracle driven by off line analysis of identical prior program runs subsequent work developed hardware based on line mechanism that averages of the energy delay improvement achieved via off line analysisin this paper we consider the automatic insertion of reconfiguration instructions into applications using profile driven binary rewriting profile based reconfiguration introduces the need for training runs prior to production use of given application but avoids the hardware complexity of on line reconfiguration it also has the potential to yield significantly greater energy savings experimental results training on small data sets and then running on larger alternative data sets indicate that the profile driven approach is more stable than hardware based reconfiguration and yields virtually all of the energy delay improvement achieved via off line analysis
an accurate and rapid method is required to retrieve the overwhelming majority of digital images to date image retrieval methods include content based retrieval and keyword based retrieval the former utilizing visual features such as color and brightness and the latter utilizing keywords that describe the image however the effectiveness of these methods in providing the exact images the user wants has been under scrutiny hence many researchers have been working on relevance feedback process in which responses from the user are given as feedback during the retrieval session in order to define user's need and provide an improved result methods that employ relevance feedback however do have drawbacks because several pieces of feedback are necessary to produce an appropriate result and the feedback information cannot be reused in this paper novel retrieval model is proposed which annotates an image with keywords and modifies the confidence level of the keywords in response to the user's feedback in the proposed model not only the images that have been given feedback but also other images with visual features similar to the features used to distinguish the positive images are subjected to confidence modification this allows for modification of large number of images with relatively little feedback ultimately leading to faster and more accurate retrieval results an experiment was performed to verify the effectiveness of the proposed model and the result demonstrated rapid increase in recall and precision using the same amount of feedback
we present framework for designing efficient distributed data structures for multi dimensional data our structures which we call skip webs extend and improve previous randomized distributed data structures including skipnets and skip graphs our framework applies to general class of data querying scenarios which include linear one dimensional data such as sorted sets as well as multi dimensional data such as dimensional octrees and digital tries of character strings defined over fixed alphabetwe show how to perform query over such set of items spread among hosts using log log log messages for one dimensional data or log messages for fixed dimensional data while using only log space per host we also show how to make such structures dynamic so as to allow for insertions and deletions in log messages for quadtrees octrees and digital tries and log log log messages for one dimensional data finally we show how to apply blocking strategy to skip webs to further improve message complexity for one dimensional data when hosts can store more data
by using elliptic curve cryptography ecc it has been recently shown that public key cryptography pkc is indeed feasible on resource constrained nodes this feasibility however does not necessarily mean attractiveness as the obtained results are still not satisfactory enough in this paper we present results on implementing ecc as well as the related emerging field of pairing based cryptography pbc on two of the most popular sensor nodes by doing that we show that pkc is not only viable but in fact attractive for wsns as far as we know pairing computations presented in this paper are the most efficient results on the mica bit mhz atmegal and tmote sky bit mhz msp nodes
animation techniques for controlling passive simulation are commonly based on an optimization paradigm the user provides goals priori and sophisticated numerical methods minimize cost function that represents these goals unfortunately for multibody systems with discontinuous contact events these optimization problems can be highly nontrivial to solve and many hour offline optimizations unintuitive parameters and convergence failures can frustrate end users and limit usage on the other hand users are quite adaptable and systems which provide interactive feedback via an intuitive interface can leverage the user's own abilities to quickly produce interesting animations however the online computation necessary for interactivity limits scene complexity in practice we introduce many worlds browsing method which circumvents these limits by exploiting the speed of multibody simulators to compute numerous example simulations in parallel offline and online and allow the user to browse and modify them interactively we demonstrate intuitive interfaces through which the user can select among the examples and interactively adjust those parts of the scene that do not match his requirements we show that using combination of our techniques unusual and interesting results can be generated for moderately sized scenes with under an hour of user time scalability is demonstrated by sampling much larger scenes using modest offline computations
current capacity planning practices based on heavy over provisioning of power infrastructure hurt the operational costs of data centers as well as ii the computational work they can support we explore combination of statistical multiplexing techniques to improve the utilization of the power hierarchy within data center at the highest level of the power hierarchy we employ controlled underprovisioning and over booking of power needs of hosted workloads at the lower levels we introduce the novel notion of soft fuses to flexibly distribute provisioned power among hosted workloads based on their needs our techniques are built upon measurement driven profiling and prediction framework to characterize key statistical properties of the power needs of hosted workloads and their aggregates we characterize the gains in terms of the amount of computational work cpu cycles per provisioned unit of power computation per provisioned watt cpw our technique is able to double the cpwoffered by power distribution unit pdu running the commerce benchmark tpc compared to conventional provisioning practices over booking the pdu by based on tails of power profiles yields further improvement of reactive techniques implemented on our xen vmm based servers dynamically modulate cpu dvfs states to ensure power draw below the limits imposed by soft fuses finally information captured in our profiles also provide ways of controlling application performance degradation despite overbooking the th percentile of tpc session response time only grew from sec to sec degradation of
resource aware random key predistribution schemes have been proposed to overcome the limitations of energy constrained wireless sensor networks wsns in most of these schemes each sensor node is loaded with key ring neighbouring nodes are considered to be connected through secure link if they share common key nodes which are not directly connected establish secure path which is then used to negotiate symmetric key however since different symmetric keys are used for different links along the secure path each intermediate node must first decrypt the message received from the upstream node notice that during this process the negotiated key is revealed to each node along the secure path the objective of this paper is to address this shortcoming to this end we propose an end to end pairwise key establishment scheme which uses properly selected set of node disjoint paths to securely negotiate symmetric keys between sensor nodes we show through analysis and simulation that our scheme is highly secure against node captures in wsns
silicon technology advances have made it possible to pack millions of transistors switching at high clock speeds on single chip while these advances bring unprecedented performance to electronic products they also pose difficult power energy consumption problems for example large number of transistors in dense on chip cache memories consume significant static leakage power even if the cache is not used by the current computation while previous compiler research studied code and data restructuring for improving data cache performance to our knowledge there exists no compiler based study that targets data cache leakage power consumption in this paper we present code restructuring techniques for array based and pointer intensive applications for reducing data cache leakage energy consumption the idea is to let the compiler analyze the application code and insert instructions that turn off cache lines that keep variables not used by the current computation this turning off does not destroy contents of cache line and waking up the cache line when it is accessed later does not incur much overhead due to inherent data locality in applications we find that at given time only small portion of the data cache needs to be active the remaining part can be placed into leakage saving mode state ie they can be turned off our experimental results indicate that the proposed compiler based strategy reduces the cache energy consumption significantly we also demonstrate how different compiler optimizations can increase the effectiveness of our strategy
modern source control systems such as subversion preserve change sets of files as atomic commits however the specific ordering information in which files were changed is typically not found in these source code repositories in this paper set of heuristics for grouping change sets ie log entries found in source code repositories is presented given such groups of change sets sequences of files that frequently change together are uncovered this approach not only gives the unordered sets of files but supplements them with partial temporal ordering information the technique is demonstrated on subset of kde source code repository the results show that the approach is able to find sequences of changed files
it is crucial to maximize targeting efficiency and customer satisfaction in personalized marketing state of the art techniques for targeting focus on the optimization of individual campaigns our motivation is the belief that the effectiveness of campaign with respect to customer is affected by how many precedent campaigns have been recently delivered to the customer we raise the multiple recommendation problem which occurs when performing several personalized campaigns simultaneously we formulate the multicampaign assignment problem to solve this issue and propose algorithms for the problem the algorithms include dynamic programming and efficient heuristic methods we verify by experiments the effectiveness of the problem formulation and the proposed algorithms
given an incorrect value produced during failed program run eg wrong output value or value that causes the program to crash the backward dynamic slice of the value very frequently captures the faulty code responsible for producing the incorrect value although the dynamic slice often contains only small percentage of the statements executed during the failed program run the dynamic slice can still be large and thus considerable effort may be required by the programmer to locate the faulty codein this paper we develop strategy for pruning the dynamic slice to identify subset of statements in the dynamic slice that are likely responsible for producing the incorrect value we observe that some of the statements used in computing the incorrect value may also have been involved in computing correct values eg value produced by statement in the dynamic slice of the incorrect value may also have been used in computing correct output value prior to the incorrect value for each such executed statement in the dynamic slice using the value profiles of the executed statements we compute confidence value ranging from to higher confidence value corresponds to greater likelihood that the execution of the statement produced correct value given failed run involving execution of single error we demonstrate that the pruning of dynamic slice by excluding only the statements with the confidence value of is highly effective in reducing the size of the dynamic slice while retaining the faulty code in the slice our experiments show that the number of distinct statements in pruned dynamic slice are to times less than the full dynamic slice confidence values also prioritize the statements in the dynamic slice according to the likelihood of them being faulty we show that examining the statements in the order of increasing confidence values is an effective strategy for reducing the effort of fault location
dynamic inference techniques have been demonstrated to provide useful support for various software engineering tasks including bug finding test suite evaluation and improvement and specification generation to date however dynamic inference has only been used effectively on small programs under controlled conditions in this paper we identify reasons why scaling dynamic inference techniques has proven difficult and introduce solutions that enable dynamic inference technique to scale to large programs and work effectively with the imperfect traces typically available in industrial scenarios we describe our approximate inference algorithm present and evaluate heuristics for winnowing the large number of inferred properties to manageable set of interesting properties and report on experiments using inferred properties we evaluate our techniques on jboss and the windows kernel our tool is able to infer many of the properties checked by the static driver verifier and leads us to discover previously unknown bug in windows
the last fifteen years has seen vast proliferation of middleboxes to solve all manner of persistent limitations in the internet protocol suite examples include firewalls nats load balancers traffic shapers deep packet intrusion detection virtual private networks network monitors transparent web caches content delivery networks and the list goes on and on however most smaller networks in homes small businesses and the developing world are left without this level of support further the management burden and limitations of middleboxes are apparent even in enterprise networks we argue for shift from using proprietary middle box harware as the dominant tool for managing networks toward using open software running on end hosts we show that functionality that seemingly must be in the network such as nats and traffic prioritization can be more cheaply flexibly and securely provided by distributed software running on end hosts working in concert with vastly simplified physical network hardware
set valued ordered information systems can be classified into two categories disjunctive and conjunctive systems through introducing two new dominance relations to set valued information systems we first introduce the conjunctive disjunctive set valued ordered information systems and develop an approach to queuing problems for objects in presence of multiple attributes and criteria then we present dominance based rough set approach for these two types of set valued ordered information systems which is mainly based on substitution of the indiscernibility relation by dominance relation through the lower upper approximation of decision some certain possible decision rules from so called set valued ordered decision table can be extracted finally we present attribute reduction also called criteria reduction in ordered information systems approaches to these two types of ordered information systems and ordered decision tables which can be used to simplify set valued ordered information system and find decision rules directly from set valued ordered decision table these criteria reduction approaches can eliminate those criteria that are not essential from the viewpoint of the ordering of objects or decision rules
nowadays embedded systems are growing at an impressive rate and provide more and more sophisticated applications characterized by having complex array index manipulation and large number of data accesses those applications require high performance specific computation that general purpose processors can not deliver at reasonable energy consumption very long instruction word architectures seem good solution providing enough computational performance at low power with the required programmability to speed up the time to market those architectures rely on compiler effort to exploit the available instruction and data parallelism to keep the data path busy all the time with the density of transistors doubling each months more and more sophisticated architectures with high number of computational resources running in parallel are emerging with this increasing parallel computation the access to data is becoming the main bottleneck that limits the available parallelism to alleviate this problem in current embedded architectures special unit works in parallel with the main computing elements to ensure efficient feed and storage of the data the address generator unit which comes in many flavors future architectures will have to deal with enormous memory bandwidth in distributed memories and the development of address generators units will be crucial for effective next generation of embedded processors where global trade offs between reaction time bandwidth energy and area must be achieved this paper provides survey of methods and techniques that optimize the address generation process for embedded systems explaining current research trends and needs for future
students studying topics in cyber security benefit from working with realistic training labs that test their knowledge of network security cost space time and reproducibility are major factors that prevent instructors from building realistic networks for their students this paper explores the ways that existing virtualization technologies could be packaged to provide more accessible comprehensive and realistic training and education environment the paper focuses on ways to leverage technologies such as operating system virtualization and other virtualization techniques to recreate an entire network environment consisting of dozens of nodes on moderately equipped hardware
in this paper we consider the problem of web page usage prediction in web site by modeling users navigation history with weighted suffix trees this user's navigation prediction can be exploited either in an on line recommendation system in website or in web page cache system the method proposed has the advantage that it demands constant amount of computational effort per user action and consumes relatively small amount of extra memory space these features make the method ideal for an on line working environment finally we have performed an evaluation of the proposed scheme with experiments on various website logfiles and we have found that its prediction quality is fairly good in many cases outperforming existing solutions
current systems on chip soc execute applications that demand extensive parallel processing networks on chip noc provide structured way of realizing interconnections on silicon and obviate the limitations of bus based solution nocs can have regular or ad hoc topologies and functional validation is essential to assess their correctness and performance in this paper we present flexible emulation environment implemented on an fpga that is suitable to explore evaluate and compare wide range of noc solutions with very limited effort our experimental results show speed up of four orders of magnitude with respect to cycle accurate hdl simulation while retaining cycle accuracy with our emulation framework designers can explore and optimize various range of solutions as well as characterize quickly performance figures
approaches for indexing proteins and for fast and scalable searching for structures similar to query structure have important applications such as protein structure and function prediction protein classification and drug discovery in this paper we develop new method for extracting local structural or geometric features from protein structures these feature vectors are in turn converted into set of symbols which are then indexed using suffix tree for given query the suffix tree index can be used effectively to retrieve the maximal matches which are then chained to obtain the local alignments finally similar proteins are retrieved by their alignment score against the query our results show classification accuracy up to and at the topology and class level according to the cath classification these results outperform the best previous methods we also show that psist is highly scalable due to the external suffix tree indexing approach it uses it is able to index about domains from scop in under an hour
we introduce rich language of descriptions for semistructured tree like data and we explain how such descriptions relate to the data they describe various query languages and data schemas can be based on such descriptions
internet routing is mostly based on static information it's dynamicity is limited to reacting to changes in topology adaptive performance based routing decisions would not only improve the performance itself of the internet but also its security and availability however previous approaches for making internet routing adaptive based on optimizing network wide objectives are not suited for an environment in which autonomous and possibly malicious entities interact in this paper we propose different framework for adaptive routing decisions based on regret minimizing online learning algorithms these algorithms as applied to routing are appealing because adopters can independently improve their own performance while being robust to adversarial behavior however in contrast to approaches based on optimization theory that provide guarantees from the outset about network wide behavior the network wide behavior if online learning algorithms were to interact with each other is less understood in this paper we study this interaction in realistic internet environment and find that the outcome is stable state and that the optimality gap with respect to the network wide optimum is small our findings suggest that online learning may be suitable framework for adaptive routing decisions in the internet
real time embedded systems are typically constrained in terms of three system performance criteria space time and energy the performance requirements are directly translated into constraints imposed on the system's resources such as code size execution time and energy consumption these resource constraints often interact or even conflict with each other in complex manner making it difficult for system developer to apply well defined design methodology in developing real time embedded system motivated by this observation we propose design framework that can flexibly balance the tradeoff involving the system's code size execution time and energy consumption given system specification and an optimization criteria the proposed technique generates set of design parameters in such way that system cost function is minimized while the given resource constraints are satisfied specifically the technique derives code generation decision for each task so that specific version of code is selected among number of different ones that have distinct characteristics in terms of code size and execution time in addition the design framework determines the voltage frequency setting for variable voltage processor whose supply voltage can be adjusted at runtime in order to minimize the energy consumption while execution performance is degraded accordingly the proposed technique formulates this design process as constrained optimization problem we show that this optimization problem is np hard and then provide heuristic solution to it we show that these seemingly conflicting design goals can be pursued by using simple optimization algorithm that works with single optimization criteria moreover the optimization is driven by an abstract system specification given by the system developer so that the system development process can be automated the results from our simulation show that the proposed algorithm finds solution that is close to the optimal one with the average error smaller than percent
the goal of the research described here is to develop multistrategy classifier system that can be used for document categorization the system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample the learners work in parallel manner where each learner carries out its own feature selection based on evolutionary techniques and then obtains classification model in classifying documents the system combines the predictions of the learners by applying evolutionary techniques as well the system relies on modular flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain
we show that for several natural classes of structured matrices including symmetric circulant hankel and toeplitz matrices approximating the permanent modulo prime is as hard as computing its exact value results of this kind are well known for arbitrary matrices however the techniques used do not seem to apply to structured matrices our approach is based on recent advances in the hidden number problem introduced by boneh and venkatesan in combined with some bounds of exponential sums motivated by the waring problem in finite fields
nowadays it is widely accepted that the data warehouse design task should be largely automated furthermore the data warehouse conceptual schema must be structured according to the multidimensional model and as consequence the most common way to automatically look for subjects and dimensions of analysis is by discovering functional dependencies as dimensions functionally depend on the fact over the data sources most advanced methods for automating the design of the data warehouse carry out this process from relational oltp systems assuming that rdbms is the most common kind of data source we may find and taking as starting point relational schema in contrast in our approach we propose to rely instead on conceptual representation of the domain of interest formalized through domain ontology expressed in the dl lite description logic we propose an algorithm to discover functional dependencies from the domain ontology that exploits the inference capabilities of dl lite thus fully taking into account the semantics of the domain we also provide an evaluation of our approach in real world scenario
while set associative caches incur fewer misses than direct mapped caches they typically have slower hit times and higher power consumption when multiple tag and data banks are probed in parallel this paper presents the location cache structure which significantly reduces the power consumption for large set associative caches we propose to use small cache called location cache to store the location of future cache references if there is hit in the location cache the supported cache is accessed as direct mapped cache otherwise the supported cache is referenced as conventional set associative cachethe worst case access latency of the location cache system is the same as that of conventional cache the location cache is virtually indexed so that operations on it can be performed in parallel with the tlb address translation these advantages make it ideal for cache systems where traditional way predication strategies perform poorlywe used the cacti cache model to evaluate the power con sumption and access latency of proposed cache architecture simplescalar cpu simulator was used to produce final results it is shown that the proposed location cache architecture is power efficient in the simulated cache configurations up to of cache accessing energy and of average cache access latency can be reduced
middleware is often built using layered architectural style layered design provides good separation of the different concerns of middleware such as communication marshaling request dispatching thread management etc layered architecture helps in the development and evolution of the middleware it also provides tactical side benefits layers provide convenient protection boundaries for enforcing security policies however the benefits of this layered structure come at cost layered designs can hinder performance related optimizations and actually make it more difficult to adapt systems to conveniently address late bound requirements such as dependability access control virus protection and so on we present some examples of this issue and outline new approach under investigation at uc davis which includes ideas in middleware architectures and programming models
symmetric multiprocessor smp servers provide superior performance for the commercial workloads that dominate the internet our simulation results show that over one third of cache misses by these applications result in cache to cache transfers where the data is found in another processor's cache rather than in memory smps are optimized for this case by using snooping protocols that broadcast address transactions to all processors conversely directory based shared memory systems must indirectly locate the owner and sharers through directory resulting in larger average miss latenciesthis paper proposes timestamp snooping technique that allows smps to utilize high speed switched interconnection networks and ii exploit physical locality by delivering address transactions to processors and memories without regard to order traditional snooping requires physical ordering of transactions timestamp snooping works by processing address transactions in logical order logical time is maintained by adding few bits per address transaction and having network switches perform handshake to ensure on time delivery processors and memories then reorder transactions based on their timestamps to establish total orderwe evaluate timestamp snooping with commercial workloads on processor sparc system using the simics full system simulator we simulate both an indirect butterfly and direct torus network design for oltp dss web serving web searching and one scientific application timestamp snooping with the butterfly network runs faster than directories at cost of more link traffic similarly with the torus network timestamp snooping runs faster for more link traffic thus timestamp snooping is worth considering when buying more interconnect bandwidth is easier than reducing interconnect latency
scientific research and practical applications of solar physics require data and computational services to be integrated seamlessly and efficiently the european grid for solar observations egso leverages grid oriented concepts and technology to provide high performance infrastructure for solar applications in this paper an architecture for data brokerage service is proposed brokers interact with providers and consumers in order to build profile of both parties in particular broker interacts with providers in order to gather information on the data potentially available to consumers and with the consumers in order to identify the set of providers that are most likely to satisfy specific data needs the brokerage technique is based on multi tier management of metadata copyright copy john wiley sons ltd
superimposition is composition technique that has been applied successfully in many areas of software development although superimposition is general purpose concept it has been re invented and implemented individually for various kinds of software artifacts we unify languages and tools that rely on superimposition by using the language independent model of feature structure trees fsts on the basis of the fst model we propose general approach to the composition of software artifacts written in different languages furthermore we offer supporting framework and tool chain called featurehouse we use attribute grammars to automate the integration of additional languages in particular we have integrated java haskell javacc and xml several case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties language must have in order to be ready for superimposition
probabilistic top ranking queries have been extensively studied due to the fact that data obtained can be uncertain in many real applications probabilistic top ranking query ranks objects by the interplay of score and probability with an implicit assumption that both scores based on which objects are ranked and probabilities of the existence of the objects are stored in the same relation we observe that in general scores and probabilities are highly possible to be stored in different relations for example in column oriented dbmss and in data warehouses in this paper we study probabilistic top ranking queries when scores and probabilities are stored in different relations we focus on reducing the join cost in probabilistic top ranking we investigate two probabilistic score functions discuss the upper lower bounds in random access and sequential access and provide insights on the advantages and disadvantages of random sequential access in terms of upper lower bounds we also propose random sequential and hybrid algorithms to conduct probabilistic top ranking we conducted extensive performance studies using real and synthetic datasets and report our findings in this paper
we describe polynomial time algorithm for global value numbering which is the problem of discovering equivalences among program sub expressions we treat all conditionals as non deterministic and all program operators as uninterpreted we show that there are programs for which the set of all equivalences contains terms whose value graph representation requires exponential size our algorithm discovers all equivalences among terms of size at most in time that grows linearly with for global value numbering it suffices to choose to be the size of the program earlier deterministic algorithms for the same problem are either incomplete or take exponential time we provide detailed analytical comparison of some of these algorithms
level one cache normally resides on processor's critical path which determines the clock frequency directmapped caches exhibit fast access time but poor hit rates compared with same sized set associative caches due to nonuniform accesses to the cache sets which generate more conflict misses in some sets while other sets are underutilized we propose technique to reduce the miss rate of direct mapped caches through balancing the accesses to cache sets we increase the decoder length and thus reduce the accesses to heavily used sets without dynamically detecting the cache set usage information we introduce replacement policy to direct mapped cache design and increase the access to the underutilized cache sets with the help of programmable decoders on average the proposed balanced cache or bcache achieves and miss rate reductions on all speck benchmarks for the instruction and data caches respectively this translates into an average ipc improvement of the cache consumes more power per access but exhibits total memory access related energy saving due to the miss rate reductions and hence the reduction to applications execution time compared with previous techniques that aim at reducing the miss rate of direct mapped caches our technique requires only one cycle to access all cache hits and has the same access time of direct mapped cache
the problem of results merging in distributed information retrieval environments has been approached by two different directions in research estimation approaches attempt to calculate the relevance of the returned documents through ad hoc methodologies weighted score merging regression etc while download approaches download all the documents locally partially or completely in order to estimate first hand their relevance both have their advantages and disadvantages it is assumed that download algorithms are more effective but they are very expensive in terms of time and bandwidth estimation approaches on the other hand usually rely on document relevance scores being returned by the remote collections in order to achieve maximum performance in addition to that regression algorithms which have proved to be more effective than weighted scores merging rely on significant number of overlap documents in order to function effectively practically requiring multiple interactions with the remote collections the new algorithm that is introduced reconciles the above two approaches combining their strengths while minimizing their weaknesses it is based on downloading limited selected number of documents from the remote collections and estimating the relevance of the rest through regression methodologies the proposed algorithm is tested in variety of settings and its performance is found to be better than estimation approaches while approximating that of download
it is well known that grid technology has the ability to coordinate shared resources and scheduled tasks however the problem of resource management and task scheduling has always been one of the main challenges in this paper we present performance effective pre scheduling strategy for dispatching tasks onto heterogeneous processors the main extension of this study is the consideration of heterogeneous communication overheads in grid systems one significant improvement of our approach is that average turnaround time could be minimized by selecting the processor that has the smallest communication ratio first the other advantage of the proposed method is that system throughput can be increased by dispersing processor idle time our proposed technique can be applied on heterogeneous cluster systems as well as computational grid environments in which the communication costs vary in different clusters to evaluate performance of the proposed techniques we have implemented the proposed algorithms along with previous methods the experimental results show that our techniques outperform other algorithms in terms of lower average turnaround time higher average throughput less processor idle time and higher processors utilization
recent years have witnessed large body of research work on mining concept drifting data streams where primary assumption is that the up to date data chunk and the yet to come data chunk share identical distributions so classifiers with good performance on the up to date chunk would also have good prediction accuracy on the yet to come data chunk this stationary assumption however does not capture the concept drifting reality in data streams more recently learnable assumption has been proposed and allows the distribution of each data chunk to evolve randomly although this assumption is capable of describing the concept drifting in data streams it is still inadequate to represent real world data streams which usually suffer from noisy data as well as the drifting concepts in this paper we propose realistic assumption which asserts that the difficulties of mining data streams are mainly caused by both concept drifting and noisy data chunks consequently we present new aggregate ensemble ae framework which trains base classifiers using different learning algorithms on different data chunks all the base classifiers are then combined to form classifier ensemble through model averaging experimental results on synthetic and real world data show that ae is superior to other ensemble methods under our new realistic assumption for noisy data streams
it is becoming apparent that the next generation ip route lookup architecture needs to achieve speeds of gbps and beyond while supporting both ipv and ipv with fast real time updates to accommodate ever growing routing tables some of the proposed multibit trie based schemes such as tree bitmap have been used in today's high end routers however their large data structure often requires multiple external memory accesses for each route lookup pipelining technique is widely used to achieve high speed lookup with cost of using many external memory chips pipelining also often leads to poor memory load balancing in this paper we propose new ip route lookup architecture called flashtrie that overcomes the shortcomings of the multibit trie based approach we use hash based membership query to limit off chip memory accesses per lookup to one and to balance memory utilization among the memory modules we also develop new data structure called prefix compressed trie that reduces the size of bitmap by more than our simulation and implementation results show that flashtrie can achieve gbps worst case throughput while simultaneously supporting prefixes for ipv and prefixes for ipv using one fpga chip and four ddr sdram chips flashtrie also supports incremental real time updates
we present an adaptive fault tolerant wormhole routing algorithm for hypercubes by using virtual networks the routing algorithm can tolerate at least faulty nodes and can route message via path of length no more than the shortest path plus four previous algorithms which achieve the same fault tolerant ability need virtual networks simulation results are also given in this paper
the advent of strong multi level partitioners has made topdown min cut placers favored choice for modern placer implementations we examine terminal propagation an important step in min cut placers because it is responsible for translating partitioning results into global placement wirelength assumptions in this work we identify previously overlooked problem ambiguous terminal propagation and propose solution based on the concept of feedback from automatic control systems implementing our approach in capo version and applying it to standard benchmark circuits yields up to wirelength reductions for the ibm benchmarks and reductions for peko instances experiments also show consistent improvements for routed wirelength yielding up to wirelength reductions with practical increase in placement runtime in addition our method significantly improves routability without building congestion maps and reduces the number of vias
efficient fine grain synchronization is extremely important to effectively harness the computational power of many core architectures however designing and implementing finegrain synchronization in such architectures presents several challenges including issues of synchronization induced overhead storage cost scalability and the level of granularity to which synchronization is applicable this paper proposes the synchronization state buffer ssb scalable architectural design for fine grain synchronization that efficiently performs synchronizations between concurrent threads the design of ssb is motivated by the following observation at any instance during the parallel execution only small fraction of memory locations are actively participating in synchronization based on this observation we present fine grain synchronization design that records and manages the states of frequently synchronized data using modest hardware support we have implemented the ssb design in the context of the core ibm cyclops architecture using detailed simulation we present our experience for set of benchmarks with different workload characteristics
pattern matching in concrete syntax is very useful in program manipulation tools in particular user defined extensions to such tools are written much easier using concrete syntax patterns few advanced frameworks for language development implement support for concrete syntax patterns but mainstream frameworks used today still do not support them this prevents most existing program manipulation tools from using concrete syntax matching which in particular severely limits the writing of tool extensions to few language experts this paper argues that the major implementation obstacle to the pervasive use of concrete syntax patterns is the pattern parser we propose an alternative approach based on unparsed patterns which are concrete syntax patterns that can be efficiently matched without being parsed this lighter approach gives up static checks that parsed patterns usually do in turn it can be integrated within any existing parser based software tool almost for free one possible consequence is enabling widespread adoption of extensible program manipulation tools by the majority of programmers unparsed patterns can be used in any programing language including multi lingual environments to demonstrate our approach we implemented it both as minimal patch for the gcc compiler allowing to scan source code for user defined patterns and as stand alone prototype called matchbox
depth of field refers to the swath through scene that is imaged in acceptable focus through an optics system such as camera lens control over depth of field is an important artistic tool that can be used to emphasize the subject of photograph in real camera the control over depth of field is limited by the nature of the image formation process and by physical constraints the depth of field effect has been simulated in computer graphics but with the same limited control as found in real camera lenses in this paper we use diffusion in non homogeneous medium to generalize depth of field in computer graphics by enabling the user to independently specify the degree of blur at each point in three dimensional space generalized depth of field provides novel tool to emphasize an area of interest within scene to pick objects out of crowd and to render busy complex picture more understandable by focusing only on relevant details that may be scattered throughout the scene our algorithm operates by blurring sequence of nonplanar layers that form the scene choosing suitable blur algorithm for the layers is critical thus we develop appropriate blur semantics such that the blur algorithm will properly generalize depth of field we found that diffusion in non homogeneous medium is the process that best suits these semantics
this paper introduces the expander new object oriented oo programming language construct designed to support object adaptation expanders allow existing classes to be noninvasively updated with new methods fields and superinterfaces each client can customize its view of class by explicitly importing any number of expanders this view then applies to all instances of that class including objects passed to the client from other components form of expander overriding allows expanders to interact naturally with oo style inheritancewe describe the design implementation and evaluation of ejava an extension to java supporting expanders we illustrate ejava's syntax and semantics through several examples the statically scoped nature of expander usage allows for modular static type system that prevents several important classes of errors we describe this modular static type system informally formalize ejava and its type system in an extension to featherweight java and prove type soundness theorem for the formalization we also describe modular compilation strategy for ejava which we have implemented using the polyglot extensible compiler framework finally we illustrate the practical benefits of ejava by using this compiler in two experiments
methods for triangle mesh decimation are common however most existing techniques operate only on static geometry in this paper we present view and pose independent method for the automatic simplification of skeletally articulated meshes such meshes have associated kinematic skeletons that are used to control their deformation with the position of each vertex influenced by linear combination of bone transformations our method extends the commonly used quadric error metric by incorporating knowledge of potential poses into probability function we minimize the average error of the deforming mesh over all possible configurations weighted by the probability this is possible by transforming the quadrics from each configuration into common coordinate system our simplification algorithm runs as preprocess and the resulting meshes can be seamlessly integrated into existing systems we demonstrate the effectiveness of this approach for generating highly simplified models while preserving necessary detail in deforming regions near joints
reliable broadband communication is becoming increasingly important during disaster recovery and emergency response operations in situations where infrastructure based communication is not available or has been disrupted an incident area network needs to be dynamically deployed ie temporary network that provides communication services for efficient crisis management at an incident site wireless mesh networks wmns are multi hop wireless networks with self healing and self configuring capabilities these features combined with the ability to provide wireless broadband connectivity at comparably low cost make wmns promising technology for incident management communications this paper specifically focuses on hybrid wmns which allow both mobile client devices as well as dedicated infrastructure nodes to form the network and provide routing and forwarding functionality hybrid wmns are the most generic and most flexible type of mesh networks and are ideally suited to meet the requirements of incident area communications however current wireless mesh and ad hoc routing protocols do not perform well in hybrid wmn and are not able to establish stable and high throughput communication paths one of the key reasons for this is their inability to exploit the typical high degree of heterogeneity in hybrid wmns safemesh the routing protocol presented in this paper addresses the limitations of current mesh and ad hoc routing protocols in the context of hybrid wmns safemesh is based on the well known aodv routing protocol and implements number of modifications and extensions that significantly improve its performance in hybrid wmns this is demonstrated via an extensive set of simulation results we further show the practicality of the protocol through prototype implementation and provide performance results obtained from small scale testbed deployment
topic modeling has been key problem for document analysis one of the canonical approaches for topic modeling is probabilistic latent semantic indexing which maximizes the joint probability of documents and terms in the corpus the major disadvantage of plsi is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus which leads to serious problems with overfitting latent dirichlet allocation lda is proposed to overcome this problem by treating the probability distribution of each document over topics as hidden random variable both of these two methods discover the hidden topics in the euclidean space however there is no convincing evidence that the document space is euclidean or flat therefore it is more natural and reasonable to assume that the document space is manifold either linear or nonlinear in this paper we consider the problem of topic modeling on intrinsic document manifold specifically we propose novel algorithm called laplacian probabilistic latent semantic indexing lapplsi for topic modeling lapplsi models the document space as submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question we compare the proposed lapplsi approach with plsi and lda on three text data sets experimental results show that lapplsi provides better representation in the sense of semantic structure
the system and network architecture for static sensor nets is largely solved today with many stable commercial solutions now available and standardization efforts underway at the ieee ietf isa and within many industry groups as result many researchers have begun to explore new domains like mobile sensor networks or mobiscopes since they enable new applications in the home and office for health and safety and in transportation and asset management this paper argues that mobility invalidates many assumptions implicit in low power static designs so the architecture for micropower mobiscopes is still very much an open research question in this paper we explore several mobile sensing applications identify research challenges to their realization and explore how emerging technologies and real time motion data could help ease these challenges
software maintenance tools for program analysisand refactoring rely on meta model capturing the relevantproperties of programs however what is considered relevantmay change when the tools are extended with new analyses andrefactorings and new programming languages this paper proposesa language independent meta model and an architecture toconstruct instances thereof which is extensible for new analyses refactorings and new front ends of programming languages dueto the loose coupling between analysis refactoring and frontend components new components can be added independentlyand reuse existing ones two maintenance tools implementingthe meta model and the architecture vizzanalyzer and xdevelop serve as proof of concept
this article explores the architectural challenges introduced by emerging bottom up fabrication of nanoelectronic circuits the specific nanotechnology we explore proposes patterned dna nanostructures as scaffold for the placement and interconnection of carbon nanotube or silicon nanorod fets to create limited size circuit node three characteristics of this technology that significantly impact architecture are limited node size random node interconnection and high defect rates we present and evaluate an accumulator based active network architecture that is compatible with any technology that presents these three challenges this architecture represents an initial unoptimized solution for understanding the implications of dna guide self assembly
structured peer to peer pp overlays have been successfully employed in many applications to locate content however they have been less effective in handling massive amounts of data because of the high overhead of maintaining indexes in this paper we propose pisces peer based system that indexes selected content for efficient search unlike traditional approaches that index all data pisces identifies subset of tuples to index based on some criteria such as query frequency update frequency index cost etc in addition coarse grained range index is built to facilitate the processing of queries that cannot be fully answered by the tuple level index more importantly pisces can adaptively self tune to optimize the subset of tuples to be indexed that is the partial index in pisces is built in just in time jit manner beneficial tuples for current users are pulled for indexing while indexed tuples with infrequent access and high maintenance cost are discarded we also introduce light weight monitoring scheme for structured networks to collect the necessary statistics we have conducted an extensive experimental study on planetlab to illustrate the feasibility practicality and efficiency of pisces the results show that pisces incurs lower maintenance cost and offers better search and query efficiency compared to existing methods
vast amount of valuable information produced and consumed by people and institutions is currently stored in relational databases for many purposes there is an ever increasing demand for having these databases published on the web so that users can query the data available in them an important requirement for this to happen is that query interfaces must be as simple and intuitive as possible in this paper we present labrador system for efficiently publishing relational databases on the web by using simple text box query interface the system operates by taking an unstructured keyword based query posed by user and automatically deriving an equivalent sql query that fits the user's information needs as expressed by the original query the sql query is then sent to dbms and its results are processed by labrador to create relevance based ranking of the answers experiments we present show that labrador can automatically find the most suitable sql query in more than of the cases and that the overhead introduced by the system in the overall query processing time is almost insignificant furthermore the system operates in non intrusive way since it requires no modifications to the target database schema
discriminative probabilistic models are very popular in nlp because of the latitude they afford in designing features but training involves complex trade offs among weights which can be dangerous few highly indicative features can swamp the contribution of many individually weaker features causing their weights to be undertrained such model is less robust for the highly indicative features may be noisy or missing in the test data to ameliorate this weight undertraining we introduce several new feature bagging methods in which separate models are trained on subsets of the original features and combined using mixture model or product of experts these methods include the logarithmic opinion pools used by smith et al we evaluate feature bagging on linear chain conditional random fields for two natural language tasks on both tasks the feature bagged crf performs better than simply training single crf on all the features
java programmers can document that the relationship between two objects is unchanging by declaring the field that encodes that relationship to be final this information can be used in program understanding and detection of errors in new code additions unfortunately few fields in programs are actually declared final programs often contain fields that could be final but are not declared so moreover the definition of final has restrictions on initializationthat limit its applicability we introduce stationary fields as generalization of final field in program is stationary if for every object that contains it all writes to the field occur before all the reads unlike the definition of final fields there can be multiple writes during initialization and initialization can span multiple methods we have developed an efficient algorithm for inferring which fields are stationary in program based on the observation that many fields acquire their value very close to object creation we presume that an object's initialization phase has concluded when its reference is saved in some heap object we perform precise analysis only regarding recently created objects applying our algorithm to real world java programs demonstrates that stationary fields are more common than final fields vs respectively in our benchmarks these surprising results have several significant implications first substantial portions of java programs appear to be written in functional style second initialization of these fields occurs very close to object creation when very good alias information is available these results open the door for more accurate and efficient pointer alias analysis
this article proposes statistical approach for fast articulated body tracking similar to the loose limbed model but using the factor graph representation and fast estimation algorithm fast nonparametric belief propagation on factor graphs is used to estimate the current marginal for each limb all belief propagation messages are represented as sums of weighted samples the resulting algorithm corresponds to set of particle filters one for each limb where an extra step recomputes the weight of each sample by taking into account the links between limbs applied to upper body tracking with stereo and colour images the resulting algorithm estimates the body pose in quasi real time hz results on sequences illustrate the effectiveness of this approach
we focus on collaborative filtering dealing with self organizing communities host mobility wireless access and ad hoc communications in such domain knowledge representation and users profiling can be hard remote servers can be often unreachable due to client mobility and feedback ratings collected during random connections to other users ad hoc devices can be useless because of natural differences between human beings our approach is based on so called affinity networks and on novel system called mobhinter that epidemically spreads recommendations through spontaneous similarities between users main results of our study are two fold firstly we show how to reach comparable recommendation accuracies in the mobile domain as well as in complete knowledge scenario secondly we propose epidemic collaborative strategies that can reduce rapidly and realistically the cold start problem
different notions of provenance for database queries have been proposed and studied in the past few years in this article we detail three main notions of database provenance some of their applications and compare and contrast amongst them specifically we review why how and where provenance describe the relationships among these notions of provenance and describe some of their applications in confidence computation view maintenance and update debugging and annotation propagation
we present novel approach for classifying documents that combines different pieces of evidence eg textual features of documents links and citations transparently through data mining technique which generates rules associating these pieces of evidence to predefined classes these rules can contain any number and mixture of the available evidence and are associated with several quality criteria which can be used in conjunction to choose the best rule to be applied at classification time our method is able to perform evidence enhancement by link forwarding backwarding ie navigating among documents related through citation so that new pieces of link based evidence are derived when necessary furthermore instead of inducing single model or rule set that is good on average for all predictions the proposed approach employs lazy method which delays the inductive process until document is given for classification therefore taking advantage of better qualitative evidence coming from the document we conducted systematic evaluation of the proposed approach using documents from the acm digital library and from brazilian web directory our approach was able to outperform in both collections all classifiers based on the best available evidence in isolation as well as state of the art multi evidence classifiers we also evaluated our approach using the standard webkb collection where our approach showed gains of in accuracy being times faster further our approach is extremely efficient in terms of computational performance showing gains of more than one order of magnitude when compared against other multi evidence classifiers
distributed digital libraries dls integration is significant for the enforcement of novel searching mechanisms in the internet the great heterogeneity of systems storing and providing digital content requires the introduction of interoperability aspects in order to resolve integration problems in flexible and dynamic way our approach introduces an innovative service oriented pp system which initialises distributed ontology schema semantically describing and indexing the digital content stored in distributed dls the proposed architecture enforces the distributed semantic index by defining virtual clusters consisting of nodes peers with similar or related content in order to provide efficient searching and recommendation mechanisms furthermore use case example is presented in order to demonstrate the functionalities of the proposed architecture in pp network with dls containing cultural material
the maintenance of an existing database depends on the depth of understanding of its characteristics such an understanding is easily lost when the developers disperse the situation becomes worse when the related documentation is missing this paper addresses this issue by extracting the extended entity relationship schema from the relational schema we developed algorithms that investigate characteristics of an existing legacy database in order to identify candidate keys of all relations in the relational schema to locate foreign keys and to decide on the appropriate links between the given relations based on this analysis graph consistent with the entity relationship diagram is derived to contain all possible uniary and binary relationships between the given relations the minimum and maximum cardinalities of each link in the mentioned graph are determined and extra links within the graph are identified and categorized if any the latter information is necessary to optimize foreign keys related information finally the last steps in the process involve when applicable suggesting improvements on the original conceptual design deciding on relationships with attributes many to many and ary relationships and identifying is links user involvement in the process is minimized to the case of having multiple choices where the system does not have the semantic knowledge required to decide on certain choice
in this paper we propose new learning method for extracting bilingual word pairs from parallel corpora in various languages in cross language information retrieval the system must deal with various languages therefore automatic extraction of bilingual word pairs from parallel corpora with various languages is important however previous works based on statistical methods are insufficient because of the sparse data problem our learning method automatically acquires rules which are effective to solve the sparse data problem only from parallel corpora without any prior preparation of bilingual resource eg bilingual dictionary machine translation system we call this learning method inductive chain learning icl moreover the system using icl can extract bilingual word pairs even from bilingual sentence pairs for which the grammatical structures of the source language differ from the grammatical structures of the target language because the acquired rules have the information to cope with the different word orders of source language and target language in local parts of bilingual sentence pairs evaluation experiments demonstrated that the recalls of systems based on several statistical approaches were improved through the use of icl
distributed shared memory dsm is an abstraction of shared memory on distributed memory machine hardware dsm systems support this abstraction at the architecture level software dsm systems support the abstraction within the runtime system one of the key problems in building an efficient software dsm system is to reduce the amount of communication needed to keep the distributed memories consistent in this article we present four techniques for doing so software release consistency multiple consistency protocols write shared protocols and an update with timeout mechanism these techniques have been implemented in the munin dsm system we compare the performance of seven munin application programs first to their performance when implemented using message passing and then to their performance when running on conventional software dsm system that does not embody the preceding techniques on processor cluster of workstations munin's performance is within of message passing for four out of the seven applications for the other three performance is within to detailed analysis of two of these three applications indicates that the addition of function shipping capability would bring their performance to within of the message passing performance compared to conventional dsm system munin achieves performance improvements ranging from few to several hundred percent depending on the application
in this paper we present our experiences concerning the enforcement of access rights extracted from odrl based digital contracts we introduce the generalized contract schema cosa which is an approach to provide generic representation of contract information on top of rights expression languages we give an overview of the design and implementation of the xorelinterpreter software component in particular the xorelinterpreter interprets digital contracts that are based on rights expression languages eg odrl or xrml and builds runtime cosa object model we describe how the xorbac access control component and the xorelinterpreter component are used to enforce access rights that we extract from odrl based digital contracts thus our approach describes how odrl based contracts can be used as means to disseminate certain types of access control information in distributed systems
we propose framework and methodology for quantifying the effect of denial of service dos attacks on distributed system we present systematic study of the resistance of gossip based multicast protocols to dos attacks we show that even distributed and randomized gossip based protocols which eliminate single points of failure do not necessarily eliminate vulnerabilities to dos attacks we propose drum simple gossip based multicast protocol that eliminates such vulnerabilities drum was implemented in java and tested on large cluster we show using closed form mathematical analysis simulations and empirical tests that drum survives severe dos attacks
we propose general approach for frequency based string mining which has many applications eg in contrast data mining our contribution is novel algorithm based on deferred data structure despite its simplicity our approach is up to times faster and uses about half the memory compared to the best known algorithm of fischer et al applications in various string domains eg natural language dna or protein sequences demonstrate the improvement of our algorithm
volumetric displays which provide view of imagery illuminated in true space are promising platform for interactive applications however presenting text in volumetric displays can be challenge as the text may not be oriented towards the user this is especially problematic with multiple viewers as the text could for example appear forwards to one user and backwards to another in first experiment we determined the effects of rotations on text readability based on the results we developed and evaluated new technique which optimizes text orientation for multiple viewers this technique provided faster group reading times in collaborative experimental task
although data broadcast has been shown to be an efficient method for disseminating data items in mobile computing systems the issue on how to ensure consistency and currency of data items provided to mobile transactions mt which are generated by mobile clients has not been examined adequately while data items are being broadcast update transactions may install new values for them if the executions of update transactions and the broadcast of data items are interleaved without any control mobile transactions may observe inconsistent data values the problem will be more complex if the mobile clients maintain some cached data items for their mobile transactions in this paper we propose concurrency control method called ordered update first with order oufo for the mobile computing systems where mobile transaction consists of sequence of read operations and each mt is associated with time constraint on its completion time besides ensuring data consistency and maximizing currency of data to mobile transactions oufo also aims at reducing data access delay of mobile transactions using client caches hybrid re broadcast invalidation report ir mechanism is designed in oufo for checking the validity of cached data items so as to improve cache consistency and minimize the overhead of transaction restarts due to data conflicts this is highly important to the performance of the mobile computing systems where the mobile transactions are associated with deadline constraint on their completion times extensive simulation experiments have been performed to compare the performance of oufo with two other efficient schemes the multi version broadcast method and the periodic ir method the performance results show that oufo offers better performance in most aspects even when network disconnection is common
at present the search for specific information on the world wide web is faced with several problems which arise on the one hand from the vast number of information sources available and on the other hand from their intrinsic heterogeneity since standards are missing promising approach for solving the complex problems emerging in this context is the use of multi agent systems of information agents which cooperatively solve advanced information retrieval problems this requires advanced capabilities to address complex tasks such as search and assessment of information sources query planning information merging and fusion dealing with incomplete information and handling of inconsistency in this paper our interest lies in the role which some methods from the field of declarative logic programming can play in the realization of reasoning capabilities for information agents in particular we are interested to see how they can be used extended and further developed for the specific needs of this application domain we review some existing systems and current projects which typically address information integration problems we then focus on declarative knowledge representation methods and review and evaluate approaches and methods from logic programming and nonmonotonic reasoning for information agents we discuss advantages and drawbacks and point out the possible extensions and open issues
overall performance of the data mining process depends not just on the value of the induced knowledge but also on various costs of the process itself such as the cost of acquiring and pre processing training examples the cpu cost of model induction and the cost of committed errors recently several progressive sampling strategies for maximizing the overall data mining utility have been proposed all these strategies are based on repeated acquisitions of additional training examples until utility decrease is observed in this paper we present an alternative projective sampling strategy which fits functions to partial learning curve and partial run time curve obtained from small subset of potentially available data and then uses these projected functions to analytically estimate the optimal training set size the proposed approach is evaluated on variety of benchmark datasets using the rapidminer environment for machine learning and data mining processes the results show that the learning and run time curves projected from only several data points can lead to cheaper data mining process than the common progressive sampling methods
as most current query processing architectures are already pipelined it seems logical to apply them to data streams however two classes of query operators are impractical for processing long or infinite data streams unbounded stateful operators maintain state with no upper bound in size and so run out of memory blocking operators read an entire input before emitting single output and so might never produce result we believe that priori knowledge of data stream can permit the use of such operators in some cases we discuss kind of stream semantics called punctuated streams punctuations in stream mark the end of substreams allowing us to view an infinite stream as mixture of finite streams we introduce three kinds of invariants to specify the proper behavior of operators in the presence of punctuation pass invariants define when results can be passed on keep invariants define what must be kept in local state to continue successful operation propagation invariants define when punctuation can be passed on we report on our initial implementation and show strategy for proving implementations of these invariants are faithful to their relational counterparts
with the explosive growth of the world wide web the public is gaining access to massive amounts of information however locating needed and relevant information remains difficult task whether the information is textual or visual text search engines have existed for some years now and have achieved certain degree of success however despite the large number of images available on the web image search engines are still rare in this article we show that in order to allow people to profit from all this visual information there is need to develop tools that help them to locate the needed images with good precision in reasonable time and that such tools are useful for many applications and purposes the article surveys the main characteristics of the existing systems most often cited in the literature such as imagerover webseek diogenes and atlas wise it then examines the various issues related to the design and implementation of web image search engine such as data gathering and digestion indexing query specification retrieval and similarity web coverage and performance evaluation general discussion is given for each of these issues with examples of the ways they are addressed by existing engines and related references are given some concluding remarks and directions for future research are also presented
load sensitive faults cause program to fail when it is executed under heavy load or over long period of time but may have no detrimental effect under small loads or short executions in addition to testing the functionality of these programs testing how well they perform under stress is very important current approaches to stress or load testing treat the system as black box generating test data based on parameters specified by the tester within an operational profile in this paper we advocate structural approach to load testing there exist many structural testing methods however their main goal is generating test data for executing all statements branches definition use pairs or paths of program at least once without consideration for executing any particular path extensivelyour initial work has focused on the identification of potentially load sensitive modules based on static analysis of the module's code and then limiting the stress testing to the regions of the modules that could be the potential causes of the load sensitivity this analysis will be incorporated into testing tool for structural load testing which takes program as input and automatically determines whether that program needs to be load tested and if so automatically generates test data for structural load testing of the program
developing countries face significant challenges in network access making even simple network tasks unpleasant many standard techniques caching and predictive prefetching help somewhat but provide little or no assistance for personal data that is needed only by single user sulula addresses this problem by leveraging the near ubiquity of cellular phones able to send and receive simple sms messages rather than visit kiosk and fetch data on demand tiresome process at best users request future visit if capacity exists the kiosk can schedule secure retrieval of that user's data saving time and more efficiently utilizing the kiosk's limited connectivity when the user arrives at provisioned kiosk she need only obtain the session key on demand and thereafter has instant access in addition sulula allows users to schedule data uploads experimental results show significant gains for the end user saving tens of minutes of time for typical email news reading session we also describe small ongoing deployment in country for proof of concept lessons learned from that experience and provide discussion on pricing and marketplace issues that remain to be addressed to make the system viable for developing world access
process support systems psss are software systems supporting the modeling enactment monitoring and analysis of business processes process automation technology can be fully exploited when predictable and repetitive processes are executed unfortunately many processes are faced with the need of managing exceptional situations that may occur during their execution and possibly even more exceptions and failures can occur when the process execution is supported by pss exceptional situations may be caused by system hardware or software failures or may by related to the semantics of the business processin this paper we introduce taxonomy of failures and exceptions and discuss the effect that they can have on pss and on its ability to support business processes then we present the main approaches that commercial psss and research prototypes offer in order to capture and react to exceptional situations and we show which classes of failure or exception can be managed by each approach
we report on model based approach to system software co engineering which is tailored to the specific characteristics of critical on board systems for the aerospace domain the approach is supported by system level integrated modeling slim language by which engineers are provided with convenient ways to describe nominal hardware and software operation probabilistic faults and their propagation error recovery and degraded modes of operationcorrectness properties safety guarantees and performance and dependability requirements are given using property patterns which act as parameterized templates to the engineers and thus offer comprehensible and easy to use framework for requirement specification instantiated properties are checked on the slim specification using state of the art formal analysis techniques such as bounded sat based and symbolic model checking and probabilistic variants thereof the precise nature of these techniques together with the formal slim semantics yield trustworthy modeling and analysis framework for system and software engineers supporting among others automated derivation of dynamic ie randomly timed fault trees fmea tables assessment of fdir and automated derivation of observability requirements
there is tremendous amount of web content available today but it is not always in form that supports end users needs in many cases all of the data and services needed to accomplish goal already exist but are not in form amenable to an end user to address this problem we have developed an end user programming tool called marmite which lets end users create so called mashups that re purpose and combine existing web content and services in this paper we present the design implementation and evaluation of marmite an informal user study found that programmers and some spreadsheet users had little difficulty using the system
we present system that composes realistic picture from simple freehand sketch annotated with text labels the composed picture is generated by seamlessly stitching several photographs in agreement with the sketch and text labels these are found by searching the internet although online image search generates many inappropriate results our system is able to automatically select suitable photographs to generate high quality composition using filtering scheme to exclude undesirable images we also provide novel image blending algorithm to allow seamless image composition each blending result is given numeric score allowing us to find an optimal combination of discovered images experimental results show the method is very successful we also evaluate our system using the results from two user studies
in large scale sensor networks sensor nodes are at high risk of being captured and compromised once sensor node is compromised all the secret keys data and code stored on it are exposed to the attacker the attacker can insert arbitrary malicious code in the compromised node moreover he can easily replicate it in large number of clones and deploy them on the network this node replication attack can form the basis of variety of attacks such as dos attacks and sybil attacks previous studies of node replication attacks have had some drawbacks they need central trusted entity or they become vulnerable when many nodes are compromised therefore we propose distributed protocol for detecting node replication attacks that is resilient to many compromised nodes our method does not need any reliable entities and has high detection rate of replicated nodes our analysis and simulations demonstrate our protocol is effective even when there are large number of compromised nodes
developing and maintaining open source software has become an important source of profit for many companies change prone classes in open source products increase project costs by requiring developers to spend effort and time identifying and characterizing change prone classes can enable developers to focus timely preventive actions for example peer reviews and inspections on the classes with similar characteristics in the future releases or products in this study we collected set of static metrics and change data at class level from two open source projects koffice and mozilla using these data we first tested and validated pareto's law which implies that great majority around of change is rooted in small proportion around of classes then we identified and characterized the change prone classes in the two products by producing tree based models in addition using tree based models we suggested prioritization strategy to use project resources for focused preventive actions in an efficient manner our empirical results showed that this strategy was effective for prioritization purposes this study should provide useful guidance to practitioners involved in development and maintenance of large scale open source products
we study auctions whose bidders are embedded in social or economic network as result even bidders who do not win the auction themselves might derive utility from the auction namely when friend wins on the other hand when an enemy or competitor wins bidder might derive negative utility such spite and altruism will alter the bidding strategies simple and natural model for bidders utilities in these settings posits that the utility of losing bidder as result of bidder winning is constant positive or negative fraction of bidder j's utilitywe study such auctions under bayesian model in which all valuations are distributed independently according to known distribution but the actual valuations are private we describe and analyze nash equilibrium bidding strategies in two broad classes regular friendship networks with arbitrary valuation distributions and arbitrary friendship networks with identical uniform valuation distributions
the process of network debugging is commonly guided by decision trees that describe and attempt to address the most common failure modes we show that troubleshooting can be made more effective by converting decision trees into suites of convergent troubleshooting scripts that do not change network attributes unless these are out of compliance with accepted norms maelstrom is tool for managing and coordinating execution of these scripts maelstrom exploits convergence of individual scripts to dynamically infer an appropriate execution order for the scripts it accomplishes this in procedure trials where is the number of troubleshooting scripts this greatly eases adding scripts to troubleshooting scheme and thus makes it easier for people to cooperate in producing more exhaustive and effective troubleshooting schemes
recent studies have indicated an increase in customer profiling techniques used by commerce businesses commerce businesses are creating maintaining and utilising customer profiles to assist in personalisation personalisation can help improve customers satisfaction levels purchasing behaviour loyalty and subsequently improve sales the continuously changing customer needs and preferences pose challenge to commerce businesses on how to maintain and update individual customer profiles to reflect any changes in customers needs and preferences this research set out to investigate how dynamic customer profile for on line customers can be updated and maintained taking into consideration individual web visitors activities the research designed and implemented decision model that analysed on line customers activities during interaction sessions and determined whether to update customers profiles or not evaluation results indicated that the model was able to analyse the on line customers activities from log file and successfully updated the customers profiles based on the customer activities undertaken during the interaction session
unlike conventional rule based knowledge bases kbs that support monotonic reasoning key correctness issue ie the correctness of sub kb with respect to the full kb arises when using kb represented by non monotonic reasoning languages such as answer set programming asp since user may have rights to access only subset of kb the non monotonic nature of asp may cause the occurrence of consequences which are erroneous in the sense that the consequences are not reasonable in the full kb this paper proposes an approach dealing with the problem the main idea is to let the usage of closed world assumptions cwas for literals in kb satisfy certain constraints two kinds of access right propositions are created rule retrieval right propositions to control the access to rules and cwa right propositions to control the usage of cwas for literals based on these right propositions this paper first defines an algorithm for translating an original kb into kb tagged by right propositions and then discusses the right dependency in kb and proposes methods for checking and obtaining set of rights that is closed under set of dependency rules finally several results on the correctness of set of rights in kb are presented which serve as guidelines for the correct use of kb as an example kb of illness related financial support for teachers of university is presented to illustrate the application of our approach
the minimum energy broadcast routing problem was extensively studied during the last years given sample space where wireless devices are distributed the aim is to perform the broadcast communication pattern from given source while minimising the total energy consumption while many papers deal with the dimensional case where the sample space is given by plain area few results are known about the more interesting and practical dimensional case in this paper we study this case and we present tighter analysis of the minimum spanning tree heuristic in order to considerably decrease its approximation factor from the known to roughly this decreases the gap with the known lower bound of given by the so called dimensional kissing number
the collection of digital information by governments corporations and individuals has created tremendous opportunities for knowledge and information based decision making driven by mutual benefits or by regulations that require certain data to be published there is demand for the exchange and publication of data among various parties data in its original form however typically contains sensitive information about individuals and publishing such data will violate individual privacy the current practice in data publishing relies mainly on policies and guidelines as to what types of data can be published and on agreements on the use of published data this approach alone may lead to excessive data distortion or insufficient protection privacy preserving data publishing ppdp provides methods and tools for publishing useful information while preserving data privacy recently ppdp has received considerable attention in research communities and many approaches have been proposed for different data publishing scenarios in this survey we will systematically summarize and evaluate different approaches to ppdp study the challenges in practical data publishing clarify the differences and requirements that distinguish ppdp from other related problems and propose future research directions
customized processors use compiler analysis and design automation techniques to take generalized architectural model and create specific instance of it which is optimized to given application or set of applications these processors offer the promise of satisfying the high performance needs of the embedded community while simultaneously shrinking design times finite state machines fsm are fundamental building block in computer architecture and are used to control and optimize all types of prediction and speculation now even in the embedded space they are used for branch prediction cache replacement policies and confidence estimation and accuracy counters for variety of optimizations in this paper we present framework for automated design of small fsm predictors for customized processors our approach can be used to automatically generate small fsm predictors to perform well over suite of applications tailored to specific application or even specific instruction we evaluate the use of these customized fsm predictors for branch prediction over set of benchmarks
scaling of interconnects exacerbates the already challenging reliability of on chip networks although many researchers have provided various fault handling techniques in chip multi processors cmps the fault tolerance of the interconnection network is yet to adequately evolve as an end to end recovery approach delays fault detection and complicates recovery to consistent global state in such system link level retransmission is endorsed for recovery making higher level protocol simple in this paper we introduce fault tolerant flow control scheme for soft error handling in on chip networks the fault tolerant flow control recovers errors at link level by requesting retransmission and ensures an error free transmission on flit basis with incorporation of dynamic packet fragmentation dynamic packet fragmentation is adopted as part of fault tolerant flow control to disengage flits from the fault containment and recover the faulty flit transmission thus the proposed router provides high level of dependability at the link level for both datapath and control planes in simulation with injected faults the proposed router is observed to perform well gracefully degrading while exhibiting error coverage in datapath elements the proposed router has been implemented using tsmc nm standard cell library as compared to router which employs triple modular redundancy tmr in datapath elements the proposed router takes less area and consumes less energy per packet on average
systems providing only exact match answers without allowing any kind of preference or approximate queries are not sufficient in many contexts many different approaches have been introduced often incompatible in their setup or proposed implementation this work shows how different kinds of preference queries prefer preference sql and skyline can be combined and answered efficiently using bit sliced index bsi arithmetic this approach has been implemented in dbms and performance results are included showing that the bit sliced index approach is efficient not only in prototype system but in real system
the management of hierarchically organized data is starting to play key role in the knowledge management community due to the proliferation of topic hierarchies for text documents the creation and maintenance of such organized repositories of information requires great deal of human interventionthe machine learning community has partially addressed this problem by developing hierarchical supervised classifiers that help people categorize new resources within given hierarchies the worst problem of hierarchical supervised classifiers however is their high demand in terms of labeled examples the number of examples required is related to the number of topics in the taxonomy bootstrapping huge hierarchy with proper set of labeled examples is therefore critical issuethis paper proposes some solutions for the bootstrapping problem that implicitly or explicitly use taxonomy definition baseline approach that classifies documents according to the class terms and two clustering approaches whose training is constrained by the priori knowledge encoded in the taxonomy structure which consists of both terminological and relational aspects in particular we propose the tax som model that clusters set of documents in predefined hierarchy of classes directly exploiting the knowledge of both their topological organization and their lexical description experimental evaluation was performed on set of taxonomies taken from the googletm and looksmarttm web directories obtaining good results
the traditional interaction mechanism with database system is through the use of query language the most widely used one being sql however when one is facing situation where he or she has to make minor modification to previously issued sql query either the whole query has to be written from scratch or one has to invoke an editor to edit the query this however is not the way we converse with each other as humans during the course of conversation the preceding interaction is used as context within which many incomplete and or incremental phrases are uniquely and unambiguously interpreted sparing the need to repeat the same things again and again in this paper we present an effective mechanism that allows user to interact with database system in way similar to the way humans converse more specifically incomplete sql queries are accepted as input which are then matched to identified parts of previously issued queries disambiguation is achieved by using various types of semantic information the overall method works independently of the domain under which it is used ie independently of the database schema several algorithms that are variations of the same basic mechanism are proposed they are mutually compared with respect to efficiency and accuracy through limited set of experiments on human subjects the results have been encouraging especially when semantic knowledge from the schema is exploited laying potential foundation for conversational querying in databases
learned activity specific motion models are useful for human pose and motion estimation nevertheless while the use of activity specific models simplifies monocular tracking it leaves open the larger issues of how one learns models for multiple activities or stylistic variations and how such models can be combined with natural transitions between activities this paper extends the gaussian process latent variable model gp lvm to address some of these issues we introduce new approach to constraining the latent space that we refer to as the locally linear gaussian process latent variable model ll gplvm the ll gplvm allows for an explicit prior over the latent configurations that aims to preserve local topological structure in the training data we reduce the computational complexity of the gplvm by adapting sparse gaussian process regression methods to the gp lvm by incorporating sparsification dynamics and back constraints within the ll gplvm we develop general framework for learning smooth latent models of different activities within shared latent space allowing the learning of specific topologies and transitions between different activities
effective system verification requires good specifications the lack of sufficient specifications can lead to misses of critical bugs design re spins and time to market slips in this paper we present new technique for mining temporal specifications from simulation or execution traces of digital hardware design given an execution trace we mine recurring temporal behaviors in the trace that match set of pattern templates subsequently we synthesize them into complex patterns by merging events in time and chaining the patterns using inference rules we specifically designed our algorithm to make it highly efficient and meaningful for digital circuits in addition we propose pattern mining diagnosis framework where specifications mined from correct and erroneous traces are used to automatically localize an error we demonstrate the effectiveness of our approach on industrial size examples by mining specifications from traces of over million cycles in few minutes and use them to successfully localize errors of different types to within module boundaries
mobile robots that interact with humans in an intuitive way must be able to follow directions provided by humans in unconstrained natural language in this work we investigate how statistical machine translation techniques can be used to bridge the gap between natural language route instructions and map of an environment built by robot our approach uses training data to learn to translate from natural language instructions to an automatically labeled map the complexity of the translation process is controlled by taking advantage of physical constraints imposed by the map as result our technique can efficiently handle uncertainty in both map labeling and parsing our experiments demonstrate the promising capabilities achieved by our approach
haskell programmers often use multi parameter type class in which one or more type parameters are functionally dependent on the first although such functional dependencies have proved quite popular in practice they express the programmer's intent somewhat indirectly developing earlier work on associated data types we propose to add functionally dependent types as type synonyms to type class bodies these associated type synonyms constitute an interesting new alternative to explicit functional dependencies
software monitoring is technique that is well suited to supporting the development of dependable system it has been widely applied not only for this purpose but also for other purposes such as debugging security performance evaluation and enhancement etc however there is an inherent gap between the levels of abstraction of the information that is collected during software monitoring the implementation level and that of the software architecture level where many design decisions are made unless an immediate structural one to one architecture to implementation mapping takes place we need specification language to describe how low level events are related to higher level ones although some specification languages for monitoring have been proposed in the literature they do not provide support up to the software architecture level in addition these languages make it harder to link to and reuse information from other event based models often employed for reliability analysis in this paper we discuss the importance of event description as an integration element for architecting dependable systems
there is growing interest in algorithms for processing and querying continuous data streams ie data seen only once in fixed order with limited memory resources in its most general form data stream is actually an update stream ie comprising data item deletions as well as insertions such massive update streams arise naturally in several application domains eg monitoring of large ip network installations or processing of retail chain transactions estimating the cardinality of set expressions defined over several possibly distributed update streams is perhaps one of the most fundamental query classes of interest as an example such query may ask xc what is the number of distinct ip source addresses seen in passing packets from both router sub sub and sub sub but not router sub sub xd earlier work only addressed very restricted forms of this problem focusing solely on the special case of insert only streams and specific operators eg union in this paper we propose the first space efficient algorithmic solution for estimating the cardinality of full fledged set expressions over general update streams our estimation algorithms are probabilistic in nature and rely on novel hash based synopsis data structure termed xd level hash sketch xd we demonstrate how our level hash sketch synopses can be used to provide low error high confidence estimates for the cardinality of set expressions including operators such as set union intersection and difference over continuous update streams using only space that is significantly sublinear in the sizes of the streaming input multi sets furthermore our estimators never require rescanning or resampling of past stream items regardless of the number of deletions in the stream we also present lower bounds for the problem demonstrating that the space usage of our estimation algorithms is within small factors of the optimal finally we propose an optimized time efficient stream synopsis based on level hash sketches that provides similar strong accuracy space guarantees while requiring only guaranteed logarithmic maintenance time per update thus making our methods applicable for truly rapid rate data streams our results from an empirical study of our synopsis and estimation techniques verify the effectiveness of our approach
internet hosting centers serve multiple service sites from common hardware base this paper presents the design and implementation of an architecture for resource management in hosting center operating system with an emphasis on energy as driving resource management issue for large server clusters the goals are to provision server resources for co hosted services in way that automatically adapts to offered load improve the energy efficiency of server clusters by dynamically resizing the active server set and respond to power supply disruptions or thermal events by degrading service in accordance with negotiated service level agreements slas our system is based on an economic approach to managing shared server resources in which services bid for resources as function of delivered performance the system continuously monitors load and plans resource allotments by estimating the value of their effects on service performance greedy resource allocation algorithm adjusts resource prices to balance supply and demand allocating resources to their most efficient use reconfigurable server switching infrastructure directs request traffic to the servers assigned to each service experimental results from prototype confirm that the system adapts to offered load and resource availability and can reduce server energy usage by or more for typical web workload
user behavior information analysis has been shown important for optimization and evaluation of web search and has become one of the major areas in both information retrieval and knowledge management researches this paper focuses on users searching behavior reliability study based on large scale query and click through logs collected from commercial search engines the concept of reliability is defined in probabilistic notion the context of user click behavior on search results is analyzed in terms of relevance five features namely query number click entropy first click ratio last click ratio and rank position are proposed and studied to separate reliable user clicks from the others experimental results show that the proposed method evaluates the reliability of user behavior effectively the auc value of the roc curve is and the algorithm maintains relevant clicks when filtering out low quality clicks
we present our experience in implementing group communication toolkit in objective caml dialect of the ml family of programming languages we compare the toolkit both quantitatively and qualitatively to predecessor toolkit which was implemented in our experience shows that using the high level abstraction features of ml gives substantial advantages some of these features such as automatic memory management and message marshalling allowed us to concentrate on those pieces of the implementation which required careful attention in order to achieve good performance we conclude with set of suggested changes to ml implementations
soft state is an often cited yet vague concept in network protocol design in which two or more network entities intercommunicate in loosely coupled often anonymous fashion researchers often define this concept operationally if at all rather than analytically source of soft state transmits periodic refresh messages over lossy communication channel to one or more receivers that maintain copy of that state which in turn expires if the periodic updates cease though number of crucial internet protocol building blocks are rooted in soft state based designs eg rsvp refresh messages pim membership updates various routing protocol updates rtcp control messages directory services like sap and so forth controversy is building as to whether the performance overhead of soft state refresh messages justify their qualitative benefit of enhanced system robustness we believe that this controversy has risen not from fundamental performance tradeoffs but rather from our lack of comprehensive understanding of soft state to better understand these tradeoffs we propose herein formal model for soft state communication based on probabilistic delivery model with relaxed reliability using this model we conduct queueing analysis and simulation to characterize the data consistency and performance tradeoffs under range of workloads and network loss rates we then extend our model with feedback and show through simulation that adding feedback dramatically improves data consistency by up to without increasing network resource consumption our model not only provides foundation for understanding soft state but also induces new fundamental transport protocol based on probabilistic delivery toward this end we sketch our design of the soft state transport protocol sstp which enjoys the robustness of soft state while retaining the performance benefit of hard state protocols like tcp through its judicious use of feedback
the development of the semantic web will require agents to use common domain ontologies to facilitate communication of conceptual knowledge however the proliferation of domain ontologies may also result in conflicts between the meanings assigned to the various terms that is agents with diverse ontologies may use different terms to refer to the same meaning or the same term to refer to different meanings agents will need method for learning and translating similar semantic concepts between diverse ontologies only until recently have researchers diverged from the last decade's ldquo common ontology rdquo paradigm to paradigm involving agents that can share knowledge using diverse ontologies this paper describes how we address this agent knowledge sharing problem of how agents deal with diverse ontologies by introducing methodology and algorithms for multi agent knowledge sharing and learning in peer to peer setting we demonstrate how this approach will enable multi agent systems to assist groups of people in locating translating and sharing knowledge using our distributed ontology gathering group integration environment doggie and describe our proof of concept experiments doggie synthesizes agent communication machine learning and reasoning for information sharing in the web domain
we propose an energy efficient framework called saf for approximate querying and clustering of nodes in sensor network saf uses simple time series forecasting models to predict sensor readings the idea is to build these local models at each node transmit them to the root of the network the sink and use them to approximately answer user queries our approach dramatically reduces communication relative to previous approaches for querying sensor networks by exploiting properties of these local models since each sensor communicates with the sink only when its local model varies due to changes in the underlying data distribution in our experimental results performed on trace of real data we observed on average about message transmissions from each sensor over week including the learning phase to correctly predict temperatures to within csaf also provides mechanism to detect data similarities between nodes and organize nodes into clusters at the sink at no additional communication cost this is again achieved by exploiting properties of our local time series models and by means of novel definition of data similarity between nodes that is based not on raw data but on the prediction values our clustering algorithm is both very efficient and provably optimal in the number of clusters our clusters have several interesting features first they can capture similarity between far away nodes that are not geographically adjacent second cluster membership to variations in sensors local models third nodes within cluster are not required to track the membership of other nodes in the cluster we present number of simulation based experimental results that demonstrate these properties of saf
good spatial locality alleviates both the latency and bandwidth problem of memory by boosting the effect of prefetching and improving the utilization of cache however conventional definitions of spatial locality are inadequate for programmer to precisely quantify the quality of program to identify causes of poor locality and to estimate the potential by which spatial locality can be improved this paper describes new component based model for spatial locality it is based on measuring the change of reuse distances as function of the data block size it divides spatial locality into components at program and behavior levels while the base model is costly because it requires the tracking of the locality of every memory access the overhead can be reduced by using small inputs and by extending sampling based tool the paper presents the result of the analysis for large set of benchmarks the cost of the analysis and the experience of user study in which the analysis helped to locate data layout problem and improve performance by with line change in an application with over lines
this paper presents novel technique anatomy for publishing sensitive data anatomy releases all the quasi identifier and sensitive values directly in two separate tables combined with grouping mechanism this approach protects privacy and captures large amount of correlation in the microdata we develop linear time algorithm for computing anatomized tables that obey the diversity privacy requirement and minimize the error of reconstructing the microdata extensive experiments confirm that our technique allows significantly more effective data analysis than the conventional publication method based on generalization specifically anatomy permits aggregate reasoning with average error below which is lower than the error obtained from generalized table by orders of magnitude
in response to long lasting anticipation by the java community version of the java platform referred to as java introduced generic types and methods to the java language the java generics are significant enhancement to the language expressivity because they allow straightforward composition of new generic classes from existing ones while reducing the need for plethora of type casts while the java generics are expressive the chosen implementation method type erasure has triggered undesirable orthogonality violations this paper identifies six cases of orthogonality violations in the java generics and demonstrates how these violations are mandated by the use of type erasure the paper also compares the java cases of orthogonality violations to compatible cases in and nextgen and analyzes the tradeoffs in the three approaches the conclusion is that java users face new challenges number of generic type expressions are forbidden while others that are allowed are left unchecked by the compiler
compared to traditional text classification with flat category set or small hierarchy of categories classifying web pages to large scale hierarchy such as open directory project odp and yahoo directory is challenging while recently proposed deep classification method makes the problem tractable it still suffers from low classification performance major problem is the lack of training data which is unavoidable with such huge hierarchy training pages associated with the category nodes are short and their distributions are skewed to alleviate the problem we propose new training data selection strategy and na iuml ve bayes combination model which utilize both local and global information we conducted series of experiments with the odp hierarchy containing more than categories to show that the proposed method of using both local and global information indeed helps avoiding the training data sparseness problem outperforming the state of art method
without proper simplification techniques database integrity checking can be prohibitively time consuming several methods have been developed for producing simplified incremental checks for each update but none until now of sufficient quality and generality for providing true practical impact and the present paper is an attempt to fill this gap on the theoretical side general characterization is introduced of the problem of simplification of integrity constraints and natural definition is given of what it means for simplification procedure to be ideal we prove that ideality of simplification is strictly related to query containment in fact an ideal simplification pro cedure can only exist in database languages for which query containment is decidable however simplifications that do not qualify as ideal may also be relevant for practical purposes we present concrete approach based on transformation operators that apply to integrity constraints written in rich datalog like language with negation the resulting procedure produces at design time simplified constraints for parametric transaction patterns which can then be instantiated and checked for consistency at run time these tests take place before the execution of the update so that only consistency preserving updates are eventually given to the database the extension to more expressive languages and the application of the framework to other contexts such as data integration and concurrent database systems are also discussed our experiments show that the simplifications obtained with our method may give rise to much better performance than with previous methods and that further improvements are achieved by checking consistency before executing the update
what distinguishes commerce from ordinary commerce what distinguishes it from distributed computation in this paper we propose performative theory of commerce drawing on speech act theory in which commerce exchanges are promises of future commercial actions whose real world meanings are constructed jointly and incrementally we then define computational model for this theory called posit spaces along with the syntax and semantics for an agent interaction protocol the posit spaces protocol or psp this protocol enables participants in multi agent commercial interaction to propose accept modify and revoke joint commitments our work integrates three strands of prior research the theory of tuple spaces in distributed computation formal dialogue games from argumentation theory and the study of commitments in multi agent systems
traffic anomalies such as failures and attacks are increasing in frequency and severity and thus identifying them rapidly and accurately is critical for large network operators the detection typically treats the traffic as collection of flows and looks for heavy changes in traffic patterns eg volume number of connections however as link speeds and the number of flows increase keeping per flow state is not scalable the recently proposed sketch based schemes are among the very few that can detect heavy changes and anomalies over massive data streams at network traffic speeds however sketches do not preserve the key eg source ip address of the flows hence even if anomalies are detected it is difficult to infer the culprit flows making it big practical hurdle for online deployment meanwhile the number of keys is too large to record to address this challenge we propose efficient reversible hashing algorithms to infer the keys of culprit flows from sketches without storing any explicit key information no extra memory or memory accesses are needed for recording the streaming data meanwhile the heavy change detection daemon runs in the background with space complexity and computational time sublinear to the key space size this short paper describes the conceptual framework of the reversible sketches as well as some initial approaches for implementation see for the optimized algorithms in details comment we further apply various emph ip mangling algorithms and emph bucket classification methods to reduce the false positives and false negatives evaluated with netflow traffic traces of large edge router we demonstrate that the reverse hashing can quickly infer the keys of culprit flows even for many changes with high accuracy
software bugs in routers lead to network outages security vulnerabilities and other unexpected behavior rather than simply crashing the router bugs can violate protocol semantics rendering traditional failure detection and recovery techniques ineffective handling router bugs is an increasingly important problem as new applications demand higher availability and networks become better at dealing with traditional failures in this paper we tailor software and data diversity sdd to the unique properties of routing protocols so as to avoid buggy behavior at run time our bug tolerant router executes multiple diverse instances of routing software and uses voting to determine the output to publish to the forwarding table or to advertise to neighbors we design and implement router hypervisor that makes this parallelism transparent to other routers handles fault detection and booting of new router instances and performs voting in the presence of routing protocol dynamics without needing to modify software of the diverse instances experiments with bgp message traces and open source software running on our linux based router hypervisor demonstrate that our solution scales to large networks and efficiently masks buggy behavior
commercially available database systems do not meet the information and processing needs of design and manufacturing environments new generation of systems engineering information systems must be built to meet these needs the architectural and computational aspects of such systems are addressed and solutions are proposed the authors argue that mainframe workstation architecture is needed to provide distributed functionality while ensuring high availability and low communication overhead that explicit control of metaknowledge is needed to support extendibility and evolution that large rule bases are needed to make the knowledge of the systems active and that incremental computation models are needed to achieve the required performance of such engineering information systems
large number of call graph construction algorithms for object oriented and functional languages have been proposed each embodying different tradeoffs between analysis cost and call graph precision in this article we present unifying framework for understanding call graph construction algorithms and an empirical comparison of representative set of algorithms we first present general parameterized algorithm that encompasses many well known and novel call graph construction algorithms we have implemented this general algorithm in the vortex compiler infrastructure mature multilanguage optimizing compiler the vortex implementation provides level playing field for meaningful cross algorithm performance comparisons the costs and benefits of number of call graph construction algorithms are empirically assessed by applying their vortex implementation to suite of sizeable to lines of code cecil and java programs for many of these applications interprocedural analysis enabled substantial speed ups over an already highly optimized baseline furthermore significant fraction of these speed ups can be obtained through the use of scalable near linear time call graph construction algorithm
in this paper we present the java aspect components jac framework for building aspect oriented distributed applications in java this paper describes the aspect oriented programming model and the architectural details of the framework implementation the framework enables extension of application semantics for handling well separated concerns this is achieved with software entity called an aspect component ac acs provide distributed pointcuts dynamic wrappers and metamodel annotations distributed pointcuts are key feature of our framework they enable the definition of crosscutting structures that do not need to be located on single host acs are dynamic they can be added removed and controlled at runtime this enables our framework to be used in highly dynamic environments where adaptable software is needed
scheduling algorithms used in compilers traditionally focus on goals such as reducing schedule length and register pressure or producing compact code in the context of hardware synthesis system where the schedule is used to determine various components of the hardware including datapath storage and interconnect the goals of scheduler change drastically in addition to achieving the traditional goals the scheduler must proactively make decisions to ensure efficient hardware is produced this paper proposes two exact solutions for cost sensitive modulo scheduling one based on an integer linear programming formulation and another based on branch and bound search to achieve reasonable compilation times decomposition techniques to break down the complex scheduling problem into phase ordered sub problems are proposed the decomposition techniques work either by partitioning the dataflow graph into smaller subgraphs and optimally scheduling the subgraphs or by splitting the scheduling problem into two phases time slot and resource assignment the effectiveness of cost sensitive modulo scheduling in minimizing the costs of function units register structures and interconnection wires are evaluated within fully automatic synthesis system for loop accelerators the cost sensitive modulo scheduler increases the efficiency of the resulting hardware significantly compared to both traditional cost unaware and greedy cost aware modulo schedulers
this study develops theory of personal values and trust in mobile commerce commerce service systems research hypotheses involve the satisfaction formation processes and usage continuance intentions in commerce service context and the participator is selected from three private wireless telecommunication service providers in taiwan results showed consumers might not use the commerce service in the future without being totally satisfied with the system and their customer values mobile technology trusting expectations were very important in the continued commerce service usage behaviour and the providers might not fulfil the commerce service need for consumers but satisfied with the commerce service delivered
files classes or methods have frequently been investigated in recent research on co change in this paper we present first study at the level of lines to identify line changes across several versions we define the annotation graph which captures how lines evolve over time the annotation graph provides more fine grained software evolution information such as life cycles of each line and related changes whenever developer changed line of versiontxt she also changed line of libraryjava
we consider lock free synchronization for dynamic embedded real time systems that are subject to resource overloads and arbitrary activity arrivals we model activity arrival behaviors using the unimodal arbitrary arrival model or uam uam embodies stronger ldquo adversary rdquo than most traditional arrival models we derive an upper bound on lock free retries under the uam with utility accrual scheduling mdash the first such result we establish the tradeoffs between lock free and lock based sharing under uam these include conditions under which activities accrued timeliness utility is greater under lock free than lock based and the consequent lower and upper bound on the total accrued utility that is possible with lock free and lock based sharing we confirm our analytical results with posix rtos implementation
in this paper study is described which investigates differences in game experience between the use of iconic and symbolic tangibles in digital tabletop interaction to enable this study new game together with two sets of play pieces iconic and symbolic was developed and used in an experiment with participants in this experiment the understanding of the game the understanding of the play pieces and the fun experience were tested both the group who played with iconic play pieces and the group who played with symbolic play pieces were proven to have comparable fun experience and understanding of the game however the understanding of the play pieces was higher in the iconic group and large majority of both groups preferred to play with iconic play pieces rather then symbolic play pieces
packet based on chip networks are increasingly being adopted in complex system on chip soc designs supporting numerous homogeneous and heterogeneous functional blocks these network on chip noc architectures are required to not only provide ultra low latency but also occupy small footprint and consume as little energy as possible further reliability is rapidly becoming major challenge in deep sub micron technologies due to the increased prominence of permanent faults resulting from accelerated aging effects and manufacturing testing challenges towards the goal of designing low latency energyefficient and reliable on chip communication networks we propose novel fine grained modular router architecture the proposed architecture employs decoupled parallel arbiters and uses smaller crossbars for row and column connections to reduce output port contention probabilities as compared to existing designs furthermore the router employs new switch allocation technique known as mirroring effect to reduce arbitration depth and increase concurrency in addition the modular design permits graceful degradation of the network in the event of permanent faults and also helps to reduce the dynamic power consumption our simulation results indicate that in an mesh network the proposed architecture reduces packet latency by and power consumption by as compared to two existing router architectures evaluation using combined performance energy and fault tolerance metric indicates that the proposed architecture provides overall improvement compared to the two earlier routers
the world wide web has undergone major changes in recent years the idea to see the web as platform for services instead of one way source of information has come along with number of new applications such as photo and video sharing portals and wikisin this paper we study how these changes affect the nature of the data distributed over the world wide web to do so we compare two data traces collected at the web proxy server of the rwth aachen the first trace was recorded in the other more than seven years later in we show the major differences and the similarities between the two traces and compare our observations with other work the results indicate that traditional proxy caching is no longer effective in typical university networks
transactions have been around since the seventies to provide reliable information processing in automated information systems originally developed for simple debit credit style database operations in centralized systems they have moved into much more complex application domains including aspects like distribution process orientation and loose coupling the amount of published research work on transactions is huge and number of overview papers and books already exist concise historic analysis providing an overview of the various phases of development of transaction models and mechanisms in the context of growing complexity of application domains is still missing however to fill this gap this paper presents historic overview of transaction models organized in several transaction management eras thereby investigating numerous transaction models ranging from the classical flat transactions via advanced and workflow transactions to the web services and grid transaction models the key concepts and techniques with respect to transaction management are investigated placing well known research efforts in historical perspective reveals specific trends and developments in the area of transaction management as such this paper provides comprehensive structured overview of developments in the area
we describe compression model for semistructured documents called structural contexts model scm which takes advantage of the context information usually implicit in the structure of the text the idea is to use separate model to compress the text that lies inside each different structure type eg different xml tag the intuition behind scm is that the distribution of all the texts that belong to given structure type should be similar and different from that of other structure types we mainly focus on semistatic models and test our idea using word based huffman method this is the standard for compressing large natural language text databases because random access partial decompression and direct search of the compressed collection is possible this variant dubbed scmhuff retains those features and improves huffman's compression ratios we consider the possibility that storing separate models may not pay off if the distribution of different structure types is not different enough and present heuristic to merge models with the aim of minimizing the total size of the compressed database this gives an additional improvement over the plain technique the comparison against existing prototypes shows that among the methods that permit random access to the collection scmhuff achieves the best compression ratios better than the closest alternative from purely compression aimed perspective we combine scm with ppm modeling separate ppm model is used to compress the text that lies inside each different structure type the result scmppm does not permit random access nor direct search in the compressed text but it gives better compression ratios than other techniques for texts longer than mb
this paper presents general framework for determining average program execution times and their variance based on the program's interval structure and control dependence graph average execution times and variance values are computed using frequency information from an optimized counter based execution profile of the program
using analysis simulation and experimentation we examine the threat against anonymous communications posed by passive logging attacks in previous work we analyzed the success of such attacks under various assumptions here we evaluate the effects of these assumptions more closely first we analyze the onion routing based model used in prior work in which fixed set of nodes remains in the system indefinitely we show that for this model by removing the assumption of uniformly random selection of nodes for placement in the path initiators can greatly improve their anonymity second we show by simulation that attack times are significantly lower in practice than bounds given by analytical results from prior work third we analyze the effects of dynamic membership model in which nodes are allowed to join and leave the system we show that all known defenses fail more quickly when the assumption of static node set is relaxed fourth intersection attacks against peer to peer systems are shown to be an additional danger either on their own or in conjunction with the predecessor attack finally we address the question of whether the regular communication patterns required by the attacks exist in real traffic we collected and analyzed the web requests of users to determine the extent to which basic patterns can be found we show that for our study frequent and repeated communication to the same web site is common
methods of top search with no random access can be used to find best objects using sorted access to the sources of attribute values in this paper we present new heuristics over the nra algorithm that can be used for fast search of top objects using wide range of user preferences nra algorithm usually needs periodical scan of large number of candidates during the computation in this paper we propose methods of no random access top search that optimize the candidate list maintenance during the computation to speed up the search the proposed methods are compared to table scan method typically used in databases we present results of experiments showing speed improvement depending on number of object attributes expressed in user preferences or selectivity of user preferences
the mapreduce framework is increasingly being used to analyze large volumes of data one important type of data analysis done with mapreduce is log processing in which click stream or an event log is filtered aggregated or mined for patterns as part of this analysis the log often needs to be joined with reference data such as information about users although there have been many studies examining join algorithms in parallel and distributed dbmss the mapreduce framework is cumbersome for joins mapreduce programmers often use simple but inefficient algorithms to perform joins in this paper we describe crucial implementation details of number of well known join strategies in mapreduce and present comprehensive experimental comparison of these join techniques on node hadoop cluster our results provide insights that are unique to the mapreduce platform and offer guidance on when to use particular join algorithm on this platform
we propose dialogue game protocol for purchase negotiation dialogues which identifies appropriate speech acts defines constraints on their utterances and specifies the different sub tasks agents need to perform in order to engage in dialogues according to this protocol our formalism combines dialogue game similar to those in the philosophy of argumentation with model of rational consumer purchase decision behaviour adopted from marketing theory in addition to the dialogue game protocol we present portfolio of decision mechanisms for the participating agents engaged in the dialogue and use these to provide our formalism with an operational semantics we show that these decision mechanisms are sufficient to generate automated purchase decision dialogues between autonomous software agents interacting according to our proposed dialogue game protocol
the increasing size and complexity of many software systems demand greater emphasis on capturing and maintaining knowledge at many different levels within the software development process this knowledge includes descriptions of the hardware and software components and their behavior external and internal design specifications and support for system testing the knowledge based software engineering kbse research paradigm is concerned with systems that use formally represented knowledge with associated inference precedures to support the various subactivities of software development as they growing scale kbse systems must balance expressivity and inferential power with the real demands of knowledge base construction maintenance performance and comprehensibility description logics dls possess several features mdash terminological orientation formal semantics and efficient reasoning procedures mdash which offer an effective tradeoff of these factors we discuss three kbse systems in which dls capture some of the requisite knowledge needed to support design coding and testing activities we then survey some alternative approaches to dls in kbse systems we close with discussion of the benefits of dls and ways to address some of their limitations
this paper shows an asymptotically tight analysis of the certified write all algorithm called awt that was introduced by anderson and woll siam comput and method for creating near optimal instances of the algorithm this algorithm is the best known deterministic algorithm that can be used to simulate synchronous parallel processors on asynchronous processors the algorithm is instantiated with permutations on where can be chosen from wide range of values when implementing simulation on specific parallel system with processors one would like to select the best possible value of and the best possible permutations in order to maximize the efficiency of the simulationthis paper shows that work complexity of any instance of awt is logq where is the number of permutations selected and is value related to their combinatorial properties the choice of turns out to be critical for obtaining an instance of the awt algorithm with near optimal work for any and any large enough work of any instance of the algorithm must be at least ln ln ln under certain conditions however that is about ln ln ln and for infinitely many large enough this lower bound can be nearly attained by instances of the algorithm that use certain permutations and have work at most ln ln ln the paper also shows penalty for not selecting well when is significantly away from ln ln ln then work of any instance of the algorithm with this displaced must be considerably higher than otherwise
two or more components eg objects modules or programs interoperate when they exchange data such as xml data using application programming interface api calls exported by xml parsers remains primary mode of accessing and manipulating xml and these api calls lead to various run time errors in components that exchange xml data currently no tool checks the source code of interoperating components for potential flaws caused by third party api calls that lead to incorrect xml data exchanges and runtime errors even when components are located within the same application our solution combines program abstraction and symbolic execution in order to reengineer the approximate schema of xml data that would be output by component this schema is compared using bisimulation with the schema of xml data that is expected by some other components we describe our approach and give our error checking algorithm we implemented our approach in tool that we used on open source and commercial systems and discovered errors that were not detected during their design and testing
outsourcing the training of support vector machines svm to external service providers benefits the data owner who is not familiar with the techniques of the svm or has limited computing resources in outsourcing the data privacy is critical issue for some legal or commercial reasons since there may be sensitive information contained in the data existing privacy preserving svm works are either not applicable to outsourcing or weak in security in this paper we propose scheme for privacy preserving outsourcing the training of the svm without disclosing the actual content of the data to the service provider in the proposed scheme the data sent to the service provider is perturbed by random transformation and the service provider trains the svm for the data owner from the perturbed data the proposed scheme is stronger in security than existing techniques and incurs very little redundant communication and computation cost
representation exposure is well known problem in the object oriented realm object encapsulation mechanisms have established tradition for solving this problem based on principle of reference containment this paper proposes novel type system which is based on different principle we call effect encapsulation which confines side effects rather than object references according to an ownership structure compared to object encapsulation effect encapsulation liberates us from the restriction on object referenceability and offers more flexibility in this paper we show that effect encapsulation can be statically type checked
set based program analysis establishes constraints between sets of abstract values for all expressions in program solving the system of constraints produces conservative approximation to the program's runtime flow of valuessome practical set based analyses use explicit selectors to extract the relevant values from an approximation set for example if the analysis needs to determine the possible return values of procedure it uses the appropriate selector to extract the relevant component from the abstract representation of the procedurein this paper we show that this selector based approach complicates the constraint solving phase of the analysis too much and thus fails to scale up to realistic programming languages we demonstrate this claim with full fledged value flow analysis for case lambda multi branched version of lambda we show how both the theoretical underpinnings and the practical implementation become too complex in response we present variant of set based closure analysis that computes equivalent results in much more efficient manner
we argue that runtime program transformation partial evaluation and dynamic compilation are essential tools for automated generation of flexible highly interactive graphical interfaces in particular these techniques help bridge the gap between high level functional description and an efficient implementation to support our claim we describe our application of these techniques to functional implementation of vision real time visualization system that represents multivariate relations as nested interactors and to auto visual rule based system that designs vision visualizations from high level task specifications vision visualizations are specified using simple functional language these programs are transformed into cached dataflow graph partial evaluator is used on particular computation intensive function applications and the results are compiled to native code the functional representation simplifies generation of correct code and the program transformations ensure good performance we demonstrate why these transformations improve performance and why they cannot be done at compile time
building rules on top of ontologies is the ultimate goal of the logical layer of the semantic web to this aim an ad hoc markup language for this layer is currently under discussion it is intended to follow the tradition of hybrid knowledge representation and reasoning systems such as inline graphic mime subtype gif xlink sinline alt text mathcal al alt text inline graphic log that integrates the description logic inline graphic mime subtype gif xlink sinline alt text mathcal alc alt text inline graphic and the function free horn clausal language datalog in this paper we consider the problem of automating the acquisition of these rules for the semantic web we propose general framework for rule induction that adopts the methodological apparatus of inductive logic programming and relies on the expressive and deductive power of inline graphic mime subtype gif xlink sinline alt text mathcal al alt text inline graphic log the framework is valid whatever the scope of induction description versus prediction is yet for illustrative purposes we also discuss an instantiation of the framework which aims at description and turns out to be useful in ontology refinement
we present set of techniques for reducing the memory consumption of object oriented programs these techniques include analysis algorithms and optimizations that use the results of these analyses to eliminate fields with constant values reduce the sizes of fields based on the range of values that can appear in each field and eliminate fields with common default values or usage patterns we apply these optimizations both to fields declared by the programmer and to implicit fields in the runtime object header although it is possible to apply these techniques to any object oriented program we expect they will be particularly appropriate for memory limited embedded systemswe have implemented these techniques in the mit flex compiler system and applied them to the programs in the specjvm benchmark suite our experimental results show that our combined techniques can reduce the maximum live heap size required for the programs in our benchmark suite by as much as some of the optimizations reduce the overall execution time others may impose modest performance penalties
next generation decision support applications besides being capable of processing huge amounts of data require the ability to integrate and reason over data from multiple heterogeneous data sources often these data sources differ in variety of aspects such as their data models the query languages they support and their network protocols also typically they are spread over wide geographical area the cost of processing decision support queries in such setting is quite high however processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences minimizing these redundancies would significantly reduce the query processing cost in this paper we propose an architecture for processing complex decision support queries involving multiple heterogeneous data sources introduce the notion of transient views mdash materialized views that exist only in the context of execution of query mdash that is useful for minimizing the redundancies involved in the execution of these queries develop cost based algorithm that takes query plan as input and generates an optimal ldquo covering plan rdquo by minimizing redundancies in the original plan validate our approach by means of an implementation of the algorithms and detailed performance study based on tpc benchmark queries on commercial database system and finally compare and contrast our approach with work in related areas in particular the areas of answering queries using views and optimization using common sub expressions our experiments demonstrate the practicality and usefulness of transient views in significantly improving the performance of decision support queries
editor's note this article describes web based energy estimation tool for embedded systems an interesting feature of this tool is that it performs real time cycle accurate energy measurements on hardware prototype of the processor the authors describe the various steps involved in using the tool and present case studies to illustrate its utility anand raghunathan nec laboratories
warping the pointer across monitor bezels has previously been demonstrated to be both significantly faster and preferred to the standard mouse behavior when interacting across displays in homogeneous multi monitor configurations complementing this work we present user study that compares the performance of four pointer warping strategies including previously untested frame memory placement strategy in heterogeneous multi monitor environments where displays vary in size resolution and orientation our results show that new frame memory pointer warping strategy significantly improved targeting performance up to in some cases in addition our study showed that when transitioning across screens the mismatch between the visual and the device space has significantly bigger impact on performance than the mismatch in orientation and visual size alone for mouse operation in highly heterogeneous multi monitor environment all our participants strongly preferred using pointer warping over the regular mouse behavior
cloud computing is disruptive trend that is changing the way we use computers the key underlying technology in cloud infrastructures is virtualization so much so that many consider virtualization to be one of the key features rather than simply an implementation detail unfortunately the use of virtualization is the source of significant security concern because multiple virtual machines run on the same server and since the virtualization layer plays considerable role in the operation of virtual machine malicious party has the opportunity to attack the virtualization layer successful attack would give the malicious party control over the all powerful virtualization layer potentially compromising the confidentiality and integrity of the software and data of any virtual machine in this paper we propose removing the virtualization layer while retaining the key features enabled by virtualization our nohype architecture named to indicate the removal of the hypervisor addresses each of the key roles of the virtualization layer arbitrating access to cpu memory and devices acting as network device eg ethernet switch and managing the starting and stopping of guest virtual machines additionally we show that our nohype architecture may indeed be no hype since nearly all of the needed features to realize the nohype architecture are currently available as hardware extensions to processors and devices
effort prediction is very important issue for software project management historical project data sets are frequently used to support such prediction but missing data are often contained in these data sets and this makes prediction more difficult one common practice is to ignore the cases with missing data but this makes the originally small software project database even smaller and can further decrease the accuracy of prediction the alternative is missing data imputation there are many imputation methods software data sets are frequently characterised by their small size but unfortunately sophisticated imputation methods prefer larger data sets for this reason we explore using simple methods to impute missing data in small project effort data sets we propose class mean imputation cmi method based on the nn hot deck imputation method mini to impute both continuous and nominal missing data in small data sets we use an incremental approach to increase the variance of population to evaluate mini and nn and cmi methods as benchmarks we use data sets with cases and cases sampled from larger industrial data set with and missing data percentages respectively we also simulate missing completely at random mcar and missing at random mar missingness mechanisms the results suggest that the mini method outperforms both cmi and the nn methods we conclude that this new imputation technique can be used to impute missing values in small data sets
the number of mobile phone users has been steadily increasing due to the development of microtechnology and human needs for ubiquitous communication menu design features play significant role in cell phone design from the perspective of customer satisfaction moreover small screens of the type used on mobile phones are limited in the amount of available space therefore it is important to obtain good menu design review of previous menu design studies for human computer interaction suggests that design guidelines for mobile phones need to be reappraised especially display features we propose conceptual model for cell phone menu design with displays the three main factors included in the model are the number of items task complexity and task type
an ad hoc data source is any semistructured data source for which useful data analysis and transformation tools are not readily available such data must be queried transformed and displayed by systems administrators computational biologists financial analysts and hosts of others on regular basis in this paper we demonstrate that it is possible to generate suite of useful data processing tools including semi structured query engine several format converters statistical analyzer and data visualization routines directly from the ad hoc data itself without any human intervention the key technical contribution of the work is multi phase algorithm that automatically infers the structure of an ad hoc data source and produces format specification in the pads data description language programmers wishing to implement custom data analysis tools can use such descriptions to generate printing and parsing libraries for the data alternatively our software infrastructure will push these descriptions through the pads compiler creating format dependent modules that when linked with format independent algorithms for analysis and transformation result infully functional tools we evaluate the performance of our inference algorithm showing it scales linearlyin the size of the training data completing in seconds as opposed to the hours or days it takes to write description by hand we also evaluate the correctness of the algorithm demonstrating that generating accurate descriptions often requires less than of theavailable data
to manage the evolution of software systems effectively software developers must understand software systems identify and evaluate alternative modification strategies implement appropriate modifications and validate the correctness of the modifications one analysis technique that assists in many of these activities is program slicing to facilitate the application of slicing to large software systems we adapted control flow based interprocedural slicing algorithm so that it accounts for interprocedural control dependencies not recognized by other slicing algorithms and reuses slicing information for improved efficiency our initial studies suggest that additional slice accuracy and slicing efficiency may be achieved with our algorithm
this paper addresses necessary modification and extensions to existing grid computing approaches in order to meet modern business demand grid computing has been traditionally used to solve large scientific problems focussing more on accumulative use of computing power and processing large input and output files typical for many scientific problems nowadays businesses have increasing computational demands such that grid technologies are of interest however the existing business requirements introduce new constraints on the design configuration and operation of the underlying systems including availability of resources performance monitoring aspects security and isolation issues this paper addresses the existing grid computing capabilities discussing the additional demands in detail this results in suggestion of problem areas that must be investigated and corresponding technologies that should be used within future business grid systems
existing work on scheduling with energy concern has focused on minimizing the energy for completing all jobs or achieving maximum throughput that is energy usage is secondary concern when compared to throughput and the schedules targeted may be very poor in energy efficiency in this paper we attempt to put energy efficiency as the primary concern and study how to maximize throughput subject to user defined threshold of energy efficiency we first show that all deterministic online algorithms have competitive ratio at least where is the max min ratio of job size nevertheless allowing the online algorithm to have slightly poorer energy efficiency leads to constant ie independent of competitive online algorithm on the other hand using randomization we can reduce the competitive ratio to logŒ¥ without relaxing the efficiency threshold finally we consider special case where no jobs are demanding and give deterministic online algorithm with constant competitive ratio for this case
mapreduce and similar systems significantly ease the task of writing data parallel code however many real world computations require pipeline of mapreduces and programming and managing such pipelines can be difficult we present flumejava java library that makes it easy to develop test and run efficient data parallel pipelines at the core of the flumejava library are couple of classes that represent immutable parallel collections each supporting modest number of operations for processing them in parallel parallel collections and their operations present simple high level uniform abstraction over different data representations and execution strategies to enable parallel operations to run efficiently flumejava defers their evaluation instead internally constructing an execution plan dataflow graph when the final results of the parallel operations are eventually needed flumejava first optimizes the execution plan and then executes the optimized operations on appropriate underlying primitives eg mapreduces the combination of high level abstractions for parallel data and computation deferred evaluation and optimization and efficient parallel primitives yields an easy to use system that approaches the efficiency of hand optimized pipelines flumejava is in active use by hundreds of pipeline developers within google
in clustering based wireless sensor network wsn cluster heads play an important role by serving as data forwarders amongst other network organisation functions thus malfunctioning and or compromised sensor nodes that serve as cluster head ch can lead to unreliable data delivery in this paper we propose scheme called secure low energy clustering seclec that incorporates secure cluster head selection and distributed cluster head monitoring to achieve reliable data delivery the seclec framework is flexible and can accommodate various tracking and monitoring mechanisms our goal in this paper is to understand the performance impact of adding such security mechanisms to the clustering architecture in terms of energy consumed and prevented data losses our experiments show that with seclec bs detects such anomalous nodes with the latency of one network operation round and the data loss due to malicious nodes is up to with reasonable communication overhead
we present new technique failure oblivious computing that enables servers to execute through memory errors without memory corruption our safe compiler for inserts checks that dynamically detect invalid memory accesses instead of terminating or throwing an exception the generated code simply discards invalid writes and manufactures values to return for invalid reads enabling the server to continue its normal execution path we have applied failure oblivious computing to set of widely used servers from the linux based open source computing environment our results show that our techniques make these servers invulnerable to known security attacks that exploit memory errors and enable the servers to continue to operate successfully to service legitimate requests and satisfy the needs of their users even after attacks trigger their memory errors we observed several reasons for this successful continued execution when the memory errors occur in irrelevant computations failure oblivious computing enables the server to execute through the memory errors to continue on to execute the relevant computation even when the memory errors occur in relevant computations failure oblivious computing converts requests that trigger unanticipated and dangerous execution paths into anticipated invalid inputs which the error handling logic in the server rejects because servers tend to have small error propagation distances localized errors in the computation for one request tend to have little or no effect on the computations for subsequent requests redirecting reads that would otherwise cause addressing errors and discarding writes that would otherwise corrupt critical data structures such as the call stack localizes the effect of the memory errors prevents addressing exceptions from terminating the computation and enables the server to continue on to successfully process subsequent requests the overall result is substantial extension of the range of requests that the server can successfully process
multidatabase system mdbs integrates information from autonomous local databases managed by heterogeneous database management systems dbms in distributed environment for query involving more than one database global query optimization should be performed to achieve good overall system performance the significant differences between an mdbs and traditional distributed database system ddbs make query optimization in the former more challenging than in the latter challenges for query optimization in an mdbs are discussed in this paper two phase optimization approach for processing query in an mdbs is proposed several global query optimization techniques suitable for an mdbs such as semantic query optimization query optimization via probing queries parametric query optimization and adaptive query optimization are suggested the architecture of global query optimizer incorporating these techniques is designed
we present the design implementation and evaluation of beepbeep high accuracy acoustic based ranging system it operates in spontaneous ad hoc and device to device context without leveraging any pre planned infrastructure it is pure software based solution and uses only the most basic set of commodity hardware speaker microphone and some form of device to device communication so that it is readily applicable to many low cost sensor platforms and to most commercial off the shelf mobile devices like cell phones and pdas it achieves high accuracy through combination of three techniques two way sensing self recording and sample counting the basic idea is the following to estimate the range between two devices each will emit specially designed sound signal beep and collect simultaneous recording from its microphone each recording should contain two such beeps one from its own speaker and the other from its peer by counting the number of samples between these two beeps and exchanging the time duration information with its peer each device can derive the two way time of flight of the beeps at the granularity of sound sampling rate this technique cleverly avoids many sources of inaccuracy found in other typical time of arrival schemes such as clock synchronization non real time handling software delays etc our experiments on two common cell phone models have shown that we can achieve around one or two centimeters accuracy within range of more than ten meters despite series of technical challenges in implementing the idea
the ability of reconfiguring software architectures in order to adapt them to new requirements or changing environment has been of growing interest we propose uniform algebraic approach that improves on previous formal work in the area due to the following characteristics first components are written in high level program design language with the usual notion of state second the approach deals with typical problems such as guaranteeing that new components are introduced in the correct state possibly transferred from the old components they replace and that the resulting architecture conforms to certain structural constraints third reconfigurations and computations are explicitly related by keeping them separate this is because the approach provides semantics to given architecture through the algebraic construction of an equivalent program whose computations can be mirrored at the architectural level
developers and designers always strive for quality software quality software tends to be robust reliable and easy to maintain and thus reduces the cost of software development and maintenance several methods have been applied to improve software quality refactoring is one of those methods the goal of this paper is to validate invalidate the claims that refactoring improves software quality we focused this study on different external quality attributes which are adaptability maintainability understandability reusability and testability we found that refactoring does not necessarily improve these quality attributes
we investigate new approach to editing spatially and temporally varying measured materials that adopts stroke based workflow in our system user specifies small number of editing constraints with painting interface which are smoothly propagated to the entire dataset through an optimization that enforces similar edits are applied to areas with similar appearance the sparse nature of this appearance driven optimization permits the use of efficient solvers allowing the designer to interactively refine the constraints we have found this approach supports specifying wide range of complex edits that would not be easy with existing techniques which present the user with fixed segmentation of the data furthermore it is independent of the underlying reflectance model and we show edits to both analytic and non parametric representations in examples from several material databases
the circular sensing model has been widely used to estimate performance of sensing applications in existing analysis and simulations while this model provides valuable high level guidelines the quantitative results obtained may not reflect the true performance of these applications due to the existence of obstacles and sensing irregularity introduced by insufficient hardware calibration in this project we design and implement two sensing area modeling sam techniques useful in the real world they complement each other in the design space sam provides accurate sensing area models for individual nodes using controlled or monitored events while sam provides continuous sensing similarity models using natural events in an environment with these two models we pioneer an investigation of the impact of sensing irregularity on application performance such as coverage scheduling we evaluate sam extensively in real world settings using three testbeds consisting of micaz motes and xsm motes to study the performance at scale we also provide an extensive node simulation evaluation results reveal several serious issues concerning circular models and demonstrate significant improvements
the successful design and implementation of secure systems must occur from the beginning component that must process data at multiple security levels is very critical and must go through additional evaluation to ensure the processing is secure it is common practice to isolate and separate the processing of data at different levels into different components in this paper we present architecture based refinement techniques for the design of multilevel secure systems we discuss what security requirements must be satisfied through the refinement process including when separation works and when it does not the process oriented approach will lead to verified engineering techniques for secure systems which should greatly reduce the cost of certification of those systems
this paper presents an analysis of the performance effects of burstiness in multi tiered systems we introduce compact characterization of burstiness based on autocorrelation that can be used in capacity planning performance prediction and admission control we show that if autocorrelation exists either in the arrival or the service process of any of the tiers in multi tiered system then autocorrelation propagates to all tiers of the system we also observe the surprising result that in spite of the fact that the bottleneck resource in the system is far from saturation and that the measured throughput and utilizations of other resources are also modest user response times are very high when autocorrelation is not considered this underutilization of resources falsely indicates that the system can sustain higher capacities we examine the behavior of small queuing system that helps us understand this counter intuitive behavior and quantify the performance degradation that originates from autocorrelated flows we present case study in an experimental multi tiered internet server and devise model to capture the observed behavior our evaluation indicates that the model is in excellent agreement with experimental results and captures the propagation of autocorrelation in the multi tiered system and resulting performance trends finally we analyze an admission control algorithm that takes autocorrelation into account and improves performance by reducing the long tail of the response time distribution
in this article we propose techniques that enable efficient exploration of the design space where each logical block can span more than one silicon layer fine grain integration provides reduced intrablock wire delay as well as improved power consumption however the corresponding power and performance advantage is usually underutilized since various implementations of multilayer blocks require novel physical design and microarchitecture infrastructure to explore microarchitecture design space we develop cubic packing engine which can simultaneously optimize physical and architectural design for efficient vertical integration this technique selects the individual unit designs from set of single layer or multilayer implementations to get the best microarchitectural design in terms of performance temperature or both our experimental results using design driver of high performance superscalar processor show percnt performance improvement over traditional for layers and percnt over with single layer unit implementations since thermal characteristics of integrated circuits are among the main challenges thermal aware floorplanning and thermal via insertion techniques are employed to keep the peak temperatures below threshold
large scheduling windows are an effective mechanism for increasing microprocessor performance through the extraction of instruction level parallelism current techniques do not scale effectively for very large windows leading to slow wakeup and select logic as well as large complicated bypass networks this paper introduces new instruction scheduler implementation referred to as hierarchical scheduling windows or hsw which exploits latency tolerant instructions in order to reduce implementation complexity hsw yields very large instruction window that tolerates wakeup select and bypass latency while extracting significant far flung ilpresults it is shown that hsw loses performance per additional cycle of bypass select wakeup latency as compared to monolithic window that loses per additional cycle also hsw achieves the performance of traditional implementations with only to the number of entries in the critical timing path
bubba is highly parallel computer system for data intensive applications the basis of the bubba design is scalable shared nothing architecture which can scale up to thousands of nodes data are declustered across the nodes ie horizontally partitioned via hashing or range partitioning and operations are executed at those nodes containing relevant data in this way parallelism can be exploited within individual transactions as well as among multiple concurrent transactions to improve throughput and response times for data intensive applications the current bubba prototype runs on commercial node multicomputer and includes parallelizing compiler distributed transaction management object management and customized version of unix the current prototype is described and the major design decisions that went into its construction are discussed the lessons learned from this prototype and its predecessors are presented
in this article we consider whether traditional index structures are effective in processing unstable nearest neighbors workloads it is known that under broad conditions nearest neighbors workloads become unstable distances between data points become indistinguishable from each other we complement this earlier result by showing that if the workload for an application is unstable you are not likely to be able to index it efficiently using almost all known multidimensional index structures for broad class of data distributions we prove that these index structures will do no better than linear scan of the data as dimensionality increasesour result has implications for how experiments should be designed on index structures such as trees trees and sr trees simply put experiments trying to establish that these index structures scale with dimensionality should be designed to establish crossover points rather than to show that the methods scale to an arbitrary number of dimensions in other words experiments should seek to establish the dimensionality of the dataset at which the proposed index structure deteriorates to linear scan for each data distribution of interest that linear scan will eventually dominate is givenan important problem is to analytically characterize the rate at which index structures degrade with increasing dimensionality because the dimensionality of real data set may well be in the range that particular method can handle the results in this article can be regarded as step toward solving this problem although we do not characterize the rate at which structure degrades our techniques allow us to reason directly about broad class of index structures rather than the geometry of the nearest neighbors problem in contrast to earlier work
modeling is core software engineering practice conceptual models are constructed to establish an abstract understanding of the domain among stakeholders these are then refined into computational models that aim to realize conceptual specification the refinement process yields sets of models that are initially incomplete and inconsistent by nature the aim of the engineering process is to negotiate consistency and completeness toward stable state sufficient for deployment implementation this paper presents the notion of model ecosystem which permits the capability to guide analyst edits toward stability by computing consistency and completeness equilibria for conceptual models during periods of model change
several forms of reasoning in ai like abduction closed world reasoning circumscription and disjunctive logic programming are well known to be intractable in fact many of the relevant problems are on the second or third level of the polynomial hierarchy in this paper we show how the notion of treewidth can be fruitfully applied to this area in particular we show that all these problems become tractable actually even solvable in linear time if the treewidth of the involved formulae or programs is bounded by some constant clearly these theoretical tractability results as such do not immediately yield feasible algorithms however we have recently established new method based on monadic datalog which allowed us to design an efficient algorithm for related problem in the database area in this work we exploit the monadic datalog approach to construct new algorithms for logic based abduction
we propose different implementations of the sparse matrix dense vector multiplication spmv for finite fields and rings we take advantage of graphic card processors gpu and multi core architectures our aim is to improve the speed of spmv in the linbox library and henceforth the speed of its black box algorithms besides we use this library and new parallelisation of the sigma basis algorithm in parallel block wiedemann rank implementation over finite fields
fairground thrill laboratory was series of live events that augmented the experience of amusement rides wearable telemetry system captured video audio heart rate and acceleration data streaming them live to spectator interfaces and watching audience in this paper we present study of this event which draws on video recordings and post event interviews and which highlights the experiences of riders spectators and ride operators our study shows how the telemetry system transformed riders into performers spectators into an audience and how the role of ride operator began to include aspects of orchestration with the relationship between all three roles also transformed critically the introduction of telemetry system seems to have had the potential to re connect riders performers back to operators orchestrators and spectators audience re introducing closer relationship that used to be available with smaller rides introducing telemetry to real world situation also creates significant complexity which we illustrate by focussing on moment of perceived crisis
in order to achieve good performance in object classification problems it is necessary to combine information from various image features because the large margin classifiers are constructed based on similarity measures between samples called kernels finding appropriate feature combinations boils down to designing good kernels among set of candidates for example positive mixtures of predetermined base kernels there are couple of ways to determine the mixing weights of multiple kernels uniform weights brute force search over validation set and multiple kernel learning mkl mkl is theoretically and technically very attractive because it learns the kernel weights and the classifier simultaneously based on the margin criterion however we often observe that the support vector machine svm with the average kernel works at least as good as mkl in this paper we propose as an alternative two step approach at first the kernel weights are determined by optimizing the kernel target alignment score and then the combined kernel is used by the standard svm with single kernel the experimental results with the voc data set show that our simple procedure outperforms the average kernel and mkl
reinhard wilhelm's career in computer science spans more than third of century during this time he has made numerous research contributions in the areas of programming languages compilers and compiler generators static program analysis program transformation algorithm animation and real time systems co founded company to transfer some of these ideas to industry held the chair for programming languages and compiler construction at saarland university and served since its inception as the scientific director of the international conference and research center for computer science at schlo√ü dagstuhl
in this paper we evaluate the atomic region compiler abstraction by incorporating it into commercial system we find that atomic regions are simple and intuitive to integrate into an binary translation system furthermore doing so trivially enables additional optimization opportunities beyond that achievable by high performance dynamic optimizer which already implements superblocks we show that atomic regions can suffer from severe performance penalties if misspeculations are left uncontrolled but that simple software control mechanism is sufficient to reign in all detrimental side effects we evaluate using full reference runs of the spec cpu integer benchmarks and find that atomic regions enable up to on average improvement beyond the performance of tuned product these performance improvements are achieved without any negative side effects performance side effects such as code bloat are absent with atomic regions in fact static code size is reduced the hardware necessary is synergistic with other needs and was already available on the commercial product used in our evaluation finally the software complexity is minimal as single developer was able to incorporate atomic regions into sophisticated line code base in three months despite never having seen the translator source code beforehand
twin page storage method which is an alternative to the twist twin slot approach by reuter
the primary business model behind web search is based on textual advertising where contextually relevant ads are displayed alongside search results we address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine showing that optimizing ad relevance and revenue is not equivalent selecting the best ads that satisfy these constraints also naturally incurs high computational costs and time constraints can lead to reduced relevance and profitability we propose novel two stage approach which conducts most of the analysis ahead of time an offine preprocessing phase leverages additional knowledge that is impractical to use in real time and rewrites frequent queries in way that subsequently facilitates fast and accurate online matching empirical evaluation shows that our method optimized for relevance matches state of the art method while improving expected revenue when optimizing for revenue we see even more substantial improvements in expected revenue
modern web search engines use different strategies to improve the overall quality of their document rankings usually the strategy adopted involves the combination of multiple sources of relevance into single ranking this work proposes the use of evolutionary techniques to derive good evidence combination functions using three different sources of evidence of relevance the textual content of documents the reputation of documents extracted from the connectivity information available in the processed collection and the anchor text concatenation the combination functions discovered by our evolutionary strategies were tested using collection containing queries extracted from real nation wide search engine query log with over million documents the experiments performed indicate that our proposal is an effective and practical alternative for combining sources of evidence into single ranking we also show that different types of queries submitted to search engine can require different combination functions and that our proposal is useful for coping with such differences
we propose framework for integrating data from multiple relational sources into an xml document that both conforms to given dtd and satisfies predefined xml constraints the framework is based on specification language aig that extends dtd by associating element types with semantic attributes inherited and synthesized inspired by the corresponding notions from attribute grammars computing these attributes via parameterized sql queries over multiple data sources and incorporating xml keys and inclusion constraints the novelty of aig consists in semantic attributes and their dependency relations for controlling context dependent dtd directed construction of xml documents as well as for checking xml constraints in parallel with document generation we also present cost based optimization techniques for efficiently evaluating aigs including algorithms for merging queries and for scheduling queries on multiple data sources this provides new grammar based approach for data integration under both syntactic and semantic constraints
technology trends present new challenges for processor architectures and their instruction schedulers growing transistor density will increase the number of execution units on single chip and decreasing wire transmission speeds will cause long and variable on chip latencies these trends will severely limit the two dominant conventional architectures dynamic issue superscalars and static placement and issue vliws we present new execution model in which the hardware and static scheduler instead work cooperatively called static placement dynamic issue spdi this paper focuses on the static instruction scheduler for spdi we identify and explore three issues spdi schedulers must consider locality contention and depth of speculation we evaluate range of spdi scheduling algorithms executing on an explicit data graph execution edge architecture we find that surprisingly simple one achieves an average of instructions per cycle ipc for spec wide issue machine and is within of the performance without on chip latencies these results suggest that the compiler is effective at balancing on chip latency and parallelism and that the division of responsibilities between the compiler and the architecture is well suited to future systems
the operations and management activities of enterprises are mainly task based and knowledge intensive accordingly an important issue in deploying knowledge management systems is the provision of task relevant information codified knowledge to meet the information needs of knowledge workers during the execution of task codified knowledge extracted from previously executed tasks can provide valuable knowledge about conducting the task at hand current task and is valuable information source for constructing task profile that models worker's task needs ie information needs for the current task in this paper we propose novel task relevance assessment approach that evaluates the relevance of previous tasks in order to construct task profile for the current task the approach helps knowledge workers assess the relevance of previous tasks through linguistic evaluation and the collaboration of knowledge workers in addition applying relevance assessment to large number of tasks may create an excessive burden for workers thus we propose novel two phase relevance assessment method to help workers conduct relevance assessment effectively furthermore modified relevance feedback technique which is integrated with the task relevance assessment method is employed to derive the task profile for the task at hand consequently task based knowledge support can be enabled to provide knowledge workers with task relevant information based on task profiles empirical experiments demonstrate that the proposed approach models workers task needs effectively and helps provide task relevant knowledge
some significant progress related to multidimensional data analysis has been achieved in the past few years including the design of fast algorithms for computing datacubes selecting some precomputed group bys to materialize and designing efficient storage structures for multidimensional data however little work has been carried out on multidimensional query optimization issues particularly the response time or evaluation cost for answering several related dimensional queries simultaneously is crucial to the olap applications recently zhao et al first exploited this problem by presenting three heuristic algorithms in this paper we first consider in detail two cases of the problem in which all the queries are either hash based star joins or index based star joins only in the case of the hash based star join we devise polynomial approximation algorithm which delivers plan whose evaluation cost is epsilon times the optimal where is the number of queries and epsilon is fixed constant with epsilon leq we also present an exponential algorithm which delivers plan with the optimal evaluation cost in the case of the index based star join we present heuristic algorithm which delivers plan whose evaluation cost is times the optimal and an exponential algorithm which delivers plan with the optimal evaluation cost we then consider general case in which both hash based star join and index based star join queries are included for this case we give possible improvement on the work of zhao et al based on an analysis of their solutions we also develop another heuristic and an exact algorithm for the problem we finally conduct performance study by implementing our algorithms the experimental results demonstrate that the solutions delivered for the restricted cases are always within two times of the optimal which confirms our theoretical upper bounds actually these experiments produce much better results than our theoretical estimates to the best of our knowledge this is the only development of polynomial algorithms for the first two cases which are able to deliver plans with deterministic performance guarantees in terms of the qualities of the plans generated the previous approaches including that of zdns may generate feasible plan for the problem in these two cases but they do not provide any performance guarantee ie the plans generated by their algorithms can be arbitrarily far from the optimal one
recently method for removing shadows from colour images was developed finlayson et al in ieee trans pattern anal mach intell that relies upon finding special direction in chromaticity feature space this invariant direction is that for which particular colour features when projected into produce greyscale image which is approximately invariant to intensity and colour of scene illumination thus shadows which are in essence particular type of lighting are greatly attenuated the main approach to finding this special angle is camera calibration colour target is imaged under many different lights and the direction that best makes colour patch images equal across illuminants is the invariant direction here we take different approach in this work instead of camera calibration we aim at finding the invariant direction from evidence in the colour image itself specifically we recognize that producing projection in the correct invariant direction will result in distribution of pixel values that have smaller entropy than projecting in the wrong direction the reason is that the correct projection results in probability distribution spike for pixels all the same except differing by the lighting that produced their observed rgb values and therefore lying along line with orientation equal to the invariant direction hence we seek that projection which produces type of intrinsic independent of lighting reflectance information only image by minimizing entropy and from there go on to remove shadows as previously to be able to develop an effective description of the entropy minimization task we go over to the quadratic entropy rather than shannon's definition replacing the observed pixels with kernel density probability distribution the quadratic entropy can be written as very simple formulation and can be evaluated using the efficient fast gauss transform the entropy written in this embodiment has the advantage that it is more insensitive to quantization than is the usual definition the resulting algorithm is quite reliable and the shadow removal step produces good shadow free colour image results whenever strong shadow edges are present in the image in most cases studied entropy has strong minimum for the invariant direction revealing new property of image formation
we investigate the relationship between symmetry reduction and inductive reasoning when applied to model checking networks of featured components popular reduction techniques for combatting state space explosion in model checking like abstraction and symmetry reduction can only be applied effectively when the natural symmetry of system is not destroyed during specification we introduce property which ensures this is preserved open symmetry we describe template based approach for the construction of open symmetric promela specifications of featured systems for certain systems safely featured parameterised systems our generated specifications are suitable for conversion to abstract specifications representing any size of network this enables feature interaction analysis to be carried out via model checking and induction for systems of any number of featured components in addition we show how for any balanced network of components by using graphical representation of the features and the process communication structure group of permutations of the underlying state space of the generated specification can be determined easily due to the open symmetry of our promela specifications this group of permutations can be used directly for symmetry reduced model checking the main contributions of this paper are an automatic method for developing open symmetric specifications which can be used for generic feature interaction analysis and the novel application of symmetry detection and reduction in the context of model checking featured networks we apply our techniques to well known example of featured network an email system
log polar imaging consists of type of methods that represent visual information with space variant resolution inspired by the visual system of mammals it has been studied for about three decades and has surpassed conventional approaches in robotics applications mainly the ones where real time constraints make it necessary to utilize resource economic image representations and processing methodologies this paper surveys the application of log polar imaging in robotic vision particularly in visual attention target tracking egomotion estimation and perception the concise yet comprehensive review offered in this paper is intended to provide novel and experienced roboticists with quick and gentle overview of log polar vision and to motivate vision researchers to investigate the many open problems that still need solving to help readers identify promising research directions possible research agenda is outlined finally since log polar vision is not restricted to robotics couple of other areas of application are discussed
to understand how and why individuals make use of emerging information assimilation services on the web as part of their daily routine we combined video recordings of online activity with targeted interviews of eleven experienced web users from these observations we describe their choice of systems the goals they are trying to achieve their information diets the basic process they use for assimilating information and the impact of user interface speed
in this article we explore the syntactic and semantic properties of prepositions in the context of the semantic interpretation of nominal phrases and compounds we investigate the problem based on cross linguistic evidence from set of six languages english spanish italian french portuguese and romanian the focus on english and romance languages is well motivated most of the time english nominal phrases and compounds translate into constructions of the form in romance languages where the preposition may vary in ways that correlate with the semantics thus we present empirical observations on the distribution of nominal phrases and compounds and the distribution of their meanings on two different corpora based on two state of the art classification tag sets lauer's set of eight prepositions and our list of semantic relations mapping between the two tag sets is also provided furthermore given training set of english nominal phrases and compounds along with their translations in the five romance languages our algorithm automatically learns classification rules and applies them to unseen test instances for semantic interpretation experimental results are compared against two state of the art models reported in the literature
this paper investigates the data exchange problem among distributed independent sources it is based on previous works in in which declarative semantics for pp systems in this semantics only facts not making the local databases inconsistent are imported weak models and the preferred weak models are those in which peers import maximal sets of facts not violating integrity constraints the framework proposed in does not provide any mechanism to set priorities among mapping rules anyhow while collecting data it is quite natural for source peer to associate different degrees of reliability to the portion of data provided by its neighbor peers starting from this observation this paper enhances previous semantics by using priority levels among mapping rules in order to select the weak models containing maximum number of mapping atoms according to their importance we will call these weak models trusted weak models and we will show they can be computed as stable models of logic program with weak constraints
significant effort has been invested in developing expressive and flexible access control languages and systems however little has been done to evaluate these systems in practical situations with real users and few attempts have been made to discover and analyze the access control policies that users actually want to implement we report on user study in which we derive the ideal access policies desired by group of users for physical security in an office environment we compare these ideal policies to the policies the users actually implemented with keys and with smartphone based distributed access control system we develop methodology that allows us to show quantitatively that the smartphone system allowed our users to implement their ideal policies more accurately and securely than they could with keys and we describe where each system fell short
location based routing lbr is one of the most widely used routing strategies in large scale wireless sensor networks with lbr small cheap and resource constrained nodes can perform the routing function without the need of complex computations and large amounts of memory space further nodes do not need to send energy consuming periodic advertisements because routing tables in the traditional sense are not needed one important assumption made by most lbr protocols is the availability of location service or mechanism to find other nodes positions although several mechanisms exist most of them rely on some sort of flooding procedure unsuitable for large scale wireless sensor networks especially with multiple and moving sinks and sources in this paper we introduce the anchor location service als protocol grid based protocol that provides sink location information in scalable and efficient manner and therefore supports location based routing in large scale wireless sensor networks the location service is evaluated mathematically and by simulations and also compared with well known grid based routing protocol our results demonstrate that als not only provides an efficient and scalable location service but also reduces the message overhead and the state complexity in scenarios with multiple and moving sinks and sources which are not usually included in the literature
ontology mapping is mandatory requirement for enabling semantic interoperability among different agents and services relying on different ontologies this aspect becomes more critical in peer to peer pp networks for several reasons the number of different ontologies can dramatically increase ii mappings among peer ontologies have to be discovered on the fly and only on the parts of ontologies contextual to specific interaction in which peers are involved iii complex mapping strategies eg structural mapping based on graph matching cannot be exploited since peers are not aware of one another's ontologies in order to address these issues we developed new ontology mapping algorithm called semantic coordinator secco secco is composed by three individual matchers syntactic lexical and contextual the syntactic matcher in order to discover mappings exploits different kinds of linguistic information eg comments labels encoded in ontology entities the lexical matcher enables discovering mappings in semantic way since it interprets the semantic meaning of concepts to be compared the contextual matcher relies on how it fits strategy inspired by the contextual theory of meaning and by taking into account the contexts in which the concepts to be compared are used refines similarity values we show through experimental results that secco fulfills two important requirements fastness and accuracy ie quality of mappings secco differently from other semantic pp applications eg piazza gridvine that assume the preexistence of mappings for achieving semantic interoperability focuses on the problem of finding mappings therefore if coupled with pp platform it paves the way towards comprehensive semantic pp solution for content sharing and retrieval semantic query answering and query routing we report on the advantages of integrating secco in the link system
in this paper we develop an automatic wrapper for the extraction of multiple sections data records from search engine results pages in the information extraction world less attention has been focused on the development of wrappers for the extraction of multiple sections data records this is evidenced by the fact that there is only one automatic wrapper mse developed for this purpose using the separation distance of data records and sections mse is able to distinguish sections and data records and extract them from search engine results pages in this study our approach is the use of dom tree properties to develop an adaptive search method which is able to detect differentiate and partition sections and data records the multiple sections data records labeled are used to pass through few filtering stages each filter is designed to filter out particular group of irrelevant data until one data region containing the relevant records is found our filtering rules are designed based on visual cue such as text and image size obtained from the browser rendering engine experimental results show that our wrapper is able to obtain better results than the currently available mse wrapper
discovery of sequential patterns is an essential data mining task with broad applications among several variations of sequential patterns closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it unfortunately there is no parallel closed sequential pattern mining method proposed yet in this paper we develop an algorithm called par csp parallel closed sequential pattern mining to conduct parallel mining of closed sequential patterns on distributed memory system par csp partitions the work among the processors by exploiting the divide and conquer property so that the overhead of interprocessor communication is minimized par csp applies dynamic scheduling to avoid processor idling moreover it employs technique called selective sampling to address the load imbalance problem we implement par csp using mpi on node linux cluster our experimental results show that par csp attains good parallelization efficiencies on various input datasets
spearman's footrule and kendall's tau are two well established distances between rankings they however fail to take into account concepts crucial to evaluating result set in information retrieval element relevance and positional information that is changing the rank of highly relevant document should result in higher penalty than changing the rank of an irrelevant document similar logic holds for the top versus the bottom of the result ordering in this work we extend both of these metrics to those with position and element weights and show that variant of the diaconis graham inequality still holds the generalized two measures remain within constant factor of each other for all permutations we continue by extending the element weights into distance metric between elements for example in search evaluation swapping the order of two nearly duplicate results should result in little penalty even if these two are highly relevant and appear at the top of the list we extend the distance measures to this more general case and show that they remain within constant factor of each other we conclude by conducting simple experiments on web search data with the proposed measures our experiments show that the weighted generalizations are more robust and consistent with each other than their unweighted counter parts
we propose new approach for constructing pp networks based on dynamic decomposition of continuous space into cells corresponding to servers we demonstrate the power of this approach by suggesting two new pp architectures and various algorithms for them the first serves as dht distributed hash table and the other is dynamic expander network the dht network which we call distance halving allows logarithmic routing and load while preserving constant degrees it offers an optimal tradeoff between degree and path length in the sense that degree guarantees path length of logd another advantage over previous constructions is its relative simplicity major new contribution of this construction is dynamic caching technique that maintains low load and storage even under the occurrence of hot spots our second construction builds network that is guaranteed to be an expander the resulting topologies are simple to maintain and implement their simplicity makes it easy to modify and add protocols small variation yields dht which is robust against random byzantine faults finally we show that using our approach it is possible to construct any family of constant degree graphs in dynamic environment though with worse parameters therefore we expect that more distributed data structures could be designed and implemented in dynamic environment
wormhole attack is particularly harmful against routing in sensor networks where an attacker receives packets at one location in the network tunnels and then replays them at another remote location in the network wormhole attack can be easily launched by an attacker without compromising any sensor nodes since most of the routing protocols do not have mechanisms to defend the network against wormhole attacks the route request can be tunneled to the target area by the attacker through wormholes thus the sensor nodes in the target area build the route through the attacker later the attacker can tamper the data messages or selectively forward data messages to disrupt the functions of the sensor network researchers have used some special hardware such as the directional antenna and the precise synchronized clock to defend the sensor network against wormhole attacks during the neighbor discovery process in this paper we propose secure routing protocol against wormhole attacks in sensor networks serwa serwa protocol avoids using any special hardware such as the directional antenna and the precise synchronized clock to detect wormhole moreover it provides real secure route against the wormhole attack simulation results show that serwa protocol only has very small false positives for wormhole detection during the neighbor discovery process less than the average energy usage at each node for serwa protocol during the neighbor discovery and route discovery is below mj which is much lower than the available energy kj at each node the cost analysis shows that serwa protocol only needs small memory usage at each node below kb if each node has neighbors which is suitable for the sensor network
exception handling mechanisms are intended to support the development of robust software however the implementation of such mechanisms with aspect oriented ao programming might lead to error prone scenarios as aspects extend or replace existing functionality at specific join points in the code execution aspects behavior may bring new exceptions which can flow through the program execution in unexpected ways this paper presents systematic study that assesses the error proneness of aop mechanisms on exception flows of evolving programs the analysis was based on the object oriented and the aspect oriented versions of three medium sized systems from different application domains our findings show that exception handling code in ao systems is error prone since all versions analyzed presented an increase in the number of uncaught exceptions and exceptions caught by the wrong handler the causes of such problems are characterized and presented as catalogue of bug patterns
service oriented systems are constructed using web services as first class programmable units and subsystems and there have been many successful applications of such systems however there is major unresolved problem with the software development and subsequent management of these applications and systems web service interfaces and implementations may be developed and changed autonomously which makes traditional configuration management practices inadequate for web services checking the compatibility of these programmable units turns out to be difficult task in this paper we present technique for checking compatibility of web service interfaces and implementations based on categorizing domain ontology instances of service description documents this technique is capable of both assessing the compatibility and identifying incompatibility factors of service interfaces and implementations the design details of system model for web service compatibility checking and the key operator for evaluating compatibility within the model are discussed we present simulation experiments and analyze the results to show the effectiveness and performance variations of our technique with different data source patterns
in this paper we are interested in minimizing the delay and maximizing the lifetime of event driven wireless sensor networks for which events occur infrequently in such systems most of the energy is consumed when the radios are on waiting for packet to arrive sleep wake scheduling is an effective mechanism to prolong the lifetime of these energy constrained wireless sensor networks however sleep wake scheduling could result in substantial delays because transmitting node needs to wait for its next hop relay node to wake up an interesting line of work attempts to reduce these delays by developing anycast based packet forwarding schemes where each node opportunistically forwards packet to the first neighboring node that wakes up among multiple candidate nodes in this paper we first study how to optimize the anycast forwarding schemes for minimizing the expected packet delivery delays from the sensor nodes to the sink based on this result we then provide solution to the joint control problem of how to optimally control the system parameters of the sleep wake scheduling protocol and the anycast packet forwarding protocol to maximize the network lifetime subject to constraint on the expected end to end packet delivery delay our numerical results indicate that the proposed solution can outperform prior heuristic solutions in the literature especially under practical scenarios where there are obstructions eg lake or mountain in the coverage area of the wireless sensor network
in this paper we propose sharpness dependent filter design based on the fairing of surface normal whereby the filtering algorithm automatically selects filter this may be mean filter min filter or filter ranked between these two depending on the local sharpness value and the sharpness dependent weighting function selected to recover the original shape of noisy model the algorithm selects mean filter for flat regions and min filter for distinguished sharp regions the selected sharpness dependent weighting function has gaussian laplacian or el fallah ford form that approximately fits the sharpness distribution found in all tested noisy models we use sharpness factor in the weighting function to control the degree of feature preserving the appropriate sharpness factor can be obtained by sharpness analysis based on the bayesian classification our experiment results demonstrate that the proposed sharpness dependent filter is superior to other approaches for smoothing polygon mesh as well as for preserving its sharp features
singleton kinds provide an elegant device for expressing type equality information resulting from modern module languages but they can complicate the metatheory of languages in which they appear present translation from language with singleton kinds to one without and prove this translation to be sound and complete this translation is useful for type preserving compilers generating typed target languages the proof of soundness and completeness is done by normalizing type equivalence derivations using stone and harper's type equivalence decision procedure
the paper investigates geometric properties of quasi perspective projection model in one and two view geometry the main results are as follows quasi perspective projection matrix has nine degrees of freedom dof and the parallelism along and directions in world system are preserved in images ii quasi fundamental matrix can be simplified to special form with only six dofs the fundamental matrix is invariant to any non singular projective transformation iii plane induced homography under quasi perspective model can be simplified to special form defined by six dofs the quasi homography may be recovered from two pairs of corresponding points with known fundamental matrix iv any two reconstructions in quasi perspective space are defined up to non singular quasi perspective transformation the results are validated experimentally on both synthetic and real images
this paper presents an end user oriented programming environment called mashroom major contributions herein include an end user programming model with an expressive data structure as well as set of formally defined mashup operators the data structure takes advantage of nested table and maintains the intuitiveness while allowing users to express complex data objects the mashup operators are visualized with contextual menu and formula bar and can be directly applied on the data experiments and case studies reveal that end users have little difficulty in effectively and efficiently using mashroom to build mashup applications
how to save energy is critical issue for the life time of sensor networks under continuously changing environments sensor nodes have varying sampling rates in this paper we present an online algorithm to minimize the total energy consumption while satisfying sampling rate with guaranteed probability we model the sampling rate as random variable which is estimated over finite time window an efficient algorithm eosp energy aware online algorithm to satisfy sampling rates with guaranteed probability is proposed our approach can adapt the architecture accordingly to save energy experimental results demonstrate the effectiveness of our approach
policies in modern systems and applications play an essential role we argue that decisions based on policy rules should take into account the possibility for the users to enable specific policy rules by performing actions at the time when decisions are being rendered and or by promising to perform other actions in the future decisions should also consider preferences among different sets of actions enabling different rules we adopt formalism and mechanism devised for policy rule management in this context and investigate in detail the notion of obligations which are those actions users promise to perform in the future upon firing of specific policy rule we also investigate how obligations can be monitored and how the policy rules should be affected when obligations are either fulfilled or defaulted
vision based human action recognition provides an advanced interface and research in the field of human action recognition has been actively carried out however an environment from dynamic viewpoint where we can be in any position any direction etc must be considered in our living space in order to overcome the viewpoint dependency we propose volume motion template vmt and projected motion template pmt the proposed vmt method is an extension of the motion history image mhi method to space the pmt is generated by projecting the vmt into plane that is orthogonal to an optimal virtual viewpoint where the optimal virtual viewpoint is viewpoint from which an action can be described in greatest detail in space from the proposed method any actions taken from different viewpoints can be recognized independent of the viewpoints the experimental results demonstrate the accuracies and effectiveness of the proposed vmt method for view independent human action recognition
in blaze bleumer and strauss bbs proposed an application called atomic proxy re encryption in which semitrusted proxy converts ciphertext for alice into ciphertext for bob without seeing the underlying plaintext we predict that fast and secure re encryption will become increasingly popular as method for managing encrypted file systems although efficiently computable the wide spread adoption of bbs re encryption has been hindered by considerable security risks following recent work of dodis and ivan we present new re encryption schemes that realize stronger notion of security and demonstrate the usefulness of proxy re encryption as method of adding access control to secure file system performance measurements of our experimental file system demonstrate that proxy re encryption can work effectively in practice
advances in biological experiments such as dna microarrays have produced large multidimensional data sets for examination and retrospective analysis scientists however heavily rely on existing biomedical knowledge in order to fully analyze and comprehend such datasets our proposed framework relies on the gene ontology for integrating priori biomedical knowledge into traditional data analysis approaches we explore the impact of considering each aspect of the gene ontology individually for quantifying the biological relatedness between gene products we discuss two figure of merit scores for quantifying the pair wise biological relatedness between gene products and the intra cluster biological coherency of groups of gene products finally we perform cluster deterioration simulation experiments on well scrutinized saccharomyces cerevisiae data set consisting of hybridization measurements the results presented illustrate strong correlation between the devised cluster coherency figure of merit and the randomization of cluster membership
modern business process management expands to cover the partner organisations business processes across organisational boundaries and thereby supports organisations to coordinate the flow of information among organisations and link their business processes with collaborative business processes organisations can create dynamic and flexible collaborations to synergically adapt to the changing conditions and stay competitive in the global market due to its significant potential and value collaborative business processes are now turning to be an important issue of contemporary business process management and attracts lots of attention and efforts from both academic and industry sides in this paper we review the development of bb collaboration and collaborative business processes provide an overview of related issues in managing collaborative business processes and discuss some emerging technologies and their relationships to collaborative business processes finally we introduce the papers that are published in this special issue
we apply an extension of the nelson oppen combination method to develop decision procedure for the non disjoint union of theories modeling data structures with counting operator and fragments of arithmetic we present some data structures and some fragments of arithmetic for which the combination method is complete and effective to achieve effectiveness the combination method relies on particular procedures to compute sets that are representative of all the consequences over the shared theory we show how to compute these sets by using superposition calculus for the theories of the considered data structures and various solving and reduction techniques for the fragments of arithmetic we are interested in including gauss elimination fourier motzkin elimination and groebner bases computation
we present the first to our knowledge approximation algorithm for tensor clustering powerful generalization to basic clustering tensors are increasingly common in modern applications dealing with complex heterogeneous data and clustering them is fundamental tool for data analysis and pattern discovery akin to their cousins common tensor clustering formulations are np hard to optimize but unlike the case no approximation algorithms seem to be known we address this imbalance and build on recent co clustering work to derive tensor clustering algorithm with approximation guarantees allowing metrics and divergences eg bregman as objective functions therewith we answer two open questions by anagnostopoulos et al our analysis yields constant approximation factor independent of data size worst case example shows this factor to be tight for euclidean co clustering however empirically the approximation factor is observed to be conservative so our method can also be used in practice
many multicast overlay networks maintain application specific performance goals by dynamically adapting the overlay structure when the monitored performance becomes inadequate this adaptation results in an unstructured overlay where no neighbor selection constraints are imposed although such networks provide resilience to benign failures they are susceptible to attacks conducted by adversaries that compromise overlay nodes previous defense solutions proposed to address attacks against overlay networks rely on strong organizational constraints and are not effective for unstructured overlays in this work we identify demonstrate and mitigate insider attacks against measurement based adaptation mechanisms in unstructured multicast overlay networks we propose techniques to decrease the number of incorrect adaptations by using outlier detection and limit the impact of malicious nodes by aggregating local information to derive global reputation for each node we demonstrate the attacks and mitigation techniques through real life deployments of mature overlay multicast system
due to the rapid development in mobile communication technologies the usage of mobile devices such as cell phone or pda has increased significantly as different devices require different applications various new services are being developed to satisfy the needs one of the popular services under heavy demand is the location based service lbs that exploits the spatial information of moving objects per temporal changes in order to support lbs well in this paper we investigate how spatio temporal information of moving objects can be efficiently stored and indexed in particular we propose novel location encoding method based on hierarchical administrative district information our proposal is different from conventional approaches where moving objects are often expressed as geometric points in two dimensional space instead in ours moving objects are encoded as one dimensional points by both administrative district as well as road information our method becomes especially useful for monitoring traffic situation or tracing location of moving objects through approximate spatial queries
as current trends in software development move toward more complex object oriented programming inlining has become vital optimization that provides substantial performance improvements to and java programs yet the aggressiveness of the inlining algorithm must be carefully monitored to effectively balance performance and code size the state of the art is to use profile information associated with call edges to guide inlining decisions in the presence of virtual method calls profile information for one call edge may not be sufficient for making effectual inlining decisions therefore we explore the use of profiling data with additional levels of context sensitivity in addition to exploring fixed levels of context sensitivity we explore several adaptive schemes that attempt to find the ideal degree of context sensitivity for each call site our techniques are evaluated on the basis of runtime performance code size and dynamic compilation time on average we found that with minimal impact on performance context sensitivity can enable reductions in compiled code space and compile time performance on individual programs varied from minus to while reductions in compile time and code space of up to and respectively were obtained
suffix trees are by far the most important data structure in stringology with myriads of applications in fields like bioinformatics and information retrieval classical representations of suffix trees require log bits of space for string of size this is considerably more than the log bits needed for the string itself where is the alphabet size the size of suffix trees has been barrier to their wider adoption in practice recent compressed suffix tree representations require just the space of the compressed string plus extra bits this is already spectacular but still unsatisfactory when is small as in dna sequences in this paper we introduce the first compressed suffix tree representation that breaks this linear space barrier our representation requires sublinear extra space and supports large set of navigational operations in logarithmic time an essential ingredient of our representation is the lowest common ancestor lca query we reveal important connections between lca queries and suffix tree navigation
approximation algorithms for clustering points in metric spaces is flourishing area of research with much research effort spent on getting better understanding of the approximation guarantees possible for many objective functions such as median means and min sum clustering this quest for better approximation algorithms is further fueled by the implicit hope that these better approximations also yield more accurate clusterings eg for many problems such as clustering proteins by function or clustering images by subject there is some unknown correct target clustering and the implicit hope is that approximately optimizing these objective functions will in fact produce clustering that is close pointwise to the truth in this paper we show that if we make this implicit assumption explicit that is if we assume that any approximation to the given clustering objective phi is epsilon close to the target then we can produce clusterings that are epsilon close to the target even for values for which obtaining approximation is np hard in particular for median and means objectives we show that we can achieve this guarantee for any constant and for the min sum objective we can do this for any constant our results also highlight surprising conceptual difference between assuming that the optimal solution to say the median objective is epsilon close to the target and assuming that any approximately optimal solution is epsilon close to the target even for approximation factor say in the former case the problem of finding solution that is epsilon close to the target remains computationally hard and yet for the latter we have an efficient algorithm
decoupled software pipelining dswp is one approach to automatically extract threads from loops it partitions loops into long running threads that communicate in pipelined manner via inter core queues this work recognizes that dswp can also be an enabling transformation for other loop parallelization techniques this use of dswp called dswp splits loop into new loops with dependence patterns amenable to parallelization using techniques that were originally either inapplicable or poorly performing by parallelizing each stage of the dswp pipeline using potentially different techniques not only is the benefit of dswp increased but the applicability and performance of other parallelization techniques are enhanced this paper evaluates dswp as an enabling framework for other transformations by applying it in conjunction with doall localwrite and specdoall to individual stages of the pipeline this paper demonstrates significant performance gains on commodity core multicore machine running variety of codes transformed with dswp
insufficiency of labeled training data is major obstacle for automatic video annotation semi supervised learning is an effective approach to this problem by leveraging large amount of unlabeled data however existing semi supervised learning algorithms have not demonstrated promising results in large scale video annotation due to several difficulties such as large variation of video content and intractable computational cost in this paper we propose novel semi supervised learning algorithm named semi supervised kernel density estimation sskde which is developed based on kernel density estimation kde approach while only labeled data are utilized in classical kde in sskde both labeled and unlabeled data are leveraged to estimate class conditional probability densities based on an extended form of kde it is non parametric method and it thus naturally avoids the model assumption problem that exists in many parametric semi supervised methods meanwhile it can be implemented with an efficient iterative solution process so this method is appropriate for video annotation furthermore motivated by existing adaptive kde approach we propose an improved algorithm named semi supervised adaptive kernel density estimation ssakde it employs local adaptive kernels rather than fixed kernel such that broader kernels can be applied in the regions with low density in this way more accurate density estimates can be obtained extensive experiments have demonstrated the effectiveness of the proposed methods
acquiring models of intricate objects like tree branches bicycles and insects is challenging task due to severe self occlusions repeated thin structures and surface discontinuities in theory shape from silhouettes sfs approach can overcome these difficulties and reconstruct visual hulls that are close to the actual shapes regardless of the complexity of the object in practice however sfs is highly sensitive to errors in silhouette contours and the calibration of the imaging system and has therefore not been used for obtaining accurate shapes with large number of views in this work we present practical approach to sfs using novel technique called coplanar shadowgram imaging that allows us to use dozens to even hundreds of views for visual hull reconstruction point light source is moved around an object and the shadows silhouettes cast onto single background plane are imaged we characterize this imaging system in terms of image projection reconstruction ambiguity epipolar geometry and shape and source recovery the coplanarity of the shadowgrams yields unique geometric properties that are not possible in traditional multi view camera based imaging systems these properties allow us to derive robust and automatic algorithm to recover the visual hull of an object and the positions of the light source simultaneously regardless of the complexity of the object we demonstrate the acquisition of several intricate shapes with severe occlusions and thin structures using to views
the unix fast file system ffs is probably the most widely used file system for performance comparisons however such comparisons frequently overlook many of the performance enhancements that have been added over the past decade in this paper we explore the two most commonly used approaches for improving the performance of meta data operations and recovery journaling and soft updates journaling systems use an auxiliary log to record meta data operations and soft updates uses ordered writes to ensure metadata consistency the commercial sector has moved en masse to journaling file systems as evidenced by their presence on nearly every server platform available today solaris aix digital unix hp ux irix and windows nt on all but solaris the default file system uses journaling in the meantime soft updates holds the promise of providing stronger reliability guarantees than journaling with faster recovery and superior performance in certain boundary cases in this paper we explore the benefits of soft updates and journaling comparing their behavior on both microbenchmarks and workload based macrobenchmarks we find that journaling alone is not sufficient to solve the meta data update problem if synchronous semantics are required ie meta data operations are durable once the system call returns then the journaling systems cannot realize their full potential only when this synchronicity requirement is relaxed can journaling systems approach the performance of systems like soft updates which also relaxes this requirement our asynchronous journaling and soft updates systems perform comparably in most cases while soft updates excels in some meta data intensive microbenchmarks the macrobenchmark results are more ambiguous in three cases soft updates and journaling are comparable in file intensive news workload journaling prevails and in small isp workload soft updates prevails
currently there are no known explicit algorithms for the great majority of graph problems in the dynamic distributed message passing model instead most state of the art dynamic distributed algorithms are constructed by composing static algorithm for the problem at hand with simulation technique that converts static algorithms to dynamic ones we argue that this powerful methodology does not provide satisfactory solutions for many important dynamic distributed problems and this necessitates developing algorithms for these problems from scratch in this paper we develop fully dynamic distributed algorithm for maintaining sparse spanners our algorithm improves drastically the quiescence time of the state of the art algorithm for the problem moreover we show that the quiescence time of our algorithm is optimal up to small constant factor in addition our algorithm improves significantly upon the state of the art algorithm in all efficiency parameters specifically it has smaller quiescence message and space complexities and smaller local processing time finally our algorithm is self contained and fairly simple and is consequently amenable to implementation on unsophisticated network devices
modeling spatiotemporal data in particular fuzzy and complex spatial objects representing geographic entities and relations is topic of great importance in geographic information systems computer vision environmental data management systems etc because of complex requirements it is challenging to represent spatiotemporal data and its features in databases and to effectively query them this article presents new approach to model and query the spatiotemporal data of fuzzy spatial and complex objects and or spatial relations in our case study we use meteorological database application in an intelligent database architecture which combines an object oriented database with knowledgebase for modeling and querying spatiotemporal objects
we present synchroscalar tile based architecture forembedded processing that is designed to provide the flexibilityof dsps while approaching the power efficiency ofasics we achieve this goal by providing high parallelismand voltage scaling while minimizing control and communicationcosts specifically synchroscalar uses columnsof processor tiles organized into statically assignedfrequency voltage domains to minimize power consumptionfurthermore while columns use simd control to minimizeoverhead data dependent computations can besupported by extremely flexible statically scheduled communicationbetween columnswe provide detailed evaluation of synchroscalar includingspice simulation wire and device models synthesisof key components cycle level simulation andcompiler and hand optimized signal processing applicationswe find that the goal of meeting not exceeding performancetargets with data parallel applications leads todesigns that depart significantly from our intuitions derivedfrom general purpose microprocessor design inparticular synchronous design and substantial global interconnectare desirable in the low frequency low powerdomain this global interconnect supports parallelizationand reduces processor idle time which are critical to energyefficient implementations of high bandwidth signalprocessing overall synchroscalar provides programmabilitywhile achieving power efficiencies within ofknown asic implementations which is better thanconventional dsps in addition frequency voltage scalingin synchroscalar provides between power savingsin our application suite
adaptive object models aom are sophisticated way of building object oriented systems that let non programmers customize the behavior of the system and that are most useful for businesses that are rapidly changing although systems based on an aom are often much smaller than competitors they can be difficult to build and to learn we believe that the problems with aom are due in part to mismatch between their design and the languages that are used to build them this paper describes how to avoid this mismatch by using implicit and explicit metaclasses
this paper proposes using user level memory thread ulmt for correlation prefetching in this approach user thread runs on general purpose processor in main memory either in the memory controller chip or in dram chip the thread performs correlation prefetching in software sending the prefetched data into the cache of the main processor this approach requires minimal hardware beyond the memory processor the correlation table is software data structure that resides in main memory while the main processor only needs few modifications to its cache so that it can accept incoming prefetches in addition the approach has wide applicability as it can effectively prefetch even for irregular applications finally it is very flexible as the prefetching algorithm can be customized by the user on an application basis our simulation results show that through new design of the correlation table and prefetching algorithm our scheme delivers good results specifically nine mostly irregular applications show an average speedup of furthermore our scheme works well in combination with conventional processor side sequential prefetcher in which case the average speedup increases to finally by exploiting the customization of the prefetching algorithm we increase the average speedup to
this paper describes enhanced subquery optimizations in oracle relational database system it discusses several techniques subquery coalescing subquery removal using window functions and view elimination for group by queries these techniques recognize and remove redundancies in query structures and convert queries into potentially more optimal forms the paper also discusses novel parallel execution techniques which have general applicability and are used to improve the scalability of queries that have undergone some of these transformations it describes new variant of antijoin for optimizing subqueries involved in the universal quantifier with columns that may have nulls it then presents performance results of these optimizations which show significant execution time improvements
the replica placement problem rpp aims at creating set of duplicated data objects across the nodes of distributed system in order to optimize certain criteria typically rpp formulations fall into two categories static and dynamic the first assumes that access statistics are estimated in advance and remain static and therefore one time replica distribution is sufficient irpp in contrast dynamic methods change the replicas in the network potentially upon every request this paper proposes an alternative technique named continuous replica placement problem crpp which falls between the two extreme approaches crpp can be defined as given an already implemented replication scheme and estimated access statistics for the next time period define new replication scheme subject to optimization criteria and constraints as we show in the problem formulation crpp is different in that the existing heuristics in the literature cannot be used either statically or dynamically to solve the problem in fact even with the most careful design their performance will be inferior since crpp embeds scheduling problem to facilitate the proposed mechanism we provide insight on the intricacies of crpp and propose various heuristics
software classification models have been regarded as an essential support tool in performing measurement and analysis processes most of the established models are single cycled in the model usage stage and thus require the measurement data of all the model's variables to be simultaneously collected and utilized for classifying an unseen case within only single decision cycle conversely the multi cycled model allows the measurement data of all the model's variables to be gradually collected and utilized for such classification within more than one decision cycle and thus intuitively seems to have better classification efficiency but poorer classification accuracy software project managers often have difficulties in choosing an appropriate classification model that is better suited to their specific environments and needs however this important topic is not adequately explored in software measurement and analysis literature by using an industrial software measurement dataset of nasa kc this paper explores the quantitative performance comparisons of the classification accuracy and efficiency of the discriminant analysis da and logistic regression lr based single cycled models and the decision tree dt based and echaid algorithms multi cycled models the experimental results suggest that the re appraisal cost of the type mr the software failure cost of type ii mr and the data collection cost of software measurements should be considered simultaneously when choosing an appropriate classification model
cyber physical systems increasingly rely on dynamically adaptive programs to respond to changes in their physical environment examples include ecosystem monitoring and disaster relief systems these systems are considered high assurance since errors during execution could result in injury loss of life environmental impact and or financial loss in order to facilitate the development and verification of dynamically adaptive systems we separate functional concerns from adaptive concerns specifically we model dynamically adaptive program as collection of non adaptive steady state programs and set of adaptations that realize transitions among steady state programs in response to environmental changes we use linear temporal logic ltl to specify properties of the non adaptive portions of the system and we use ltl an adapt operator extension toltl to concisely specify properties that hold during the adaptation process model checking offers an attractive approach to automatically analyzing models for adherence to formal properties and thus providing assurance however currently model checkers are unable to verify properties specified using ltl moreover as the number of steady state programs and adaptations increase the verification costs in terms of space and time potentially become unwieldy to address these issues we propose modular model checking approach to verifying that formal model of an adaptive program satisfies its requirements specified in ltl and ltl respectively
as the size of an rfid tag becomes smaller and the price of the tag gets lower rfid technology has been applied to wide range of areas recently rfid has been adopted in the business area such as supply chain management since companies can get movement information for products easily using the rfid technology it is expected to revolutionize supply chain management however the amount of rfid data in supply chain management is huge therefore it requires much time to extract valuable information from rfid data for supply chain management in this paper we define query templates for tracking queries and path oriented queries to analyze the supply chain we then propose an effective path encoding scheme to encode the flow information for products to retrieve the time information for products efficiently we utilize numbering scheme used in the xml area based on the path encoding scheme and the numbering scheme we devise storage scheme to process tracking queries and path oriented queries efficiently finally we propose method which translates the queries to sql queries experimental results show that our approach can process the queries efficiently on the average our approach is about times better than recent technique in terms of query performance
we review two areas of recent research linking proportional fairness with product form networks the areas concern respectively the heavy traffic and the large deviations limiting regimes for the stationary distribution of flow model where the flow model is stochastic process representing the randomly varying number of document transfers present in network sharing capacity according to the proportional fairness criterion in these two regimes we postulate the limiting form of the stationary distribution by comparison with several variants of the fairness criterion we outline how product form results can help provide insight into the performance consequences of resource pooling
large scale supercomputing is revolutionizing the way science is conducted growing challenge however is understanding the massive quantities of data produced by large scale simulations the data typically time varying multivariate and volumetric can occupy from hundreds of gigabytes to several terabytes of storage space transferring and processing volume data of such sizes is prohibitively expensive and resource intensive although it may not be possible to entirely alleviate these problems data compression should be considered as part of viable solution especially when the primary means of data analysis is volume rendering in this paper we present our study of multivariate compression which exploits correlations among related variables for volume rendering two configurations for multidimensional compression based on vector quantization are examined we emphasize quality reconstruction and interactive rendering which leads us to solution using graphics hardware to perform on the fly decompression during rendering
abstract in this paper we extend the standard for object oriented databases odmg with reactive features by proposing language for specifying triggers and defining its semantics this extension has several implications thus this work makes three different specific contributions first the definition of declarative data manipulation language for odmg which is missing in the current version of the standard such definition requires revisiting data manipulation in odmg and also addressing issues related to set oriented versus instance oriented computation then the definition of trigger language for odmg unifying also the sql proposal and providing support for trigger inheritance and overriding finally the development of formal semantics for the proposed data manipulation and trigger languages
most future large scale sensor networks are expected to follow two tier architecture which consists of resource rich master nodes at the upper tier and resource poor sensor nodes at the lower tier sensor nodes submit data to nearby master nodes which then answer the queries from the network owner on behalf of sensor nodes relying on master nodes for data storage and query processing raises severe concerns about data confidentiality and query result correctness when the sensor network is deployed in hostile environments in particular compromised master node may leak hosted sensitive data to the adversary it may also return juggled or incomplete query results to the network owner this paper for the first time in the literature presents suite of novel schemes to secure multidimensional range queries in tiered sensor networks the proposed schemes can ensure data confidentiality against master nodes and also enable the network owner to verify with very high probability the authenticity and completeness of any query result by inspecting the spatial and temporal relationships among the returned data detailed performance evaluations confirm the high efficacy and efficiency of the proposed schemes
applications ranging from location based services to multi player online gaming require continuous query support to monitor track and detect events of interest among sets of moving objects examples are alerting capabilities for detecting whether the distance the travel cost or the travel time among set of moving objects exceeds threshold these types of queries are driven by continuous streams of location updates simultaneously evaluated over many queries in this paper we define three types of proximity relations that induce location constraints to model continuous spatio temporal queries among sets of moving objects in road networks our focus lies on evaluating large number of continuous queries simultaneously we introduce novel moving object indexing technique that together with novel road network partitioning scheme restricts computations within the partial road network these techniques reduce query processing overhead by more than experiments over real world data sets show that our approach is twenty times faster than baseline algorithm
wireless sensor networks produce large amount of data that needs to be processed delivered and assessed according to the application objectives the way these data are manipulated by the sensor nodes is fundamental issue information fusion arises as response to process data gathered by sensor nodes and benefits from their processing capability by exploiting the synergy among the available data information fusion techniques can reduce the amount of data traffic filter noisy measurements and make predictions and inferences about monitored entity in this work we survey the current state of the art of information fusion by presenting the known methods algorithms architectures and models of information fusion and discuss their applicability in the context of wireless sensor networks
the technique of latent semantic indexing is used in wide variety of commercial applications in these applications the processing time and ram required for svd computation and the processing time and ram required during lsi retrieval operations are all roughly linear in the number of dimensions chosen for the lsi representation space in large scale commercial lsi applications reducing values could be of significant value in reducing server costs this paper explores the effects of varying dimensionality the approach taken here focuses on term comparisons pairs of terms are considered which have strong real world associations the proximities of members of these pairs in the lsi space are compared at multiple values of the testing is carried out for collections of from one to five million documents for the five million document collection value of provides the best performance the results suggest that there is something of an island of stability in the to range the results also indicate that there is relatively little room to employ values outside of this range without incurring significant distortions in at least some term term correlations
traditional content based image retrieval systems typically compute single descriptor per image based for example on color histograms the result of query is in general the images from the database whose descriptors are the closest to the descriptor of the query image systems built this way are able to return images that are globally similar to the query image but can not return images that contain some of the objects that are in the query as opposed to this traditional coarse grain recognition scheme recent advances in image processing make fine grain image recognition possible notably by computing local descriptors that can detect similar objects in different images obviously powerful fine grain recognition in images also changes the retrieval process instead of submitting single query to retrieve similar images multiple queries must be submitted and their partial results must be post processed before delivering the answer this paper first presents family of local descriptors supporting fine grain image recognition these descriptors enforce robust recognition despite image rotations and translations illumination variations and partial occlusions many multi dimensional indexes have been proposed to speed up the retrieval process these indexes however have been mostly designed for and evaluated against databases where each image is described by single descriptor while this paper does not present any new indexing scheme it shows that the three most efficient indexing techniques known today are still too slow to be used in practice with local descriptors because of the changes in the retrieval process
the advance of the web has significantly and rapidly changed the way of information organization sharing and distribution the next generation of the web the semantic web seeks to make information more usable by machines by introducing more rigorous structure based on ontologies in this context we try to propose novel and integrated approach for semi automated extraction of ontology based semantic web from data intensive web application and thus make the web content machine understandable our approach is based on the idea that semantics can be extracted by applying reverse engineering technique on the structures and the instances of html forms which are the most convenient interface to communicate with relational databases on the current data intensive web application this semantics is exploited to produce over several steps personalised ontology
we consider the problem of flow coordination in distributed multimedia applications most transport level protocols are designed to operate independently and lack mechanisms for sharing information with other flows and coordinating data transport in various ways this limitation becomes problematic in distributed applications that employ numerous flows between two computing clusters sharing the same intermediary forwarding path across the internet in this article we propose an open architecture that supports the sharing of network state information peer flow information and application specific information called simply the coordination protocol cp the scheme facilitates coordination of network resource usage across flows belonging to the same application as well as aiding other types of coordination the effectiveness of our approach is illustrated in the context of multistreaming in tele immersion where consistency of network information across flows both greatly improves frame transport synchrony and minimizes buffering delay
nodes in the hexagonal mesh and torus network are placed at the vertices of regular triangular tessellation so that each node has up to six neighbors the routing algorithm for the hexagonal torus is very complicated and it is an open problem by now hexagonal mesh and torus are known to belong to the class of cayley digraphs in this paper we use cayley formulations for the hexagonal torus along with some result on subgraphs and coset graphs to develop the optimal routing algorithm for the hexagonal torus and then we draw conclusions to the network diameter of the hexagonal torus
deformable isosurfaces implemented with level set methods have demonstrated great potential in visualization and computer graphics for applications such as segmentation surface processing and physically based modeling their usefulness has been limited however by their high computational cost and reliance on signi pound cant parameter tuning this paper presents solution to these challenges by describing graphics processor gpu based algorithms for solving and visualizing level set solutions at interactive rates the proposed solution is based on new streaming implementation of the narrow band algorithm the new algorithm packs the level set isosurface data into texture memory via multi dimensional virtual memory system as the level set moves this texture based representation is dynamically updated via novel gpu to cpu message passing scheme by integrating the level set solver with real time volume renderer user can visualize and intuitively steer the level set surface as it evolves we demonstrate the capabilities of this technology for interactive volume segmentation and visualization
usage data captured and logged by computers has long been an essential source of information for software developers support services personnel usability designers and learning researchers whether from mainframes file servers network devices or workstations the user event data logged in its many forms has served as an essential source of information for those who need to improve software analyze problems monitor security track workflow report on resource usage evaluate learning activities etc with today's generation of open and community source web based frameworks however new challenges arise as to how where and when user activity gets captured and analyzed these frameworks flexibility in allowing easy integration of different applications presentation technologies middleware and data sources has side effects on usage data fragmented logs in wide range of formats often bestrewn across many locations this paper focuses on common issues faced especially by academic computing support personnel who need to gather and analyze user activity information within heterogeneous distributed open source web frameworks like sakai and uportal as described in this paper these kinds of challenges can be met by drawing upon techniques for coordinated distributed event monitoring along with some basic data mining and data visualization approaches in particular this paper describes work in progress to develop an approach towards building distributed capture and analysis systems for large production deployment of the sakai collaboration and learning environment in order to meet wide range of tracking monitoring and reporting log analysis in one university setting
in the sponsored search model search engines are paid by businesses that are interested in displaying ads for their site alongside the search results businesses bid for keywords and their ad is displayed when the keyword is queried to the search engine an important problem in this process is keyword generation given business that is interested in launching campaign suggest keywords that are related to that campaign we address this problem by making use of the query logs of the search engine we identify queries related to campaign by exploiting the associations between queries and urls as they are captured by the user's clicks these queries form good keyword suggestions since they capture the wisdom of the crowd as to what is related to site we formulate the problem as semi supervised learning problem and propose algorithms within the markov random field model we perform experiments with real query logs and we demonstrate that our algorithms scale to large query logs and produce meaningful results
tracked objects rarely move alone they are often temporarily accompanied by other objects undergoing similar motion we propose novel tracking algorithm called sputnik tracker it is capable of identifying which image regions move coherently with the tracked object this information is used to stabilize tracking in the presence of occlusions or fluctuations in the appearance of the tracked object without the need to model its dynamics in addition sputnik tracker is based on novel template tracker integrating foreground and background appearance cues the time varying shape of the target is also estimated in each video frame together with the target position the time varying shape is used as another cue when estimating the target position in the next frame
in this position paper we discuss number of issues relating to model metrics with particular emphasis on metrics for uml models our discussion is presented as series of nine observations where we examine some of the existing work on applying metrics to uml models present some of our own work in this area and specify some topics for future research that we regard as important furthermore we identify three categories of challeges for model metrics and describe how our nine observations can be partitioned into these categories
relatively small set of static instructions has significant leverage on program execution performance these problem instructions contribute disproportionate number of cache misses and branch mispredictions because their behavior cannot be accurately anticipated using existing prefetching or branch prediction mechanisms the behavior of many problem instructions can be predicted by executing small code fragment called speculative slice if speculative slice is executed before the corresponding problem instructions are fetched then the problem instructions can move smoothly through the pipeline because the slice has tolerated the latency of the memory hierarchy for loads or the pipeline for branches this technique results in speedups up to percent over an aggressive baseline machine to benefit from branch predictions generated by speculative slices the predictions must be bound to specific dynamic branch instances we present technique that invalidates predictions when it can be determined by monitoring the program's execution path that they will not be used this enables the remaining predictions to be correctly correlated
for any outsourcing service privacy is major concern this paper focuses on outsourcing frequent itemset mining and examines the issue on how to protect privacy against the case where the attackers have precise knowledge on the supports of some items we propose new approach referred to as support anonymity to protect each sensitive item with other items of similar support to achieve support anonymity we introduce pseudo taxonomy tree and have the third party mine the generalized frequent itemsets under the corresponding generalized association rules instead of association rules the pseudo taxonomy is construct to facilitate hiding of the original items where each original item can map to either leaf node or an internal node in the taxonomy tree the rationale for this approach is that with taxonomy tree the nodes to satisfy the support anonymity may be any nodes in the taxonomy tree with the appropriate supports so this approach can provide more candidates for support anonymity with limited fake items as only the leaf nodes not the internal nodes of the taxonomy tree need to appear in the transactions otherwise for the association rule mining the nodes to satisfy the support anonymity have to correspond to the leaf nodes in the taxonomy tree this is far more restricted the challenge is thus on how to generate the pseudo taxonomy tree to facilitate support anonymity and to ensure the conservation of original frequent itemsets the experimental results showed that our methods of support anonymity can achieve very good privacy protection with moderate storage overhead
topical crawlers are increasingly seen as way to address the scalability limitations of universal search engines by distributing the crawling process across users queries or even client computers the context available to such crawlers can guide the navigation of links with the goal of efficiently locating highly relevant target pages we developed framework to fairly evaluate topical crawling algorithms under number of performance metrics such framework is employed here to evaluate different algorithms that have proven highly competitive among those proposed in the literature and in our own previous research in particular we focus on the tradeoff between exploration and exploitation of the cues available to crawler and on adaptive crawlers that use machine learning techniques to guide their search we find that the best performance is achieved by novel combination of explorative and exploitative bias and introduce an evolutionary crawler that surpasses the performance of the best nonadaptive crawler after sufficiently long crawls we also analyze the computational complexity of the various crawlers and discuss how performance and complexity scale with available resources evolutionary crawlers achieve high efficiency and scalability by distributing the work across concurrent agents resulting in the best performance cost ratio
this paper argues for set of requirements that an architectural style for self healing systems should satisfy adaptability dynamicity awareness autonomy robustness distributability mobility and traceability support for these requirements is discussed along five dimensions we have identified as distinguishing characteristics of architectural styles external structure topology rules behavior interaction and data flow as an illustration these requirements are used to assess an existing architectural style while this initial formulation of the requirements appears to have utility much further work remains to be done in order to apply it in evaluating and comparing architectural styles for self healing systems
leakage energy will be the major energy consumer in futuredeep sub micron designs especially the memory sub systemof future socs will be negatively affected by this trend inorder to reduce the leakage energy memory banks are transitionedto low energy state when possible this transitionitself costs some energy which is termed as the transition energyin this paper we present as the first approach of its kind novel energy saving replacement policy called lru seq forinstruction caches evaluation of the policy on various architecturesin system level environment has shown that upto energy savings can be obtained considering the negligiblehardware impact lru seq offers viable choice foran energy saving policy
in this work we propose method to reduce the impact of process variations by adapting the application's algorithm at the software layer we introduce the concept of hardware signatures as the measured post manufacturing hardware characteristics that can be used to drive software adaptation across different die using encoding as an example we demonstrate significant yield improvements as much as points at over design reduction in over design by as much as points at yield as well as application quality improvements about db increase in average psnr at yield further we investigate implications of limited information exchange ie signature measurement granularity on yield and quality we show that our proposed technique for determining optimal signature measurement points results in an improvement in psnr of about db over naive sampling for the encoder we conclude that hardware signature based application adaptation is an easy and inexpensive to implement better informed by actual application requirements and ffective way to manage yield cost quality tradeoffs in application implementation design flows
modern embedded consumer devices execute complex network and multimedia applications that require high performance and low energy consumption for implementing complex applications on network on chips nocs design methodology is needed for performing exploration at noc system level in order to select the optimal application specific noc architecture serving the application requirements in the best way the design methodology we present in this paper is based on the exploration of different noc characteristics and is supported by flexible noc simulator which provides the essential evaluation metrics in order to select the optimal communication parameters of the noc architectures we illustrated that it is possible with the evaluation metrics provided by the simulator we present to perform exploration of several noc aspects and select the optimal communication characteristics for noc platforms having network and multimedia applications as the target domains with our methodology we can achieve gain of in the energy times delay product on average
in order to manage service to meet the agreed upon sla it is important to design service of the required capacity and to monitor the service thereafter for violations at runtime this objective can be achieved by translating slos specified in the sla into lower level policies that can then be used for design and enforcement purposes such design and operational policies are often constraints on thresholds of lower level metrics in this paper we propose systematic and practical approach that combines fine grained performance modeling with regression analysis to translate service level objectives into design and operational policies for multi tier applications we demonstrate that our approach can handle both request based and session based workloads and deal with workload changes in terms of both request volume and transaction mix we validate our approach using both the rubis commerce benchmark and trace driven simulation of business critical enterprise application these results show the effectiveness of our approach
classical compiler optimizations assume fixed cache architecture and modify the program to take best advantage of it in some cases this may not be the best strategy because each nest might work best with different cache configuration and transforming nest for given fixed cache configuration may not be possible due to data and control dependences working with fixed cache configuration can also increase energy consumption in loops where the best required configuration is smaller than the default fixed one in this paper we take an alternate approach and modify the cache configuration for each nest depending on the access pattern exhibited by the nest we call this technique compiler directed cache polymorphism cdcp more specifically in this paper we make the following contributions first we present an approach for analyzing data reuse properties of loop nests second we give algorithms to simulate the footprints of array references in their reuse space third based on our reuse analysis we present an optimization algorithm to compute the cache configurations for each loop nest our experimental results show that cdcp is very effective in finding the near optimal data cache configurations for different nests in array intensive applications
it is envisaged that the application of the multilevel security mls scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on per user basis however as advances in this area are being made and ideas crystallized the concomitant weaknesses of the mls databases are also surfacing we insist that the critical problem with the current model is that the belief at higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported critics also argue that it is imperative for mls database users to theorize about the belief of others perhaps at different security levels an apparatus that is currently missing and the absence of which is seriously felt the impetus for our current research is this need to provide an adequate framework for belief reasoning in mls databases we demonstrate that prudent application of the concept of inheritance in deductive database setting will help capture the notion of declarative belief and belief reasoning in mls databases in an elegant way to this end we develop function to compute belief in multiple modes which can be used to reason about the beliefs of other users we strive to develop poised and practical logical characterization of mls databases for the first time based on the inherently difficult concept of non monotonic inheritance we present an extension of the acclaimed datalog language called the multilog and show that datalog is special case of our language we also suggest an implementation scheme for multilog as front end for coral
in this paper we propose formal analysis approach to estimate the expected average data cache access time of an application across all possible program inputs towards this goal we introduce the notion of probabilistic access history that intuitively summarizes the history of data memory accesses along different program paths to reach particular program point and their associated probabilities an efficient static program analysis technique has been developed to compute the access history at all program points we estimate the cache hit miss probabilities and hence the expected access time of each data memory reference from the access history our experimental evaluation confirms the accuracy and viability of the probabilistic data cache modeling approach
this paper presents new static type system for multithreaded programs well typed programs in our system are guaranteed to be free of data races and deadlocks our type system allows programmers to partition the locks into fixed number of equivalence classes and specify partial order among the equivalence classes the type checker then statically verifies that whenever thread holds more than one lock the thread acquires the locks in the descending orderour system also allows programmers to use recursive tree based data structures to describe the partial order for example programmers can specify that nodes in tree must be locked in the tree order our system allows mutations to the data structure that change the partial order at runtime the type checker statically verifies that the mutations do not introduce cycles in the partial order and that the changing of the partial order does not lead to deadlocks we do not know of any other sound static system for preventing deadlocks that allows changes to the partial order at runtimeour system uses variant of ownership types to prevent data races and deadlocks ownership types provide statically enforceable way of specifying object encapsulation ownership types are useful for preventing data races and deadlocks because the lock that protects an object can also protect its encapsulated objects this paper describes how to use our type system to statically enforce object encapsulation as well as prevent data races and deadlocks the paper also contains detailed discussion of different ownership type systems and the encapsulation guarantees they provide
distributed information brokering system dibs is peer to peer overlay network that comprises diverse data servers and brokering components helping client queries locate the data server many existing information brokering systems adopt server side access control deployment and honest assumptions on brokers however little attention has been drawn on privacy of data and metadata stored and exchanged within dibs in this paper we address privacy preserving information sharing via on demand information access we propose flexible and scalable system using broker coordinator overlay network through an innovative automaton segmentation scheme distributed access control enforcement and query segment encryption our system integrates security enforcement and query forwarding while preserving system wide privacy we present the automaton segmentation approach analyze privacy preservation in details and finally examine the end to end performance and scalability through experiments and analysis
we study the problem of finding in given word all maximal gapped palindromes verifying two types of constraints that we call long armed and length constrained palindromes for each of the two classes we propose an algorithm that runs in time for constant size alphabet where is the number of output palindromes both algorithms can be extended to compute biological gapped palindromes within the same time bound
many of the existing approaches in software comprehension focus on program structure or external documentation however by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked to understand software as whole we need to enrich software analysis with the developer knowledge hidden in the code naming this paper proposes the use of information retrieval to exploit linguistic information found in source code such as identifier names and comments we introduce semantic clustering technique based on latent semantic indexing and clustering to group source artifacts that use similar vocabulary we call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code we compare the topics to each other identify links between them provide automatically retrieved labels and use visualization to illustrate how they are distributed over the system our approach is language independent as it works at the level of identifier names to validate our approach we applied it on several case studies two of which we present in this paper note some of the visualizations presented make heavy use of colors please obtain color copy of the article for better understanding
existing work on software connectors shows significant disagreement on both their definition and their relationships with components coordinators and adaptors we propose precise characterisation of connectors discuss how they relate to the other three classes and contradict the suggestion that connectors and components are disjoint we discuss the relationship between connectors and coupling and argue the inseparability of connection models from component programming models finally we identify the class of configuration languages show how it relates to primitive connectors and outline relevant areas for future work
the area under the roc receiver operating characteristics curve or simply auc has been traditionally used in medical diagnosis since the it has recently been proposed as an alternative single number measure for evaluating the predictive ability of learning algorithms however no formal arguments were given as to why auc should be preferred over accuracy in this paper we establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that auc is better measure defined precisely than accuracy we then reevaluate well established claims in machine learning based on accuracy using auc and obtain interesting and surprising new results for example it has been well established and accepted that naive bayes and decision trees are very similar in predictive accuracy we show however that naive bayes is significantly better than decision trees in auc the conclusions drawn in this paper may make significant impact on machine learning and data mining applications
this paper presents search engine architecture retin aiming at retrieving complex categories in large image databases for indexing scheme based on two step quantization process is presented to compute visual codebooks the similarity between images is represented in kernel framework such similarity is combined with online learning strategies motivated by recent machine learning developments such as active learning additionally an offline supervised learning is embedded in the kernel framework offering real opportunity to learn semantic categories experiments with real scenario carried out from the corel photo database demonstrate the efficiency and the relevance of the retin strategy and its outstanding performances in comparison to up to date strategies
automated verification is one of the most successful applications of automated reasoning in computer science in automated verification one uses algorithmic techniques to establish the correctness of the design with respect to given property automated verification is based on small number of key algorithmic ideas tying together graph theory automata theory and logic in this self contained talk will describe how this holy trinity gave rise to automated verification tools and mention some applications to planning
many data warehouse systems have been developed recently yet data warehouse practice is not sufficiently sophisticated for practical usage most data warehouse systems have some limitations in terms of flexibility efficiency and scalability in particular the sizes of these data warehouses are forever growing and becoming overloaded with data scenario that leads to difficulties in data maintenance and data analysis this research focuses on data information integration between data cubes this research might contribute to the resolution of two concerns the problem of redundancy and the problem of data cubes independent information this work presents semantic cube model which extends object oriented technology to data warehouses and which enables users to design the generalization relationship between different cubes in this regard this work's objectives are to improve the performance of query integrity and to reduce data duplication in data warehouse to deal with the handling of increasing data volume in data warehouses we discovered important inter relationships that hold among data cubes that facilitate information integration and that prevent the loss of data semantics
the vision of future electronic marketplaces markets is that of markets being populated by autonomous intelligent entities software trading agents representing their users or owners and conducting business on their behalf for this vision to materialize one fundamental issue that needs to be addressed is that of trust first users need to be able to trust that the agents will do what they say they do second they need to be confident that their privacy is protected and that the security risks involved in entrusting agents to perform transactions on their behalf are minimized finally users need to be assured that any legal issues relating to agents trading electronically are fully covered as they are in traditional trading practices in this paper we consider the barriers for the adoption of agent technology in electronic commerce commerce which pertain to trust security and legal issues we discuss the perceived risks of the use of agents in commerce and the fundamental issue of trust in this context issues regarding security and how some of these can be addressed through the use of cryptography are described the impact of the use of agent technology on the users privacy and how it can be both protected as well as hindered by it is also examined finally we discuss the legal issues that arise in agent mediated commerce and discuss the idea of attributing to software agents the status of legal persons or persons and the various implications
wireless sensor networks have been proposed for multitude of location dependent applications for such systems the cost and limitations of the hardware on sensing nodes prevent the use of range based localization schemes that depend on absolute point to point distance estimates because coarse accuracy is sufficient for most sensor network applications solutions in range free localization are being pursued as cost effective alternative to more expensive range based approaches in this paper we present apit novel localization algorithm that is range free we show that our apit scheme performs best when an irregular radio pattern and random node placement are considered and low communication overhead is desired we compare our work via extensive simulation with three state of the art range free localization schemes to identify the preferable system configurations of each in addition we study the effect of location error on routing and tracking performance we show that routing performance and tracking accuracy are not significantly affected by localization error when the error is less than times the communication radio radius
xml data can be represented by tree or graph structure and xml query processing requires the information of structural relationships among nodes the basic structural relationships are parent child and ancestor descendant and finding all occurrences of these basic structural relationships in an xml data is clearly core operation in xml query processing several node labeling schemes have been suggested to support the determination of ancestor descendant or parent child structural relationships simply by comparing the labels of nodes however the previous node labeling schemes have some disadvantages such as large number of nodes that need to be relabeled in the case of an insertion of xml data huge space requirements for node labels and inefficient processing of structural joins in this paper we propose the nested tree structure that eliminates the disadvantages and takes advantage of the previous node labeling schemes the nested tree structure makes it possible to use the dynamic interval based labeling scheme which supports xml data updates with almost no node relabeling as well as efficient structural join processing experimental results show that our approach is efficient in handling updates with the interval based labeling scheme and also significantly improves the performance of the structural join processing compared with recent methods
component programming techniques encourage abstraction and reuse through external linking some parts of program however must use concrete internally specified references so pure component system is not sufficient mechanism for structuring programs we present the combination of static internally linked module system and purely abstractive component system the latter extends our previous model of typed units to properly account for translucency and sharing we also show how units and modules can express an sml style system of structures and functors and we explore the consequences for recursive structures and functors
in this paper we kernelize conventional clustering algorithms from novel point of view based on the fully mathematical proof we first demonstrate that kernel kmeans kkmeans is equivalent to kernel principal component analysis kpca prior to the conventional kmeans algorithm by using kpca as preprocessing step we also generalize gaussian mixture model gmm to its kernel version the kernel gmm kgmm consequently conventional clustering algorithms can be easily kernelized in the linear feature space instead of nonlinear one to evaluate the newly established kkmeans and kgmm algorithms we utilized them to the problem of semantic object extraction segmentation of color images based on series of experiments carried out on set of color images we indicate that both kkmeans and kgmm can offer more elaborate output than the conventional kmeans and gmm respectively
various index structures have been proposed to speed up the evaluation of xml path expressions however existing xml path indices suffer from at least one of three limitations they focus only on indexing the structure relying on separate index for node content they are useful only for simple path expressions such as root to leaf paths or they cannot be tightly integrated with relational query processor moreover there is no unified framework to compare these index structures in this paper we present framework defining family of index structures that includes most existing xml path indices we also propose two novel index structures in this family with different space time tradeoffs that are effective for the evaluation of xml branching path expressions ie twigs with value conditions we also show how this family of index structures can be implemented using the access methods of the underlying relational database system finally we present an experimental evaluation that shows the performance tradeoff between index space and matching time the experimental results show that our novel indices achieve orders of magnitude improvement in performance for evaluating twig queries albeit at higher space cost over the use of previously proposed xml path indices that can be tightly integrated with relational query processor
this paper addresses performance bottleneck in time series subsequence matching first we analyze the disk access and cpu processing times required during the index searching and post processing steps of subsequence matching through preliminary experiments based on their results we show that the post processing step is main performance bottleneck in subsequence matching in order to resolve the performance bottleneck we propose simple yet quite effective method that processes the post processing step by rearranging the order of candidate subsequences to be compared with query sequence our method completely eliminates the redundancies of disk accesses and cpu processing occurring in the post processing step we show that our method is optimal and also does not incur any false dismissal also we justify the effectiveness of our method by extensive experiments
software maintenance is expensive and difficult because soft ware is complex and maintenance requires the understanding of code written by someone else prerequisite to maintainability is program understanding specifically understanding the control flows between software components this is especially problematic for emerging software technologies such as the world wide web because of the lack of formal development practices and because web applications comprise mix of static and dynamic content adequate representations are therefore necessary to facilitate program understanding this research proposes an approach called readable readable executable augmentable database linked environment that generates executable tabular representations that can be used to both understand and manipulate software applications controlled laboratory experiment carried out to test the efficacy of the approach demonstrates that the representations significantly enhance program understanding the results suggest that the approach and the corresponding environment may be useful to alleviate problems associated with the software maintainability of new web applications
structured texts for example dictionaries and user manuals typically have heirarchical tree like structure we describe query language for retrieving information from collections of hierarchical text the language is based on tree pattern matching notion called tree inclusion tree inclusion allows easy expression of queries that use the structure and the content of the document in using it user need not be aware of the whole structure of the database thus language based on tree inclusion is data independent property made necessary because of the great variance in the structure of the texts
this paper presents an approach to matching parts of deformable shapes multiscale salient parts of the two shapes are first identified then these parts are matched if their immediate properties are similar the same holds recursively for their subparts and the same holds for their neighbor parts the shapes are represented by hierarchical attributed graphs whose node attributes encode the photometric and geometric properties of corresponding parts and edge attributes capture the strength of neighbor and part of interactions between the parts their matching is formulated as finding the subgraph isomorphism that minimizes quadratic cost the dimensionality of the matching space is dramatically reduced by convexifying the cost experimental evaluation on the benchmark mpeg and brown datasets demonstrates that the proposed approach is robust
tree automata completion is technique for the verification of infinite state systems it has already been used for the verification of cryptographic protocols and the prototyping of java static analyzers however as for many other verification techniques the correctness of the associated tool becomes more and more difficult to guarantee it is due to the size of the implementation that constantly grows and due to optimizations which are necessary to scale up the efficiency of the tool to verify real size systems in this paper we define and develop checker for tree automata produced by completion the checker is defined using coq and its implementation is automatically extracted from its formal specification using extraction gives checker that can be run independently of the coq environment specific algorithm for tree automata inclusion checking has been defined so as to avoid the exponential blow up the obtained checker is certified in coq independent of the implementation of completion usable with any approximation performed during completion small and fast some benchmarks are given to show how efficient the tool is
shape skeletons are fundamental concepts for describing the shape of geometric objects and have found variety of applications in number of areas where geometry plays an important role two types of skeletons commonly used in geometric computations are the straight skeleton of linear polygon and the medial axis of bounded set of points in the dimensional euclidean space however exact computation of these skeletons of even fairly simple planar shapes remains an open problem in this paper we propose novel approach to construct exact or approximate continuous distance functions and the associated skeletal representations skeleton and the corresponding radius function for solid semi analytic sets that can be either rigid or undergoing topological deformations our approach relies on computing constructive representations of shapes with functions that operate on real valued halfspaces as logic operations we use our approximate distance functions to define new type of skeleton ie the skeleton which is piecewise linear for polygonal domains generalizes naturally to planar and spatial domains with curved boundaries and has attractive properties we also show that the exact distance functions allow us to compute the medial axis of any closed bounded and regular planar domain importantly our approach can generate the medial axis the straight skeleton and the skeleton of possibly deformable shapes within the same formulation extends naturally to and can be used in variety of applications such as skeleton based shape editing and adaptive motion planning
this paper offers an exploration of the attitudes of older adults to keeping in touch with people who are important to them we present findings from three focus groups with people from to years of age themes emerging from the findings suggest that older adults view the act of keeping in touch as being worthy of time and dedication but also as being something that needs to be carefully managed within the context of daily life communication is seen as means through which skill should be demonstrated and personality expressed and is understood in very different context to the lightweight interaction that is increasingly afforded by new technologies the themes that emerged are used to elicit number of design implications and to promote some illustrative design concepts for new communication devices
software has spent the bounty of moore's law by solving harder problems and exploiting abstractions such as high level languages virtual machine technology binary rewriting and dynamic analysis abstractions make programmers more productive and programs more portable but usually slow them down since moore's law is now delivering multiple cores instead of faster processors future systems must either bear relatively higher cost for abstractions or use some cores to help tolerate abstraction costs this paper presents the design implementation and evaluation of novel concurrent configurable dynamic analysis framework that efficiently utilizes multicore cache architectures it introduces cache friendly asymmetric buffering cab lock free ring buffer that implements efficient communication between application and analysis threads we guide the design and implementation of our framework with model of dynamic analysis overheads the framework implements exhaustive and sampling event processing and is analysis neutral we evaluate the framework with five popular and diverse analyses and show performance improvements even for lightweight low overhead analyses efficient inter core communication is central to high performance parallel systems and we believe the cab design gives insight into the subtleties and difficulties of attaining it for dynamic analysis and other parallel software
recent work has shown that the physical connectivity of the internet exhibits small world behavior characterizing such behavior is important not only for generating realistic internet topology but also for the proper evaluation of large scale content delivery mechanisms along this line this paper tries to understand how small world behavior arises in the internet topologies and how it impacts the performance of multicast techniques first we attribute small world behavior to two possible causes namely the variability of vertex degree and the preference for local connections for vertices we have found that both factors contribute with different relative degrees to the small world behavior of autonomous system as level and router level internet topologies for as level topology we observe that high variability of vertex degree is sufficient to cause small world behavior but for router level topology preference for local connectivity plays more important role second we propose better models to generate small world internet topologies our models incorporate both causes of small world behavior and generate graphs closely resemble real internet graphs third using simulation we demonstrate the importance of our work by studying the scaling behavior of multicast techniques we show that multicast tree size largely depends on network topology if topology generators capture only the variability of vertex degree they are likely to underestimate the benefit of multicast techniques
there is currently an abundance of social network services available on the internet in addition examples of location aware social network services are emerging the use of such services presents interesting consequences for users privacy and behaviour and ultimately the adoption of such services yet not lot of explicit knowledge is available that addresses these issues the work presented here tries to answer this by investigating the willingness to use location aware service in population of students during three week festival the main findings show that most users are willing to use such systems also on larger scale however some reservations particularly with regard to privacy are uncovered
cache memories in embedded systems play an important role in reducing the execution time of the applications various kinds of extensions have been added to cache hardware to enable software involvement in replacement decisions thus improving the run time over purely hardware managed cache novel embedded systems like intel's xscale and arm cortex processors provide the facility of locking one or more lines in cache this feature is called cache locking this paper presents the first method in the literature for instruction cache locking that is able to reduce the average case run time of the program we devise cost benefit model to discover the memory addresses which should be locked in the cache we implement our scheme inside binary rewriter thus widening the applicability of our scheme to binaries compiled using any compiler results obtained on suite of mibench and mediabench benchmarks show up to improvement in the instruction cache miss rate on average and up to improvement in the execution time on average for applications having instruction accesses as bottleneck depending on the cache configuration the improvement in execution time is as high as for some benchmarks
regular path expressions are essential for formulating queries over the semistructured data without specifying the exact structure the query pruning is an important optimization technique to avoid useless traversals in evaluating regular path expressions while the previous query pruning optimizes single regular path expression well it often fails to fully optimize multiple regular path expressions nevertheless multiple regular path expressions are very frequently used in nontrivial queries and so an effective optimization technique for them is required in this paper we present new technique called the two phase query pruning that consists of the preprocessing phase and the pruning phase our two phase query pruning is effective in optimizing multiple regular path expressions and is more scalable and efficient than the combination of the previous query pruning and post processing in that it never deals with exponentially many combinations of sub results produced from all the regular path expressions
web search engines typically provide search results without considering user interests or context we propose personalized search approach that can easily extend conventional search engine on the client side our mapping framework automatically maps set of known user interests onto group of categories in the open directory project odp and takes advantage of manually edited data available in odp for training text classifiers that correspond to and therefore categorize and personalize search results according to user interests in two sets of controlled experiments we compare our personalized categorization system pcat with list interface system list that mimics typical search engine and with nonpersonalized categorization system cat in both experiments we analyze system performances on the basis of the type of task and query length we find that pcat is preferable to list for information gathering types of tasks and for searches with short queries and pcat outperforms cat in both information gathering and finding types of tasks and for searches associated with free form queries from the subjects answers to questionnaire we find that pcat is perceived as system that can find relevant web pages quicker and easier than list and cat
the discovery of biclusters which denote groups of items that show coherent values across subset of all the transactions in data set is an important type of analysis performed on real valued data sets in various domains such as biology several algorithms have been proposed to find different types of biclusters in such data sets however these algorithms are unable to search the space of all possible biclusters exhaustively pattern mining algorithms in association analysis also essentially produce biclusters as their result since the patterns consist of items that are supported by subset of all the transactions however major limitation of the numerous techniques developed in association analysis is that they are only able to analyze data sets with binary and or categorical variables and their application to real valued data sets often involves some lossy transformation such as discretization or binarization of the attributes in this paper we propose novel association analysis framework for exhaustively and efficiently mining range support patterns from such data set on one hand this framework reduces the loss of information incurred by the binarization and discretization based approaches and on the other it enables the exhaustive discovery of coherent biclusters we compared the performance of our framework with two standard biclustering algorithms through the evaluation of the similarity of the cellular functions of the genes constituting the patterns biclusters derived by these algorithms from microarray data these experiments show that the real valued patterns discovered by our framework are better enriched by small biologically interesting functional classes also through specific examples we demonstrate the ability of the rap framework to discover functionally enriched patterns that are not found by the commonly used biclustering algorithm isa the source code and data sets used in this paper as well as the supplementary material are available at http wwwcsumnedu vk gaurav rap
in building large scale video server it is highly desirable to use heterogeneous disk subsystems for the following reasons first existing disks may fail especially in an environment with large number of disks enforcing the use of new disks second for scalable server to cope with the increasing demand of customers new disks may be needed to increase the server apos storage capacity and throughput with rapid advances in the performance of disks the newly added disks generally have higher data transfer rate and larger storage capacity than the disks originally in the system in this paper we propose novel striping scheme termed as resource based striping rbs for video servers built on heterogeneous disks rbs combines the techniques of wide striping and narrow striping so that it can obtain the optimal stripe allocation and efficiently utilize both the bandwidth and storage capacity of all disks rbs is suitable for applications whose files are not updated frequently such as course on demand and movie on demand we examine the performance of rbs via simulation experiments our results show that rbs greatly outperforms the conventional striping schemes proposed for video servers with heterogeneous or homogeneous disks in terms of the number of simultaneous streams supported and the number of files that can be stored
this paper describes the moveme interaction prototype developed in conjunction with vlab in rotterdam moveme proposes scenario for social interaction and the notion of social intimacy interaction with sensory enhanced soft pliable tactile throw able cushions afford new approaches to pleasure movement and play somatics approach to touch and kinaesthesia provides an underlying design framework the technology developed for moveme uses the surface of the cushion as an intelligent tactile interface making use of movement analysis system called laban effort shape we have developed model that provides high level interpretation of varying qualities of touch and motion trajectory we describe the notion of social intimacy and how we model it through techniques in somatics and performance practice we describe the underlying concepts of moveme and its motivations we illustrate the structural layers of interaction and related technical detail finally we discuss the related body of work in the context of evaluating our approach and conclude with plans for future work
this paper proposes set of new software test diversity measures based on control oscillations of test suites oscillation diversity uses conversion inversion and phase transformation to vary test suite amplitudes frequencies and phases resistance and inductance are defined as measures of diversification difficulty the experimental results show correlation between some oscillation diversity measures and fault detection effectiveness
this paper presents techniques and tools to transform spreadsheets into relational databases and back set of data refinement rules is introduced to map tabular datatype into relational database schema having expressed the transformation of the two data models as data refinements we obtain for free the functions that migrate the data we use well known relational database techniques to optimize and query the data because data refinements define bi directional transformations we can map such database back to an optimized spreadsheet we have implemented the data refinement rules and we constructed haskell based tools to manipulate optimize and refactor excel like spreadsheets
we introduce new acceleration to the standard splatting volume rendering algorithm our method achieves full colour bit depth sorted and shaded volume rendering significantly faster than standard splatting the speedup is due to dimensional adjacency data structure that efficiently skips transparent parts of the data and stores only the voxels that are potentially visible our algorithm is robust and flexible allowing for depth sorting of the data including correct back to front ordering for perspective projections this makes interactive splatting possible for applications such as medical visualizations that rely on structure and depth information
in this paper new method to improve the utilization of main memory systems is presented the new method is based on prestoring in main memory number of query answers each evaluated out of single memory page to this end the ideas of page answers and page traces are formally described and their properties analyzed the query model used here allows for selection projection join recursive queries as well as arbitrary combinations we also show how to apply the approach under update traffic this concept is especially useful in managing the main memories of an important class of applications this class includes the evaluation of triggers and alerters performance improvement of rule based systems integrity constraint checking and materialized views these applications are characterized by the existence at compile time of predetermined set of queries by slow but persistent update traffic and by their need to repetitively reevaluate the query set the new approach represents new type of intelligent database caching which contrasts with traditional caching primarily in that the cache elements are derived data and as consequence they overlap arbitrarily and do not have fixed length the contents of the main memory cache are selected based on the data distribution within the database the set of fixed queries to preprocess and the paging characteristics page answers and page traces are used as the smallest indivisible units in the cache an efficient heuristic to select near optimal set of page answers and page traces to populate the main memory has been developed implemented and tested finally quantitative measurements of performance benefits are reported
all frequent itemset mining algorithms rely heavily on the monotonicity principle for pruning this principle allows for excluding candidate itemsets from the expensive counting phase in this paper we present sound and complete deduction rules to derive bounds on the support of an itemset based on these deduction rules we construct condensed representation of all frequent itemsets by removing those itemsets for which the support can be derived resulting in the so called non derivable itemsets ndi representation we also present connections between our proposal and recent other proposals for condensed representations of frequent itemsets experiments on real life datasets show the effectiveness of the ndi representation making the search for frequent non derivable itemsets useful and tractable alternative to mining all frequent itemsets
programming web applications in direct style with the help of continuations is much simpler safer modular and better performing technology than the current dominating page centric technology combining cgi scripts active pages or servlets this paper discusses the use of continuations in the context of web applications the problems they solve as well as some new problems they introduce
in degree pagerank number of visits and other measures of web page popularity significantly influence the ranking of search results by modern search engines the assumption is that popularity is closely correlated with quality more elusive concept that is difficult to measure directly unfortunately the correlation between popularity and quality is very weak for newly created pages that have yet to receive many visits and or in links worse since discovery of new content is largely done by querying search engines and because users usually focus their attention on the top few results newly created but high quality pages are effectively shut out and it can take very long time before they become popularwe propose simple and elegant solution to this problem the introduction of controlled amount of randomness into search result ranking methods doing so offers new pages chance to prove their worth although clearly using too much randomness will degrade result quality and annul any benefits achieved hence there is tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality we study this tradeoff both analytically and via simulation in the context of an economic objective function based on aggregate result quality amortized over time we show that modest amount of randomness leads to improved search results
the ever growing needs of large multimedia systems cannot be met by magnetic disks due to their high cost and low storage density consequently cheaper and denser tertiary storage systems are being integrated into the storage hierarchies of these applications although tertiary storage is cheaper the access latency is very high due to the need to load and unload media on the drives this high latency and the bursty nature of traffic result in the accumulation of requests for tertiary storage we study the problem of scheduling these requests to improve performance in particular we address the issues of scheduling across multiple tapes or disks as opposed to most other studies which consider only one or two media we focus on algorithms that minimize the number of switches and show through simulation that these result in near optimal schedules for single drive libraries an efficient algorithm that produces optimal schedules is developed for multiple drives the problem is shown to be np complete efficient and effective heuristics are presented for both single and multiple drives the scheduling policies developed achieve significant performance gains over naive policies the algorithms are simple to implement and are not restrictive the study encompasses all types of storage libraries handling removable media such as tapes and optical disks
botnets are networks of compromised computers infected with malicious code that can be controlled remotely under common command and control channel recognized as one the most serious security threats on current internet infrastructure advanced botnets are hidden not only in existing well known network applications eg irc http or peer to peer but also in some unknown or novel creative applications which makes the botnet detection challenging problem most current attempts for detecting botnets are to examine traffic content for bot signatures on selected network links or by setting up honeypots in this paper we propose new hierarchical framework to automatically discover botnets on large scale wifi isp network in which we first classify the network traffic into different application communities by using payload signatures and novel cross association clustering algorithm and then on each obtained application community we analyze the temporal frequent characteristics of flows that lead to the differentiation of malicious channels created by bots from normal traffic generated by human beings we evaluate our approach with about million flows collected over three consecutive days on large scale wifi isp network and results show the proposed approach successfully detects two types of botnet application flows ie blackenergy http bot and kaiten irc bot from about million flows with high detection rate and an acceptable low false alarm rate
in this paper we present multimodal approach for the recognition of eight emotions our approach integrates information from facial expressions body movement and gestures and speech we trained and tested model with bayesian classifier using multimodal corpus with eight emotions and ten subjects firstly individual classifiers were trained for each modality next data were fused at the feature level and the decision level fusing the multimodal data resulted in large increase in the recognition rates in comparison with the unimodal systems the multimodal approach gave an improvement of more than when compared to the most successful unimodal system further the fusion performed at the feature level provided better results than the one performed at the decision level
this study investigates level set multiphase image segmentation by kernel mapping and piecewise constant modeling of the image data thereof kernel function maps implicitly the original data into data of higher dimension so that the piecewise constant model becomes applicable this leads to flexible and effective alternative to complex modeling of the image data the method uses an active curve objective functional with two terms an original term which evaluates the deviation of the mapped image data within each segmentation region from the piecewise constant model and classic length regularization term for smooth region boundaries functional minimization is carried out by iterations of two consecutive steps minimization with respect to the segmentation by curve evolution via euler lagrange descent equations and minimization with respect to the regions parameters via fixed point iterations using common kernel function this step amounts to mean shift parameter update we verified the effectiveness of the method by quantitative and comparative performance evaluation over large number of experiments on synthetic images as well as experiments with variety of real images such as medical satellite and natural images as well as motion maps
in main memory databases the number of processor cache misses has critical impact on the performance of the system cache conscious indices are designed to improve performance by reducing the number of processor cache misses that are incurred during search operation conventional wisdom suggests that the index's node size should be equal to the cache line size in order to minimize the number of cache misses and improve performance as we show in this paper this design choice ignores additional effects such as the number of instructions executed and the number of tlb misses which play significant role in determining the overall performance to capture the impact of node size on the performance of cache conscious tree csb tree we first develop an analytical model based on the fundamental components of the search process this model is then validated with an actual implementation demonstrating that the model is accurate both the analytical model and experiments confirm that using node sizes much larger than the cache line size can result in better search performance for the csb tree
adaptive user interface composition is the ability of software system to compose its user interface at runtime according to given deployment profile and to possibly drop running components and activate better alternatives in their place in response to deployment profile modifications while adaptive behavior has gained interest for wide range of software products and services its support is very demanding requiring adoption of user interface architectural patterns from the early software design stages while previous research addressed the issue of engineering adaptive systems from scratch there is an important methodological gap since we lack processes to reform existing non adaptive systems towards adaptive behavior we present stepwise transformation process of user interface software by incrementally upgrading relevant class structures towards adaptive composition by treating adaptive behavior as cross cutting concern all our refactoring examples have emerged from real practice
with the recent dramatic increase in electronic access to documents text categorization mdash the task of assigning topics to given document mdash has moved to the center of the information sciences and knowledge management this article uses the structure that is present in the semantic space of topics in order to improve performance in text categorization according to their meaning topics can be grouped together into ldquo meta topics rdquo eg gold silver and copper are all metals the proposed architecture matches the hierarchical structure of the topic space as opposed to flat model that ignores the structure it accommodates both single and multiple topic assignments for each document its probabilistic interpretation allows its predictions to be combined in principled way with information from other sources the first level of the architecture predicts the probabilities of the meta topic groups this allows the individual models for each topic on the second level to focus on finer discriminations within the group evaluating the performance of two level implementation on the reuters testbed of newswire articles shows the most significant improvement for rare classes
unanticipated connection of independently developed componentsis one of the key issues in component oriented programming while variety of component oriented languages have been proposed none of them has achieved breakthrough yet in this paper we present scl simple language dedicated to component oriented programming scl integrates well known features such as component class component interface port or service all these well known features are presented discussed and compared to existing approaches because they vary quite widely from one language to another but these features are not enough to build component language indeed most approaches use language primitives and shared interfaces to connect components but shared interfaces are in contradiction with the philosophy of independently developed components to this issue scl provides new features such as uniform component composition model based on connectors connectors represent interactions between independently developed components scl also integrates component properties which enable connections based on component state changes with no requirements of specific code in components
we provide type system inspired by affine intuitionistic logic for the calculus of higher order mobile embedded resources homer resulting in the first process calculus combining affine linear non copyable and non linear copyable higher order mobile processes nested locations and local names the type system guarantees that linear resources are neither copied nor embedded in non linear resources during computation we exemplify the use of the calculus by modelling simplistic cash smart card system the security of which depends on the interplay between linear mobile hardware embedded non linear mobile processes and local names purely linear calculus would not be able to express that embedded software processes may be copied conversely purely non linear calculus would not be able to express that mobile hardware processes cannot be copied
cryptographic operations are essential for many security critical systems reasoning about information flow in such systems is challenging because typical noninterference based information flow definitions allow no flow from secret to public data unfortunately this implies that programs with encryption are ruled out because encrypted output depends on secret inputs the plaintext and the key however it is desirable to allow flows arising from encryption with secret keys provided that the underlying cryptographic algorithm is strong enough in this article we conservatively extend the noninterference definition to allow safe encryption decryption and key generation to illustrate the usefulness of this approach we propose and implement type system that guarantees noninterference for small imperative language with primitive cryptographic operations the type system prevents dangerous program behavior eg giving away secret key or confusing keys and nonkeys which we exemplify with secure implementations of cryptographic protocols because the model is based on standard noninterference property it allows us to develop some natural extensions in particular we consider public key cryptography and integrity which accommodate reasoning about primitives that are vulnerable to chosen ciphertext attacks
with the multiplication of xml data sources many xml data warehouse models have been proposed to handle data heterogeneity and complexity in way relational data warehouses fail to achieve however xml native database systems currently suffer from limited performances both in terms of manageable data volume and response time fragmentation helps address both these issues derived horizontal fragmentation is typically used in relational data warehouses and can definitely be adapted to the xml context however the number of fragments produced by classical algorithms is difficult to control in this paper we propose the use of means based fragmentation approach that allows to master the number of fragments through its parameter we experimentally compare its efficiency to classical derived horizontal fragmentation algorithms adapted to xml data warehouses and show its superiority
with over us science and mathematics education standards and rapid proliferation of web enabled curriculum retrieving curriculum that aligns with the standards to which teachers must teach is key objective for educational digital libraries however previous studies of such alignment use single dimensional and binary measures of the alignment concept as consequence they suffer from low inter rater reliability irr with experts agreeing about alignments only some of the time we present the results of an experiment in which the alignment variable was operationalized using the saracevic model of relevance clues taken from the everyday practice of teaching results show high irr across all clues with irr on several specific alignment dimensions significantly higher than on overall alignment in addition model of overall alignment is derived and estimated the structure and explanatory power of the model as well as the relationships between alignment clues differ significantly between alignments of curriculum found by users themselves and curriculum found by others these results illustrate the usefulness of clue based relevance measures for information retrieval and have important consequences for both the formulation of automated retrieval mechanisms and the construction of gold standard or benchmark set of standard curriculum alignments
open distributed systems are becoming increasingly popular such systems include components that may be obtained from number of different sources for example java allows run time loading of software components residing on remote machines one unfortunate side effect of this openness is the possibility that hostile software components may compromise the security of both the program and the system on which it runs java offers built in security mechanism using which programmers can give permissions to distributed components and check these permissions at run time this security model is flexible but using it is not straightforward which may lead to insufficiently tight permission checking and therefore breaches of securityin this paper we propose data flow algorithm for automated analysis of the flow of permissions in java programs our algorithm produces for given instruction in the program set of permissions that are checked on all possible executions up to this instruction this information can be used in program understanding tools or directly for checking properties that assert what permissions must always be checked before access to certain functionality is allowed the worst case complexity of our algorithm is low order polynomial in the number of program statements and permission types while comparable previous approaches have exponential costs
in this paper we present microsearch search system suitable for small devices used in ubiquitous computing environments akin to desktop search engine microsearch indexes the information inside small device and accurately resolves user queries given the very limited hardware resources conventional search engine designs and algorithms cannot be used we adopt information retrieval techniques for query resolution and propose space efficient algorithm to perform top query on limited hardware resources finally we present theoretical model of microsearch to better understand the tradeoffs in system design parameters by implementing microsearch on actual hardware for evaluation we demonstrate the feasibility of scaling down information retrieval systems onto very small devices
data cube construction has been the focus of much research due to its importance in improving efficiency of olap significant fraction of this work has been on rolap techniques which are based on relational technology existing rolap cubing solutions mainly focus on flat datasets which do not include hierarchies in their dimensions nevertheless the nature of hierarchies introduces several complications into cube construction making existing techniques essentially inapplicable in significant number of real world applications in particular hierarchies raise three main challenges the number of nodes in cube lattice increases dramatically and its shape is more involved these require new forms of lattice traversal for efficient execution the number of unique values in the higher levels of dimension hierarchy may be very small hence partitioning data into fragments that fit in memory and include all entries of particular value may often be impossible this requires new partitioning schemes the number of tuples that need to be materialized in the final cube increases dramatically this requires new storage schemes that remove all forms of redundancy for efficient space utilization in this paper we propose cure novel rolap cubing method that addresses these issues and constructs complete data cubes over very large datasets with arbitrary hierarchies cure contributes novel lattice traversal scheme an optimized partitioning method and suite of relational storage schemes for all forms of redundancy we demonstrate the effectiveness of cure through experiments on both real world and synthetic datasets among the experimental results we distinguish those that have made cure the first rolap technique to complete the construction of the cube of the highest density dataset in the apb benchmark gb cure was in fact quite efficient on this showing great promise with respect to the potential of the technique overall
in this article we present the integration of shape knowledge into variational model for level set based image segmentation and contour based pose tracking given the surface model of an object that is visible in the image of one or multiple cameras calibrated to the same world coordinate system the object contour extracted by the segmentation method is applied to estimate the pose parameters of the object vice versa the surface model projected to the image plane helps in top down manner to improve the extraction of the contour while common alternative segmentation approaches which integrate shape knowledge face the problem that an object can look very differently from various viewpoints free form model ensures that for each view the model can fit the data in the image very well moreover one additionally solves the problem of determining the object's pose in space the performance is demonstrated by numerous experiments with monocular and stereo camera system
we introduce novel representation for random access rendering of antialiased vector graphics on the gpu along with efficient encoding and rendering algorithms the representation supports broad class of vector primitives including multiple layers of semitransparent filled and stroked shapes with quadratic outlines and color gradients our approach is to create coarse lattice in which each cell contains variable length encoding of the graphics primitives it overlaps these cell specialized encodings are interpreted at runtime within pixel shader advantages include localized memory access and the ability to map vector graphics onto arbitrary surfaces or under arbitrary deformations most importantly we perform both prefiltering and supersampling within single pixel shader invocation achieving inter primitive antialiasing at no added memory bandwidth cost we present an efficient encoding algorithm and demonstrate high quality real time rendering of complex real world examples
transaction management on mobile database systems mds has to cope with number of constraints such as limited bandwidth low processing power unreliable communication and mobility etc as result of these constraints traditional concurrency control mechanisms are unable to manage transactional activities to maintain availability innovative transaction execution schemes and concurrency control mechanisms are therefore required to exploit the full potential of mds in this paper we report our investigation on multi versions transaction processing approach and deadlock free concurrency control mechanism based on multiversion two phase locking scheme integrated with timestamp approach we study the behavior of the proposed model with simulation study in mds environment we have compared our schemes using reference model to argue that such performance comparison helps to show the superiority of our model over others experimental results demonstrate that our model provide significantly higher throughput by improving degree of concurrency by reducing transaction wait time and by minimizing restarts and aborts
the cade atp system competition casc is an annual evaluation of fully automatic first order automated theorem proving atp systems casc was the thirteenth competition in the casc series twenty six atp systems and system variants competed in the various competition and demonstration divisions an outline of the competition design and commentated summary of the results are presented
this paper presents theoretical approach that has been developed to capture the computational intensity and computing resource requirements of geographical data and analysis methods these requirements are then transformed into common framework grid based representation of spatial computational domain which supports the efficient use of emerging cyberinfrastructure environments two key types of transformational functions data centric and operation centric are identified and their relationships are explained the application of the approach is illustrated using two geographical analysis methods inverse distance weighted interpolation and the spatial statistic we describe the underpinnings of these two methods present their conventional sequential algorithms and then address their latent parallelism based on spatial computational domain representation through the application of this theoretical approach the development of domain decomposition methods is decoupled from specific high performance computer architectures and task scheduling implementations which makes the design of generic parallel processing solutions feasible for geographical analyses
we present and evaluate simple yet efficient optimization technique that improves memory hierarchy performance for pointer centric applications by up to and reduces cache misses by up to this is achieved by selecting an improved ordering for the data members of pointer based data structures our optimization is applicable to all type safe programming languages that completely abstract from physical storage layout examples of such languages are java and oberon our technique does not involve programmers in the optimization process but runs fully automatically guided by dynamic profiling information that captures which paths through the program are taken with that frequencey the algorithm first strives to cluster data members that are accessed closely after one another onto the same cache line increasing spatial locality then the data members that have been mapped to particular cache line are ordered to minimize load latency in case of cache miss
we propose method for discovering the dependency relationships between the topics of documents shared in social networks using the latent social interactions attempting to answer the question given seemingly new topic from where does this topic evolve in particular we seek to discover the pair wise probabilistic dependency in topics of documents which associate social actors from latent social network where these documents are being shared by viewing the evolution of topics as markov chain we estimate markov transition matrix of topics by leveraging social interactions and topic semantics metastable states in markov chain are applied to the clustering of topics applied to the citeseer dataset collection of documents in academia we show the trends of research topics how research topics are related and which are stable we also show how certain social actors authors impact these topics and propose new ways for evaluating author impact
munin is distributed shared memory dsm system that allows shared memory parallel programs to be executed efficiently on distributed memory multiprocessors munin is unique among existing dsm systems in its use of multiple consistency protocols and in its use of release consistency in munin shared program variables are annotated with their expected access pattern and these annotations are then used by the runtime system to choose consistency protocol best suited to that access pattern release consistency allows munin to mask network latency and reduce the number of messages required to keep memory consistent munin's multiprotocol release consistency is implemented in software using delayed update queue that buffers and merges pending outgoing writes sixteen processor prototype of munin is currently operational we evaluate its implementation and describe the execution of two munin programs that achieve performance within ten percent of message passing implementations of the same programs munin achieves this level of performance with only minor annotations to the shared memory programs
this paper presents generative model for textures that uses local sparse description of the image content this model enforces the sparsity of the expansion of local texture patches on adapted atomic elements the analysis of given texture within this framework performs the sparse coding of all the patches of the texture into the dictionary of atoms conversely the synthesis of new texture is performed by solving an optimization problem that seeks for texture whose patches are sparse in the dictionary this paper explores several strategies to choose this dictionary set of hand crafted dictionaries composed of edges oscillations lines or crossings elements allows to synthesize synthetic images with geometric features another option is to define the dictionary as the set of all the patches of an input exemplar this leads to computer graphics methods for synthesis and shares some similarities with non local means filtering the last method we explore learns the dictionary by an optimization process that maximizes the sparsity of set of exemplar patches applications of all these methods to texture synthesis inpainting and classification shows the efficiency of the proposed texture model
optical flow computation is well known technique and there are important fields in which the application of this visual modality commands high interest nevertheless most real world applications require real time processing an issue which has only recently been addressed most real time systems described to date use basic models which limit their applicability to generic tasks especially when fast motion is presented or when subpixel motion resolution is required therefore instead of implementing complex optical flow approach we describe here very high frame rate optical flow processing system recent advances in image sensor technology make it possible nowadays to use high frame rate sensors to properly sample fast motion ie as low motion scene which makes gradient based approach one of the best options in terms of accuracy and consumption of resources for any real time implementation taking advantage of the regular data flow of this kind of algorithm our approach implements novel superpipelined fully parallelized architecture for optical flow processing the system is fully working and is organized into more than pipeline stages which achieve data throughput of one pixel per clock cycle this computing scheme is well suited to fpga technology and vlsi implementation the developed customized dsp architecture is capable of processing up to frames per second at resolution of pixels we discuss the advantages of high frame rate processing and justify the optical flow model chosen for the implementation we analyze this architecture measure the system resource requirements using fpga devices and finally evaluate the system's performance and compare it with other approaches described in the literature
hardware systems and reactive software systems can be described as the composition of several concurrently active processes automated reasoning based on model checking algorithms can substantially increase confidence in the overall reliability of system direct methods for model checking concurrent composition however usually suffer from the explosion in the number of program states that arises from concurrency reasoning compositionally about individual processes helps mitigate this problem number of rules have been proposed for compositional reasoning typically based on an assume guarantee reasoning paradigm reasoning with these rules can be delicate as some are syntactically circular in nature in that assumptions and guarantees are mutually dependent this is known to be source of unsoundness in this article we investigate rules for compositional reasoning from the viewpoint of completeness we show that several rules are incomplete that is there are properties whose validity cannot be established using only these rules we derive new circular reasoning rule and show it to be sound and complete we show that the auxiliary assertions needed for completeness need be defined only on the interface of the component processes we also show that the two main paradigms of circular and noncircular reasoning are closely related in that proof of one type can be transformed in straightforward manner to one of the other type these results give some insight into the applicability of compositional reasoning methods
the development of new techniques and the emergence of new high throughput tools have led to new information revolution the amount and the diversity of the information that need to be stored and processed have led to the adoption of data integration systems in order to deal with information extraction from disparate sources the mediation between traditional databases and ontologies has been recognized as cornerstone issue in bringing in legacy data with formal semantic meaning however our knowledge evolves due to the rapid scientific development so ontologies and schemata need to change in order to capture and accommodate such an evolution when ontologies change these changes should somehow be rendered and used by the pre existing data integration systems problem that most of the integration systems seem to ignore in this paper we review existing approaches for ontology schema evolution and examine their applicability in state of the art ontology based data integration setting then we show that changes in schemata differ significantly from changes in ontologies this strengthens our position that current state of the art systems are not adequate for ontology based data integration so we give the requirements for an ideal data integration system that will enable and exploit ontology evolution
abstract recurrent neural networks readily process recognize and generate temporal sequences by encoding grammatical strings as temporal sequences recurrent neural networks can be trained to behave like deterministic sequential finite state automata algorithms have been developed for extracting grammatical rules from trained networks using simple method for inserting prior knowledge or rules into recurrent neural networks we show that recurrent neural networks are able to perform rule revision rule revision is performed by comparing the inserted rules with the rules in the finite state automata extracted from trained networks the results from training recurrent neural network to recognize known non trivial randomly generated regular grammar show that not only do the networks preserve correct rules but that they are able to correct through training inserted rules which were initially incorrect by incorrect we mean that the rules were not the ones in the randomly generated grammar
recently mining from data streams has become an important and challenging task for many real world applications such as credit card fraud protection and sensor networking one popular solution is to separate stream data into chunks learn base classifier from each chunk and then integrate all base classifiers for effective classification in this paper we propose new dynamic classifier selection dcs mechanism to integrate base classifiers for effective mining from data streams the proposed algorithm dynamically selects single best classifier to classify each test instance at run time our scheme uses statistical information from attribute values and uses each attribute to partition the evaluation set into disjoint subsets followed by procedure that evaluates the classification accuracy of each base classifier on these subsets given test instance its attribute values determine the subsets that the similar instances in the evaluation set have constructed and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance experimental results and comparative studies demonstrate the efficiency and efficacy of our method such dcs scheme appears to be promising in mining data streams with dramatic concept drifting or with significant amount of noise where the base classifiers are likely conflictive or have low confidence
in this work we address the issue of efficient processing of range queries in dht based pp data networks the novelty of the proposed approach lies on architectures algorithms and mechanisms for identifying and appropriately exploiting powerful nodes in such networks the existence of such nodes has been well documented in the literature and plays key role in the architecture of most successful real world pp applications however till now this heterogeneity has not been taken into account when architecting solutions for complex query processing especially in dht networks with this work we attempt to fill this gap for optimizing the processing of range queries significant performance improvements are achieved due to ensuring much smaller hop count performance for range queries and ii avoiding the dangers and inefficiencies of relying for range query processing on weak nodes with respect to processing storage and communication capacities and with intermittent connectivity we present detailed experimental results validating our performance claims
we present the auckland layout model alm constraint based technique for specifying layout as it is used for arranging the controls in graphical user interface gui most gui frameworks offer layout managers that are basically adjustable tables often adjacent table cells can be merged in the alm the focus switches from the table cells to vertical and horizontal tabulators between the cells on the lowest level of abstraction the model applies linear constraints and an optimal layout is calculated using linear programming however bare linear programming makes layout specification cumbersome and unintuitive especially for gui domain experts who are often not used to such mathematical formalisms in order to improve the usability of the model alm offers several other layers of abstraction that make it possible to define common gui layout more easily in the domain of user interfaces it is important that specifications are not over constrained therefore alm introduces soft constraints which are automatically translated to appropriate hard linear constraints and terms in the objective function guis are usually composed of rectangular areas containing controls therefore alm offers an abstraction for such areas dynamic resizing behavior is very important for guis hence areas have domain specific parameters specifying their minimum maximum and preferred sizes from such definitions hard and soft constraints are automatically derived third level of abstraction allows designers to arrange guis in tabular fashion using abstractions for columns and rows which offer additional parameters for ordering and alignment row and column definitions are used to automatically generate definitions from lower levels of abstraction such as hard and soft constraints and areas specifications from all levels of abstraction can be consistently combined offering gui developers rich set of tools that is much closer to their needs than pure linear constraints incremental computation of solutions makes constraint solving fast enough for near real time use
in the area of image retrieval from data bases and for copyright protection of large image collections there is growing demand for unique but easily computable fingerprints for images these fingerprints can be used to quickly identify every image within larger set of possibly similar images this paper introduces novel method to automatically obtain such fingerprints from an image it is based on reinterpretation of an image as riemannian manifold this representation is feasible for gray value images and color images we discuss the use of the spectrum of eigenvalues of different variants of the laplace operator as fingerprint and show the usability of this approach in several use cases contrary to existing works in this area we do not only use the discrete laplacian but also with particular emphasis the underlying continuous operator this allows better results in comparing the resulting spectra and deeper insights in the problems arising we show how the well known discrete laplacian is related to the continuous laplace beltrami operator furthermore we introduce the new concept of solid height functions to overcome some potential limitations of the method
liveness analysis is an important analysis in optimizing compilers liveness information is used in several optimizations and is mandatory during the code generation phase two drawbacks of conventional liveness analyses are that their computations are fairly expensive and their results are easily invalidated by program transformations we present method to check liveness of variables that overcomes both obstacles the major advantage of the proposed method is that the analysis result survives all program transformations except for changes in the control flow graph for common program sizes our technique is faster and consumes less memory than conventional data flow approaches thereby we heavily make use of ssa form properties which allow us to completely circumvent data flow equation solving we evaluate the competitiveness of our approach in an industrial strength compiler our measurements use the integer part of the spec benchmarks and investigate the liveness analysis used by the ssa destruction pass we compare the net time spent in liveness computations of our implementation against the one provided by that compiler the results show that in the vast majority of cases our algorithm while providing the same quality of information needs less time an average speed up of
we have developed self healing key distribution scheme for secure multicast group communications for wireless sensor network environment we present strategy for securely distributing rekeying messages and specify techniques for joining and leaving group access control in multicast system is usually achieved by encrypting the content using an encryption key known as the group key session key that is only known by the group controller and all legitimate group members in our scheme all rekeying messages except for unicast of an individual key are transmitted without any encryption using one way hash function and xor operation in our proposed scheme nodes are capable of recovering lost session keys on their own without requesting additional transmission from the group controller the proposed scheme provides both backward and forward secrecy we analyze the proposed scheme to verify that it satisfies the security and performance requirements for secure group communication
name ambiguity problem has raised an urgent demand for efficient high quality named entity disambiguation methods the key problem of named entity disambiguation is to measure the similarity between occurrences of names the traditional methods measure the similarity using the bag of words bow model the bow however ignores all the semantic relations such as social relatedness between named entities associative relatedness between concepts polysemy and synonymy between key terms so the bow cannot reflect the actual similarity some research has investigated social networks as background knowledge for disambiguation social networks however can only capture the social relatedness between named entities and often suffer the limited coverage problem to overcome the previous methods deficiencies this paper proposes to use wikipedia as the background knowledge for disambiguation which surpasses other knowledge bases by the coverage of concepts rich semantic information and up to date content by leveraging wikipedia's semantic knowledge like social relatedness between named entities and associative relatedness between concepts we can measure the similarity between occurrences of names more accurately in particular we construct large scale semantic network from wikipedia in order that the semantic knowledge can be used efficiently and effectively based on the constructed semantic network novel similarity measure is proposed to leverage wikipedia semantic knowledge for disambiguation the proposed method has been tested on the standard weps data sets empirical results show that the disambiguation performance of our method gets improvement over the traditional bow based methods and improvement over the traditional social network based methods
physically motivated method for surface reconstruction is proposed that can recover smooth surfaces from noisy and sparse data sets no orientation information is required by new technique based on regularized membrane potentials the input sample points are aggregated leading to improved noise tolerability and outlier removal without sacrificing much with respect to detail feature recovery after aggregating the sample points on volumetric grid novel iterative algorithm is used to classify grid points as exterior or interior to the surface this algorithm relies on intrinsic properties of the smooth scalar field on the grid which emerges after the aggregation step second mesh smoothing paradigm based on mass spring system is introduced by enhancing this system with bending energy minimizing term we ensure that the final triangulated surface is smoother than piecewise linear in terms of speed and flexibility the method compares favorably with respect to previous approaches most parts of the method are implemented on modern graphics processing units gpus results in wide variety of settings are presented ranging from surface reconstruction on noise free point clouds to grayscale image segmentation
we present memory management scheme for java based on thread local heaps assuming most objects are created and used by single thread it is desirable to free the memory manager from redundant synchronization for thread local objects therefore in our scheme each thread receives partition of the heap in which it allocates its objects and in which it does local garbage collection without synchronization with other threads we dynamically monitor to determine which objects are local and which are global furthermore we suggest using profiling to identify allocation sites that almost exclusively allocate global objects and allocate objects at these sites directly in global areawe have implemented the thread local heap memory manager and preliminary mechanism for direct global allocation on an ibm prototype of jdk for windows our measurements of thread local heaps with direct global allocation on way multiprocessor ibm netfinity server show that the overall garbage collection times have been substantially reduced and that most long pauses have been eliminated
the recent proliferation of location based services lbss has necessitated the development of effective indoor positioning solutions in such context wireless local area network wlan positioning is particularly viable solution in terms of hardware and installation costs due to the ubiquity of wlan infrastructures this paper examines three aspects of the problem of indoor wlan positioning using received signal strength rss first we show that due to the variability of rss features over space spatially localized positioning method leads to improved positioning results second we explore the problem of access point ap selection for positioning and demonstrate the need for further research in this area third we present kernelized distance calculation algorithm for comparing rss observations to rss training records experimental results indicate that the proposed system leads to percent improvement over the widely used nearest neighbor and histogram based methods
grids involve coordinated resource sharing and problem solving in heterogeneous dynamic environments to meet the needs of generation of researchers requiring large amounts of bandwidth and more powerful computational resources the lack of resource ownership by grid schedulers and fluctuations in resource availability require mechanisms which will enable grids to adjust themselves to cope with fluctuations the lack of central controller implies need for self adaptation grids must thus be enabled with the ability to discover monitor and manage the use of resources so they can operate autonomously two different approaches have been conceived to match the resource demands of grid applications to resource availability dynamic scheduling and adaptive scheduling however these two approaches fail to address at least one of three important issues the production of feasible schedules in reasonable amount of time in relation to that required for the execution of an application ii the impact of network link availability on the execution time of an application and iii the necessity of migrating codes to decrease the execution time of an application to overcome these challenges this paper proposes procedure for enabling grid applications composed of various dependent tasks to deal with the availability of hosts and links bandwidth this procedure involves task scheduling resource monitoring and task migration with the goal of decreasing the execution time of grid applications the procedure differs from other approaches in the literature because it constantly considers changes in resource availability especially network bandwidth availability to trigger task migration the proposed procedure is illustrated via simulation using various scenarios involving fluctuation of resource availability an additional contribution of this paper is the introduction of set of schedulers offering solutions which differ in terms of both schedule length and computational complexity the distinguishing aspect of this set of schedulers is the consideration of time requirements in the production of feasible schedules performance is then evaluated considering various network topologies and task dependencies
this paper proposes weighted power series model for face verification scores fusion essentially linear parametric power series model is adopted to directly minimize an approximated total error rate for fusion of multi modal face verification scores unlike the conventional least squares error minimization approach which involves fitting of learning model to data density and then perform threshold process for error counting this work directly formulates the required target error count rate in terms of design model parameters with closed form solution the solution is found to belong to specific setting of the weighted least squares our experiments on fusing scores from visual and infra red face images as well as on public data sets show promising results
the increasing amount of communication between individuals in formats eg email instant messaging and the web has motivated computational research in social network analysis sna previous work in sna has emphasized the social network sn topology measured by communication frequencies while ignoring the semantic information in sns in this paper we propose two generative bayesian models for semantic community discovery in sns combining probabilistic modeling with community detection in sns to simulate the generative models an enf gibbs sampling algorithm is proposed to address the efficiency and performance problems of traditional methods experimental studies on enron email corpus show that our approach successfully detects the communities of individuals and in addition provides semantic topic descriptions of these communities
how many pages are there on the web more less big bets on clusters in the clouds could be wiped out if small cache of few million urls could capture much of the value language modeling techniques are applied to msn's search logs to estimate entropy the perplexity is surprisingly small millions not billions entropy is powerful tool for sizing challenges and opportunities how hard is search how hard are query suggestion mechanisms like auto complete how much does personalization help all these difficult questions can be answered by estimation of entropy from search logs what is the potential opportunity for personalization in this paper we propose new way to personalize search personalization with backoff if we have relevant data for particular user we should use it but if we don't back off to larger and larger classes of similar users as proof of concept we use the first few bytes of the ip address to define classes the coefficients of each backoff class are estimated with an em algorithm ideally classes would be defined by market segments demographics and surrogate variables such as time and geography
we address the problem of finding best deterministic query answer to query over probabilistic database for this purpose we propose the notion of consensus world or consensus answer which is deterministic world answer that minimizes the expected distance to the possible worlds answers this problem can be seen as generalization of the well studied inconsistent information aggregation problems eg rank aggregation to probabilistic databases we consider this problem for various types of queries including spj queries top ranking queries group by aggregate queries and clustering for different distance metrics we obtain polynomial time optimal or approximation algorithms for computing the consensus answers or prove np hardness most of our results are for general probabilistic database model called and xor tree model which significantly generalizes previous probabilistic database models like tuples and block independent disjoint models and is of independent interest
this paper presents novel architectural solution to address the problem of scalable routing in very large sensor networks the control complexities of the existing sensor routing protocols both flat and with traditional hierarchy do not scale very well for large networks with potentially hundreds of thousands of embedded sensor devices this paper develops novel routing solution off network control processing oncp that achieves control scalability in large sensor networks by shifting certain amount of routing functions off network this routing approach consisting of coarse grain global routing and distributed fine grain local routing is proposed for achieving scalability by avoiding network wide control message dissemination we present the oncp architectural concepts and analytically characterize its performance in relation to both flat and traditional hierarchical sensor routing architectures we also present ns based experimental results which indicate that for very large networks the packet drop latency and energy performance of oncp can be significantly better than those for flat sensor routing protocols such as directed diffusion and cluster based traditional hierarchical protocols such as cbrp
constrained clustering has recently become an active research topic this type of clustering methods takes advantage of partial knowledge in the form of pairwise constraints and acquires significant improvement beyond the traditional unsupervised clustering however most of the existing constrained clustering methods use constraints which are selected at random recently active constrained clustering algorithms utilizing active constraints have proved themselves to be more effective and efficient in this paper we propose an improved algorithm which introduces multiple representatives into constrained clustering to make further use of the active constraints experiments on several benchmark data sets and public image data sets demonstrate the advantages of our algorithm over the referenced competitors
this paper analyzes memory access scheduling and virtual channels as mechanisms to reduce the latency of main memory accesses by the cpu and peripherals in web servers despite the address filtering effects of the cpu's cache hierarchy there is significant locality and bank parallelism in the dram access stream of web server which includes traffic from the operating system application and peripherals however sequential memory controller leaves much of this locality and parallelism unexploited as serialization and bank conflicts affect the realizable latency aggressive scheduling within the memory controller to exploit the available parallelism and locality can reduce the average read latency of the sdram however bank conflicts and the limited ability of the sdram's internal row buffers to act as cache hinder further latency reduction virtual channel sdramovercomes these limitations by providing set of channel buffers that can hold segments from rows of any internal sdram bank this paper presents memory controller policies that can make effective use of these channel buffers to further reduce the average read latency of the sdram
locating faults in program can be very time consuming and arduous and therefore there is an increased demand for automated techniques that can assist in the fault localization process in this paper code coverage based method with family of heuristics is proposed in order to prioritize suspicious code according to its likelihood of containing program bugs highly suspicious code ie code that is more likely to contain bug should be examined before code that is relatively less suspicious and in this manner programmers can identify and repair faulty code more efficiently and effectively we also address two important issues first how can each additional failed test case aid in locating program faults and second how can each additional successful test case help in locating program faults we propose that with respect to piece of code the contribution of the first failed test case that executes it in computing its likelihood of containing bug is larger than or equal to that of the second failed test case that executes it which in turn is larger than or equal to that of the third failed test case that executes it and so on this principle is also applied to the contribution provided by successful test cases that execute the piece of code tool gdebug was implemented to automate the computation of the suspiciousness of the code and the subsequent prioritization of suspicious code for locating program faults to validate our method case studies were performed on six sets of programs siemens suite unix suite space grep gzip and make data collected from the studies are supportive of the above claim and also suggest heuristics iii and of our method can effectively reduce the effort spent on fault localization
in order to achieve high performance wide issue superscalar processors have to fetch large number of instructions per cycle conditional branches are the primary impediment to increasing the fetch bandwidth because they can potentially alter the flow of control and are very frequent to overcome this problem these processors need to predict the outcome of multiple branches in cycle this paper investigates two control flow prediction schemes that predict the effective outcome of multiple branches with the help of single prediction instead of considering branches as the basic units of prediction these schemes consider subgraphs of the control flow graph of the executed program as the basic units of prediction and predict the target of an entire subgraph at time thereby allowing the superscalar fetch mechanism to go past multiple branches in cycle the first control flow prediction scheme investigated considers sequential block like subgraphs and the second scheme considers tree like subgraphs to make the control flow predictions both schemes do out of prediction as opposed to the out of prediction done by branch level prediction schemes these two schemes are evaluated using mips isa based way superscalar microarchitecture an improvement in effective fetch size of approximately percent and percent respectively is observed over identical microprocessors that use branch level prediction no appreciable difference in the prediction accuracy was observed although the control flow prediction schemes predicted out of outcomes
wireless mesh networks wmns are considered as cost effective easily deployable and capable of extending internet connectivity however one of the major challenges in deploying reliable wmns is preventing their nodes from malicious attacks which is of particular concern as attacks can severely degrade network performance when dos attack is targeted over an entire communication path it is called path based dos attack we study the performance impact of path based dos attacks by considering attack intensity medium errors physical diversity collusion and hop count we setup wireless mesh testbed and configure set of experiments to gather realistic measurements and assess the effects of different factors we find that medium errors have significant impact on the performance of wmns when path based dos attack is carried out and the impact is exacerbated by the mac layer retransmissions we show that due to physical diversity far attacker can lead to an increased performance degradation than close by attacker additionally we demonstrate that the joint impact of two colluding attackers is not as severe as the joint result of individual attacks we also discuss strategy to counter path based dos attacks which can potentially alleviate the impact of the attack significantly
current cscw applications support one or more modes of cooperative work the selection of and transition between these modes is usually placed on the users at ipsi we built the sepia cooperative hypermedia authoring environment supporting whole range of situations arising during collaborative work and the smooth transitions between them while early use of the system shows the benefits of supporting smooth transitions between different collaborative modes it also reveals some deficits regarding parallel work management of alternative documents or reuse of document parts we propose to integrate version support to overcome these limitations this leads to versioned data management and an extended user interface enabling concurrent users to select certain state of their work to be aware of related changes and to cooperate with others either asynchronously or synchronously
trust between users is an important piece of knowledge that can be exploited in search and recommendationgiven that user supplied trust relationships are usually very sparse we study the prediction of trust relationships using user interaction features in an online user generated review application context we show that trust relationship prediction can achieve better accuracy when one adopts personalized and cluster based classification methods the former trains one classifier for each user using user specific training data the cluster based method first constructs user clusters before training one classifier for each user cluster our proposed methods have been evaluated in series of experiments using two datasets from epinionscom it is shown that the personalized and cluster based classification methods outperform the global classification method particularly for the active users
due to increasing system decentralization component heterogeneity and interface complexities many trustworthiness challenges become more and more complicated and intertwined moreover there is lack of common understanding of software trustworthiness and its related development methodology this paper reports preliminary results from an ongoing collaborative research project among international research units which aims at exploring theories and methods for enhancing existing software process techniques for trustworthy software development the results consist in two parts the proposal of new concept of process trustworthiness as capability indicator to measure the relative degree of confidence for certain software processes to deliver trustworthy software and the introduction of the architecture of trustworthy process management framework tpmf toolkit for process runtime support in measuring and improving process trustworthiness in order to assess and assure software trustworthiness
the research issue of broadcasting has attracted considerable amount of attention in mobile computing system by utilizing broadcast channels server is able to continuously and repeatedly broadcast data to mobile users from these broadcast channels mobile users obtain the data of interest efficiently and only need to wait for the required data to be present on the broadcast channel given the access frequencies of data items one can design proper data allocation in the broadcast channels to reduce the average expected delay of data items in practice the data access frequencies may vary with time we explore in this paper the problem of adjusting broadcast programs to effectively respond to the changes of data access frequencies and develop an efficient algorithm dl to address this problem performance of algorithm dl is analyzed and system simulator is developed to validate our results sensitivity analysis on several parameters including the number of data items the number of broadcast disks and the variation of access frequencies is conducted it is shown by our results that the broadcast programs adjusted by algorithm dl are of very high quality and are in fact very close to the optimal ones
contemporary database applications often perform queries in hybrid data spaces hds where vectors can have mix of continuous valued and non ordered discrete valued dimensions to support efficient query processing for an hds robust indexing method is required existing indexing techniques to process queries efficiently either apply to continuous data spaces eg the tree or non ordered discrete data spaces eg the nd tree no techniques directly indexing vectors in hdss have been reported in the literature in this paper we propose new multidimensional indexing technique called the nd tree to directly index vectors in an hds to build such an index we first introduce some essential geometric concepts eg hybrid bounding rectangle in hdss the nd tree structure and the relevant tree building and query processing algorithms based on these geometric concepts in hdss are then presented strategies have been suggested to make the values in continuous dimensions and non ordered discrete dimensions comparable and controllable novel node splitting heuristics which exploit characteristics of both continuous and discrete dimensions are proposed performance of the nd tree is compared with that of linear scan tree and nd tree using range queries on hybrid data experimental results demonstrate that the nd tree is quite promising in supporting range queries in hdss
it is now well admitted that formal methods are helpful for many issues raised in the web service area in this paper we present framework for the design and the verification of wss using process algebras and their tools we define two way mapping between abstract specifications written using these calculi and executable web services written in bpelws the translation includes also compensation event and fault handlers the following choices are available design and verification in bpelws using process algebra tools or design and verification in process algebra and automatically obtaining the corresponding bpelws code the approaches can be combined process algebras are not useful only for temporal logic verification we remark the use of simulation bisimulation for verification for the hierarchical refinement design method for the service redundancy analysis in community and for replacing service with another one in composition
with the increased use of virtual machines vms as vehicles that isolate applications running on the same host it is necessary to devise techniques that enable multiple vms to share underlying resources both fairly and efficiently to that end one common approach is to deploy complex resource management techniques in the hosting infrastructure alternately in this paper we advocate the use of self adaptation in the vms themselves based on feedback about resource usage and availability consequently we define friendly vm fvm to be virtual machine that adjusts its demand for system resources so that they are both efficiently and fairly allocated to competing fvms such properties are ensured using one of many provably convergent control rules such as additive increase multiplicative decrease aimd by adopting this distributed application based approach to resource management it is not necessary to make assumptions about the underlying resources nor about the requirements of fvms competing for these resources to demonstrate the elegance and simplicity of our approach we present prototype implementation of our fvm framework in user mode linux uml an implementation that consists of less than lines of code changes to uml we present an analytic control theoretic model of fvm adaptation which establishes convergence and fairness properties these properties are also backed up with experimental results using our prototype fvm implementation
with power consumption becoming increasingly critical in interconnected systems power aware networks will become part and parcel of many single chip and multichip systems as communication links consume significant power regardless of utilization mechanism to realize such power aware networks is on off links network links that can be turned on off as function of traffic in this paper we investigate and propose self regulating power aware interconnection networks that turn their links on off in response to bursts and dips in traffic in distributed fashion we explore the design space of such on off networks outlining step design methodology along with various building block solutions at each step that can be effectively assembled to develop various on off network designs we applied our methodology to the design of two classes of on off networks with links that possess substantially different on off delays an on chip network as well as chip to chip network and show that our designs are able to adapt dynamically to variations in network traffic three specific network designs are then constructed presented and evaluated our simulations show that link power consumption can be reduced by up to percent with modest increase in network latency
fraud is serious problem that costs the worldwide economy billions of dollars annually however fraud detection is difficult as perpetrators actively attempt to masquerade their actions among typically overwhelming large volumes of legitimate activity in this paper we investigate the fraud detection problem and examine how learning classifier systems can be applied to it we describe the common properties of fraud introducing an abstract problem which can be tuned to exhibit those characteristics we report experiments on this abstract problem with popular real time learning classifier system algorithm results from our experiments demonstrating that this approach can overcome the difficulties inherent to the fraud detection problem finally we apply the algorithm to real world problem and show that it can achieve good performance in this domain
the predictability of data values is studied at fundamental level two basic predictor models are defined computational predictors perform an operation on previous values to yield predicted next values examples we study are stride value prediction which adds delta to previous value and last value prediction which performs the trivial identity operation on the previous value context based predictors match recent value history context with previous value history and predict values based entirely on previously observed patterns to understand the potential of value prediction we perform simulations with unbounded prediction tables that are immediately updated using correct data values simulations of integer spec benchmarks show that data values can be highly predictable best performance is obtained with context based predictors overall prediction accuracies are between and the context based predictor typically has an accuracy about better than the computational predictors last value and stride comparison of context based prediction and stride prediction shows that the higher accuracy of context based prediction is due to relatively few static instructions giving large improvements this suggests the usefulness of hybrid predictors among different instruction types predictability varies significantly in general load and shift instructions are more difficult to predict correctly whereas add instructions are more predictable
data clustering has been discussed extensively but almost all known conventional clustering algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the data points existing subspace clustering algorithms for handling high dimensional data focus on numerical dimensions in this paper we designed an iterative algorithm called subcad for clustering high dimensional categorical data sets based on the minimization of an objective function for clustering we deduced some cluster memberships changing rules using the objective function we also designed an objective function to determine the subspace associated with each cluster we proved various properties of this objective function that are essential for us to design fast algorithm to find the subspace associated with each cluster finally we carried out some experiments to show the effectiveness of the proposed method and the algorithm
with more and more large networks becoming available mining and querying such networks are increasingly important tasks which are not being supported by database models and querying languages this paper wants to alleviate this situation by proposing data model and query language for facilitating the analysis of networks key features include support for executing external tools on the networks flexible contexts on the network each resulting in different graph primitives for querying subgraphs including paths and transforming graphs the data model provides for closure property in which the output of every query can be stored in the database and used for further querying
large real time software systems such as real time java virtual machines often use barrier protocols which work for dynamically varying number of threads without using centralized locking such barrier protocols however still suffer from priority inversion similar to centralized locking we introduce gang priority management as generic solution for avoiding unbounded priority inversion in barrier protocols our approach is either kernel assisted for efficiency or library based for portability but involves cooperation from the protocol designer for generality we implemented gang priority management in the linux kernel and rewrote the garbage collection safe point barrier protocol in ibm's websphere real time java virtual machine to exploit it we run experiments on an way smp machine in multi user and multi process environment and show that by avoiding unbounded priority inversion the maximum latency to reach barrier point is reduced by factor of and the application jitter is reduced by factor of
achieving intuitive control of animated surface deformation while observing specific style is an important but challenging task in computer graphics solutions to this task can find many applications in data driven skin animation computer puppetry and computer games in this paper we present an intuitive and powerful animation interface to simultaneously control the deformation of large number of local regions on deformable surface with minimal number of control points our method learns suitable deformation subspaces from training examples and generate new deformations on the fly according to the movements of the control points our contributions include novel deformation regression method based on kernel canonical correlation analysis cca and poisson based translation solving technique for easy and fast deformation control based on examples our run time algorithm can be implemented on gpus and can achieve few hundred frames per second even for large datasets with hundreds of training examples
the specification of schema mappings has proved to be time and resource consuming and has been recognized as critical bottleneck to the large scale deployment of data integration systems in an attempt to address this issue dataspaces have been proposed as data management abstraction that aims to reduce the up front cost required to setup data integration system by gradually specifying schema mappings through interaction with end users in pay as you go fashion as step in this direction we explore an approach for incrementally annotating schema mappings using feedback obtained from end users in doing so we do not expect users to examine mapping specifications rather they comment on results to queries evaluated using the mappings using annotations computed on the basis of user feedback we present method for selecting from the set of candidate mappings those to be used for query evaluation considering user requirements in terms of precision and recall in doing so we cast mapping selection as an optimization problem mapping annotations may reveal that the quality of schema mappings is poor we also show how feedback can be used to support the derivation of better quality mappings from existing mappings through refinement an evolutionary algorithm is used to efficiently and effectively explore the large space of mappings that can be obtained through refinement the results of evaluation exercises show the effectiveness of our solution for annotating selecting and refining schema mappings
watermarking algorithms provide way of hiding or embedding some bits of information in watermark in the case of watermarking model many algorithms employ so called indexed localization scheme in this paper we propose an optimization framework with two new steps for such watermarking algorithms to improve their capacity and invisibility the first step is to find an optimal layout of invariant units to improve capacity the second step is to rearrange the correspondence between the watermark units and the invariant units to improve invisibility experimental tests show that by using this framework the capacity and invisibility of watermarking algorithms can be greatly improved
many virtual environments and games must be populated with synthetic characters to create the desired experience these characters must move with sufficient realism so as not to destroy the visual quality of the experience yet be responsive controllable and efficient to simulate in this paper we present an approach to character motion called snap together motion that addresses the unique demands of virtual environments snap together motion stm preprocesses corpus of motion capture examples into set of short clips that can be concatenated to make continuous streams of motion the result process is simple graph structure that facilitates efficient planning of character motions user guided process selects common character poses and the system automatically synthesizes multi way transitions that connect through these poses in this manner well connected graphs can be constructed to suit particular application allowing for practical interactive control without the effort of manually specifying all transitions
as more and more documents become electronically available finding documents in large databases that fit users needs is becoming increasingly important in the past the document search problem was dealt with using the database query approach or the text based search approach in this paper we investigate this problem focusing on the sci ssci databases from isi specifically we design our search methodology based on the four fields commonly seen in scientific research document abstract title keywords and reference list of these four only the abstract field can be viewed as normal text while the other three have their own characteristics to differentiate them from texts therefore we first develop method to compute the similarity value for each field our next problem is combining the four similarity values into final value one approach is to assign weights to each and compute the weighted sum we have not adopted this simple weighting method however because it is difficult to determine appropriate weights instead we use the back propagation neural network to combine them finally extensive experiments have been carried out using real documents drawn from tkde journal and the results indicate that in all situations our method has much higher accuracy than the traditional text based search approach
denial of service dos attacks are arguably one of the most cumbersome problems in the internet this paper presents distributed information system over set of completely connected servers called chameleon which is robust to dos attacks on the nodes as well as the operations of the system in particular it allows nodes to efficiently look up and insert data items at any time despite powerful past insider adversary which has complete knowledge of the system up to some time point and can use that knowledge in order to block constant fraction of the nodes and inject lookup and insert requests to selected data this is achieved with smart randomized replication policy requiring polylogarithmic overhead only and the interplay of permanent and temporary distributed hash table all requests in chameleon can be processed in polylogarithmic time and work at every node
energy consumption is major concern in many embedded computing systems several studies have shown that cache memories account for about of the total energy consumed in these systems the performance of given cache architecture is largely determined by the behavior of the application using that cache desktop systems have to accommodate very wide range of applications and therefore the manufacturer usually sets the cache architecture as compromise given current applications technology and cost unlike desktop systems embedded systems are designed to run small range of well defined applications in this context cache architecture that is tuned for that narrow range of applications can have both increased performance as well as lower energy consumption we introduce novel cache architecture intended for embedded microprocessor platforms the cache can be configured by software to be direct mapped two way or four way set associative using technique we call way concatenation having very little size or performance overhead we show that the proposed cache architecture reduces energy caused by dynamic power compared to way shutdown cache furthermore we extend the cache architecture to also support way shutdown method designed to reduce the energy from static power that is increasing in importance in newer cmos technologies our study of programs drawn from powerstone mediabench and spec show that tuning the cache's configuration saves energy for every program compared to conventional four way set associative as well as direct mapped caches with average savings of compared to four way conventional cache
this paper describes the generation of model capturing information on how placenames co occur together the advantages of the co occurrence model over traditional gazetteers are discussed and the problem of placename disambiguation is presented as case study we begin by outlining the problem of ambiguous placenames we demonstrate how analysis of wikipedia can be used in the generation of co occurrence model the accuracy of our model is compared to handcrafted ground truth then we evaluate alternative methods of applying this model to the disambiguation of placenames in free text using the geoclef evaluation forum we conclude by showing how the inclusion of placenames in both the text and geographic parts of query provides the maximum mean average precision and outline the benefits of co occurrence model as data source for the wider field of geographic information retrieval gir
with the proliferation of mobile streaming multimedia available battery capacity constrains the end user experience since streaming applications are expected to be long running wireless network interface card's wnic energy consumption is particularly an acute problem in this work we explore various mechanisms to conserve client wnic energy consumption for popular streaming formats such as microsoft windows media real and apple quicktime first we investigate the wnic energy consumption characteristics for these popular multimedia streaming formats under varying stream bandwidth and network loss rates we show that even for high bandwidth kbps stream the wnic unnecessarily spent over of the time in idle state illustrating the potential for significant energy savingsbased on these observations we explore two mechanisms to conserve the client wnic energy consumption first we show the limitations of ieee power saving mode for multimedia streams without an understanding of the stream requirements these scheduled rendezvous mechanisms do not offer any energy savings for multimedia streams over kbps we also develop history based client side strategies to reduce the energy consumed by transitioning the wnics to lower power consuming sleep state we show that streams optimized for kbps can save over in energy consumption with data loss high bandwidth stream kbps can still save in energy consumption with less than data loss we also show that real and quicktime packets are harder to predict at the network level without understanding the packet semantics as the amount of cross traffic generated by other clients that share the same wireless segment increases the potential energy savings from our client side policies deteriorate further our work enables multimedia proxy and server developers to suitably customize the stream to lower client energy consumption
this paper describes an approach of representing shape by using set of invariant spherical harmonic sh coefficients after conformal mapping specifically genus zero mesh object is first conformally mapped onto the unit sphere by using modified discrete conformal mapping where the modification is based on mobius factorization and aims at obtaining canonical conformal mapping then sh analysis is applied to the resulting conformal spherical mesh the obtained sh coefficients are further made invariant to translation and rotation while at the same time retain the completeness thanks to which the original shape information has been faithfully preserved
active harmony is an automated runtime performance tuning system in this paper we describe parameter prioritizing tool to help focus on those parameters that are performance critical historical data is also utilized to further speed up the tuning process we first verify our proposed approaches with synthetic data and finally we verify all the improvements on real cluster based web service system taken together these changes allow the active harmony system to reduce the time spent tuning from up to and at the same time reduce the variation in performance while tuning
an overwhelming volume of news videos from different channels and languages is available today which demands automatic management of this abundant information to effectively search retrieve browse and track cross lingual news stories news story similarity measure plays critical role in assessing the novelty and redundancy among them in this paper we explore the novelty and redundancy detection with visual duplicates and speech transcripts for cross lingual news stories news stories are represented by sequence of keyframes in the visual track and set of words extracted from speech transcript in the audio track major difference to pure text documents is that the number of keyframes in one story is relatively small compared to the number of words and there exist large number of non near duplicate keyframes these features make the behavior of similarity measures different compared to traditional textual collections furthermore the textual features and visual features complement each other for news stories they can be further combined to boost the performance experiments on the trecvid cross lingual news video corpus show that approaches on textual features and visual features demonstrate different performance and measures on visual features are quite effective overall the cosine distance on keyframes is still robust measure language models built on visual features demonstrate promising performance the fusion of textual and visual features improves overall performance
the ready availability of online source code examples has fundamentally changed programming practices however current search tools are not designed to assist with programming tasks and are wholly separate from editing tools this paper proposes that embedding task specific search engine in the development environment can significantly reduce the cost of finding information and thus enable programmers to write better code more easily this paper describes the design implementation and evaluation of blueprint web search interface integrated into the adobe flex builder development environment that helps users locate example code blueprint automatically augments queries with code context presents code centric view of search results embeds the search experience into the editor and retains link between copied code and its source comparative laboratory study found that blueprint enables participants to write significantly better code and find example code significantly faster than with standard web browser analysis of three months of usage logs with users suggests that task specific search interfaces can significantly change how and when people search the web
efforts to improve application reliability can be irrelevant if the reliability of the underlying operating system on which the application resides is not seriously considered an important first step in improving the reliability of an operating system is to gain insights into why and how the bugs originate contributions of the different modules to the bugs their distribution across severities the different ways in which the bugs may be resolved and the impact of bug severities on their resolution times to acquire this insight we conducted an extensive analysis of the publicly available bug data on the linux kernel over period of seven years we also justify and explain the statistical bug occurrence trends observed from the data using the architecture of the linux kernel as an anchor the statistical analysis of the linux bug data suggests that the linux kernel may draw significant benefits from the continual reliability improvement efforts of its developers these efforts however are disproportionately targeted towards popular configurations and hardware platforms due to which the reliability of these configurations may be better than those that are not commonly used thus key finding of our study is that it may be prudent to restrict to using common configurations and platforms when using open source systems such as linux in applications with stringent reliability expectations finally our study of the architectural properties of the bugs suggests that the dependence among the modules rather than the unreliabilities of the individual modules is the primary cause of the bugs and their impact on system reliability
major obstacle in the technology transfer agenda of behavioral analysis and design methods is the need for logics or automata to express properties for control intensive systems interaction modeling notations may offer replacement or complement with practitioner appealing and lightweight flavor due partly to the subspecification of intended behavior by means of scenarios we propose novel approach consisting of engineering new formal notation of this sort based on simple compact declarative semantics vts visual timed event scenarios scenarios represent event patterns graphically depicting conditions over traces they predicate general system events and provide features to describe complex properties not expressible with msc like notations the underlying formalism supports partial orders and real time constraints the problem of checking whether timed automaton model has matching trace is proven decidable on top of this kernel we introduce notation to state properties over all system traces conditional scenarios allowing engineers to describe uniquely rich connections between antecedent and consequent portions of the scenario an undecidability result is presented for the general case of the model checking problem over dense time domains to later identify decidable yet practically relevant subclass where verification is solvable by generating antiscenarios expressed in the vts hbox rm kernel notation
data generalization is widely used to protect identities and prevent inference of sensitive information during the public release of microdata the anonymity model has been extensively applied in this context the model seeks generalization scheme such that every individual becomes indistinguishable from at least other individuals and the loss in information while doing so is kept at minimum the search is performed on domain hierarchy lattice where every node is vector signifying the level of generalization for each attribute an effort to understand privacy and data utility trade offs will require knowing the minimum possible information losses of every possible value of however this can easily lead to an exhaustive evaluation of all nodes in the hierarchy lattice in this paper we propose using the concept of pareto optimality to obtain the desired trade off information pareto optimal generalization is one in which no other generalization can provide higher value of without increasing the information loss we introduce the pareto optimal anonymization poka algorithm to traverse the hierarchy lattice and show that the number of node evaluations required to find the pareto optimal generalizations can be significantly reduced results on benchmark data set show that the algorithm is capable of identifying all pareto optimal nodes by evaluating only of nodes in the lattice
distributed simulation techniques are commonly used to improve the speed and scalability of wireless sensor network simulators however accurate simulations of dynamic interactions of sensor network applications incur large synchronization overheads and severely limit the performance of existing distributed simulators in this paper we present two novel techniques that significantly reduce such overheads by minimizing the number of sensor node synchronizations during simulations these techniques work by exploiting radio and mac specific characteristics without reducing simulation accuracy in addition we present new probing mechanism that makes it possible to exploit any potential application specific characteristics for synchronization reductions we implement and evaluate these techniques in cycle accurate distributed simulation framework that we developed based on avrora in our experiments the radio level technique achieves speedup of to times in simulating hop networks with to nodes with default backoffs themac level technique achieves speedup of to times in the best case scenarios of simulating and nodes in our multi hop flooding tests together they achieve speedup of to times in simulating networks with to nodes the experiments also demonstrate that the speedups can be significantly larger as the techniques scale with the number of processors and radio off mac backoff time
an important problem in software engineering is the automated discovery of noncrashing occasional bugs in this work we address this problem and show that mining of weighted call graphs of program executions is promising technique we mine weighted graphs with combination of structural and numerical techniques more specifically we propose novel reduction technique for call graphs which introduces edge weights then we present an analysis technique for such weighted call graphs based on graph mining and on traditional feature selection schemes the technique generalises previous graph mining approaches as it allows for an analysis of weights our evaluation shows that our approach finds bugs which previous approaches cannot detect so far our technique also doubles the precision of finding bugs which existing techniques can already localise in principle
this article presents new technique adaptive replication for automatically eliminating synchronization bottlenecks in multithreaded programs that perform atomic operations on objects synchronization bottlenecks occur when multiple threads attempt to concurrently update the same object it is often possible to eliminate synchronization bottlenecks by replicating objects each thread can then update its own local replica without synchronization and without interacting with other threads when the computation needs to access the original object it combines the replicas to produce the correct values in the original object one potential problem is that eagerly replicating all objects may lead to performance degradation and excessive memory consumptionadaptive replication eliminates unnecessary replication by dynamically detecting contention at each object to find and replicate only those objects that would otherwise cause synchronization bottlenecks we have implemented adaptive replication in the context of parallelizing compiler for subset of given an unannotated sequential program written in the compiler automatically extracts the concurrency determines when it is legal to apply adaptive replication and generates parallel code that uses adaptive replication to efficiently eliminate synchronization bottlenecksin addition to automatic parallelization and adaptive replication our compiler also implements lock coarsening transformation that increases the granularity at which the computation locks objects the advantage is reduction in the frequency with which the computation acquires and releases locks the potential disadvantage is the introduction of new synchronization bottlenecks caused by increases in the sizes of the critical sections because the adaptive replication transformation takes place at lock acquisition sites there is synergistic interaction between lock coarsening and adaptive replication lock coarsening drives down the overhead of using adaptive replication and adaptive replication eliminates synchronization bottlenecks associated with the overaggressive use of lock coarseningour experimental results show that for our set of benchmark programs the combination of lock coarsening and adaptive replication can eliminate synchronization bottlenecks and significantly reduce the synchronization and replication overhead as compared to versions that use none or only one of the transformations
in this paper we investigate reduced representations for the emerging cube we use the borders classical in data mining for the emerging cube these borders can support classification tasks to know whether trend is emerging or not however the borders do not make possible to retrieve the measure values this is why we introduce two new and reduced representations without measure loss the emerging closed cube and emerging quotient cube we state the relationship between the introduced representations experiments performed on various data sets are intended to measure the size of the three reduced representations
even great efforts have been made for decades the recognition of human activities is still an unmature technology that attracted plenty of people in computer vision in this paper system framework is presented to recognize multiple kinds of activities from videos by an svm multi class classifier with binary tree architecture the framework is composed of three functionally cascaded modules detecting and locating people by non parameter background subtraction approach extracting various of features such as local ones from the minimum bounding boxes of human blobs in each frames and newly defined global one contour coding of the motion energy image ccmei and recognizing activities of people by svm multi class classifier whose structure is determined by clustering process the thought of hierarchical classification is introduced and multiple svms are aggregated to accomplish the recognition of actions each svm in the multi class classifier is trained separately to achieve its best classification performance by choosing proper features before they are aggregated experimental results both on home brewed activity data set and the public schuldt's data set show the perfect identification performance and high robustness of the system
the ability to walk up to any computer personalize it and use it as one's own has long been goal of mobile computing research we present soulpad new approach based on carrying an auto configuring operating system along with suspended virtual machine on small portable device with this approach the computer boots from the device and resumes the virtual machine thus giving the user access to his personal environment including previously running computations soulpad has minimal infrastructure requirements and is therefore applicable to wide range of conditions particularly in developing countries we report our experience implementing soulpad and using it on variety of hardware configurations we address challenges common to systems similar to soulpad and show that the soulpad model has significant potential as mobility solution
data races occur when multiple threads are about to access the same piece of memory and at least one of those accesses is write such races can lead to hard to reproduce bugs that are time consuming to debug and fix we present relay static and scalable race detection analysis in which unsoundness is modularized to few sources we describe the analysis and results from our experiments using relay to find data races in the linux kernel which includes about million lines of code
one popular approach to object design proposes to identify responsibilities from software contracts apply number of principles to assign them to objects and finally construct an object interaction that realizes the contract this three step activity is currently manual process that is time consuming and error prone and is among the most challenging activities in object oriented development in this paper we present model transformation that partially automates this activity such transformation is modularized in three stages the first stage automatically transforms software contract to trace of state modification actions in the second stage the designer manually extends the trace with design decisions finally the extended trace is automatically transformed to an object interaction in the third stage prototype of the whole transformation was developed and successfully applied to case study from the literature our technique allows the extraction of valuable information from software contracts provides bridge between analysis and design artifacts and significantly reduces the effort of interaction design
operational transformation ot is an optimistic concurrency control method that has been well established in realtime group editors and has drawn significant research attention in the past decade it is generally believed that the use of ot automatically achieves high local responsiveness in group editors however no performance study has been reported previously on ot algorithms to the best of our knowledge this paper extends recent ot algorithm and studies its performance by theoretical analyses and performance experiments this paper proves that the worst case execution time of ot only appears in rare cases and shows that local responsiveness of ot based group editors in fact depends on number of factors such as the size of the operation log the paper also reveals that these two results have general implications on ot algorithms and hence the design of ot based group editors must pay attention to performance issues
addressed in this paper is the issue of email data cleaning for text mining many text mining applications need take emails as input email data is usually noisy and thus it is necessary to clean it before mining several products offer email cleaning features however the types of noises that can be eliminated are restricted despite the importance of the problem email cleaning has received little attention in the research community thorough and systematic investigation on the issue is thus needed in this paper email cleaning is formalized as problem of non text filtering and text normalization in this way email cleaning becomes independent from any specific text mining processing cascaded approach is proposed which cleans up an email in four passes including non text filtering paragraph normalization sentence normalization and word normalization as far as we know non text filtering and paragraph normalization have not been investigated previously methods for performing the tasks on the basis of support vector machines svm have also been proposed in this paper features in the models have been defined experimental results indicate that the proposed svm based methods can significantly outperform the baseline methods for email cleaning the proposed method has been applied to term extraction typical text mining processing experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method
due to the reliance on the textual information associated with an image image search engines on the web lack the discriminative power to deliver visually diverse search results the textual descriptions are key to retrieve relevant results for given user query but at the same time provide little information about the rich image content in this paper we investigate three methods for visual diversification of image search results the methods deploy lightweight clustering techniques in combination with dynamic weighting function of the visual features to best capture the discriminative aspects of the resulting set of images that is retrieved representative image is selected from each cluster which together form diverse result set based on performance evaluation we find that the outcome of the methods closely resembles human perception of diversity which was established in an extensive clustering experiment carried out by human assessors
inspired by the surprising discovery of several recurring structures in various complex networks in recent years number of related works treated software systems as complex network and found that software systems might expose the small world effects and follow scale free degree distributions different from the research perspectives adopted in these works the work presented in this paper treats software execution processes as an evolving complex network for the first time the concept of software mirror graph is introduced as new model of complex networks to incorporate the dynamic information of software behavior the experimentation paradigm with statistical repeatability was applied to three distinct subject programs to conduct several software experiments the corresponding experimental results are analyzed by treating the software execution processes as an evolving directed topological graph as well as an evolving software mirror graph this results in several new findings while the software execution processes may demonstrate as small world complex network in the topological sense they no longer expose the small world effects in the temporal sense further the degree distributions of the software execution processes may follow power law however they may also follow an exponential function or piecewise power law
we overview the development of first order automated reasoning systems starting from their early years based on the analysis of current and potential applications of such systems we also try to predict new trends in first order automated reasoning our presentation will be centered around two main motives efficiency and usefulness for existing and future potential applications
dynamic slicing algorithms have been considered to aid in debugging for many years however as far as we know no detailed studies on evaluating the benefits of using dynamic slicing for locating real faults present in programs have been carried out in this paper we study the effectiveness of fault location using dynamic slicing for set of real bugs reported in some widely used software programs our results show that of the faults studied faults were captured by data slices required the use of full slices and none of them required the use of relevant slices moreover it was observed that dynamic slicing considerably reduced the subset of program statements that needed to be examined to locate faulty statements interestingly we observed that all of the memory bugs in the faulty versions were captured by data slices the dynamic slices that captured faulty code included to of statements that were executed at least once
video on demand vod service offers large selection of videos from which customers can choose designers of vod systems strive to achieve low access latency for customers one approach that has been investigated by several researchers allows the server to batch clients requesting the same video and to serve clients in the same batch with one multicast video stream this approach has the advantage that it can save server resources as well as server access and network bandwidth thus allowing the server to handle large number of customers without sacrificing access latency vod server replication is another approach that can allow vod service to handle large number of clients albeit at the additional cost of providing more servers while replication is an effective way to increase the service capacity it needs to be coupled with appropriate selection techniques in order to make efficient use of the increased capacity in this paper we investigate the design of server selection techniques for system of replicated batching vod servers we design and evaluate range of selection algorithms as they would be applied to three batching approaches batching with persistent channel allocation patching and hierarchical multicast stream merging hmsm we demonstrate that server replication combined with appropriate server selection scheme can indeed be used to increase the capacity of the service leading to improved performance
this work addresses data warehouse maintenance ie how changes to autonomous heterogeneous and distributed sources should be detected and propagated to warehouse the research community has mainly addressed issues relating to the internal operation of data warehouse servers work related to data warehouse maintenance has received less attention and only limited set of maintenance alternatives are considered while ignoring the autonomy and heterogeneity of sourcesin this paper we extend work on single source view maintenance to views with multiple heterogeneous sources we present tool pam which allows for comparison of large number of relevant maintenance policies under different configurations based on such analysis and previous studies we propose set of heuristics to guide in policy selection the quality of these heuristics is evaluated empirically using test bed developed for this purpose this is done for number of different criteria and for different data sources and computer systems the performance gained using the policy selected through the heuristics is compared with the performance of all identified policies based on these experiments we claim that heuristic based selections are good
massive data streams are now fundamental to many data processing applications for example internet routers produce large scale diagnostic data streams such streams are rarely stored in traditional databases and instead must be processed on the fly as they are produced similarly sensor networks produce multiple data streams of observations from their sensors there is growing focus on manipulating data streams and hence there is need to identify basic operations of interest in managing data streams and to support them efficiently we propose computation of the hamming norm as basic operation of interest the hamming norm formalises ideas that are used throughout data processing when applied to single stream the hamming norm gives the number of distinct items that are present in that data stream which is statistic of great interest in databases when applied to pair of streams the hamming norm gives an important measure of dis similarity the number of unequal item counts in the two streams hamming norms have many uses in comparing data streams we present novel approximation technique for estimating the hamming norm for massive data streams this relies on what we call the sketch and we prove its accuracy we test our approximation method on large quantity of synthetic and real stream data and show that the estimation is accurate to within few percentage points
we present framework for specification and security analysis of communication protocols for mobile wireless networks this setting introduces new challenges which are not being addressed by classical protocol analysis techniques the main complication stems from the fact that the actions of intermediate nodes and their connectivity can no longer be abstracted into single unstructured adversarial environment as they form an inherent part of the system's security in order to model this scenario faithfully we present broadcast calculus which makes clear distinction between the protocol processes and the network's connectivity graph which may change independently from protocol actions we identify property characterising an important aspect of security in this setting and express it using behavioural equivalences of the calculus we complement this approach with control flow analysis which enables us to automatically check this property on given network and attacker specification
characterizing the communication behavior of large scale applications is difficult and costly task due to code system complexity and long execution times while many tools to study this behavior have been developed these approaches either aggregate information in lossy way through high level statistics or produce huge trace files that are hard to handle we contribute an approach that provides orders of magnitude smaller if not near constant size communication traces regardless of the number of nodes while preserving structural information we introduce intra and inter node compression techniques of mpi events that are capable of extracting an application's communication structure we further present replay mechanism for the traces generated by our approach and discuss results of our implementation for bluegene given this novel capability we discuss its impact on communication tuning and beyond to the best of our knowledge such concise representation of mpi traces in scalable manner combined with deterministic mpi call replay is without any precedent
over the last decade feature creep and the convergence of multiple devices have increased the complexity of both design and use one way to reduce the complexity of device without sacrificing its features is to design the ui consistently however designing consistent user interface of multifunction device often becomes formidable task especially when the logical interaction is concerned this paper presents systematic method for consistent design of user interaction called cuid consistent user interaction design and validates its usefulness through case study cuid focusing on ensuring consistency of logical interaction rather than physical or visual interfaces employs constraint based interactive approach it strives for consistency as the main goal but also considers efficiency and safety of use cuid will reduce the cognitive complexity of the task of interaction design to help produce devices that are easier to learn and use
the web graph is giant social network whose properties have been measured and modeled extensively in recent years most such studies concentrate on the graph structure alone and do not consider textual properties of the nodes consequently web communities have been characterized purely in terms of graph structure and not on page content we propose that topic taxonomy such as yahoo or the open directory provides useful framework for understanding the structure of content based clusters and communities in particular using topic taxonomy and an automatic classifier we can measure the background distribution of broad topics on the web and analyze the capability of recent random walk algorithms to draw samples which follow such distributions in addition we can measure the probability that page about one broad topic will link to another broad topic extending this experiment we can measure how quickly topic context is lost while walking randomly on the web graph estimates of this topic mixing distance may explain why global pagerank is still meaningful in the context of broad queries in general our measurements may prove valuable in the design of community specific crawlers and link based ranking systems
current techniques and tools for automated termination analysis of term rewrite systems trss are already very powerful however they fail for algorithms whose termination is essentially due to an inductive argument therefore we show how to couple the dependency pair method for trs termination with inductive theorem proving as confirmed by the implementation of our new approach in the tool aprove now trs termination techniques are also successful on this important class of algorithms
we present freeform modeling framework for unstructured triangle meshes which is based on constraint shape optimization the goal is to simplify the user interaction even for quite complex freeform or multiresolution modifications the user first sets various boundary constraints to define custom tailored abstract basis function which is adjusted to given design task the actual modification is then controlled by moving one single dof manipulator object the technique can handle arbitrary support regions and piecewise boundary conditions with smoothness ranging continuously from to to more naturally adapt the modification to the shape of the support region the deformed surface can be tuned to bend with anisotropic stiffness we are able to achieve real time response in an interactive design session even for complex meshes by precomputing set of scalar valued basis functions that correspond to the degrees of freedom of the manipulator by which the user controls the modification
balancing peer to peer graphs including zone size distributions has recently become an important topic of peer to peer pp research to bring analytical understanding into the various peer join mechanisms we study how zone balancing decisions made during the initial sampling of the peer space affect the resulting zone sizes and derive several asymptotic results for the maximum and minimum zone sizes that hold with high probability
we explore making virtual desktops behave in more physically realistic manner by adding physics simulation and using piling instead of filing as the fundamental organizational structure objects can be casually dragged and tossed around influenced by physical characteristics such as friction and mass much like we would manipulate lightweight objects in the real world we present prototype called bumptop that coherently integrates variety of interaction and visualization techniques optimized for pen input we have developed to support this new style of desktop organization
in distributed shared memory multiprocessors remote memory references generate processor to memory traffic which may result in bottleneck it is therefore important to design algorithms that minimize the number of remote memory references we establish lower bound of three on remote reference time complexity for mutual exclusion algorithms in model where processes communicate by means of general read modify write primitive that accesses at most one shared variable in one instruction since the general read modify write primitive is generalization of variety of atomic primitives that have been implemented in multiprocessor systems our lower bound holds for all mutual exclusion algorithms that use such primitives furthermore this lower bound is shown to be tight by presenting an algorithm with the matching upper bound
in this experience report we present an evaluation of different techniques to manage concurrency in the context of application servers traditionally using entity beans is considered as the only way to synchronize concurrent access to data in jave ee and using mechanism such as synchronized blocks within ejbs is strongly not recommended in our evaluation we consider the use of software transactional memory to enable concurrent accesses to shared data across different session beans we are also comparing our approach with using entity beans and session beans synchronized by global lock
in this paper the problem of optimizing svr automatically for time series forecasting is considered which involves introducing auto adaptive parameters and to depict the non uniform distribution of the information offered by the training data developing multiple kernel function to rescale different attributes of input space optimizing all the parameters involved simultaneously with genetic algorithm and performing feature selection to reduce the redundant information experimental results assess the feasibility of our approach called model optimizing svr or briefly mo svr and demonstrate that our method is promising alternative for time series forecasting
information dissemination is powerful mechanism for finding information in wide area environments an information dissemination server accepts long term user queries collects new documents from information sources matches the documents against the queries and continuously updates the users with relevant information this paper is retrospective of the stanford information filtering service sift system that as of april was processing over worldwide subscriptions and over daily documents the paper describes some of the indexing mechanisms that were developed for sift as well as the evaluations that were conducted to select scheme to implement it also describes the implementation of sift and experimental results for the actual system finally it also discusses and experimentally evaluates techniques for distributing service such as sift for added performance and availability
with the diverse new capabilities that sensor and ad hoc networks can provide applicability of data aggregation is growing data aggregation is useful in dealing with multi value domain information which often requires approximate agreement decisions among nodes in contrast to fully connected networks the research on data aggregation for partially connected networks is very limited this is due to the complexity of formal proofs and the fact that node may not have global view of the entire network which makes it difficult to attain the convergence properties the complexity of the problem is compounded in the presence of message dropouts faults and orchestrated attacks by exploiting the properties of discrete markov chains this study investigates the data aggregation problem for partially connected networks to obtain the number of rounds of message exchanges needed to reach network convergence the average convergence rate in round of message exchange and the number of rounds required to reach stationary convergence
the purpose of this talk is to provide comprehensive state of the art concerning the evolution of query optimization methods from centralized database systems to data grid systems through parallel distributed and data integration systems for each environment we try to describe synthetically some methods and point out their main characteristics
recently the network attached secure disk nasd model has become more widely used technique for constructing large scale storage systems however the security system proposed for nasd assumes that each client will contact the server to get capability to access one object on server while this approach works well in smaller scale systems in which each file is composed of few objects it fails for large scale systems in which thousands of clients make accesses to single file composed of thousands of objects spread across thousands of disks the file system we are building ceph distributes files across many objects and disks to distribute load and improve reliability in such system the metadata server cluster will sometimes see thousands of open requests for the same file within seconds to address this bottleneck we propose new authentication protocols for object based storage systems in which sequence of fixed size objects comprise file and flash crowds are likely we qualitatively evaluated the security and risks of each protocol and using traces of scientific application compared the overhead of each protocol we found that surprisingly protocol using public key cryptography incurred little extra cost while providing greater security than protocol using only symmetric key cryptography
this paper presents flickr distance which is novel measurement of the relationship between semantic concepts objects scenes in visual domain for each concept collection of images are obtained from flickr based on which the improved latent topic based visual language model is built to capture the visual characteristic of this concept then flickr distance between different concepts is measured by the square root of jensen shannon js divergence between the corresponding visual language models comparing with wordnet flickr distance is able to handle far more concepts existing on the web and it can scale up with the increase of concept vocabularies comparing with google distance which is generated in textual domain flickr distance is more precise for visual domain concepts as it captures the visual relationship between the concepts instead of their co occurrence in text search results besides unlike google distance flickr distance satisfies triangular inequality which makes it more reasonable distance metric both subjective user study and objective evaluation show that flickr distance is more coherent to human perception than google distance we also design several application scenarios such as concept clustering and image annotation to demonstrate the effectiveness of this proposed distance in image related applications
lifetime is very important to wireless sensor networks since most sensors are equipped with non rechargeable batteries therefore energy and delay are critical issues for the research of sensor networks that have limited lifetime due to the uncertainties in execution time of some tasks this paper models each varied execution time as probabilistic random variable with the consideration of applications performance requirements to solve the map mode assignment with probability problem using probabilistic design we propose an optimal algorithm to minimize the total energy consumption while satisfying the timing constraint with guaranteed confidence probability the experimental results show that our approach achieves significant energy saving than previous work for example our algorithm achieves an average improvement of on total energy consumption
several current support systems for travel and tourism are aimed at providing information in personalized manner taking users interests and preferences into account in this vein personalized systems observe users behavior and based thereon make generalizations and predictions about them this article describes user modeling server that offers services to personalized systems with regard to the analysis of user actions the representation of assumptions about the user and the inference of additional assumptions based on domain knowledge and characteristics of similar users the system is open and compliant with major standards allowing it to be easily accessed by clients that need personalization services
we present novel rendering system for defocus blur and lens effects it supports physically based rendering and outperforms previous approaches by involving novel gpu based tracing method our solution achieves more precision than competing real time solutions and our results are mostly indistinguishable from offline rendering our method is also more general and can integrate advanced simulations such as simple geometric lens models enabling various lens aberration effects these latter is crucial for realism but are often employed in artistic contexts too we show that available artistic lenses can be simulated by our method in this spirit our work introduces an intuitive control over depth of field effects the physical basis is crucial as starting point to enable new artistic renderings based on generalized focal surface to emphasize particular elements in the scene while retaining realistic look our real time solution provides realistic as well as plausible expressive results
miniaturization of devices and the ensuing decrease in the threshold voltage has led to substantial increase in the leakage component of the total processor energy consumption relatively simpler issue logic and the presence of large number of function units in the vliw and the clustered vliw architectures attribute large fraction of this leakage energy consumption in the functional units however functional units are not fully utilized in the vliw architectures because of the inherent variations in the ilp of the programs this underutilization is even more pronounced in the context of clustered vliw architectures because of the contentions for the limited number of slow intercluster communication channels which lead to many short idle cyclesin the past some architectural schemes have been proposed to obtain leakage energy bene ts by aggressively exploiting the idleness of functional units however presence of many short idle cycles cause frequent transitions from the active mode to the sleep mode and vice versa and adversely ffects the energy benefits of purely hardware based scheme in this paper we propose and evaluate compiler instruction scheduling algorithm that assist such hardware based scheme in the context of vliw and clustered vliw architectures the proposed scheme exploits the scheduling slacks of instructions to orchestrate the functional unit mapping with the objective of reducing the number of transitions in functional units thereby keeping them off for longer duration the proposed compiler assisted scheme obtains further reduction of energy consumption of functional units with negligible performance degradation over hardware only scheme for vliw architecture the benefits are and in the context of clustered and clustered vliw architecture respectively our test bed uses the trimaran compiler infrastructure
despite well documented advantages attempts to go truly paperless seldom succeed this is principally because computer based paperless systems typically do not support all of the affordances of paper nor the work process that have evolved with paper based systems we suggest that attention to users work environments activities and practices are critical to the success of paperless systems this paper describes the development and effective utilization of software tool for the paperless marking of student assignments which does not require users to compromise on established best practice it includes significant advance in the task management support
clustering decisions frequently arise in business applications such as recommendations concerning products markets human resources etc currently decision makers must analyze diverse algorithms and parameters on an individual basis in order to establish preferences on the decision making issues they face because there is no supportive model or tool which enables comparing different result clusters generated by these algorithms and parameters combinations the multi algorithm voting mav methodology enables not only visualization of results produced by diverse clustering algorithms but also provides quantitative analysis of the results the current research applies mav methodology to the case of recommending new car pricing the findings illustrate the impact and the benefits of such decision support system
master worker paradigm for executing large scale parallel discrete event simulation programs over networkenabled computational resources is proposed and evaluated in contrast to conventional approaches to parallel simulation client server architecture is proposed where clients workers repeatedly download state vectors of logical processes and associated message data from server master perform simulation computations locally at the client and then return the results back to the server this process offers several potential advantages over conventional parallel discrete event simulation systems including support for execution over heterogeneous distributed computing platforms load balancing efficient execution on shared platforms easy addition or removal of client machines during program execution simpler fault tolerance and improved portability prototype implementation called the aurora parallel and distributed simulation system aurora is described the structure and interaction of the aurora components is described results of an experimental performance evaluation are presented detailing primitive timings and application performance on both dedicated and shared computing platforms
most real world databases contain substantial amounts of time referenced or temporal data recent advances in temporal query languages show that such database applications may benefit substantially from built in temporal support in the dbms to achieve this temporal query representation optimization and processing mechanisms must be provided this paper presents foundation for query optimization that integrates conventional and temporal query optimization and is suitable for both conventional dbms architectures and ones where the temporal support is obtained via layer on top of conventional dbms this foundation captures duplicates and ordering for all queries as well as coalescing for temporal queries thus generalizing all existing approaches known to the authors it includes temporally extended relational algebra to which sql and temporal sql queries may be mapped six types of algebraic equivalences concrete query transformation rules that obey different equivalences procedure for determining which types of transformation rules are applicable for optimizing query and query plan enumeration algorithm the presented approach partitions the work required by the database implementor to develop provably correct query optimizer into four stages the database implementor has to specify operations formally design and prove correct appropriate transformation rules that satisfy any of the six equivalence types augment the mechanism that determines when the different types of rules are applicable to ensure that the enumeration algorithm applies the rules correctly and ensure that the mapping generates correct initial query plan
queries over sets of complex elements are performed extracting features from each element which are used in place of the real ones during the processing extracting large number of significant features increases the representative power of the feature vector and improves the query precision however each feature is dimension in the representation space consequently handling more features worsen the dimensionality curse the problem derives from the fact that the elements tends to distribute all over the space and large dimensionality allows them to spread over much broader spaces therefore in high dimensional spaces elements are frequently farther from each other so the distance differences among pairs of elements tends to homogenize when searching for nearest neighbors the first one is usually not close but as long as one is found small increases in the query radius tend to include several others this effect increases the overlap between nodes in access methods indexing the dataset both spatial and metric access methods are sensitive to the problem this paper presents general strategy applicable to metric access methods in general improving the performance of similarity queries in high dimensional spaces our technique applies function that stretches the distances thus close objects become closer and far ones become even farther experiments using the metric access method slim tree show that similarity queries performed in the transformed spaces demands up to less distance calculations less disk access and reduces up to in total time when comparing with the original spaces
differential calculus for first order logic is developed to enforce database integrity formal differentiation of first order sentences is useful in maintaining database integrity since once database constraint is expressed as first order sentence its derivative with respect to transaction provides the necessary and sufficient condition for maintaining integrity the derivative is often much simpler to test than the original constraint since it maintains integrity differentially by assuming integrity before the transaction and testing only for new violations the formal differentiation requires no resolution search but only substitution it is more efficient than resolution based approaches and it provides considerably more general solution than previous substitution based methods since it is valid for all first order sentences and with all transactions involving arbitrary collections of atomic changes to the database it also produces large number of sufficient conditions that are often less strict than those of the previous approaches and it can be extended to accommodate many dynamic constraints
in this paper we propose two languages called future temporal logic ftl and past temporal logic ptl for specifying temporal triggers some examples of trigger conditions that can be specified in our language are the following the value of certain attribute increases by more than in minutes tuple that satisfies certain predicate is added to the database at least minutes before another tuple satisfying different condition is added to the database such triggers are important for monitor and control applicationsin addition to the languages we present algorithms for processing the trigger conditions specified in these languages namely procedures for determining when the trigger conditions are satisfied these methods can be added as temporal component to an existing database management systems preliminary prototype of the temporal component that uses the ftl language has been built on top of sybase running on sun workstations
theoretical and technological progress has revived the interest in the design of services for the support of co located human human communication and collaboration witnessing the start of several large scale projects over the last few years most of these projects focus on meetings and or lecture situations however user centred design and evaluation frameworks for co located communication and collaboration are major concern in this paper we summarise the prevalent approaches towards user centred design and evaluation and we develop two different services in one service participants in small group meeting receive real time feedback about observable properties of the meeting that are directly related to the social dynamics such as individual amount of speaking time or eye gaze patterns in the other service teachers in classroom receive real time feedback about the activities and attention level of participants in the lecture we also propose ways to address the different dimensions that are relevant to the design and evaluation of these services the individual the social and the organisational dimension bringing together methods from different disciplines
the specification of an information system should include description of structural system aspects as well as description of the system behavior in this article we show how this can be achieved by high level petri nets mdash namely the so called nr nets nested relation transition nets in nr nets the structural part is modeled by nested relations and the behavioral part is modeled by novel petri net formalism each place of net represents nested relation scheme and the marking of each place is given as nested relation of the respective type insert and delete operations in nested relational database nf database are expressed by transitions in net these operations may operate not only on whole tuples of given relation but also on ldquo subtuples rdquo of existing tuples the arcs of net are inscribed with so called filter tables which allow together with an optional logical expression as transition inscription conditions to be formulated on the specified sub tuples the occurrence rule for nr net transitions is defined by the operations union intersection and ldquo negative rdquo in lattices of nested relations the structure of an nr net together with the occurrence rule defines classes of possible information system procedures ie sequences of possibly concurrent operations in an information system
tabletop systems are currently being focused on and many applications using these systems are being developed in such tabletop systems how to recognize real objects on the table is an essential and important issue in existing tabletop systems markers have been often used however their black and white pattern which means nothing to humans spoils the appearance of the object we developed transparent markers on liquid crystal display lcd tabletop system by using the polarization features of the lcd and optical lms in particular through experiments with various kinds of optical films we found that two halfwave plates make the markers rotation invariant by using the transparent markers tangible transparent magic lenses tm applications were developed
text sentiment analysis also referred to as emotional polarity computation has become flourishing frontier in the text mining community this paper studies online forums hotspot detection and forecast using sentiment analysis and text mining approaches first we create an algorithm to automatically analyze the emotional polarity of text and to obtain value for each piece of text second this algorithm is combined with means clustering and support vector machine svm to develop unsupervised text mining approach we use the proposed text mining approach to group the forums into various clusters with the center of each representing hotspot forum within the current time span the data sets used in our empirical studies are acquired and formatted from sina sports forums which spans range of different topic forums and posts experimental results demonstrate that svm forecasting achieves highly consistent results with means clustering the top hotspot forums listed by svm forecasting resembles of means clustering results both svm and means achieve the same results for the top hotspot forums of the year
semi supervised support vector machine svm attempts to learn decision boundary that traverses through low data density regions by maximizing the margin over labeled and unlabeled examples traditionally svm is formulated as non convex integer programming problem and is thus difficult to solve in this paper we propose the cutting plane semi supervised support vector machine cutsvm algorithm to solve the svm problem specifically we construct nested sequence of successively tighter relaxations of the original svm problem and each optimization problem in this sequence could be efficiently solved using the constrained concave convex procedure cccp moreover we prove theoretically that the cutsvm algorithm takes time sn to converge with guaranteed accuracy where is the total number of samples in the dataset and is the average number of non zero features ie the sparsity experimental evaluations on several real world datasets show that cutsvm performs better than existing svm methods both in efficiency and accuracy
this paper explores the embodied interactional ways in which people naturally collaborate around and share collections of photographs we employ ethnographic studies of paper based photograph use to consider requirements for distributed collaboration around digital photographs distributed sharing is currently limited to the passing on of photographs to others by email webpages or mobile phones to move beyond this fundamental challenge for photoware consists of developing support for the practical achievement of sharing at distance specifically this entails augmenting the natural production of accounts or photo talk to support the distributed achievement of sharing
in this paper we consider the problem of maximizing wireless network capacity aka one shot scheduling in both the protocol and physical models we give the first distributed algorithms with provable guarantees in the physical model and show how they can be generalized to more complicated metrics and settings in which the physical assumptions are slightly violated we also give the first algorithms in the protocol model that do not assume transmitters can coordinate with their neighbors in the interference graph so every transmitter chooses whether to broadcast based purely on local events our techniques draw heavily from algorithmic game theory and machine learning theory even though our goal is distributed algorithm indeed our main results allow every transmitter to run any algorithm it wants so long as its algorithm has learning theoretic property known as no regret in game theoretic setting
in previous paper we introduced hybrid scalable approach for data gathering and dissemination in sensor networks we called on board data dissemination obdd the approach was validated and implemented on well structured network topologies where virtual boards carry the queries are constructed guided by well defined trajectories in this paper we extend our previous work to include networks with irregular topologies in such networks boards guided by well defined trajectories are no longer enough to construct the dissemination structure we have adapted and modified an underlying topology detection algorithm to fit our protocol the resulting topology based on board data dissemination tobdd efficiently works with any network topology it also maintains the desirable features of obdd although the approach adapts both pull and push strategies it synchronizes both phases to maintain the communications mainly on demand and to prohibit problems that could be inherited from the push strategy both analysis and simulation show that tobdd promises an efficient and scalable paradigm for data gathering and dissemination in sensor networks that reduces the communication cost and leads to load balancing
several recent studies have introduced lightweight versions of java reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety we carry this process step further omitting almost all features of the full language including interfaces and even assignment to obtain small calculus featherweight java for which rigorous proofs are not only possible but easy featherweight java bears similar relation to java as the lambda calculus does to languages such as ml and haskell it offers similar computational feel providing classes methods fields inheritance and dynamic typecasts with semantics closely following java's proof of type safety for featherweight java thus illustrates many of the interesting features of safety proof for the full language while remaining pleasingly compact the minimal syntax typing rules and operational semantics of featherweight java make it handy tool for studying the consequences of extensions and variations as an illustration of its utility in this regard we extend featherweight java with generic classes in the style of gj bracha odersky stoutamire and wadler and give detailed proof of type safety the extended system formalizes for the first time some of the key features of gj
memory analysis techniques have become sophisticated enough to model with high degree of accuracy the manipulation of simple memory structures finite structures single double linked lists and trees however modern programming languages provide extensive library support including wide range of generic collection objects that make use of complex internal data structures while these data structures ensure that the collections are efficient often these representations cannot be effectively modeled by existing methods either due to excessive analysis runtime or due to the inability to represent the required information this paper presents method to represent collections using an abstraction of their semantics the construction of the abstract semantics for the collection objects is done in manner that allows individual elements in the collections to be identified our construction also supports iterators over the collections and is able to model the position of the iterators with respect to the elements in the collection by ordering the contents of the collection based on the iterator position the model can represent notion of progress when iteratively manipulating the contents of collection these features allow strong updates to the individual elements in the collection as well as strong updates over the collections themselves
the paper presents problems pertaining to spatial data mining based on the existing solutions new method of knowledge extraction in the form of spatial association rules and collocations has been worked out and is proposed herein delaunay diagram is used for determining neighborhoods based on the neighborhood notion spatial association rules and collocations are defined novel algorithm for finding spatial rules and collocations has been presented the approach allows eliminating the parameters defining neighborhood of objects thus avoiding multiple test and trial repetitions of the process of mining for various parameter values the presented method has been implemented and tested the results of the experiments have been discussed
given that contiguous reads and writes between cache and disk outperform fragmented reads and writes fragmented reads and writes are forcefully transformed into contiguous reads and writes via proposed matrix stripe cache based contiguity transform msc ct method which employs rule of consistency for data integrity at the block level and rule of performance that ensures no performance degradation msc ct performs for reads and writes both of which are produced by write requests from host as write request from host employs reads for parity update and writes to disks in redundant array of independent disks raid msc ct is compatible with existing disk technologies the proposed implementation in linux kernel delivers peak throughput that is times higher than case without msc ct on representative workloads the results demonstrate that msc ct is extremely simple to implement has low overhead and is ideally suited for raid controllers not only for random writes but also for sequential writes in various realistic scenarios
although unit tests are recognized as an important tool in software development programmers prefer to write code rather than unit tests despite the emergence of tools like junit which automate part of the process unit testing remains time consuming resource intensive and not particularly appealing activitythis paper introduces new development method called contract driven development this development method is based on novel mechanism that extracts test cases from failure producing runs that the programmers trigger it exploits actions that developers perform anyway as part of their normal process of writing code thus it takes the task of writing unit tests off the developers shoulders while still taking advantage of their knowledge of the intended semantics and structure of the code the approach is based on the presence of contracts in code which act as the oracle of the test cases the test cases are extracted completely automatically are run in the background and can easily be maintained over versions the tool implementing this methodology is called cdd and is available both in binary and in source form
as the width of the processor grows complexity of register file rf with multiple ports grows more than linearly and leads to larger register access time and higher power consumption analysis of spec programs reveals that only small portion of the instructions in program in integer and in floating point require both the source operands also when the programs are executed in an wide processor only very few two or less two source instructions are executed in cycle for significant portion of time more than for integer and for floating point leading to significant under utilization of register port bandwidth in this paper we propose novel technique to significantly reduce the number of register ports with very minor modification in the select logic to issue only limited number of two source instructions each cycle this is achieved with no significant impact on processor's overall performance the novelty of the technique is that it is easy to implement and succeeds in reducing the access time power and area of the register file without aggravating these factors in any other logic on the chip with this technique in an wide processor as compared to conventional entry rf with read ports for integer programs register file can be designed with or read ports as these configurations result in instructions per cycle ipc degradation of only and respectively this significantly low degradation in ipc is achieved while reducing the register access time by and respectively and reducing power by and respectively for fp programs register file can be designed with read ports ipc loss less access time and less power or with read ports ipc loss less access time and less power the paper analyzes the performance of all the possible flavors of the proposed technique for register file in both wide and wide processors and presents choice of the performance and register port complexity combination to the designer
in this paper we propose new and general preprocessor algorithm called csroulette which converts any cost insensitive classification algorithms into cost sensitive ones csroulette is based on cost proportional roulette sampling technique called cprs in short csroulette is closely related to costing another cost sensitive meta learning algorithm which is based on rejection sampling unlike rejection sampling which produces smaller samples cprs can generate different size samples to further improve its performance we apply ensemble bagging on cprs the resulting algorithm is called csroulette our experiments show that csroulette outperforms costing and other meta learning methods in most datasets tested in addition we investigate the effect of various sample sizes and conclude that reduced sample sizes as in rejection sampling cannot be compensated by increasing the number of bagging iterations
this paper presents physics based method for creating complex multi character motions from short single character sequences we represent multi character motion synthesis as spacetime optimization problem where constraints represent the desired character interactions we extend standard spacetime optimization with novel timewarp parameterization in order to jointly optimize the motion and the interaction constraints in addition we present an optimization algorithm based on block coordinate descent and continuations that can be used to solve large problems multiple characters usually generate this framework allows us to synthesize multi character motion drastically different from the input motion consequently small set of input motion dataset is sufficient to express wide variety of multi character motions
when web user's underlying information need is not clearly specified from the initial query an effective approach is to diversify the results retrieved for this query in this paper we introduce novel probabilistic framework for web search result diversification which explicitly accounts for the various aspects associated to an underspecified query in particular we diversify document ranking by estimating how well given document satisfies each uncovered aspect and the extent to which different aspects are satisfied by the ranking as whole we thoroughly evaluate our framework in the context of the diversity task of the trec web track moreover we exploit query reformulations provided by three major web search engines wses as means to uncover different query aspects the results attest the effectiveness of our framework when compared to state of the art diversification approaches in the literature additionally by simulating an upper bound query reformulation mechanism from official trec data we draw useful insights regarding the effectiveness of the query reformulations generated by the different wses in promoting diversity
we consider the problem to infer concise document type definition dtd for given set of xml documents problem which basically reduces to learning of concise regular expressions from positive example strings we identify two such classes single occurrence regular expressions sores and chain regular expressions chares both classes capture the far majority of the regular expressions occurring in practical dtds and are succinct by definition we present the algorithm idtd infer dtd that learns sores from strings by first inferring an automaton by known techniques and then translating that automaton to corresponding sore possibly by repairing the automaton when no equivalent sore can be found in the process we introduce novel automaton to regular expression rewrite technique which is of independent interest we show that idtd outperforms existing systems in accuracy conciseness and speed in scenario where only very small amount of xml data is available for instance when generated by web service requests or by answers to queries idtd produces regular expressions which are too specific therefore we introduce novel learning algorithm crx that directly infers chares which form subclass of sores without going through an automaton representation we show that crx performs very well within its target class on very small data sets finally we discuss incremental computation noise numerical predicates and the generation of xml schemas
in this paper we investigate future topics and challenges of interaction and user experience in multimedia we bring together different perspectives from overlapping fields of research such as multimedia human computer interaction information retrieval networked multimedia and creative arts based on potential intersections we define three application domains to be investigated further as they create high demand and good prospect for long lasting developments in the future these application domains are media working environments media enter edutainment and social media engagement each application domain is analyzed along five dimensions namely information quality presentation quality ambience interactivity and user expectations based on this analysis we identify the most pressing research questions and key challenges for each area finally we advocate user centered approach to tackle these challenges and questions in order to develop relevant multimedia applications that best meet the users expectations
this paper presents the design and implementation of compiler that translates programs written in type safe subset of the programming language into highly optimized dec alpha assembly language programs and certifier that automatically checks the type safety and memory safety of any assembly language program produced by the compiler the result of the certifier is either formal proof of type safety or counterexample pointing to potential violation of the type system by the target program the ensemble of the compiler and the certifier is called certifying compilerseveral advantages of certifying compilation over previous approaches can be claimed the notion of certifying compiler is significantly easier to employ than formal compiler verification in part because it is generally easier to verify the correctness of the result of computation than to prove the correctness of the computation itself also the approach can be applied even to highly optimizing compilers as demonstrated by the fact that our compiler generates target code for range of realistic programs which is competitive with both the cc and gcc compilers with all optimizations enabled the certifier also drastically improves the effectiveness of compiler testing because for each test case it statically signals compilation errors that might otherwise require many executions to detect finally this approach is practical way to produce the safety proofs for proof carrying code system and thus may be useful in system for safe mobile code
this paper describes the development and use of set of design representations of moving bodies in the design of bystander multi user interactive immersive artwork built on video based motion sensing technology we extended the traditional user centred design tools of personas and scenarios to explicitly address human movement characteristics embedded in social interaction set of corresponding movement schemas in labanotation was constructed to visually represent the spatial and social interaction of multiple users over time together these three design representations of moving bodies were used to enable the design team to work with the aspects of human movement relevant to bystander and to ensure that the system could respond in coherent and robust manner to the shifting configurations of visitors in the space they also supported two experiential methods of design reflection in action enactment and immersion that were vital for grounding designers understandings of the specific interactive nature of the work in their own sensing feeling and moving bodies
establishing pairwise keys for each pair of neighboring sensors is the first concern in securing communication in sensor networks this task is challenging because resources are limited several random key predistribution schemes have been proposed but they are appropriate only when sensors are uniformly distributed with high density these schemes also suffer from dramatic degradation of security when the number of compromised sensors exceeds threshold in this paper we present group based key predistribution scheme gke which enables any pair of neighboring sensors to establish unique pairwise key regardless of sensor density or distribution since pairwise keys are unique security in gke degrades gracefully as the number of compromised nodes increases in addition gke is very efficient since it requires only localized communication to establish pairwise keys thus significantly reducing the communication overhead our security analysis and performance evaluation illustrate the superiority of gke in terms of resilience connectivity communication overhead and memory requirement
we present particle based method for viscoelastic fluids simulation in the method based on the traditional navier stokes equation an additional elastic stress term is introduced to achieve viscoelastic flow behaviors which have both fluid and solid features benefiting from the lagrangian nature of smoothed particle hydrodynamics large flow deformation can be handled more easily and naturally and also by changing the viscosity and elastic stress coefficient of the particles according to the temperature variation the melting and flowing phenomena such as lava flow and wax melting are achieved the temperature evolution is determined with the heat diffusion equation the method is effective and efficient and has good controllability different kinds of viscoelastic fluid behaviors can be obtained easily by adjusting the very few experimental parameters
to address business requirements and to survive in competing markets companies or open source organizations often have to release different versions of their projects in different languages manually migrating projects from one language to another such as from java to is tedious and error prone task to reduce manual effort or human errors tools can be developed for automatic migration of projects from one language to another however these tools require the knowledge of how application programming interfaces apis of one language are mapped to apis of the other language referred to as api mapping relations in this paper we propose novel approach called mam mining api mapping that mines api mapping relations from one language to another using api client code mam accepts set of projects each with two versions in two languages and mines api mapping relations between those two languages based on how apis are used by the two versions these mined api mapping relations assist in migration of projects from one language to another we implemented tool and conducted two evaluations to show the effectiveness of mam the results show that our tool mines unique mapping relations of apis between java and with more than accuracy the results also show that mined api mapping relations help reduce compilation errors and defects during migration of projects with an existing migration tool called javacsharp the reduction in compilation errors and defects is due to our new mined mapping relations that are not available with the existing migration tool
this research reports on study of the interplay between multi tasking and collaborative work we conducted an ethnographic study in two different companies where we observed the experiences and practices of thirty six information workers we observed that people continually switch between different collaborative contexts throughout their day we refer to activities that are thematically connected as working spheres we discovered that to multi task and cope with the resulting fragmentation of their work individuals constantly renew overviews of their working spheres they strategize how to manage transitions between contexts and they maintain flexible foci among their different working spheres we argue that system design to support collaborative work should include the notion that people are involved in multiple collaborations with contexts that change continually system design must take into account these continual changes people switch between local and global perspectives of their working spheres have varying states of awareness of their different working spheres and are continually managing transitions between contexts due to interruptions
the growing cost of tuning and managing computer systems is leading to out sourcing of commercial services to hosting centers these centers provision thousands of dense servers within relatively small real estate in order to host the applications services of different customers who may have been assured by service level agreement sla power consumption of these servers is becoming serious concern in the design and operation of the hosting centers the effects of high power consumption manifest not only in the costs spent in designing effective cooling systems to ward off the generated heat but in the cost of electricity consumption itself it is crucial to deploy power management strategies in these hosting centers to lower these costs towards enhancing profitability at the same time techniques for power management that include shutting down these servers and or modulating their operational speed can impact the ability of the hosting center to meet slas in addition repeated on off cycles can increase the wear and tear of server components incurring costs for their procurement and replacement this paper presents formalism to this problem and proposes three new online solution strategies based on steady state queuing analysis feedback control theory and hybrid mechanism borrowing ideas from these two using real web server traces we show that these solutions are more adaptive to workload behavior when performing server provisioning and speed control than earlier heuristics towards minimizing operational costs while meeting the slas
in this article we present synthesis technique for generating schedulers for real time systems the aim of the scheduler is to ensure via restricting the general behaviour that the real time system satisfies the specification the real time system and the specification are described as alur dill timed automata while the synthesised scheduler is type of timed trajectory automaton this allows us to perform the synthesis without incurring the cost of constructing timed regions we also note simple constraint that the specification has to satisfy for this technique to be useful
one important trend in today's microprocessor architectures is the increase in size of the processor caches these caches also tend to be set associative as technology scales process variations are expected to increase the fault rates of the sram cells that compose such caches as an important component of the processor the parametric yield of sram cells is crucial to the overall performance and yield of the microchip in this article we propose microarchitectural solution called the buddy cache that permits large set associative caches to tolerate faults in sram cells due to process variations in essence instead of disabling faulty cache block in set as is the current practice it is paired with another faulty cache block in the same set mdash the buddy although both cache blocks are faulty if the faults of the two blocks do not overlap then instead of losing two blocks buddying will yield functional block from the nonfaulty portions of the two blocks we found that with buddying caches can better mitigate the negative impacts of process variations on performance and yield gracefully downgrading performance as opposed to catastrophic failure we will describe the details of the buddy cache and give insights as to why it is both more performance and yield resilient to faults
in this paper we propose novel application specific demand paging mechanism for low end embedded systems with flash memory as secondary storage these systems are not equipped with virtual memory small memory space called an execution buffer is allocated to page an application an application specific page manager manages the buffer the manager is generated by compiler post pass and combined with the application image our compiler post pass analyzes the elf executable image of an application and transforms function call return instructions into calls to the page manager as result each function of the code can be loaded into memory on demand at run time to minimize the overhead of demand paging code clustering algorithms are also presented we evaluate our techniques with five embedded applications we show that our approach can reduce the code memory size by on average with reasonable performance degradation and energy consumption more on average for low end embedded systems
overlapping split phase large latency operations with computations is standard technique for improving performance on modern architectures in this paper we present general interprocedural technique for overlapping such accesses with computation we have developed an interprocedural balanced code placement ibcp framework which performs analysis on arbitrary recursive procedures and arbitrary control flow and replaces synchronous operations with balanced pair of asynchronous operations we have evaluated this scheme in the context of overlapping operations with computation we demonstrate how this analysis is useful for applications which perform frequent and large accesses to disks including applications which snapshot or checkpoint their computations or out of core applications
in an effort to make robust traffic classification more accessible to human operators we present visualization techniques for network traffic our techniques are based solely on network information that remains intact after application layer encryption and so offer way to visualize traffic in the dark our visualizations clearly illustrate the differences between common application protocols both in their transient ie time dependent and steady state behavior we show how these visualizations can be used to assist human operator to recognize application protocols in unidentified traffic and to verify the results of an automated classifier via visual inspection in particular our preliminary results show that we can visually scan almost connections in less than one hour and correctly identify known application behaviors moreover using visualizations together with an automated comparison technique based on dynamic time warping of the motifs we can rapidly develop accurate recognizers for new or previously unknown applications
mobile devices used in educational settings are usually employed within collaborative learning activity in which learning takes place in the form of social interactions between team members while performing shared task we introduce mobitop mobile tagging of objects and people geospatial digital library system which allows users to contribute and share multimedia annotations via mobile devices key feature of mobitop that is well suited for collaborative learning is that annotations are hierarchical allowing annotations to be annotated by other users to an arbitrary depth group of student teachers involved in an inquiry based learning activity in geography were instructed to identify rock types and associated landforms by collaborating with each other using the mobitop system the outcome of the study and its implications are reported in this paper
we consider the setting of device that obtains its energy from battery and some regenerative source such as solar cell we consider the speed scaling problem of scheduling collection of tasks with release times deadlines and sizes so as to minimize the energy recharge rate of the regenerative source this is the first theoretical investigation of speed scaling for devices with regenerative energy source we show that the problem can be expressed as polynomial sized convex program we show that using the kkt conditions one can obtain an efficient algorithm to verify the optimality of schedule we show that the energy optimal yds schedule is approximate with respect to the recharge rate we show that the online algorithm bkp is competitive with respect to recharge rate
the pie segment slider is novel parameter control interface combining the advantages of tangible input with the customizability of graphical interface representation the physical part of the interface consists of round touchpad which serves as an appropriate sensor for manipulating ring shaped sliders arranged around virtual object the novel interface concept allows to shift substantial amount of interaction task time from task preparation to its exploratory execution our user study compared the task performance of the novel interface to common touchpad operated gui and examined the task sequences of both solutions the results confirm the benefits of exploiting tangible input and proprioception for operating graphical user interface elements
time series estimation techniques are usually employed in biomedical research to derive variables less accessible from set of related and more accessible variables these techniques are traditionally built from systems modeling approaches including simulation blind decovolution and state estimation in this work we define target time series tts and its related time series rts as the output and input of time series estimation process respectively we then propose novel data mining framework for time series estimation when tts and rts represent different sets of observed variables from the same dynamic system this is made possible by mining database of instances of tts its simultaneously recorded rts and the input output dynamic models between them the key mining strategy is to formulate mapping function for each tts rts pair in the database that translates feature vector extracted from rts to the dissimilarity between true tts and its estimate from the dynamic model associated with the same tts rts pair at run time feature vector is extracted from an inquiry rts and supplied to the mapping function associated with each tts rts pair to calculate dissimilarity measure an optimal tts rts pair is then selected by analyzing these dissimilarity measures the associated input output model of the selected tts rts pair is then used to simulate the tts given the inquiry rts as an input an exemplary implementation was built to address biomedical problem of noninvasive intracranial pressure assessment the performance of the proposed method was superior to that of simple training free approach of finding the optimal tts rts pair by conventional similarity based search on rts features
for specific set of features chosen for representing images the performance of content based image retrieval cbir system depends critically on the similarity or dissimilarity measure used instead of manually choosing distance function in advance more promising approach is to learn good distance function from data automatically in this paper we propose kernel approach to improve the retrieval performance of cbir systems by learning distance metric based on pairwise constraints between images as supervisory information unlike most existing metric learning methods which learn mahalanobis metric corresponding to performing linear transformation in the original image space we define the transformation in the kernel induced feature space which is nonlinearly related to the image space experiments performed on two real world image databases show that our method not only improves the retrieval performance of euclidean distance without distance learning but it also outperforms other distance learning methods significantly due to its higher flexibility in metric learning
large amounts of information can be overwhelming and costly process especially when transmitting data over network typical modern geographical information system gis brings all types of data together based on the geographic component of the data and provides simple point and click query capabilities well as complex analysis tools querying geographical information system however can be prohibitively expensive due to the large amounts of data which may need to be processed since the use of gis technology has grown dramatically in the past few years there is now need more than ever to provide users with the fastest and least expensive query capabilities especially since an approximated of data stored in corporate databases has geographical component however not every application requires the same high quality data for its processing in this paper we address the issues of reducing the cost and response time of gis queries by pre aggregating data by compromising the data accuracy and precision we present computational issues in generation of multi level resolutions of spatial data and show that the problem of finding the best approximation for the given region and real value function on this region under predictable error in general is np complete
transactional memory aims to provide programming model that makes parallel programming easier hardware implementations of transactional memory htm suffer from fewer overheads than implementations in software and refinements in conflict management strategies for htm allow for even larger improvements in particular lazy conflict management has been shown to deliver better performance but it has hitherto required complex protocols and implementations in this paper we show new scalable htm architecture that performs comparably to the state of the art and can be implemented by minor modifications to the mesi protocol rather than re engineering it from the ground up our approach detects conflicts eagerly while transaction is running but defers the resolution lazily until commit time we evaluate this eager lazy system eazyhtm by comparing it with the scalable tcc like approach and system employing ideal lazy conflict management with zero cycle transaction validation and fully parallel commits we show that eazyhtm performs on average faster than scalable tcc in addition eazyhtm has fast commits and aborts can commit in parallel even if there is only one directory present and does not suffer from cascading waits
single user interactive computer applications are pervasive in our daily lives and work leveraging single user applications for multi user collaboration has the potential to significantly increase the availability and improve the usability of collaborative applications in this paper we report an innovative transparent adaptation approach for this purpose the basic idea is to adapt the single user application programming interface to the data and operational models of the underlying collaboration supporting technique namely operational transformation distinctive features of this approach include application transparency it does not require access to the source code of the single user application unconstrained collaboration it supports concurrent and free interaction and collaboration among multiple users and reusable collaborative software components collaborative software components developed with this approach can be reused in adapting wide range of single user applications this approach has been applied to transparently convert ms word into real time collaborative word processor called coword which supports multiple users to view and edit any objects in the same word document at the same time over the internet the generality of this approach has been tested by re applying it to convert ms powerpoint into copowerpoint
we show that naming the existence of distinct ids known to all is hidden but necessary assumption of herlihy's universality result for consensus we then show in very precise sense that naming is harder than consensus and bring to the surface some relevant differences existing between popular shared memory models
we propose novel partition path based ppb grouping strategy to store compressed xml data in stream of blocks in addition we employ minimal indexing scheme called block statistic signature bss on the compressed data which is simple but effective technique to support evaluation of selection and aggregate xpath queries of the compressed data we present formal analysis and empirical study of these techniques the bss indexing is first extended into effective cluster statistic signature css and multiple cluster statistic signature mss indexing by establishing more layers of indexes we analyze how the response time is affected by various parameters involved in our compression strategy such as the data stream block size the number of cluster layers and the query selectivity we also gain further insight about the compression and querying performance by studying the optimal block size in stream which leads to the minimum processing cost for queries the cost model analysis provides solid foundation for predicting the querying performance finally we demonstrate that our ppb grouping and indexing strategies are not only efficient enough to support path based selection and aggregate queries of the compressed xml data but they also require relatively low computation time and storage space when compared with other state of the art compression strategies
logic programs under answer set semantics constitute an important tool for declarative problem solving in recent years two research issues received growing attention on the one hand concepts like loops and elementary sets have been proposed in order to extend clark's completion for computing answer sets of logic programs by means of propositional logic on the other hand different concepts of program equivalence like strong and uniform equivalence have been studied in the context of program optimization and modular programming in this paper we bring these two lines of research together and provide alternative characterizations for different conceptions of equivalence in terms of unfounded sets along with the related concepts of loops and elementary sets our results yield new insights into the model theory of equivalence checking we further exploit these characterizations to develop novel encodings of program equivalence in terms of standard and quantified propositional logic respectively
the acoustic ensbox is an embedded platform which enables practical distributed acoustic sensing by providing integrated hardware and software support in single platform it provides highly accurate acoustic self calibration system which eliminates the need for manual surveying of node reference positions in this paper we present an acoustic laptop that enables distributed acoustic research through the use of less resource constrained and more readily available platform it runs exactly the same software and uses the same sensor hardware as the acoustic ensbox but replaces the embedded computing platform with standard laptop we describe the advantages of using the acoustic laptop as rich prototyping platform for acoustic source localization and mote class node localization applications the acoustic laptop is not intended to replace the acoustic ensbox but to compliment it by providing an easily replicated prototyping platform that is extensible and resource rich and suitable for attended pilot deployments we show that the benefits gained by laptop's extra resources enable intensive signal processing in real time without optimization this enables on line interactive experimentation with algorithms such as approximated maximum likelihood applications developed using the acoustic laptop can subsequently be run on the more deployable acoustic ensbox platform unmodified apart from performance optimizations
in this paper we try to conclude what kind of computer architecture is efficient for executing sequential problems and what kind of an architecture is efficient for executing parallel problems from the processor architect's point of view for that purpose we analytically evaluate the performance of eight general purpose processor architectures representing widely both commercial and scientific processor designs in both single processor and multiprocessor setups the results are interesting the most efficient architecture for sequential problems is two level pipelined vliw very long instruction word architecture with few parallel functional units the most efficient architecture for parallel problems is deeply inter thread superpipelined architecture in which functional units are chained thus designing computer for efficient sequential computation leads to very different architecture than designing one for efficient parallel computation and there exists no single optimal architecture for general purpose computation
visitors enter website through variety of means including web searches links from other sites and personal bookmarks in some cases the first page loaded satisfies the visitor's needs and no additional navigation is necessary in other cases however the visitor is better served by content located elsewhere on the site found by navigating links if the path between user's current location and his eventual goal is circuitous then the user may never reach that goal or will have to exert considerable effort to reach it by mining site access logs we can draw conclusions of the form users who load page are likely to later load page if there is no direct link from to then it is advantageous to provide one the process of providing links to users eventual goals while skipping over the in between pages is called shortcutting existing algorithms for shortcutting require substantial offline training which make them unable to adapt when access patterns change between training sessions we present improved online algorithms for shortcut link selection that are based on novel analogy drawn between shortcutting and caching in the same way that cache algorithms predict which memory pages will be accessed in the future our algorithms predict which web pages will be accessed in the future our algorithms are very efficient and are able to consider accesses over long period of time but give extra weight to recent accesses our experiments show significant improvement in the utility of shortcut links selected by our algorithm as compared to those selected by existing algorithms
mobile computing systems should be self managed to simplify operation and maintenance plus meet user expectation with respect to quality of service qos when architecting self managed mobile computing systems one must take holistic view on both qos management and the entities in the mobile environment this paper presents novel model that includes both resources and context elements to illustrate the usefulness of the model it is applied to video streaming application by modelling context elements and resources in the environment specifying context dependencies and qos characteristics of the application and designing weakly integrated resource and context managers we describe middleware that uses the developed managers when evaluating context dependencies and predict offered qos of alternative implementations of the application in order to select the one that can operate in the current environment and that best satisfies given user preferences
the construction of taxonomies is considered as the first step for structuring domain knowledge many methodologies have been developed in the past for building taxonomies from classical information repositories such as dictionaries databases or domain text however in the last years scientists have started to consider the web as valuable repository of knowledge in this paper we present novel approach especially adapted to the web environment for composing taxonomies in an automatic and unsupervised way it uses combination of different types of linguistic patterns for hyponymy extraction and carefully designed statistical measures to infer information relevance the learning performance of the different linguistic patterns and statistical scores considered is carefully studied and evaluated in order to design method that maximizes the quality of the results our proposal is also evaluated for several well distinguished domains offering in all cases reliable taxonomies considering precision and recall
applications like multimedia retrieval require efficient support for similarity search on large data collections yet nearest neighbor search is difficult problem in high dimensional spaces rendering efficient applications hard to realize index structures degrade rapidly with increasing dimensionality while sequential search is not an attractive solution for repositories with millions of objects this paper approaches the problem from different angle solution is sought in an unconventional storage scheme that opens up new range of techniques for processing nn queries especially suited for high dimensional spaces the suggested physical database design accommodates well novel variant of branch and bound search that reduces the high dimensional space quickly to small candidate set the paper provides insight in applying this idea to nn search using two similarity metrics commonly encountered in image database applications and discusses techniques for its implementation in relational database systems the effectiveness of the proposed method is evaluated empirically on both real and synthetic data sets reporting the significant improvements in response time yielded
clock network power in field programmable gate arrays fpgas is considered and two complementary approaches for clock power reduction in the xilinx virtex fpga are presented the approaches are unique in that they leverage specific architectural aspects of virtex to achieve reductions in dynamic power consumed by the clock network the first approach comprises placement based technique to reduce interconnect resource usage on the clock network thereby reducing capacitance and power up to the second approach borrows the clock gating notion from the asic domain and applies it to fpgas clock enable signals on flip flops are selectively migrated to use the dedicated clock enable available on the fpga's built in clock network leading to reduced toggling on the clock interconnect and lower power up to power reductions are achieved without any performance penalty on average
the creation of realistic virtual human heads is challenging problem in computer graphics due to the diversity and individuality of the human being in this paper an easy to use geometry optimized virtual human head is presented by using uniform two step refinement operator the virtual model has natural multiresolution structure in the coarsest level the model is represented by control mesh which serves as the anatomical structure of the head to achieve fine geometric details over multi level detail set is assigned to the corresponding control vertices in with the aid of uniquely defined local frame field we show in this paper that with carefully designed control mesh the presented virtual model not only captures the physical characteristics of the head but also is optimal in the geometric sense the contributions of this paper also include that diverse applications of the presented virtual model are presented showing that the model deformation is smooth and natural and the presented virtual model is easy to use
while simulationists devise ever more efficient simulation algorithms for specific applications and infrastructures the problem of automatically selecting the most appropriate one for given problem has received little attention so far one reason for this is the overwhelming amount of performance data that has to be analyzed for deriving suitable selection mechanisms we address this problem with framework for data mining on simulation performance data which enables the evaluation of various data mining methods in this context such an evaluation is essential as there is no best data mining algorithm for all kinds of simulation performance data once an effective data mining approach has been identified for specific class of problems its results can be used to select efficient algorithms for future simulation problems this paper covers the components of the framework the integration of external tools and the re formulation of the algorithm selection problem from data mining perspective basic data mining strategies for algorithm selection are outlined and sample algorithm selection problem from computational biology is presented
major performance bottleneck for database systems is the memory hierarchy the performance of the memory hierarchy is directly related to how the content of disk pages maps to the cache lines ie to the organization of data within disk page called the page layout the prevalent page layout in database systems is the ary storage model nsm as demonstrated in this paper using nsm for temporal data deteriorates memory hierarchy performance for query intensive workloads this paper proposes two cacheconscious read optimized page layouts for temporal data experiments show that the proposed page layouts are substantially faster than nsm
in modern software development regression tests are used to confirm the fundamental functionalities of an edited program and to assure the code quality difficulties occur when testing reveals unexpected behaviors which indicate potential defects introduced by the edit however the changes that caused the failure are not always easy to find we propose heuristic that ranks method changes that might have affected failed test indicating the likelihood that they may have contributed to test failure our heuristic is based on the calling structure of the failed test eg the number of ancestors and descendents of method in the test's call graph whether the caller or callee was changed etc we evaluated the effectiveness of the heuristic in pairs of edited versions in the eclipse jdt core plug in using the test suite from its compiler tests plug in our results indicate that when failure is caused by single method change our heuristic ranked the failure inducing change as number or number of all the method changes in of the delegate tests ie representatives of all failing tests even when the failure is caused by some combination of the changes rather than single change our heuristic still helps
within the area of general purpose fine grained subjectivity analysis opinion topic identification has to date received little attention due to both the difficulty of the task and the lack of appropriately annotated resources in this paper we provide an operational definition of opinion topic and present an algorithm for opinion topic identification that following our new definition treats the task as problem in topic coreference resolution we develop methodology for the manual annotation of opinion topics and use it to annotate topic information for portion of an existing general purpose opinion corpus in experiments using the corpus our topic identification approach statistically significantly outperforms several non trivial baselines according to three evaluation measures
we study the representation and manipulation of geospatial information in database management system dbms the geospatial data model that we use as basis hinges on complex object model whose set and tuple constructors make it efficient for defining not only collections of geographic objects but also relationships among these objects in addition it allows easy manipulation of nonbasic types such as spatial data types we investigate the mapping of our reference model onto major commercial dbms models namely relational model extended to abstract data types adt and an object oriented model our analysis shows the strengths and limits of the two model types for handling highly structured data with spatial components
data integration is the problem of combining data residing at different sources and providing the user with unified view of these data one of the critical issues of data integration is the detection of similar entities based on the content this complexity is due to three factors the data type of the databases are heterogenous the schema of databases are unfamiliar and heterogenous as well and the amount of records is voluminous and time consuming to analyze as solution to these problems we extend our work in another of our papers by introducing new measure to handle heterogenous textual and numerical data type for coincident meaning extraction firstly to in order accommodate the heterogeneous data types we propose new weight called bin frequency inverse document bin frequency bf idbf for effective heterogeneous data pre processing and classification by unified vectorization secondly in order to handle the unfamiliar data structure we use the unsupervised algorithm self organizing map finally to help the user to explore and browse the semantically similar entities among the copious amount of data we use som based visualization tool to map the database tables based on their semantical content
this paper proposes and studies distributed cache management approach through page level data to cache slice mapping in future processor chip comprising many cores cache management is crucial multicore processor design aspect to overcome non uniform cache access latency for high program performance and to reduce on chip network traffic and related power consumption unlike previously studied pure hardware based private and shared cache designs the proposed os microarchitecture approach allows mimicking wide spectrum of caching policies without complex hardware support moreover processors and cache slices can be isolated from each other without hardware modifications resulting in improved chip reliability characteristics we discuss the key design issues and implementation strategies of the proposed approach and present an experimental result showing the promise of it
the one dimensional decomposition of nonuniform workload arrays with optimal load balancing is investigated the problem has been studied in the literature as the chains on chains partitioning problem despite the rich literature on exact algorithms heuristics are still used in parallel computing community with the hope of good decompositions and the myth of exact algorithms being hard to implement and not runtime efficient we show that exact algorithms yield significant improvements in load balance over heuristics with negligible overhead detailed pseudocodes of the proposed algorithms are provided for reproducibility we start with literature review and propose improvements and efficient implementation tips for these algorithms we also introduce novel algorithms that are asymptotically and runtime efficient our experiments on sparse matrix and direct volume rendering datasets verify that balance can be significantly improved by using exact algorithms the proposed exact algorithms are times faster than single sparse matrix vector multiplication for way decompositions on the average we conclude that exact algorithms with proposed efficient implementations can effectively replace heuristics
an efficient index structure for complex multi dimensional objects is one of the most challenging requirements in non traditional applications such as geographic information systems computer aided design and multimedia databases in this paper we first propose main memory data structure for complex multi dimensional objects then we present an extension of the existing multi dimensional index structure among existing multi dimensional index structures the popular ast tree is selected the ast tree is coupled with the main memory data structure to improve the performance of spatial query processing an analytical model is developed for our index structure experimental results show that the analytical model is accurate the relative error being below the performance of our index structure is compared with that of state of the art index structure by experimental measurements our index structure outperforms the state of the art index structure due to its ability to reduce large amount of storage
in this paper we propose measures for compressed data structures in which space usage is measured in data aware manner in particular we consider the fundamental dictionary problem on set data where the task is to construct data structure for representing set of items out of universe and supporting various queries on we use well known data aware measure for set data called gap to bound the space of our data structures we describe novel dictionary structure that requires gap nlog logn nloglog bits under the ram model our dictionary supports membership rank and predecessor queries in nearly optimal time matching the time bound of andersson and thorup's predecessor structure andersson thorup tight er worst case bounds on dynamic searching and priority queues in acm symposium on theory of computing stoc while simultaneously improving upon their space usage we support select queries even faster in loglogn time our dictionary structure uses exactly gap bits in the leading term ie the constant factor is and answers queries in near optimal time when seen from the worst case perspective we present the first nlog bit dictionary structure that supports these queries in near optimal time under the ram model we also build dictionary which requires the same space and supports membership select and partial rank queries even more quickly in loglogn time we go on to show that for many real world datasets data aware methods lead to worthwhile compression over combinatorial methods to the best of our knowledge these are the first results that achieve data aware space usage and retain near optimal time
topological relationships like overlap inside meet and disjoint uniquely characterize the relative position between objects in space for long time they have been focus of interdisciplinary research as in artificial intelligence cognitive science linguistics robotics and spatial reasoning especially as predicates they support the design of suitable query languages for spatial data retrieval and analysis in spatial database systems and geographical information systems while to large extent conceptual aspects of topological predicates like their definition and reasoning with them as well as strategies for avoiding unnecessary or repetitive predicate executions like predicate migration and spatial index structures have been emphasized the development of robust and efficient implementation techniques for them has been largely neglected especially the recent design of topological predicates for all combinations of complex spatial data types has resulted in large increase of their numbers and stressed the importance of their efficient implementation the goal of this article is to develop correct and efficient implementation techniques of topological predicates for all combinations of complex spatial data types including two dimensional point line and region objects as they have been specified by different authors and in different commercial and public domain software packages our solution consists of two phases in the exploration phase for given scene of two spatial objects all topological events like intersection and meeting situations are summarized in two precisely defined topological feature vectors one for each argument object of topological predicate whose specifications are characteristic and unique for each combination of spatial data types these vectors serve as input for the evaluation phase which analyzes the topological events and determines the boolean result of topological predicate predicate verification or the kind of topological predicate predicate determination by formally defined method called nine intersection matrix characterization besides this general evaluation method the article presents an optimized method for predicate verification called matrix thinning and an optimized method for predicate determination called minimum cost decision tree the methods presented in this article are applicable to all known complete collections of mutually exclusive topological predicates that are formally based on the well known nine intersection model
voltage islanding technique in network on chip noc can significantly reduce the computational energy consumption by scaling down the voltage levels of the processing elements pes this reduction in energy consumption comes at the cost of the energy consumption of the level shifters between voltage islands moreover from physical design perspective it is desirable to have limited number of voltage islands considering voltage islanding during mapping of the pes to the noc routers can significantly reduce both the computational and the level shifter energy consumptions and the communication energy consumption on the noc links in this paper we formulate the problem as an optimization problem with an objective of minimizing the overall energy consumption constrained by the performance in terms of delay and the maximum number of voltage islands we provide the optimal solution to our problem using mixed integer linear program milp formulation we also propose heuristic based on random greedy selection to solve the problem experimental results using es benchmark applications and some real applications show that the heuristic finds near optimal solution in almost all cases in very small fraction of the time required to achieve the optimal solution
we extend controlled query evaluation cqe an inference control method to enforce confidentiality in static information systems under queries to updatable databases within the framework of the lying approach to cqe we study user update requests that have to be translated into new database state in order to avoid dangerous inferences some such updates have to be denied even though the new database instance would be compatible with set of integrity constraints in contrast some other updates leading to an incompatible instance should not be denied we design control method to resolve this seemingly paradoxical situation and then prove that the general security definitions of cqe and other properties linked to user updates hold
the ambient calculus is calculus of computation that allows active processes to move between sites we present an analysis inspired by state of the art pointer analyses that safety and accurately predicts which processes may turn up at what sites during the execution of composite system the analysis models sets of processes by sets of regular tree grammars enhanced with context dependent counts and it obtains its precision by combining powerful redex materialisation with strong redex reduction in the manner of the strong updates performed in pointer analyses the underlying ideas are flexible and scale up to general tree structures admitting powerful restructuring operations
the web fosters the creation of communities by offering users wide array of social software tools while the success of these tools is based on their ability to support different interaction patterns among users by imposing as few limitations as possible the communities they support are not free of rules just think about the posting rules in community forum or the editing rules in thematic wiki in this paper we propose framework for the sharing of best community practices in the form of potentially rule based annotation layer that can be integrated with existing web community tools with specific focus on wikis this solution is characterized by minimal intrusiveness and plays nicely within the open spirit of the web by providing users with behavioral hints rather than by enforcing the strict adherence to set of rules
estimating the global data distribution in peer to peer pp networks is an important issue and has not yet been well addressed it can benefit many pp applications such as load balancing analysis query processing data mining and so on in this paper we propose novel algorithm which is based on compact multi dimensional histogram information to achieve high estimation accuracy with low estimation cost maintaining data distribution in multi dimensional histogram which is spread among peers without overlapping and each part of which is further condensed by set of discrete cosine transform coefficients each peer is capable to hierarchically accumulate the compact information to the entire histogram by information exchange and consequently estimates the global data density with accuracy and efficiency algorithms on discrete cosine transform coefficients hierarchically accumulating as well as density estimation error are introduced with detailed theoretical analysis and proof our extensive performance study confirms the effectiveness and efficiency of our methods on density estimation in dynamic pp networks
we demonstrate real time simulation system capable of automatically balancing standing character while at the same time tracking reference motion and responding to external perturbations the system is general to non human morphologies and results in natural balancing motions employing the entire body for example wind milling our novel balance routine seeks to control the linear and angular momenta of the character we demonstrate how momentum is related to the center of mass and center of pressure of the character and derive control rules to change these centers for balance the desired momentum changes are reconciled with the objective of tracking the reference motion through an optimization routine which produces target joint accelerations hybrid inverse forward dynamics algorithm determines joint torques based on these joint accelerations and the ground reaction forces finally the joint torques are applied to the free standing character simulation we demonstrate results for following both motion capture and keyframe data as well as both human and non human morphologies in presence of variety of conditions and disturbances
static program checking tools can find many serious bugs in software but due to analysis limitations they also frequently emit false error reports such false positives can easily render the error checker useless by hiding real errors amidst the false effective error report ranking schemes mitigate the problem of false positives by suppressing them during the report inspection process in this way ranking techniques provide complementary method to increasing the precision of the analysis results of checking tool weakness of previous ranking schemes however is that they produce static rankings that do not adapt as reports are inspected ignoring useful correlations amongst reports this paper addresses this weakness with two main contributions first we observe that both bugs and false positives frequently cluster by code locality we analyze clustering behavior in historical bug data from two large systems and show how clustering can be exploited to greatly improve error report ranking second we present general probabilistic technique for error ranking that exploits correlation behavior amongst reports and incorporates user feedback into the ranking process in our results we observe factor of improvement over randomized ranking for error reports emitted by both intra procedural and inter procedural analysis tools
we present framework for the formal verification of abstract state machine asm designs using the multiway decision graphs mdg tool asm is state based language for describing transition systems mdg provides symbolic representation of transition systems with support of abstract sorts and functions we implemented transformation tool that automatically generates mdg models from asm specifications then formal verification techniques provided by the mdg tool such as model checking or equivalence checking can be applied on the generated models we illustrate this work with the case study of an atm switch controller in which behavior and structure were specified in asm and using our asm mdg facility are successfully verified with the mdg tool
standard pattern discovery techniques such as association rules suffer an extreme risk of finding very large numbers of spurious patterns for many knowledge discovery tasks the direct adjustment approach to controlling this risk applies statistical test during the discovery process using critical value adjusted to take account of the size of the search space however problem with the direct adjustment strategy is that it may discard numerous true patterns this paper investigates the assignment of different critical values to different areas of the search space as an approach to alleviating this problem using variant of technique originally developed for other purposes this approach is shown to be effective at increasing the number of discoveries while still maintaining strict control over the risk of false discoveries
multi stage programming msp provides disciplined approach to run time code generation in the purely functional setting it has been shown how msp can be used to reduce the overhead of abstractions allowing clean maintainable code without paying performance penalties unfortunately msp is difficult to combine with imperative features which are prevalent in mainstream languages the central difficulty is scope extrusion wherein free variables can inadvertently be moved outside the scopes of their binders this paper proposes new approach to combining msp with imperative features that occupies sweet spot in the design space in terms of how well useful msp applications can be expressed and how easy it is for programmers to understand the key insight is that escapes or anti quotes must be weakly separable from the rest of the code ie the computational effects occurring inside an escape that are visible outside the escape are guaranteed to not contain code to demonstrate the feasibility of this approach we formalize type system based on lightweight java which we prove sound and we also provide an implementation called mint to validate both the expressivity of the type system and the effect of staging on the performance of java programs
scheduling in large scale parallel systems has been and continues to be an important and challenging research problem several key factors including the increasing use of off the shelf clusters of workstations to build such parallel systems have resulted in the emergence of new class of scheduling strategies broadly referred to as dynamic coscheduling unfortunately the size of both the design and performance spaces of these emerging scheduling strategies is quite large due in part to the numerous dynamic interactions among the different components of the parallel computing environment as well as the wide range of applications and systems that can comprise the parallel environment this in turn makes it difficult to fully explore the benefits and limitations of the various proposed dynamic coscheduling approaches for large scale systems solely with the use of simulation and or experimentationto gain better understanding of the fundamental properties of different dynamic coscheduling methods we formulate general mathematical model of this class of scheduling strategies within unified framework that allows us to investigate wide range of parallel environments we derive matrix analytic analysis based on stochastic decomposition and fixed point iteration large number of numerical experiments are performed in part to examine the accuracy of our approach these numerical results are in excellent agreement with detailed simulation results our mathematical model and analysis is then used to explore several fundamental design and performance tradeoffs associated with the class of dynamic coscheduling policies across broad spectrum of parallel computing environments
in this paper we propose new index structure for object oriented databases the main idea of this is graph structure called signature graph which is constructed over signature file generated for class and improves the search of signature file dramatically in addition the signature files accordingly the signature graphs can be organized into hierarchy according to the nested structure called the aggregation hierarchy of classes in an object oriented database which leads to another significant improvements
schema mapping is specification that describes how data structured under one schema the source schema is to be transformed into data structured under different schema the target schema although the notion of an inverse of schema mapping is important the exact definition of an inverse mapping is somewhat elusive this is because schema mapping may associate many target instances with each source instance and many source instances with each target instance based on the notion that the composition of mapping and its inverse is the identity we give formal definition for what it means for schema mapping to be an inverse of schema mapping for class of source instances we call such an inverse an inverse particular case of interest arises when is the class of all instances in which case an inverse is global inverse we focus on the important and practical case of schema mappings defined by source to target tuple generating dependencies and uncover rich theory when is defined by set of dependencies with finite chase we show how to construct an inverse when one exists in particular we show how to construct global inverse when one exists given and we show how to define the largest class such that is an inverse of
we propose group membership service for dynamic ad hoc networks it maintains as long as possible the existing groups and ensures that each group diameter is always smaller than constant fixed according to the application using the groups the proposed protocol is self stabilizing and works in dynamic distributed systems moreover it ensures kind of continuity in the service offer to the application while the system is converging except if too strong topology changes happen such best effort behavior allows applications to rely on the groups while the stabilization has not been reached which is very useful in dynamic ad hoc networks
new approach for supporting reactive capability is described in the context of an advanced object oriented database system called adome ii besides having rich set of pre defined composite event expressions and well defined execution model adome ii supports an extensible approach to reactive processing so as to be able to gracefully accommodate dynamic applications requirements in this approach production rules combined with methods are used as unifying mechanism to process rules to enable incremental detection of composite events and to allow new composite event expressions to be introduced into the system declaratively this allows the definition of new production rules each time an extension of the model takes place methods of supporting new composite event expressions are described and comparisons with other relevant approaches are also conducted prototype of adome ii has been constructed which has as its implementation base an ordinary passive oodbms and production rule base system
human societies have long used the capability of argumentation and dialogue to overcome and resolve conflicts that may arise within their communities today there is an increasing level of interest in the application of such dialogue games within artificial agent societies in particular within the field of multi agent systems this theory of argumentation and dialogue games has become instrumental in designing rich interaction protocols and in providing agents with means to manage and resolve conflicts however to date much of the existing literature focuses on formulating theoretically sound and complete models for multi agent systems nonetheless in so doing it has tended to overlook the computational implications of applying such models in agent societies especially ones with complex social structures furthermore the systemic impact of using argumentation in multi agent societies and its interplay with other forms of social influences such as those that emanate from the roles and relationships of society within such contexts has also received comparatively little attention to this end this paper presents significant step towards bridging these gaps for one of the most important dialogue game types namely argumentation based negotiation abn the contributions are three fold first we present both theoretically grounded and computationally tractable abn framework that allows agents to argue negotiate and resolve conflicts relating to their social influences within multi agent society in particular the model encapsulates four fundamental elements scheme that captures the stereotypical pattern of reasoning about rights and obligations in an agent society ii mechanism to use this scheme to systematically identify social arguments to use in such contexts iii language and protocol to govern the agent interactions and iv set of decision functions to enable agents to participate in such dialogues second we use this framework to devise series of concrete algorithms that give agents set of abn strategies to argue and resolve conflicts in multi agent task allocation scenario in so doing we exemplify the versatility of our framework and its ability to facilitate complex argumentation dialogues within artificial agent societies finally we carry out series of experiments to identify how and when argumentation can be useful for agent societies in particular our results show clear inverse correlation between the benefit of arguing and the resources available within the context that when agents operate with imperfect knowledge an arguing approach allows them to perform more effectively than non arguing one that arguing earlier in an abn interaction presents more efficient method than arguing later in the interaction and that allowing agents to negotiate their social influences presents both an effective and an efficient method that enhances their performance within society
sampling conditions for recovering the homology of set using topological persistence are much weaker than sampling conditions required by any known polynomial time algorithm for producing topologically correct reconstruction under the former sampling conditions which we call weak sampling conditions we give an algorithm that outputs topologically correct reconstruction unfortunately even though the algorithm terminates its time complexity is unbounded motivated by the question of knowing if polynomial time algorithm for reconstruction exists under the weak sampling conditions we identify at the heart of our algorithm test which requires answering the following question given two dimensional simplicial complexes does there exist simplicial complex containing and contained in which realizes the persistent homology of into we call this problem the homological simplification of the pair and prove that this problem is np complete using reduction from sat
the global spread of business is introducing new trend in the organization of work this dynamics in business has led to distribution of activities in different locations affecting also the way people develop software developers software engineers quality inspectors and also users and customers are distributed around the world since stakeholders are distributed the software development cannot be efficiently performed without support for articulating these people in consistent way issues like distance communication and different time zones introduce additional difficulties to the stakeholders involved in the process this paper explores the use of an agents architecture designed to support the requirements engineering specifically the verification and validation activities in the distributed development process goal driven approach is used to define high level goals that after refined makes possible to derive requirements and assigning responsibilities to the actors humans software agents devices and programs
the growing use of digital signal processors dsps in embedded systems necessitates the use of optimizing compilers supporting special hardware features in this paper we present compiler optimizations with the aim of minimizing energy consumption of embedded applications this comprises loop optimizations for exploitation of simd instructions and zero overhead hardware loops in order to increase performance and decrease the energy consumption in addition we use phase coupled code generator based on genetic algorithm gcg which is capable of performing energy aware instruction selection and scheduling energy aware compilation is done with respect to an instruction level energy cost model which is integrated into our code generator and simulator experimental results for several benchmarks show the effectiveness of our approach
memory latency is an important bottleneck in system performance that cannot be adequately solved by hardware alone several promising software techniques have been shown to address this problem successfully in specific situations however the generality of these software approaches has been limited because current architecturtes do not provide fine grained low overhead mechanism for observing and reacting to memory behavior directly to fill this need this article proposes new class of memory operations called informing memory operations which essentially consist of memory operatin combined either implicitly or explicitly with conditional branch and ink operation that is taken only if the reference suffers cache miss this article describes two different implementations of informing memory operations one is based on cache outcome condition code and the other is based on low overhead traps we find that modern in order issue and out of order issue superscalar processors already contain the bulk of the necessary hardware support we describe how number of software based memory optimizations can exploit informing memory operations to enhance performance and we look at cache coherence with fine grained access control as case study our performance results demonstrate that the runtime overhead of invoking the informing mechanism on the alpha and mips processors is generally small enough to provide considerable flexibility to hardware and software designers and that the cache coherence application has improved performance compared to other current solutions we believe that the inclusion of informing memory operations in future processors may spur even more innovative performance optimizations
this paper presents the creation deployment and evaluation of large scale spatially stable paper based visualization of software system the visualization was created for single team who were involved systematically in its initial design and subsequent design iterations the evaluation indicates that the visualization supported the onboarding scenario but otherwise failed to realize the research team's expectations we present several lessons learned and cautions to future research into largescale spatially stable visualizations of software systems
frequent subtree mining has attracted great deal of interest among the researchers due to its application in wide variety of domains some of the domains include bio informatics xml processing computational linguistics and web usage mining despite the advances in frequent subtree mining mining for the entire frequent subtrees is infeasible due to the combinatorial explosion of the frequent subtrees with the size of the datasets in order to provide reduced and concise representation without information loss we propose novel algorithm pcitminer prefix based closed induced tree miner pcitminer adopts the prefix based pattern growth strategy to provide the closed induced frequent subtrees efficiently the empirical analysis reveals that our algorithm significantly outperforms the current state of the art algorithm prefixtreeispan zou lu zhang hu and zhou
data on the web in html tables is mostly structured but we usually do not know the structure in advance thus we cannot directly query for data of interest we propose solution to this problem based on document independent extraction ontologies our solution entails elements of table understanding data integration and wrapper creation table understanding allows us to find tables of interest within web page recognize attributes and values within the table pair attributes with values and form records data integration techniques allow us to match source records with target schema ontologically specified wrappers allow us to extract data from source records into target schema experimental results show that we can successfully locate data of interest in tables and map the data from source html tables with unknown structure to given target database schema we can thus directly query source data with unknown structure through known target schema
recent proposals for chip multiprocessors cmps advocate speculative or implicit threading in which the hardware employs prediction to peel off instruction sequences ie implicit threads from the sequential execution stream and speculatively executes them in parallel on multiple processor cores these proposals augment conventional multiprocessor which employs explicit threading with the ability to handle implicit threads current proposals focus on only implicitly threaded code sections this paper identifies for the first time the issues in combining explicit and implicit threading we present the multiplex architecture to combine the two threading models multiplex exploits the similarities between implicit and explicit threading and provides unified support for the two threading models without additional hardware multiplex groups subset of protocol states in an implicitly threaded cmp to provide write invalidate protocol for explicit threads using fully integrated compiler infrastructure for automatic generation of multiplex code this paper presents detailed performance analysis for entire benchmarks instead of just implicitly threaded sections as done in previous papers we show that neither threading models alone performs consistently better than the other across the benchmarks cmp with four dual issue cpus achieves speedup of and over one dual issue cpu using implicit only and explicit only threading respectively multiplex matches or outperforms the better of the two threading models for every benchmark and four cpu multiplex achieves speedup of our detailed analysis indicates that the dominant overheads in an implicitly threaded cmp are speculation state overflow due to limited cache capacity and load imbalance and data dependences in fine grain threads
task scheduling is an essential aspect of parallel programming most heuristics for this np hard problem are based on simple system model that assumes fully connected processors and concurrent interprocessor communication hence contention for communication resources is not considered in task scheduling yet it has strong influence on the execution time of parallel program this paper investigates the incorporation of contention awareness into task scheduling new system model for task scheduling is proposed allowing us to capture both end point and network contention to achieve this the communication network is reflected by topology graph for the representation of arbitrary static and dynamic networks the contention awareness is accomplished by scheduling the communications represented by the edges in the task graph onto the links of the topology graph edge scheduling is theoretically analyzed including aspects like heterogeneity routing and causality the proposed contention aware scheduling preserves the theoretical basis of task scheduling it is shown how classic list scheduling is easily extended to this more accurate system model experimental results show the significantly improved accuracy and efficiency of the produced schedules
we revisit the shortest path problem in asynchronous duty cycled wireless sensor networks which exhibit time dependent features we model the time varying link cost and distance from each node to the sink as periodic functions we show that the time cost function satisfies the fifo property which makes the time dependent shortest path problem solvable in polynomial time using the synchronizer we propose fast distributed algorithm to build all to one shortest paths with polynomial message complexity and time complexity the algorithm determines the shortest paths for all discrete times with single execution in contrast with multiple executions needed by previous solutions we further propose an efficient distributed algorithm for time dependent shortest path maintenance the proposed algorithm is loop free with low message complexity and low space complexity of maxdeg where maxdeg is the maximum degree for all nodes the performance of our solution is evaluated under diverse network configurations the results suggest that our algorithm is more efficient than previous solutions in terms of message complexity and space complexity
the semantic web not only contains resources but also includes the heterogeneous relationships among them which is sharply distinguished from the current web as the growth of the semantic web specialized search techniques are of significance in this paper we present rss framework for enabling ranked semantic search on the semantic web in this framework the heterogeneity of relationships is fully exploited to determine the global importance of resources in addition the search results can be greatly expanded with entities most semantically related to the query thus able to provide users with properly ordered semantic search results by combining global ranking values and the relevance between the resources and the query the proposed semantic search model which supports inference is very different from traditional keyword based search methods moreover rss also distinguishes from many current methods of accessing the semantic web data in that it applies novel ranking strategies to prevent returning search results in disorder the experimental results show that the framework is feasible and can produce better ordering of semantic search results than directly applying the standard pagerank algorithm on the semantic web
when assessing the quality and maintainability of large code bases tools are needed for extracting several facts from the source code such as architecture structure code smells and quality metrics moreover these facts should be presented in such ways so that one can correlate them and find outliers and anomalies we present solidfx an integrated reverse engineering environment ire for and solidfx was specifically designed to support code parsing fact extraction metric computation and interactive visual analysis of the results in much the same way ides and design tools offer for the forward engineering pipeline in the design of solidfx we adapted and extended several existing code analysis and data visualization techniques to render them scalable for handling code bases of millions of lines in this paper we detail several design decisions taken to construct solidfx we also illustrate the application of our tool and our lessons learnt in using it in several types of analyses of real world industrial code bases including maintainability and modularity assessments detection of coding patterns and complexity analyses
the growing aging population faces number of challenges including rising medical cost inadequate number of medical doctors and healthcare professionals as well as higher incidence of misdiagnosis there is an increasing demand for better healthcare support for the elderly and one promising solution is the development of context aware middleware infrastructure for pervasive health wellness care this allows the accurate and timely delivery of health medical information among the patients doctors and healthcare workers through widespread deployment of wireless sensor networks and mobile devices in this paper we present our design and implementation of such context aware middleware for pervasive homecare camph the middleware offers several key enabling system services that consist of pp based context query processing context reasoning for activity recognition and context aware service management it can be used to support the development and deployment of various homecare services for the elderly such as patient monitoring location based emergency response anomalous daily activity detection pervasive access to medical data and social networking we have developed prototype of the middleware and demonstrated the concept of providing continuing care to an elderly with the collaborative interactions spanning multiple physical spaces person home office and clinic the results of the prototype show that our middleware approach achieves good efficiency of context query processing and good accuracy of activity recognition
the simulation of computer networks requires accurate models of user behavior to this end we present empirical models of end user network traffic derived from the analysis of neti home data there are two forms of models presented the first models traffic for specific tcp or udp port the second models all tcp or udp traffic for an end user these models are meant to be network independent and contain aspects such as bytes sent bytes received and user think time the empirical models derived in this study can then be used to enable more realistic simulations of computer networks
energy usage has been an important concern in recent research on online scheduling in this paper we extend the study of the tradeoff between flow time and energy from the single processor setting to the multi processor setting our main result is an analysis of simple non migratory online algorithm called crr classified round robin on processors showing that its flow time plus energy is within times of the optimal non migratory offline algorithm when the maximum allowable speed is slightly relaxed this result still holds even if the comparison is made against the optimal migratory offline algorithm the competitive ratio increases by factor of as special case our work also contributes to the traditional online flow time scheduling specifically for minimizing flow time only crr can yield competitive ratio one or even arbitrarily smaller than one when using sufficiently faster processors prior to our work similar result is only known for online algorithms that needs migration while the best non migratory result can achieve an competitive ratio the above result stems from an interesting observation that there always exists some optimal migratory schedule that can be converted in an offline sense to non migratory schedule with moderate increase in flow time plus energy more importantly this non migratory schedule always dispatches jobs in the same way as crr
in this paper we describe legal core ontology that is part of the legal knowledge interchange format knowledge representation formalism that enables the translation of legal knowledge bases written in different representation formats and formalisms legal core ontology can play an important role in the translation of existing legal knowledge bases to other representation formats in particular as the basis for articulate knowledge serving this requires that the ontology has firm grounding in commonsense and is developed in principled manner we describe the theory and methodology underlying the lkif core ontology compare it with other ontologies introduce the concepts it defines and discuss its use in the formalisation of an eu directive
after some general remarks about program verification we introduce separation logic novel extension of hoare logic that can strengthen the applicability and scalability of program verification for imperative programs that use shared mutable data structures or shared memory concurrency
we present probabilistic model of user affect designed to allow an intelligent agent to recognise multiple user emotions during the interaction with an educational computer game our model is based on probabilistic framework that deals with the high level of uncertainty involved in recognizing variety of user emotions by combining in dynamic bayesian network information on both the causes and effects of emotional reactions the part of the framework that reasons from causes to emotions diagnostic model implements theoretical model of affect the occ model which accounts for how emotions are caused by one's appraisal of the current context in terms of one's goals and preferences the advantage of using the occ model is that it provides an affective agent with explicit information not only on which emotions user feels but also why thus increasing the agent's capability to effectively respond to the users emotions the challenge is that building the model requires having mechanisms to assess user goals and how the environment fits them form of plan recognition in this paper we illustrate how we built the predictive part of the affective model by combining general theories with empirical studies to adapt the theories to our target application domain we then present results on the model's accuracy showing that the model achieves good accuracy on several of the target emotions we also discuss the model's limitations to open the ground for the next stage of the work ie complementing the model with diagnostic information
this paper evaluates the combination of two methods for adapting bipedal locomotion to explore virtual environments displayed on head mounted displays hmds within the confines of limited tracking spaces we combine method of changing the optic flow of locomotion effectively scaling the translational gain with method of intervening and manipulating user's locations in physical space while preserving their spatial awareness of the virtual space this latter technique is called resetting in two experiments we evaluate both scaling the translational gain and resetting while subject locomotes along path and then turns to face remembered object we find that the two techniques can be effectively combined although there is cognitive cost to resetting
an authoring methodology and set of checking tools let authors specify the spatial and temporal features of an application and verify the application prior to its execution the checking tools include an animation tool spatial and temporal layouts and the execution table
business process management is tightly coupled with service oriented architecture as business processes orchestrate services for business collaboration at logical level given the complexity of business processes and the variety of users it is sought after feature to show business process with different views so as to cater for the diverse interests authority levels etc of users this paper presents framework named flexview to support process abstraction and concretisation novel model is proposed to characterise the structural components of business process and describe the relations between these components two algorithms are developed to formally illustrate the realisation of process abstraction and concretisation in compliance with the defined consistency rules prototype is also implemented with ws bpel to prove the applicability of the approach
as usual the sigops workshop provided great platform for interesting discussion among other things controversy arose around the usefulness of causal ordering in distributed system in this paper explain causality in non technical terms and enumerate some of the most prevalent misconceptions that surrounded causality next present some important examples where causal delivery is necessary and sufficient ordering of events
dynamic optimizers modify the binary code of programs at runtime by profiling and optimizing certain aspects of the execution we present completely software based framework that dynamically optimizes programs for object based distributed shared memory dsm systems in dsm systems reducing the number of messages between nodes is crucial prefetching transfers data in advance from the storage node to the local node so that communication is minimized our framework uses profiler and dynamic binary rewriter that monitors the access behavior of the application and places prefetches where they are beneficial to speed up the application in addition we adapt the number of prefetches per request to best fit the application's behavior evaluation shows that the performance of our system is better than manual prefetching the number of messages sent decreases by up to performance gains of up to can be observed on the benchmarks
we present novel approach to parameterize mesh with disk topology to the plane in shape preserving manner our key contribution is local global algorithm which combines local mapping of each triangle to the plane using transformations taken from restricted set with global stitch operation of all triangles involving sparse linear system the local transformations can be taken from variety of families eg similarities or rotations generating different types of parameterizations in the first case the parameterization tries to force each triangle to be an as similar as possible version of its counterpart this is shown to yield results identical to those of the lscm algorithm in the second case the parameterization tries to force each triangle to be an as rigid as possible version of its counterpart this approach preserves shape as much as possible it is simple effective and fast due to pre factoring of the linear system involved in the global phase experimental results show that our approach provides almost isometric parameterizations and obtains more shape preserving results than other state of the art approaches we present also more general hybrid parameterization model which provides continuous spectrum of possibilities controlled by single parameter the two cases described above lie at the two ends of the spectrum we generalize our local global algorithm to compute these parameterizations the local phase may also be accelerated by parallelizing the independent computations per triangle
inserting virtual objects in real camera images with correct lighting is an active area of research current methods use high dynamic range camera with fish eye lens to capture the incoming illumination the main problem with this approach is the limitation to distant illumination therefore the focus of our work is real time description of both near and far field illumination for interactive movement of virtual objects in the camera image of real room the daylight which is coming in through the windows produces spatially varying distribution of indirect light in the room therefore near field description of incoming light is necessary our approach is to measure the daylight from outside and to simulate the resulting indirect light in the room to accomplish this we develop special dynamic form of the irradiance volume for real time updates of indirect light in the room and combine this with importance sampling and shadow maps for light from outside this separation allows object movements with interactive frame rates fps to verify the correctness of our approach we compare images of synthetic objects with real objects
denial of service dos attacks constitute one of the major threats and among the hardest security problems in today's internet of particular concern are distributed denial of service ddos attacks whose impact can be proportionally severe with little or no advance warning ddos attack can easily exhaust the computing and communication resources of its victim within short period of time because of the seriousness of the problem many defense mechanisms have been proposed to combat these attacks this paper presents structural approach to the ddos problem by developing classification of ddos attacks and ddos defense mechanisms furthermore important features of each attack and defense system category are described and advantages and disadvantages of each proposed scheme are outlined the goal of the paper is to place some order into the existing attack and defense mechanisms so that better understanding of ddos attacks can be achieved and subsequently more efficient and effective algorithms techniques and procedures to combat these attacks may be developed
novel space time representation str of iterative algorithms for their systematic mapping onto regular processor arrays is proposed timing information is introduced in the dependence graph dg by the definition and the construction of the space time dg stdg any variable instance of the loop body independently of the number of the loop indices is characterized by an integer vector composed by its indices as space part and an additional time index representing its execution time according to preliminary timing the main advantage of the str is that the need for the uniformization of the algorithm is avoided moreover it is proven that in the stdg dependence vectors having opposite directions do not exist and therefore linear mapping of the stdg onto the desired processor array can always be derived efficient and regular architectures are produced by applying the str to the original description of the warshall floyd algorithm for the algebraic path problem
we propose an algorithm to improve the quality of depth maps used for multi view stereo mvs many existing mvs techniques make use of two stage approach which estimates depth maps from neighbouring images and then merges them to extract final surface often the depth maps used for the merging stage will contain outliers due to errors in the matching process traditional systems exploit redundancy in the image sequence the surface is seen in many views in order to make the final surface estimate robust to these outliers in the case of sparse data sets there is often insufficient redundancy and thus performance degrades as the number of images decreases in order to improve performance in these circumstances it is necessary to remove the outliers from the depth maps we identify the two main sources of outliers in top performing algorithm spurious matches due to repeated texture and matching failure due to occlusion distortion and lack of texture we propose two contributions to tackle these failure modes firstly we store multiple depth hypotheses and use spatial consistency constraint to extract the true depth secondly we allow the algorithm to return an unknown state when the true depth estimate cannot be found by combining these in discrete label mrf optimisation we are able to obtain high accuracy depth maps with low numbers of outliers we evaluate our algorithm in multi view stereo framework and find it to confer state of the art performance with the leading techniques in particular on the standard evaluation sparse data sets
in this paper we describe new technique called pipeline spectroscopy and use it to measure the cost of each cache miss the cost of miss is displayed graphed as histogram which represents precise readout showing detailed visualization of the cost of each cache miss throughout all levels of the memory hierarchy we call the graphs spectrograms because they reveal certain signature features of the processor's memory hierarchy the pipeline and the miss pattern itself next we provide two examples that use spectroscopy to optimize the processor's hardware or application's software the first example demonstrates how miss spectrogram can aid software designers in analyzing the performance of an application the second example uses miss spectrogram to analyze bus queueing our experiments show that performance gains of up to are possible detailed analysis of spectrogram leads to much greater insight in pipeline dynamics including effects due to miss cluster miss overlap prefetching and miss queueing delays
in this paper we present an observational case study at major teaching hospital which both inspired and gave us valuable feedback on the design and development of lwoad lwoad is denotational language we propose to support users of an electronic document system in declaratively expressing specifying and implementing computational mechanisms that fulfill coordinative requirements our focus addresses the user friendly and formal expression of local coordinative practices the agile mocking up of corresponding functionalities the full deployment of coordination oriented and context aware behaviors into legacy electronic document systems we give examples of lwoad mechanisms taken from the case study and discuss their impact for the eud of coordinative functionalities
the rapid increase in ic design complexity and wide spread use of intellectual property ip blocks have made the so called mixed size placement very important topic in recent years although several algorithms have been proposed for mixed sized placements most of them primarily focus on the global placement aspect in this paper we propose three step approach named xdp for mixed size detailed placement first combination of constraint graph and linear programming is used to legalize macros then an enhanced greedy method is used to legalize the standard cells finally sliding window based cell swapping is applied to further reduce wirelength the impact of individual techniques is analyzed and quantified experiments show that when applied to the set of global placement results generated by aplace xdp can produce wirelength comparable to the native detailed placement of aplace and shorter wirelength compared to fengshui when applied to the set of global placements generated by mpl xdp is the only detailed placement that successfully produces legal placement for all the examples while aplace and fengshui fail for and of the examples for cases where legal placements can be compared the wirelength produced by xdp is shorter by on average compared to aplace and fengshui furthermore xdp displays higher robustness than the other tools by covering broader spectrum of examples by different global placement tools
efficient access to web content remains elusive for individuals accessing the web using assistive technology previous efforts to improve web accessibility have focused on developer awareness technological improvement and legislation but these approaches have left remaining concerns first while many tools can help produce accessible content these tools are generally difficult to integrate into existing developer workflows and rarely offer specific suggestions that developers can implement second tools that automatically improve web content for users generally solve specific problems and are difficult to combine and use on diversity of existing assistive technology finally although blind web users have proven adept at overcoming the shortcomings of the web and existing tools they have been only marginally involved in improving the accessibility of their own web experience as first step toward addressing these concerns we introduce accessmonkey common scripting framework that web users web developers and web researchers can use to collaboratively improve accessibility this framework advances the idea that javascript and dynamic web content can be used to improve inaccessible content instead of being cause of it using accessmonkey web users and developers on different platforms with potentially different goals can collaboratively make the web more accessible in this paper we first present the accessmonkey framework describe three implementations of it that we have created and offer several example scripts that demonstrate its utility we conclude by discussing future extensions of this work that will provide efficient access to scripts as users browse the web and allow non technical users be involved in creating scripts
this paper motivated by functional brain imaging applications is interested in the discovery of stable spatio temporal patterns this problem is formalized as multi objective multi modal optimization problem on one hand the target patterns must show good stability in wide spatio temporal region antagonistic objectives on the other hand experts are interested in finding all such patterns global and local optima the proposed algorithm termed miner is empirically validated on artificial and real world datasets it shows good performances and scalability detecting target spatiotemporal patterns within minutes from mo datasets
recently number of hybrid systems have been proposed to combine the advantages of shared nothing and shared everything concepts for computing relational join operations most of these proposed systems however presented few analytical results and have produced limited or no implementations on actual multiprocessors in this paper we present parallel join algorithm with load balancing for hybrid system that combines both shared nothing and shared everything architectures we derive an analytical model for the join algorithm on this architecture and validate it using both hardware software simulations and actual experimentations we study the performance of the join on the hybrid system for wide range of system parameter values we conclude that the hybrid system outperforms both shared nothing and shared everything architectures
we present new non blocking array based shared stack and queue implementations we sketch proofs of correctness and amortized time analyses for the algorithms to the best of our knowledge our stack algorithm is the first practical array based one and it is the first time that bounded counter values are employed to implement shared stack and queue we verify the correctness of our algorithms by the spin model checker and compare our algorithms to other algorithms experimentally
while significant amount of research efforts has been reported on developing algorithms based on joins and semijoins to tackle distributed query processing there is relatively little progress made toward exploring the complexity of the problems studied as result proving np hardness of or devising polynomial time algorithms for certain distributed query optimization problems has been elaborated upon by many researchers however due to its inherent difficulty the complexity of the majority of problems on distributed query optimization remains unknown in this paper we generally characterize the distributed query optimization problems and provide frame work to explore their complexity as it will be shown most distributed query optimization problems can be transformed into an optimization problem comprising set of binary decisions termed sum product optimization spo problem we first prove spo is np hard in light of the np completeness of well known problem knapsack knap then using this result as basis we prove that five classes of distributed query optimization problems which cover the majority of distributed query optimization problems previously studied in the literature are np hard by polynomially reducing spo to each of them the detail for each problem transformation is derived we not only prove the conjecture that many prior studies relied upon but also provide frame work for future related studies
it is often argued that usability problems should be identified as early as possible during software development but many usability evaluation methods do not fit well in early development activities we propose method for usability evaluation of use cases widely used representation of design ideas produced early in software development processes the method proceeds by systematic inspection of use cases with reference to set of guidelines for usable design to validate the method four evaluators inspected set of use cases for health care application the usability problems predicted by the evaluators were compared to the result of conventional think aloud test about one fourth of the problems were identified by both think aloud testing and use case inspection about half of the predicted problems not found by think aloud testing were assessed as providing useful input to early development qualitative data on the evaluators experience using the method are also presented on this background we argue that use case inspection has promising potential and discuss its limitations
compilers are critical for embedded systems and high performance computing compiler infrastructure provides an infrastructure for rapid development of high quality compilers based on main components of compiler infrastructures this paper reviews representative compiler infrastructure products and summarizes their features it focuses on an insight analysis of the key techniques for building the compiler back ends and presents our probes into compiler infrastructures for typical issues
intrusion prevention system ips has been an effective tool to detect and prevent unwanted attempts which are mainly through network and system vulnerabilities at accessing and manipulating computer systems intrusion detection and prevention are two main functions of ips as attacks are becoming massive and complex the traditional centralized ipses are incapable of detecting all those attempts the existing distributed ipses mainly based on mobile agent have some serious problems such as weak security of mobile agents response latency large code size in this paper we propose customized intrusion prevention system vmfence in distributed virtual computing environment to simplify the complexity of the management in vmfence the states of detection processes vary with those of virtual machines vms which are described by deterministic finite automata dfa the detection processes each of which detects one virtual machine reside in privileged virtual machine the processes run synchronously and outside of vms in order to achieve high performance and security the experimental results also show vmfence has higher detection efficiency than traditional intrusion detection systems and little impact on the performance of the monitored vms
the rewrite based approach provides executable specifications for security policies which can be independently designed verified and then anchored on programs using modular discipline in this paper we describe how to perform queries over these rule based policies in order to increase the trust of the policy author on the correct behavior of the policy the analysis we provide is founded on the strategic narrowing process which provides both the necessary abstraction for simulating executions of the policy over access requests and the mechanism for solving what if queries from the security administrator we illustrate this general approach by the analysis of firewall system policy
power is an important design constraint in embedded computing systems to meet the power constraint microarchitecture and hardware designed to achieve high performance need to be revisited from both performance and power angles this paper studies one of them branch predictor as well known branch prediction is critical to exploit instruction level parallelism effectively but may incur additional power consumption due to the hardware resource dedicated for branch prediction and the extra power consumed on mispredicted branches this paper explores the design space of branch prediction mechanisms and tries to find the most beneficial one to realize iow power embedded processor the sample processor studied is godson like processor which is dual issue out of order processor with deep pipeline supporting mips instruction set
we propose novel critiquing based recommender interface the hybrid critiquing interface that integrates the user self motivated critiquing facility to compensate for the limitations of system proposed critiques the results from our user study show that the integration of such self motivated critiquing support enables users to achieve higher level of decision accuracy while consuming less cognitive effort in addition users expressed higher subjective opinions of the hybrid critiquing interface than the interface simply providing system proposed critiques and they would more likely return to it for future use
many activities in business process management such as process retrieval process mining and process integration need to determine the similarity or the distance between two processes although several approaches have recently been proposed to measure the similarity between business processes neither the definitions of the similarity notion between processes nor the measure methods have gained wide recognition in this paper we define the similarity and the distance based on firing sequences in the context of workflow nets wf nets as the unified reference concepts however to many wf nets either the number of full firing sequences or the length of single firing sequence is infinite since transition adjacency relations tars can be seen as the genes of the firing sequences which describe transition orders appearing in all possible firing sequences we propose practical similarity definition based on the tar sets of two processes it is formally shown that the corresponding distance measure between processes is metric an algorithm using model reduction techniques for the efficient computation of the measure is also presented experimental results involving comparison of different measures on artificial processes and evaluations on clustering real life processes validate our approach
frequent pattern mining has been studied extensively and has many useful applications however frequent pattern mining often generates too many patterns to be truly efficient or effective in many applications it is sufficient to generate and examine frequent patterns with sufficiently good approximation of the support frequency instead of in full precision such compact but close enough frequent pattern base is called condensed frequent pattern basein this paper we propose and examine several alternatives for the design representation and implementation of such condensed frequent pattern bases several algorithms for computing such pattern bases are proposed their effectiveness at pattern compression and methods for efficiently computing them are investigated systematic performance study is conducted on different kinds of databases and demonstrates the effectiveness and efficiency of our approach in handling frequent pattern mining in large databases
ultra low voltage operation has recently drawn significant attention due to its large potential energy savings however typical design practices used for super threshold operation are not necessarily compatible with the low voltage regime here radically different guidelines may be needed since existing process technologies have been optimized for super threshold operation we therefore study the selection of the optimal technology in ultra low voltage designs to achieve minimum energy and minimum variability which are among foremost concerns we investigate five industrial technologies from nm to nm we demonstrate that mature technologies are often the best choice in very low voltage applications saving as much as in total energy consumption compared to poorly selected technology in parallel the effect of technology choice on variability is investigated when operating at the energy optimal design point the results show up to improvement in delay variation due to global process shift and mismatch when using the most advanced technologies despite their large variability at nominal vdd
self adjusting computation offers language centric approach to writing programs that can automatically respond to modifications to their data eg inputs except for several domain specific implementations however all previous implementations of self adjusting computation assume mostly functional higher order languages such as standard ml prior to this work it was not known if self adjusting computation can be made to work with low level imperative languages such as without placing undue burden on the programmer we describe the design and implementation of ceal based language for self adjusting computation the language is fully general and extends with small number of primitives to enable writing self adjusting programs in style similar to conventional programs we present efficient compilation techniques for translating ceal programs into that can be compiled with existing compilers using primitives supplied by run time library for self adjusting computation we implement the proposed compiler and evaluate its effectiveness our experiments show that ceal is effective in practice compiled self adjusting programs respond to small modifications to their data by orders of magnitude faster than recomputing from scratch while slowing down from scratch run by moderate constant factor compared to previous work we measure significant space and time improvements
in recent years privacy model called anonymity has gained popularity in the microdata releasing as the microdata may contain multiple sensitive attributes about an individual the protection of multiple sensitive attributes has become an important problem different from the existing models of single sensitive attribute extra associations among multiple sensitive attributes should be invested two kinds of disclosure scenarios may happen because of logical associations the diversity is checked to prevent the foregoing disclosure risks with an requirement definition used to ensure the diversity requirement at last two step greedy generalization algorithm is used to carry out the multiple sensitive attributes processing which deal with quasi identifiers and sensitive attributes respectively we reduce the overall distortion by the measure of masking sa
ragnarok is an experimental software development environment that focuses on enhanced support for managerial activities in large scale software development taking the daily work of the software developer as its point of departure the main emphasis is support in three areas management navigation and collaboration the leitmotif is the software architecture which is extended to handle managerial data in addition to source code this extended software architecture is put under tight version and configuration management control and furthermore used as basis for visualisation preliminary results of using the ragnarok prototype in number of projects are outlined
many web information services utilize techniques of information extraction ie to collect important facts from the web to create more advanced services one possible method is to discover thematic information from the collected facts through text classification however most conventional text classification techniques rely on manual labelled corpora and are thus ill suited to cooperate with web information services with open domains in this work we present system named liveclassifier that can automatically train classifiersthrough web corpora based on user defined topic hierarchies due to its flexibility and convenience liveclassifier can be easily adapted for various purposes new web information services can be created to fully exploit it human users can use it to create classifiers for their personal applications the effectiveness of classifiers created by liveclassifier is well supportedby empirical evidence
the amount and variety of data available electronically have dramatically increased in the last decade however data and documents are stored in different ways and do not usually show their internal structure in order to take full advantage of the topological structure of digital documents and particularly web sites their hierarchical organization should be exploited by introducing notion of query similar to the one used in database systems good approach in that respect is the one provided by graphical query languages originally designed to model object bases and later proposed for semistructured data like log the aim of this paper is to provide suitable graph based semantics to this language supporting both data structure variability and topological similarities between queries and document structures suite of operational semantics based on the notion of bisimulation is introduced both at the concrete level instances and at the abstract level schemata giving rise to semantic framework that benefits from the cross fertilization of tools originally designed in quite different research areas databases concurrency logics static analysis
multiprocessor interconnection networks may reach congestion with high traffic loads which prevents reaching the wished performance unfortunately many of the mechanisms proposed in the literature for congestion control either suffer from lack of robustness being unable to work properly with different traffic patterns or message lengths or detect congestion relying on global information that wastes some network bandwidth this paper presents family of mechanisms to avoid network congestion in wormhole networks all of them need only local information applying message throttling when it is required the proposed mechanisms use different strategies to detect network congestion and also apply different corrective actions the mechanisms are evaluated and compared for several network loads and topologies noticeably improving network performance with high loads but without penalizing network behavior for low and medium traffic rates where no congestion control is required
beyond certain number of cores multi core processing chips will require network on chip noc to interconnect the cores and overcome the limitations of bus nocs must be carefully designed to meet constraints like power consumption area and ultra low latencies although meshes with dor dimension order routing meet these constraints the need for partitioning eg virtual machines coherency domains and traffic isolation may prevent the use of dor routing also core heterogeneity and manufacturing and run time faults may lead to partially irregular topologies routing in these topologies is complex and previously proposed solutions required routing tables which drastically increase power consumption area and latency the exception is lbdr logic based distributed routing flexible routing method for irregular topologies that removes the need for using routing tables both at end nodes and switches thus achieving large savings in chip area and power consumption but lbdr lacks support for multicast and broadcast which are required to efficiently support cache coherence protocols both for single and multiple coherence domains
geography is becoming increasingly important in web search search engines can often return better results to users by analyzing features such as user location or geographic terms in web pages and user queries this is also of great commercial value as it enables location specific advertising and improved search for local businesses as result major search companies have invested significant resources into geographic search technologies also often called local search this paper studies geographic search queries ie text queries such as hotel new york that employ geographical terms in an attempt to restrict results to particular region or location our main motivation is to identify opportunities for improving geographical search and related technologies and we perform an analysis of million queries of the recently released aol query trace first we identify typical properties of geographic search geo queries based on manual examination of several thousand queries based on these observations we build classifier that separates the trace into geo and non geo queries we then investigate the properties of geo queries in more detail and relate them to web sites and users associated with such queries we also propose new taxonomy for geographic search queries
electronic voting support systems should not focus only on ballot casting and recording instead user centered perspective should be adopted for the design of system that supports information gathering organizing and sharing deliberation decision making and voting relevant social science literature on political decision making and voting is used to develop requirements design concept is presented that supports extended information browsing using combined filtering from ballot materials and voter profiles the system supports information sharing and participation in electronic dialogues voters may interweave information browsing annotation contextualized discussion and ballot markup over extended time periods
we present statistical method that uses prediction modeling to decrease the temporally redundant data transmitted back to the sink the major novelties are fourfold first prediction model is fit to the sensor data second prediction error is utilized to adaptively update the model parameters using hypothesis testing third data transformation is proposed to bring the sensor sample series closer to weak stationarity finally an efficient implementation is presented we show that our proposed prediction error based hypothesis testing drastig method achieves low energy dissipation while keeping the prediction errors at user defined tolerable magnitudes based on real data experiments
when we attempted to introduce an extractive approach to company we were faced with challenging project situation where legacy applications did not have many commonalities among their implementations as they were developed independently by different teams without sharing common code base although there were not many structural similarities we expected to find similarities if we view them from the domain model perspective as they were in the same domain and were developed with the object oriented paradigm therefore we decided to place the domain model at the center of extraction and reengineering thus developing domain model based extractive method the method has been successfully applied to introduce software product line to set top box manufacturing company
previous research on mining sequential patterns mainly focused on discovering patterns from point based event data little effort has been put toward mining patterns from interval based event data where pair of time values is associated with each event kam and fu's work in identified temporal relationships between two intervals according to these temporal relationships new variant of temporal patterns was defined for interval based event data unfortunately the patterns defined in this manner are ambiguous which means that the temporal relationships among events cannot be correctly represented in temporal patterns to resolve this problem we first define new kind of nonambiguous temporal pattern for interval based event data then the tprefixspan algorithm is developed to mine the new temporal patterns from interval based events the completeness and accuracy of the results are also proven the experimental results show that the efficiency and scalability of the tprefixspan algorithm are satisfactory furthermore to show the applicability and effectiveness of temporal pattern mining we execute experiments to discover temporal patterns from historical nasdaq data
we present functional object calculus which solves the traditional conflict between matching based functional programming and object oriented programming by treating uniformly method invocations and functional constructor applications the key feature of the calculus is that each object remembers its history that is the series of method calls that created it histories allow us to classify objects in finer way than classes do the resulting calculus has simple syntax and is very expressive we give examples finally we define type system for the calculus and show its soundness typable programs do not produce matching or method invocation errors
we present simple local protocol pcover which provides partial but high coverage in sensor networks through pcover we demonstrate that it is feasible to maintain high coverage while significantly increasing coverage duration when compared with protocols that provide full coverage in particular we show that we are able to maintain coverage for duration that is times the duration for which existing protocols maintain full coverage through simulations we show that our protocol provides load balancing ie the desired level of coverage is maintained almost until the point where all sensors deplete their batteries we also show that pcover handles failure of sensors different coverage areas different node densities and different topologies and can be used for dynamically changing the level of coverage
virtual networks provide applications with the illusion of having their own dedicated high performance networks although network interfaces posses limited shared resources we present the design of large scale virtual network system and examine the integration of communication programming interface system resource management and network interface operation our implementation on cluster of workstations quantifies the impact of virtualization on small message latencies and throughputs shows full hardware performance is delivered to dedicated applications and time shared workloads and shows robust performance under demanding workloads that overcommit interface resources
fault free path in the dimensional hypercube with faulty vertices is said to be long if it has length at least similarly fault free cycle in is long if it has length at least if all faulty vertices are from the same bipartite class of such length is the best possible we show that for every set of at most faulty vertices in and every two fault free vertices and satisfying simple necessary condition on neighbors of and there exists long fault free path between and this number of faulty vertices is tight and improves the previously known results furthermore we show for every set of at most faulty vertices in where that has long fault free cycle this is first quadratic bound which is known to be asymptotically optimal
collaborative web search cws is community based approach to web search that supports the sharing of past result selections among group of related searchers so as to personalize result lists to reflect the preferences of the community as whole in this paper we present the results of recent live user trial which demonstrates how cws elicits high levels of participation and how the search activities of community of related users form type of social search network
existing methods place data or code in scratchpad memory ie spm by either relying on heuristics or resorting to integer programming or mapping it to graph coloring problem in this work the spm allocation problem is formulated as an interval coloring problem the key observation is that in many embedded applications arrays including structs as special case are often related in the following way for any two arrays their live ranges are often such that one is either disjoint from or contains the other as result array interference graphs are often superperfect graphs and optimal interval colorings for such array interference graphs are possible this has led to the development of two new spm allocation algorithms while differing in whether live range splits and spills are done sequentially or together both algorithms place arrays in spm based on examining the cliques in an interference graph in both cases we guarantee optimally that all arrays in an interference graph can be placed in spm if its size is no smaller than the clique number of the graph in the case that the spm is not large enough we rely on heuristics to split or spill live range until the graph is colorable our experiment results using embedded benchmarks show that our algorithms can outperform graph coloring when their interference graphs are superperfect or nearly so although graph coloring is admittedly more general and may also be effective to applications with arbitrary interference graphs
this paper describes method for studying idioms based implementations of crosscutting concerns and our experiences with it in the context of real world large scale embedded software system in particular we analyse seemingly simple concern tracing and show that it exhibits significant variability despite the use of prescribed idiom we discuss the consequences of this variability in terms of how aspect oriented software development techniques could help prevent it how it paralyses automated migration efforts and which aspect language features are required in order to obtain precise and concise aspects additionally we elaborate on the representativeness of our results and on the usefulness of our proposed method
word wide web intelligent agent technology has provided researchers and practitioners such as those involved in information technology innovation knowledge management and technical collaboration with the ability to examine the design principles and performance characteristics of the various approaches to intelligent agent technology and to increase the cross fertilization of ideas on the development of autonomous agents and multi agent systems among different domains this study investigates the employment of intelligent agents in web based auction process with particular reference to the appropriateness of the agent software for the online auction task consumers value perception of the agent the effect of this consumer perception on their intention to use the tool and measure of consumer acceptance in the initial case study both consumers and web operators thought the use of software agents enhanced online auction efficiency and timeliness the second phase of the investigation established that consumer familiarity with the agent functionality was positively associated with seven dimensions online auction site's task agent's technology task technology fit perceived ease of use perceived usefulness perceived playfulness intention to use tool and negatively associated with perceived risk intelligent agents have the potential to release skilled operator time for the use of value adding tasks in the planning and expansion of online auctions
internet exchange points ixps are an important ingredient of the internet as level ecosystem logical fabric of the internet made up of about ases and their mutual business relationships whose primary purpose is to control and manage the flow of traffic despite the ixps critical role in this fabric little is known about them in terms of their peering matrices ie who peers with whom at which ixp and corresponding traffic matrices ie how much traffic do the different ases that peer at an ixp exchange with one another in this paper we report on an internet wide traceroute study that was specifically designed to shed light on the unknown ixp specific peering matrices and involves targeted traceroutes from publicly available and geographically dispersed vantage points based on our method we were able to discover and validate the existence of about ixp specific peering links nearly more links than were previously known in the process we also classified all known ixps depending on the type of information required to detect them moreover in view of the currently used inferred as level maps of the internet that are known to miss significant portion of the actual as relationships of the peer to peer type our study provides new method for augmenting these maps with ixp related peering links in systematic and informed manner
the problem of optimizing joins between two fragmented relations on broadcast local network is analyzed data redundancy is considered semantic information associated with fragments are used to eliminate necessary processing more than one physical copies of fragment is allowed to be used in strategy to achieve more parallelism join analysis graphs are introduced to represent joins on two fragmented relations the problem of optimizing join is mapped into an equivalent problem of finding minimum weight vertex cover for the corresponding join analysis graph this problem is proved to be np hard four phase approach for processing joins is proposed
we introduce computational framework for discovering regular or repeated geometric structures in shapes we describe and classify possible regular structures and present an effective algorithm for detecting such repeated geometric patterns in point or meshbased models our method assumes no prior knowledge of the geometry or spatial location of the individual elements that define the pattern structure discovery is made possible by careful analysis of pairwise similarity transformations that reveals prominent lattice structures in suitable model of transformation space we introduce an optimization method for detecting such uniform grids specifically designed to deal with outliers and missing elements this yields robust algorithm that successfully discovers complex regular structures amidst clutter noise and missing geometry the accuracy of the extracted generating transformations is further improved using novel simultaneous registration method in the spatial domain we demonstrate the effectiveness of our algorithm on variety of examples and show applications to compression model repair and geometry synthesis
we present unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within parallelizing compiler using this framework we show how to find good storage mapping for given schedule good schedule for given storage mapping and good storage mapping that is valid for all legal one dimensional affine schedules we consider storage mappings that collapse one dimension of multidimensional array and programs that are in single assignment form and accept one dimensional affine schedule our method combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests we formulate the constraints imposed by the data dependences and storage mappings as set of linear inequalities and apply numerical programming techniques to solve for the shortest occupancy vector we consider our method to be first step towards automating procedure that finds the optimal tradeoff between parallelism and storage space
high on chip temperature impairs the processor's reliability and reduces its lifetime hardware level dynamic thermal management dtm techniques can effectively constrain the chip temperature but degrades the performance we propose an os level technique that performs thermal aware job scheduling to reduce dtms the algorithm is based on the observation that hot and cool jobs executed in different order can make difference in resulting temperature real system implementation in linux shows that our scheduler can remove percnt to percnt of the hardware dtms in medium thermal environment the cpu throughput is improved by up to percnt percnt on average in severe thermal environment
texture representation should corroborate various functions of texture in this paper we present novel approach that incorporates texture features for retrieval in an examplar based texture compaction and synthesis algorithm the original texture is compacted and compressed in the encoder to obtain thumbnail texture which the decoder then synthesizes to obtain perceptually high quality texture we propose using probabilistic framework based on the generalized em algorithm to analyze the solutions of the approach our experiment results show that high quality synthesized texture can be generated in the decoder from compressed thumbnail texture the number of bits in the compressed thumbnail is times lower than that in the original texture and times lower than that needed to compress the original texture using jpeg we also show that in terms of retrieval and synthesization our compressed and compacted textures perform better than compressed cropped textures and compressed compacted textures derived by the patchwork algorithm
there is growing interest in studying dynamic graphs or graphs that evolve with time in this work we investigate new type of dynamic graph analysis finding regions of graph that are evolving in similar manner and are topologically similar over period of time for example these regions can be used to group set of changes having common cause in event detection and fault diagnosis prior work has proposed greedy framework called cstag to find these regions it was accurate in datasets where the regions are temporally and spatially well separated however in cases where the regions are not well separated cstag produces incorrect groupings in this paper we propose new algorithm called reghunter it treats the region discovery problem as multi objective optimisation problem and it uses multi level graph partitioning algorithm to discover the regions of correlated change in addition we propose an external clustering validation technique and use several existing internal measures to evaluate the accuracy of reghunter using synthetic datasets we found reghunter is significantly more accurate than cstag in dynamic graphs that have regions with small separation using two real datasets the access graph of the world cup website and the bgp connectivity graph during the landfall of hurricane katrina we found reghunter obtained more accurate results than cstag furthermore reghunter was able to discover two interesting regions for the world cup access graph that cstag was not able to find
in this work we present spice based rtl subthreshold leakage model analyzing components built in nm technology we present separation approach regarding inter and intra die threshold variations temperature supply voltage and state dependence the body effect and differences between nmos and pmos introduce leakage state dependence of one order of magnitude we show that the leakage of rt components still shows state dependencies between and leakage model not regarding the state can never be more accurate than this the proposed state aware model has an average error of for the rt components analyzed
in this paper we investigate efficient strategies for supporting on demand information dissemination and gathering in large scale vwireless sensor networks in particular we propose comb needle discovery support model resembling an ancient method use comb to help find needle in sands or haystack the model combines push and pull for information dissemination and gathering the push component features data duplication in linear neighborhood of each node the pull component features dynamic formation of an on demand routing structure resembling comb the comb needle model enables us to investigate the cost of spectrum of push and pull combinations for supporting discovery and query in large scale sensor networks our result shows that the optimal routing structure depends on the frequency of query occurrence and the spatial temporal frequency of related events in the network the benefit of balancing push and pull for discovery in large scale geometric networks are demonstrated we also raise the issue of query coverage in unreliable networks and investigate how redundancy can improve the coverage via both theoretical analysis and simulation last we study adaptive strategies for the case where the frequencies of query and events are unknown priori and time varying
traditional high performance computing systems require extensive management and suffer from security and configuration problems this paper presents two generations of cluster management system that aims at making clusters as secure and self managing as possible the goal of the system is minimality all nodes in cluster are configured with minimal software base consisting of virtual machine monitor and remote bootstrapping mechanism and customers then buy access using simple pre paid token scheme all necessary application software including the operating system is provided by the customer as full virtual machine and boot strapped or migrated into the cluster we have explored two different models for cluster control the first decentralized push model evil man requires direct network access to cluster nodes each of which is running truly minimal control plane implementation consisting of only few hundred lines of code in the second centralized pull model evil twin nodes may be running behind nats or firewalls and are controlled by centralized web service specially developed cache invalidation protocol is used for telling nodes when to reload their workload description from the centralized service
there is an increasing interest in techniques that support analysis and measurement of fielded software systems these techniques typically deploy numerous instrumented instances of software system collect execution data when the instances run in the field and analyze the remotely collected data to better understand the system's in the field behavior one common need for these techniques is the ability to distinguish execution outcomes eg to collect only data corresponding to some behavior or to determine how often and under which condition specific behavior occurs most current approaches however do not perform any kind of classification of remote executions and either focus on easily observable behaviors eg crashes or assume that outcomes classifications are externally provided eg by the users to address the limitations of existing approaches we have developed three techniques for automatically classifying execution data as belonging to one of several classes in this paper we introduce our techniques and apply them to the binary classification of passing and failing behaviors our three techniques impose different overheads on program instances and thus each is appropriate for different application scenarios we performed several empirical studies to evaluate and refine our techniques and to investigate the trade offs among them our results show that the first technique can build very accurate models but requires complete set of execution data the second technique produces slightly less accurate models but needs only small fraction of the total execution data and the third technique allows for even further cost reductions by building the models incrementally but requires some sequential ordering of the software instances instrumentation
two general techniques for implementing domain specific language dsl with less overhead are the finally tagless embedding of object programs and the direct style representation of side effects we use these techniques to build dsl for probabilistic programming for expressing countable probabilistic models and performing exact inference and importance sampling on them our language is embedded as an ordinary ocaml library and represents probability distributions as ordinary ocaml programs we use delimited continuations to reify probabilistic programs as lazy search trees which inference algorithms may traverse without imposing any interpretive overhead on deterministic parts of model we thus take advantage of the existing ocaml implementation to achieve competitive performance and ease of use inference algorithms can easily be embedded in probabilistic programs themselves
this paper discusses semantic interoperability issues in agent based commerce systems the literature reports various techniques to enable agents to understand the meanings of the messages exchanged we will argue how these different techniques can be combined in one agent communication protocol to obtain the best of each world the resulting communication protocol enables agents to sufficiently understand each other to participate in successful collaboration
several wireless sensor network applications are currently appearing in various domains their goal is often to monitor geographical area when sensor detects monitored event it informs sink node using alarm messages the area surveillance application needs to react to such an event with finite bounded and known delay these are real time constraints this work proposes real time mac protocol with realistic assumptions for random linear network where sensors are deployed randomly along line we present formal validation of this protocol both for initialization and run time and present simulation results on realistic scenario
sculpting various facial expressions from static face model is process with intensive manual tuning efforts in this paper we present an interactive facial expression posing system through portrait manipulation where manipulated portrait serves metaphor for automatically inferring its corresponding facial expression with fine details users either rapidly assemble face portrait through pre designed portrait component library or intuitively modify an initial portrait during the editing procedure when the users move one or group of control points on the portrait other portrait control points are adjusted in order to automatically maintain the faceness of the edited portrait if the automated propagation function switch is optionally turned on finally the portrait is used as query input to search for and reconstruct its corresponding facial expression from pre recorded facial motion capture database we showed that this system is effective for rapid facial expression sculpting through comparative user study
software frameworks and libraries are indispensable to today's software systems as they evolve it is often time consuming for developers to keep their code up to date so approaches have been proposed to facilitate this usually these approaches cannot automatically identify change rules for one replaced by many and many replaced by one methods and they trade off recall for higher precision using one or more experimentally evaluated thresholds we introduce aura novel hybrid approach that combines call dependency and text similarity analyses to overcome these limitations we implement it in java system and compare it on five frameworks with three previous approaches by dagenais and robillard kim et al and sch auml fer et al the comparison shows that on average the recall of aura is higher while its precision is similar eg lower
as the popularity of areas including document storage and distributed systems continues to grow the demand for high performance xml databases is increasingly evident this has led to number of research efforts aimed at exploiting the maturity of relational database systems in order to increase xml query performance in our approach we use an index structure based on metamodel for xml databases combined with relational database technology to facilitate fast access to xml document elements the query process involves transforming xpath expressions to sql which can be executed over our optimised query engine as there are many different types of xpath queries varying processing logic may be applied to boost performance not only to individual xpath axes but across multiple axes simultaneously this paper describes pattern based approach to xpath query processing which permits the execution of group of xpath location steps in parallel
in yen defines class of formulas for paths in petri nets and claims that its satisfiability problem is expspace complete in this paper we show that in fact the satisfiability problem for this class of formulas is as hard as the reachability problem for petri nets moreover we salvage almost all of yen's results by defining fragment of this class of formulas for which the satisfiability problem is expspace complete by adapting his proof
we present declarative and visual debugging environment for eclipse called jive traditional debugging is procedural in that programmer must proceed step by step and object by object in order to uncover the cause of an error in contrast we present declarative approach to debugging consisting of flexible set of queries over program's execution history as well as over individual runtime states this runtime information is depicted in visual manner during program execution in order to aid the debugging process the current state of execution is depicted through an enhanced object diagram and the history of execution is depicted by sequence diagram our methodology makes use of these diagrams as means of formulating queries and reporting results in visual manner it also supports revisiting past runtime states either through reverse stepping of the program or through queries that report information from past states eclipse serves as an ideal framework for implementing jive since like the jive architecture it makes crucial use of the java platform debugging architecture jpda this paper presents details of the jive architecture and its integration into eclipse
the wide adoption of xml has increased the interest of the database community on tree structured data management techniques querying capabilities are provided through tree pattern queries the need for querying tree structured data sources when their structure is not fully known and the need to integrate multiple data sources with different tree structures have driven recently the suggestion of query languages that relax the complete specification of tree pattern in this paper we use query language which allows partial tree pattern queries ptpqs the structure in ptpq can be flexibly specified fully partially or not at all to evaluate ptpq we exploit index graphs which generate an equivalent set of complete tree pattern queriesin order to process ptpqs we need to efficiently solve the ptpq satisfiability and containment problems these problems become more complex in the context of ptpqs because the partial specification of the structure allows new non trivial structural expressions to be derived from those explicitly specified in ptpq we address the problem of ptpq satisfiability and containment in the absence and in the presence of index graphs and we provide necessary and sufficient conditions for each case to cope with the high complexity of ptpq containment in the presence of index graphs we study family of heuristic approaches for ptpq containment based on structural information extracted from the index graph in advance and on the fly we implement our approaches and we report on their extensive experimental evaluation and comparison
modeling and simulating pipelined processors in procedurallanguages such as requires lots of cost in handlingconcurrent events which hinders fast simulation number ofresearches on simulation have devised speed up techniques toreduce the number of events this paper presents newsimulation approach developed to enhance the simulation ofpipelined processors the proposed approach is based on earlypipeline evaluation that all the intermediate values of aninstruction are computed in advance creating future state for thenext instructions the future state allows the next instructions tobe computed without considering data dependencies betweennearby instructions we apply this concept to building cycle accurate simulator for pipelined risc processor and achieve almost the same speed as the instruction level simulator
modern languages like java and rely on dynamic optimizations in virtual machines for better performance current dynamic optimizations are reactive their performance is constrained by the dependence on runtime sampling and the partial knowledge of the execution this work tackles the problems by developing set of techniques that make virtual machine evolve across production runs the virtual machine incrementally learns the relation between program inputs and optimization strategies so that it proactively predicts the optimizations suitable for new run the prediction is discriminative guarded by confidence measurement through dynamic self evaluation we employ an enriched extensible specification language to resolve the complexities in program inputs these techniques implemented in jikes rvm produce significant performance improvement on set of java applications
supporting dynamic reconfiguration is required even in highly constrained embedded systems to allow software patches and updates and to allow adaptations to changes in environmental and operating conditions without service interruption dynamic reconfiguration however is complex and error prone process in this paper we report our experience in implementing safe dynamic reconfigurations in embedded devices with limited resources our approach relies on component based framework for building reconfigurable operating systems and the use of domain specific language dsl for reconfiguration
event correlation has become the cornerstone of many reactive applications particularly in distributed systems however support for programming with complex events is still rather specific and rudimentary this paper presents eventjava an extension of java with generic support for event based distributed programming eventjava seamlessly integrates events with methods and broadcasting with unicasting of events it supports reactions to combinations of events and predicates guarding those reactions eventjava is implemented as framework to allow for customization of event semantics matching and dispatching we present its implementation based on compiler transforming specific primitives to java along with reference implementation of the framework we discuss ordering properties of eventjava through formalization of its core as an extension of featherweight java in performance evaluation we show that eventjava compares favorably to highly tuned database backed event correlation engine as well as to comparably lightweight concurrency mechanism
deriving local cost models for query optimization in dynamic multidatabase system mdbs is challenging issue in this paper we study how to evolve query cost model to capture slowly changing dynamic mdbs environment so that the cost model is kept up to date all the time two novel evolutionary techniques ie the shifting method and the block moving method are proposed the former updates cost model by taking up to date information from new sample query into consideration at each step while the latter considers block batch of new sample queries at each step the relevant issues including derivation of recurrence updating formulas development of efficient algorithms analysis and comparison of complexities and design of an integrated scheme to apply the two methods adaptively are studied our theoretical and experimental results demonstrate that the proposed techniques are quite promising in maintaining accurate cost models efficiently for slowly changing dynamic mdbs environment besides the application to mdbss the proposed techniques can also be applied to the automatic maintenance of cost models in self managing database systems
this study develops knowledge navigator model knm tm to navigate knowledge management km implementation journey the knm comprises two frameworks evaluation and calculation framework qualitative research methods including literature review in depth interviews focus groups and content analysis are conducted to construct the evaluation framework of knm an algorithm model is proposed in the calculation framework and cases survey was employed to obtain the initial version of the score ranges used to differentiate maturity levels several propositions and the corresponding knm were constructed we define the km maturity level into five stages knowledge chaotic stage knowledge conscientious stage km stage km advanced stage and km integration stage the evaluation framework of knm consists of three aspects three target management objects culture km process and information technology km activities and key areas kas the initial version of the score ranges was identified the study results can be referenced and the methodology can be applied to other countries although the sample is confined to industries in taiwan
the importance and benefits of expertise sharing for organizations in knowledge economy are well recognized however the potential cost of expertise sharing is less well understood this paper proposes conceptual framework called collective attention economy to identify the costs associated with expertise sharing and provide the basis for analyzing and understanding the cost benefit structure of different communication mechanisms to demonstrate the analytical power of the conceptual framework the paper describes new communication mechanism dynamic mailing list dml that is developed by adjusting certain cost factors
programs written in managed languages are compiled to platform independent intermediate representation such as java bytecode the relative high level of java bytecode has engendered widespread practice of changing the bytecode directly without modifying the maintained version of the source code this practice called bytecode engineering or enhancement has become indispensable in introducing various concerns including persistence distribution and security transparently for example transparent persistence architectures help avoid the entanglement of business and persistence logic in the source code by changing the bytecode directly to synchronize objects with stable storage with functionality added directly at the bytecode level the source code reflects only partial semantics of the program specifically the programmer can neither ascertain the program's runtime behavior by browsing its source code nor map the runtime behavior back to the original source code this paper presents an approach that improves the utility of source level programming tools by providing enhancement specifications written in domain specific language by interpreting the specifications source level programming tool can gain an awareness of the bytecode enhancements and improve its precision and usability we demonstrate the applicability of our approach by making source code editor and symbolic debugger enhancements aware
software changes during their life cycle software systems experience wide spectrum of changes from minor modifications to major architectural shifts small scale changes are usually performed with text editing and refactorings while large scale transformations require dedicated program transformation languages for medium scale transformations both approaches have disadvantages manual modifications may require myriad of similar yet not identical edits leading to errors and omissions while program transformation languages have steep learning curve and thus only pay off for large scale transformationswe present system supporting example based program transformation to define transformation programmer performs an example change manually feeds it into our system and generalizes it to other application contexts with time developer can build palette of reusable medium sized code transformations we provide detailed description of our approach and illustrate it with examples
future multicore processors will be more susceptible to variety of hardware failures in particular intermittent faults caused in part by manufacturing thermal and voltage variations can cause bursts of frequent faults that last from several cycles to several seconds or more due to practical limitations of circuit techniques cost effective reliability will likely require the ability to temporarily suspend execution on core during periods of intermittent faults we investigate three of the most obvious techniques for adapting to the dynamically changing resource availability caused by intermittent faults and demonstrate their different system level implications we show that system software reconfiguration has very high overhead that temporarily pausing execution on faulty core can lead to cascading livelock and that using spare cores has high fault free cost to remedy these and other drawbacks of the three baseline techniques we propose using thin hardware firmware layer to manage an overcommitted system one where the os is configured to use more virtual processors than the number of currently available physical cores we show that this proposed technique can gracefully degrade performance during intermittent faults of various duration with low overhead without involving system software and without requiring spare cores
refactorings are usually proposed in an ad hoc way because it is difficult to prove that they are sound with respect to formal semantics not guaranteeing the absence of type errors or semantic changes consequently developers using refactoring tools must rely on compilation and tests to ensure type correctness and semantics preservation respectively which may not be satisfactory to critical software development in this paper we formalize static semantics for alloy which is formal object oriented modeling language and encode it in prototype verification system pvs the static semantics formalization can be useful for specifying and proving that transformations in general not only refactorings do not introduce type errors for instance as we show here
entity matching is the problem of deciding if two given mentions in the data such as helen hunt and hunt refer to the same real world entity numerous solutions have been developed but they have not considered in depth the problem of exploiting integrity constraints that frequently exist in the domains examples of such constraints include mention with age two cannot match mention with salary and if two paper citations match then their authors are likely to match in the same order in this paper we describe probabilistic solution to entity matching that exploits such constraints to improve matching accuracy at the heart of the solution is generative model that takes into account the constraints during the generation process and provides well defined interpretations of the constraints we describe novel combination of em and relaxation labeling algorithms that efficiently learns the model thereby matching mentions in an unsupervised way without the need for annotated training data experiments on several real world domains show that our solution can exploit constraints to significantly improve matching accuracy by and that the solution scales up to large data sets
unifying terminology usages which captures more term semantics is useful for event clustering this paper proposes metric of normalized chain edit distance to mine incrementally controlled vocabulary from cross document coreference chains controlled vocabulary is employed to unify terms among different co reference chains novel threshold model that incorporates both time decay function and spanning window uses the controlled vocabulary for event clustering on streaming news under correct co reference chains the proposed system has performance increase compared to the baseline system and performance increase compared to the system without introducing controlled vocabulary furthermore chinese co reference resolution system with chain filtering mechanism is used to experiment on the robustness of the proposed event clustering system the clustering system using noisy co reference chains still achieves performance increase compared to the baseline system the above shows that our approach is promising
in this paper assuming that each node is incident with two or more fault free links we show that an dimensional alternating group graph can tolerate up to link faults where while retaining fault free hamiltonian cycle the proof is computer assisted the result is optimal with respect to the number of link faults tolerated previously without the assumption at most link faults can be tolerated for the same problem and the same graph
under the tuple level uncertainty paradigm we formalize the use of novel graphical model generator recognizer network grn as model of probabilistic databases the grn modeling framework is capable of representing much wider range of tuple dependency structure we show that grn representation of probabilistic database may undergo transitions induced by imposing constraints or evaluating queries we formalize procedures for these two types of transitions such that the resulting graphical models after transitions remain as grns this formalism makes grn self contained modeling framework and closed representation system for probabilistic databases property that is lacking in most existing models in addition we show that exploiting the transitional mechanisms allows systematic approach to constructing grns for arbitrary probabilistic data at arbitrary stages advantages of grns in query evaluation are also demonstrated
past research on probabilistic databases has studied the problem of answering queries on static database application scenarios of probabilistic databases however often involve the conditioning of database using additional information in the form of new evidence the conditioning problem is thus to transform probabilistic database of priors into posterior probabilistic database which is materialized for subsequent query processing or further refinement it turns out that the conditioning problem is closely related to the problem of computing exact tuple confidence values it is known that exact confidence computation is an np hard problem this has led researchers to consider approximation techniques for confidence computation however neither conditioning nor exact confidence computation can be solved using such techniques in this paper we present efficient techniques for both problems we study several problem decomposition methods and heuristics that are based on the most successful search techniques from constraint satisfaction such as the davis putnam algorithm we complement this with thorough experimental evaluation of the algorithms proposed our experiments show that our exact algorithms scale well to realistic database sizes and can in some scenarios compete with the most efficient previous approximation algorithms
server pages also called dynamic pages render generic web page into many similar ones the technique is commonly used for implementing web application user interfaces uis yet our previous study found high rate of repetitions also called lsquo clones rsquo in web applications particularly in uis the finding raised the question as to why such repetitions had not been averted with the use of server pages for an answer we conducted an experiment using php server pages to explore how far server pages can be pushed to achieve generic web applications our initial findings suggested that generic representation obtained using server pages sometimes compromises certain important system qualities such as run time performance it may also complicate the use of wysiwyg editors we have analysed the nature of these trade offs and now propose mixed strategy approach to obtain optimum generic representation of web applications without unnecessary compromise to critical system qualities and user experience the mixed strategy approach applies the generative technique of xvcl to achieve genericity at the meta level representation of web application leaving repetitions to the actual web application our experiments show that the mixed strategy approach can achieve good level of genericity without conflicting with other system qualities our findings should open the way for others to better informed decisions regarding generic design solutions which should in turn lead to simpler more maintainable and more reusable web applications copyright copy john wiley sons ltd
web data extraction systems in use today mainly focus on the generation of extraction rules ie wrapper induction thus they appear ad hoc and are difficult to integrate when holistic view is taken each phase in the data extraction process is disconnected and does not share common foundation to make the building of complete system straightforward in this paper we demonstrate holistic approach to web data extraction the principal component of our proposal is the notion of document schema document schemata are patterns of structures embedded in documents once the document schemata are obtained the various phases eg training set preparation wrapper induction and document classification can be easily integrated the implication of this is improved efficiency and better control over the extraction procedure our experimental results confirmed this more importantly because document can be represented as avector of schema it can be easily incorporated into existing systems as the fabric for integration
in this paper we propose the integration of services into social networks soaf service of friend to leverage the creation of the internet of services vision we show how to integrate services and humans into common network structure and discuss design and implementation issues in particular we discuss the required extensions to existing social network vocabulary with regard to services we illustrate scenario where this network structures can be applied in the context of service discovery and highlight the benefit of service enriched social network structure
recent spates of cyber attacks and frequent emergence of applications affecting internet traffic dynamics have made it imperative to develop effective techniques that can extract and make sense of significant communication patterns from internet traffic data for use in network operations and security management in this paper we present general methodology for building comprehensive behavior profiles of internet backbone traffic in terms of communication patterns of end hosts and services relying on data mining and information theoretic techniques the methodology consists of significant cluster extraction automatic behavior classification and structural modeling for in depth interpretive analyses we validate the methodology using data sets from the core of the internet the results demonstrate that it indeed can identify common traffic profiles as well as anomalous behavior patterns that are of interest to network operators and security analysts
tiling is crucial loop transformation for generating high performance code on modern architectures efficient generation of multi level tiled code is essential for maximizing data reuse in systems with deep memory hierarchies tiled loops with parametric tile sizes not compile time constants facilitate runtime feedback and dynamic optimizations used in iterative compilation and automatic tuning previous parametric multi level tiling approaches have been restricted to perfectly nested loops where all assignment statements are contained inside the innermost loop of loop nest previous solutions to tiling for imperfect loop nests have only handled fixed tile sizes in this paper we present an approach to parametric multi level tiling of imperfectly nested loops the tiling technique generates loops that iterate over full rectangular tiles making them amenable to compiler optimizations such as register tiling experimental results using number of computational benchmarks demonstrate the effectiveness of the developed tiling approach
popular web sites are expected to handle huge number of requests concurrently within reasonable time frame the performance of these web sites is largely dependent on effective thread management of their web servers although the implementation of static and dynamic thread policies is common practice remarkably little is known about the implications on performance moreover the commonly used policies do not take into account the complex interaction between the threads that compete for access to shared resource we propose new dynamic thread assignment policies that minimize the average response time of web servers the web server is modeled as two layered tandem of multi threading queues where the active threads compete for access to common resource this type of two layered queueing model which occurs naturally in the performance modeling of systems with intensive software hardware interaction are on the one hand appealing from an application point of view but on the other hand are challenging from methodological point of view our results show that the optimal dynamic thread assignment policies yield strong reductions in the response times validation on an apache web server shows that our dynamic thread policies confirm our analytical results
in constructive type theory recursive and corecursive definitions are subject to syntactic restrictions which guarantee termination for recursive functions and productivity for corecursive functions however many terminating and productive functions do not pass the syntactic tests bove proposed in her thesis an elegant reformulation of the method of accessibility predicates that widens the range of terminative recursive functions formalisable in constructive type theory in this paper we pursue the same goal for productive corecursive functions notably our method of formalisation of coinductive definitions of productive functions in coq requires not only the use of ad hoc predicates but also systematic algorithm that separates the inductive and coinductive parts of functions
probabilistic or randomized algorithms are fast becoming as commonplace as conventional deterministic algorithms this survey presents five techniques that have been widely used in the design of randomized algorithms these techniques are illustrated using randomized algorithms mdash both sequential and distributed mdash that span wide range of applications includingprimality testing classical problem in number theory interactive probabilistic proof systems new method of program testing dining philosophers classical problem in distributed computing and byzantine agreement reaching agreement in the presence of malicious processors included with each algorithm is discussion of its correctness and its computational complexity several related topics of interest are also addressed including the theory of probabilistic automata probabilistic analysis of conventional algorithms deterministic amplification and derandomization of randomized algorithms finally comprehensive annotated bibliography is given
approximate query answering has recently emerged as an effective method for generating viable answer among various techniques for approximate query answering wavelets have received lot of attention however wavelet techniques minimizing the root squared error ie the norm error have several problems such as the poor quality of reconstructed data when the original data is biased in this paper we present amid approximation of multi measured data using svd for multi measured data in amid we adapt the singular value decomposition svd to compress multi measured data we show that svd guarantees the root squared error and also drive an error bound of svd for an individual data value using mathematical analyses in addition in order to improve the accuracy of approximated data we combine svd and wavelets in amid since svd is applied to fixed matrix we use various properties of matrices to adapt svd to the incremental update environment we devise two variants of amid for the incremental update environment incremental amid and local amid to the best of our knowledge our work is the first to extend svd to incremental update environments
in this paper we propose two simple and efficient schemes for establishing anonymity in clustered wireless sensor networks cwsns the first scheme simple anonymity scheme sas uses range of pseudonyms as identifiers for node to ensure concealment of its true identifier id after deployment neighbouring nodes share their individual pseudonyms and use them for anonymous communication the second scheme cryptographic anonymity scheme cas uses keyed cryptographic one way hash function kchf for id concealment in cas neighbouring nodes securely share the information used by the kchf to generate pseudonyms even when many nodes in neighbourhood are compromised and are colluding our schemes guarantee complete anonymity to non compromised nodes during their mutual communication our schemes have reasonably low memory and computation costs they can be embedded into any wireless sensor network routing protocol to ensure anonymity and privacy during node discovery routing and data delivery
crucial step in volume rendering is the design of transfer functions that will highlight those aspects of the volume data that are of interest to the user for many applications boundaries carry most of the relevant information reliable detection of boundaries is often hampered by limitations of the imaging process such as blurring and noise we present method to identify the materials that form the boundaries these materials are then used in new domain that facilitates interactive and semiautomatic design of appropriate transfer functions we also show how the obtained boundary information can be used in region growing based segmentation
sensitivity analysis sa is novel compiler technique that complements and integrates with static automatic parallelization analysis for the cases when program behavior is input sensitive sa can extract all the input dependent statically unavailable conditions for which loops can be dynamically parallelized sa generates sequence of sufficient conditions which when evaluated dynamically in order of their complexity can each validate the dynamic parallel execution of the corresponding loop while sa's principles are fairly simple implementing it in real compiler and obtaining good experimental results on benchmark codes is difficult task in this paper we present some of the most important implementation issues that we had to overcome in order to achieve fairly successful automatic parallelizer we present techniques related to validating dependence removing transformations eg privatization or pushback parallelization and static and dynamic evaluation of complex conditions for loop parallelization we concern ourselves with multi version and parallel code generation as well as the use of speculative parallelization when other less costly options fail we present summary table of the contributions of our techniques to the successful parallelization of industry benchmark codes we also report speedups and parallel coverage of these codes on two multicore based systems and compare them to results obtained by the ifort compiler
the last years have seen the definition of many languages models and standards tailored to specify and enforce access control policies but such frameworks do not provide methodological support during the policy specification process in particular they do not provide facilities for the analysis of the social context where the system operatesin this paper we propose model driven approach for the specification and analysis of access control policies we build this framework on top of si modeling language tailored to capture and analyze functional and security requirements of socio technical systems the framework also provides formal mechanisms to assist policy writers and system administrators in the verification of access control policies and of the actual user permission assignment
we propose hybrid method for simulating multiphase fluids such as bubbly water the appearance of subgrid visual details is improved by incorporating new bubble model based on smoothed particle hydrodynamics sph into an eulerian grid based simulation that handles background flows of large bodies of water and air to overcome the difficulty in simulating small bubbles in the context of the multiphase flows on coarse grid we heuristically model the interphase properties of water and air by means of the interactions between bubble particles as result we can animate lively motion of bubbly water with small scale details efficiently
knowing patterns of relationship in social network is very useful for law enforcement agencies to investigate collaborations among criminals for businesses to exploit relationships to sell products or for individuals who wish to network with others after all it is not just what you know but also whom you know that matters however finding out who is related to whom on large scale is complex problem asking every single individual would be impractical given the huge number of individuals and the changing dynamics of relationships recent advancement in technology has allowed more data about activities of individuals to be collected such data may be mined to reveal associations between these individuals specifically we focus on data having space and time elements such as logs of people's movement over various locations or of their internet activities at various cyber locations reasoning that individuals who are frequently found together are likely to be associated with each other we mine from the data instances where several actors co occur in space and time presumably due to an underlying interaction we call these spatio temporal co occurrences events which we use to establish relationships between pairs of individuals in this paper we propose model for constructing social network from events and provide an algorithm that mines these events from the data experiments on real life data tracking people's accesses to cyber locations have also yielded encouraging results
understanding bgp routing dynamics is critical to the solid growth and maintenance of the internet routing infrastructure however while the most extensive study on bgp dynamics is nearly decade old many factors that could affect bgp dynamics have changed considerably we revisit this important topic in this paper focusing on not only comparing with the previous results but also issues not well explored before we have found that compared to almost decade ago although certain characteristics remain unchanged such as some temporal properties bgp dynamics are now busier and more importantly now have much less pathological behavior and are healthier√¢ for example forwarding dynamics are now not only dominant but also more consistent across different days contributions to bgp dynamics by different bgp peers√¢ which are not proportional to the size of peer√¢ as√¢ are also more stable and dynamics due to policy changes or duplicate announcements are usually from specific peers
the paper proposes variation of simulation for checking and proving contextual equivalence in non deterministic call by need lambda calculus with constructors case seq and letrec with cyclic dependencies it also proposes novel method to prove its correctness the calculus semantics is based on small step rewrite semantics and on may convergence the cyclic nature of letrec bindings as well as non determinism makes known approaches to prove that simulation implies contextual preorder such as howe's proof technique inapplicable in this setting the basic technique for the simulation as well as the correctness proof is called pre evaluation which computes set of answers for every closed expression if simulation succeeds in finite computation depth then it is guaranteed to show contextual preorder of expressions
automatic emotion sensing in textual data is crucial for the development of intelligent interfaces in many interactive computer applications this paper describes high precision knowledgebase independent approach for automatic emotion sensing for the subjects of events embedded within sentences the proposed approach is based on the probability distribution of common mutual actions between the subject and the object of an event we have incorporated web based text mining and semantic role labeling techniques together with number of reference entity pairs and hand crafted emotion generation rules to realize an event emotion detection system the evaluation outcome reveals satisfactory result with about accuracy for detecting the positive negative and neutral emotions
this chapter discusses usability engineering approach for the design and the evaluation of adaptive web based systems focusing on practical issues list of methods will be presented considering user centered approach after having introduced the peculiarities that characterize the evaluation of adaptive web based systems the chapter describes the evaluation methodologies following the temporal phases of evaluation according to user centered approach three phases are distinguished requirement phase preliminary evaluation phase and final evaluation phase moreover every technique is classified according to set of parameters that highlight the practical exploitation of that technique for every phase the appropriate techniques are described by giving practical examples of their application in the adaptive web number of issues that arise when evaluating an adaptive system are described and potential solutions and workarounds are sketched
we investigate the problem of refining sql queries to satisfy cardinality constraints on the query result this has applications to the many few answers problems often faced by database users we formalize the problem of query refinement and propose framework to support it in database system we introduce an interactive model of refinement that incorporates user feedback to best capture user preferences our techniques are designed to handle queries having range and equality predicates on numerical and categorical attributes we present an experimental evaluation of our framework implemented in an open source data manager and demonstrate the feasibility and practical utility of our approach
using an underlying role based model for the administration of roles has proved itself to be successful approach this paper sets out to describe the enterprise role based access control model erbac in the context of sam jupiter commercial enterprise security management softwarewe provide an overview of the role based conceptual model underlying sam jupiter having established this basis we describe how the model is used to facilitate role based administration approach in particular we discuss our notion of scopes which describe the objects over which an administrator has authority the second part provides case study based on our real world experiences in the implementation of role based administrative infrastructures finally critical evaluation and comparison with current approaches to administrative role based access control is provided
inductive logic programming or relational learning is powerful paradigm for machine learning or data mining however in order for ilp to become practically useful the efficiency of ilp systems must improve substantially to this end the notion of query pack is introduced it structures sets of similar queries furthermore mechanism is described for executing such query packs complexity analysis shows that considerable efficiency improvements can be achieved through the use of this query pack execution mechanism this claim is supported by empirical results obtained by incorporating support for query pack execution in two existing learning systems
distributed data stream processing applications are often characterized by data flow graphs consisting of large number of built in and user defined operators connected via streams these flow graphs are typically deployed on large set of nodes the data processing is carried out on the fly as tuples arrive at possibly very high rates with minimum latency it is well known that developing and debugging distributed multi threaded and asynchronous applications such as stream processing applications can be challenging thus without domain specific debugging support developers struggle when debugging distributed applications in this paper we describe tools and language support to support debugging distributed stream processing applications our key insight is to view debugging of stream processing applications from four different but related perspectives first debugging the semantics of the application involves verifying the operator level composition and inspecting the flows at the logical level second debugging the user defined operators involves traditional source code debugging but strongly tied to the stream level interactions third debugging the deployment details of the application require understanding the runtime physical layout and configuration of the application fourth debugging the performance of the application requires inspecting various performance metrics such as communication rates cpu utilization etc associated with streams operators and nodes in the system in light of this characterization we developed several tools such as debugger aware compiler and an associated stream debugger composition and deployment visualizers and performance visualizers as well as language support such as configuration knobs for logging and tracing deployment configurations such as operator to process and process to node mappings monitoring directives to inspect streams and special sink adapters to intercept and dump streaming data to files and sockets to name few we describe these tools in the context of spade mdash language for creating distributed stream processing applications and system mdash distributed stream processing middleware under development at the ibm watson research center published in by john wiley sons ltd this article is us government work and is in the public domain in the usa
most existing pp networks route requests in kn log log log hops where is the number of participating nodes and is an adjustable parameter although some can achieve hop routing for constant by tuning the parameter the neighbor locations however become function of causing considerable maintenance overhead if the user base is highly dynamic as witnessed by the deployed systems this paper explores the design space using the simple uniformly random neighbor selection strategy and proposes random peer to peer network that is the first of its kind to resolve requests in hops with chosen probability of where is constant the number of neighbors per node is within constant factor from the optimal complexity for any network whose routing paths are bounded by hops
mails concerning the development issues of system constitute an important source of information about high level design decisions low level implementation concerns and the social structure of developers establishing links between mails and the software artifacts they discuss is non trivial problem due to the inherently informal nature of human communication different approaches can be brought into play to tackle this trace ability issue but the question of how they can be evaluated remains unaddressed as there is no recognized benchmark against which they can be compared in this article we present such benchmark which we created through the manual inspection of statistically significant number of mails pertaining to six unrelated software systems we then use our benchmark to measure the effectiveness of number of approaches ranging from lightweight approaches based on regular expressions to full fledged information retrieval approaches
in this paper we propose an automated system able to detect volume based anomalies in network traffic caused by denial of service dos attacks we designed system with two stage architecture that combines more traditional change point detection approaches adaptive threshold and cumulative sum with novel one based on the continuous wavelet transform the presented anomaly detection system is able to achieve good results in terms of the trade off between correct detections and false alarms estimation of anomaly duration and ability to distinguish between subsequent anomalies we test our system using set of publicly available attack free traffic traces to which we superimpose anomaly profiles obtained both as time series of known common behaviors and by generating traffic with real tools for dos attacks extensive test results show how the proposed system accurately detects wide range of dos anomalies and how the performance indicators are affected by anomalies characteristics ie amplitude and duration moreover we separately consider and evaluate some special test cases
frequent pattern mining is popular topic in data mining with the advance of this technique privacy issues attract more and more attention in recent years in this field previous works based hiding sensitive information on uniform support threshold or disclosure threshold however in practical applications we probably need to apply different support thresholds to different itemsets for reflecting their significance in this paper we propose new hiding strategy to protect sensitive frequent patterns with multiple sensitive thresholds based on different sensitive thresholds the sanitized dataset is able to highly fulfill user requirements in real applications while preserving more information of the original dataset empirical studies show that our approach can protect sensitive knowledge well not only under multiple thresholds but also under uniform threshold moreover the quality of the sanitized dataset can be maintained
web page classification is one of the essential techniques for web mining specifically classifying web pages of user interesting class is the first step of mining interesting information from the web however constructing classifier for an interesting class requires laborious pre processing such as collecting positive and negative training examples for instance in order to construct homepage classifier one needs to collect sample of homepages positive examples and sample of non homepages negative examples in particular collecting negative training examples requires arduous work and special caution to avoid biasing them we introduce in this paper the positive example based learning pebl framework for web page classification which eliminates the need for manually collecting negative training examples in pre processing we present an algorithm called mapping convergence that achieves classification accuracy with positive and unlabeled data as high as that of traditional svm with positive and negative data our experiments show that when the algorithm uses the same amount of positive examples as that of traditional svm the algorithm performs as well as traditional svm
since software systems need to be continuously available under varying conditions their ability to evolve at runtime is increasingly seen as one key issue modern programming frameworks already provide support for dynamic adaptations however the high variability of features in dynamic adaptive systems das introduces an explosion of possible runtime system configurations often called modes and mode transitions designing these configurations and their transitions is tedious and error prone making the system feature evolution difficult while aspect oriented modeling aom was introduced to improve the modularity of software this paper presents how an aom approach can be used to tame the combinatorial explosion of das modes using aom techniques we derive wide range of modes by weaving aspects into an explicit model reflecting the runtime system we use these generated modes to automatically adapt the system we validate our approach on an adaptive middleware for home automation currently deployed in rennes metropolis
interactive multi user internet games require frequent state updates between players to accommodate the great demand for reality and interactivity the large latency and limited bandwidth on the internet greatly affects the games scalability the high level architecture hla is the ieee standard for distributed simulation with its data distribution management ddm service group assuming the functionalities of interest management with its support for reuse and interoperability and its ddm support for communication optimization the hla is promising at supporting multi user gaming on the internet however this usually requires particular prior security setup across administrative domains according to the specific run time infrastructure rti used we have previously developed service oriented hla rti sohr which enables distributed simulations to be conducted across administrated domains on the grid this paper discusses multi user gaming on the grid using sohr specifically maze game is used to illustrate how sohr enables users to join game conveniently experiments have been carried out to show how ddm can improve the communication efficiency
the world is changing and so must the data that describes its history not surprisingly considerable research effort has been spent in databases along this direction covering topics such as temporal models and schema evolution topic that has not received much attention however is that of concept evolution for example germany instance level concept has evolved several times in the last century as it went through different governance structures then split into two national entities that eventually joined again likewise caterpillar is transformed into butterfly while mother becomes two maternally related entities as well the concept of whale class level concept changed over the past two centuries thanks to scientific discoveries that led to better understanding of what the concept entails in this work we present formal framework for modeling querying and managing such evolution in particular we describe how to model the evolution of concept and how this modeling can be used to answer historical queries of the form how has concept evolved over period our proposal extends an rdf like model with temporal features and evolution operators then we provide query language that exploits these extensions and supports historical queries
blog post opinion retrieval aims at finding blog posts that are relevant and opinionated about user's query in this paper we propose simple probabilistic model for assigning relevant opinion scores to documents the key problem is how to capture opinion expressions in the document that are related to the query topic current solutions enrich general opinion lexicons by finding query specific opinion lexicons using pseudo relevance feedback on external corpora or the collection itself in this paper we use general opinion lexicon and propose using proximity information in order to capture opinion term relatedness to the query we propose proximity based opinion propagation method to calculate the opinion density at each point in document the opinion density at the position of query term in the document can then be considered as the probability of opinion about the query term at that position the effect of different kernels for capturing the proximity is also discussed experimental results on the blog dataset show that the proposed method provides significant improvement over standard trec baselines and achieves increase in map over the best performing run in the trec blog track
with the increase in the size of data sets data mining has recently become an important research topic and is receiving substantial interest from both academia and industry at the same time interest in temporal databases has been increasing and growing number of both prototype and implemented systems are using an enhanced temporal understanding to explain aspects of behavior associated with the implicit time varying nature of the universe this paper investigates the confluence of these two areas surveys the work to date and explores the issues involved and the outstanding problems in temporal data mining
this paper examines some issues that affect the efficiency and fairness of the transmission control protocol tcp the backbone of internet protocol communication in multihops satellite network systems it proposes scheme that allows satellite systems to automatically adapt to any change in the number of active tcp flows due to handover occurrence the free buffer size and the bandwidth delay product of the networkthe proposed scheme has two major design goals increasing the system efficiency and improving its fairness the system efficiency is controlled by matching the aggregate traffic rate to the sum of the link capacity and total buffer size on the other hand the system min max fairness is achieved by allocating bandwidth among individual flows in proportion with their rtts the proposed scheme is dubbed recursive explicit and fair window adjustment refwa simulation results elucidate that the refwa scheme substantially improves the system fairness reduces the number of packet drops and makes better utilization of the bottleneck link the results demonstrate also that the proposed scheme works properly in more complicated environments where connections traverse multiple bottlenecks and the available bandwidth may change over data transmission time
propositional satisfiability solving or sat is an important reasoning task arising in numerous applications such as circuit design formal verification planning scheduling or probabilistic reasoning the depth first search dpll procedure is in practice the most efficient complete algorithm to date previous studies have shown the theoretical and experimental advantages of decomposing propositional formulas to guide the ordering of variable instantiation in dpll however in practice the computation of tree decomposition may require considerable amount of time and space on large formulas existing decomposition tools are unable to handle most currently challenging sat instances because of their size in this paper we introduce simple fast and scalable method to quickly produce tree decompositions of large sat problems we show experimentally the efficiency of orderings derived from these decompositions on the solving of challenging benchmarks
understanding web document and the sections inside the document is very important for web transformation and information retrieval from web pages detecting pagelets which are small features located inside web page in order to understand web document's structure is difficult problem current work on pagelet detection focuses only on finding the location of the pagelet without regard to its functionality we describe method to detect both the location and functionality of pagelets using html element patterns for each pagelet type an html element pattern is created and matched to web page sections of the web page that matches the patterns are marked as pagelet candidates we test this technique on multiple popular web pages from the news and commerce genres we find that this method adequately recalls various pagelets from the web page
the assumption of routing symmetry is often embedded into traffic analysis and classification tools this paper uses passively captured network data to estimate the amount of traffic actually routed symmetrically on specific link we propose flow based symmetry estimator fse set of metrics to assess symmetry in terms of flows packets and bytes which disregards inherently asymmetrical traffic such as udp icmp and tcp background radiation this normalized metric allows fair comparison of symmetry across different links we evaluate our method on large heterogeneous dataset and confirm anecdotal reports that routing symmetry typically does not hold for non edge internet links and decreases as one moves toward core backbone links due to routing policy complexity our proposed metric for traffic asymmetry induced by routing policies will help the community improve traffic characterization techniques and formats but also support quantitative formalization of routing policy effects on links in the wild
we present specification language called action language for model checking software specifications action language forms an interface between transition system models that model checker generates and high level specification languages such as statecharts rsml and scr mdash similar to an assembly language between microprocessor and programming language we show that action language translations of statecharts and scr specifications are compact and they preserve the structure of the original specification action language allows specification of both synchronous and asynchronous systems it also supports modular specifications to enable compositional model checking
in previous work we have designed tracking protocol stalk for wireless sensor networks and proved it to be self stabilizing at the pseudo code automata level however it is very challenging to achieve and verify self stabilization of the same protocol at the implementation tinyos level due to the size of the corresponding program at the implementation level in this paper we present lightweight and practical method for specification based design of stabilization and illustrate this method on the stalk protocol as our case study
the analysis of large execution traces is almost impossible without efficient tool support lately there has been an increase in the number of tools for analyzing traces generated from object oriented systems this interest has been driven by the fact that polymorphism and dynamic binding pose serious limitations to static analysis however most of the techniques supported by existing tools are found in the context of very specific visualization schemes which makes them hard to reuse it is also very common to have two different tools implement the same techniques using different terminology this appears to result from the absence of common framework for trace analysis approaches this paper presents the state of the art in the area of trace analysis we do this by analyzing the techniques that are supported by eight trace exploration tools we also discuss their advantages and limitations and how they can be improved
context aware application in the pervasive computing environment provides intuitive user centric services using implicit context cues personalization and control are important issues for this class of application as they enable end users to understand and configure the behavior of an application however most development efforts for building context aware applications focus on the sensor fusion and machine learning algorithms to generate and distribute context cues that drive the application with little emphasis on user centric issues we argue that to elevate user experiences with context aware applications it is very important to address these personalization and control issues at the system interface level in parallel to context centric design towards this direction we present persona toolkit that provides support for extending context aware applications with end user personalization and control features specifically persona exposes few application programming interfaces that abstract end user customization and control mechanisms and enables developers to integrate these user centric aspects with rest of the application seamlessly there are two primary advantages of persona first it can be used with various existing middlewares as ready to use plug in to build customizable and controllable context aware applications second existing context aware applications can easily be augmented to provide end user personalization and control support in this paper we discuss the design and implementation of persona and demonstrate its usefulness through the development and augmentation of range of common context aware applications
most file systems attempt to predict which disk blocks will be needed in the near future and prefetch them into memory this technique can improve application throughput as much as but why the reasons include that the disk cache comes into play the device driver amortizes the fixed cost of an operation over larger amount of data total disk seek time can be decreased and that programs can overlap computation and however intuition does not tell us the relative benefit of each of these causes or techniques for increasing the effectiveness of prefetching to answer these questions we constructed an analytic performance model for file system reads the model is based on bsd derived file system and parameterized by the access patterns of the files layout of files on disk and the design characteristics of the file system and of the underlying disk we then validated the model against several simple workloads the predictions of our model were typically within of measured values and differed at most by from measured values using the model and experiments we explain why and when prefetching works and make proposals for how to tune file system and disk parameters to improve overall system throughput
in order to provide general access control methodology for parts of xml documents we propose combining role based access control as found in the role graph model with methodology originally designed for object oriented databases we give description of the methodology showing how different access modes xpath expressions and roles can be combined and how propagation of permissions is handled given this general approach system developer can design complex authorization model for collections of xml documents
we present new window based method for correspondence search using varying support weights we adjust the support weights of the pixels in given support window based on color similarity and geometric proximity to reduce the image ambiguity our method outperforms other local methods on standard stereo benchmarks
increasing risks of spoof attacks and other common problems of unimodal biometric systems such as intra class variations non universality and noisy data necessitate the use of multimodal biometrics the face and the ear are highly attractive biometric traits for combination because of their physiological structure and location besides both of them can be acquired non intrusively however changes of facial expressions variations in pose scale and illumination and the presence of hair and ornaments present some genuine challenges in this paper local feature based approach is proposed to fuse ear and face biometrics at the score level experiments with frgc and the university of notre dame biometric databases show that the technique achieves an identification rate of and verification rate of at far for fusion of the ear with neutral face biometrics it is also found to be fast and robust to facial expressions achieving and identification and verification rates respectively
synchronous data flow languages such as scade lustre manage infinite sequences or streams as primitive values making them naturally adapted to the description of dominated systems their conservative extension with means to define control structures or modes has been long term research topic through which several solutions have emergedin this paper we pursue this effort and generalize existing solutions by providing two constructs general form of state machines called parameterized state machines and valued signals as can be found in esterel parameterized state machines greatly reduce the reliance on error prone mechanisms such as shared memory in automaton based programming signals provide new way of programming with multi rate data in synchronous dataflow languages together they allow for much more direct and natural programming of systems that combine dataflow and state machinesthe proposed extension is fully implemented in the new lucid synchrone compiler
this paper describes new hierarchical approach to content based image retrieval called the customized queries approach cqa contrary to the single feature vector approach which tries to classify the query and retrieve similar images in one step cqa uses multiple feature sets and two step approach to retrieval the first step classifies the query according to the class labels of the images using the features that best discriminate the classes the second step then retrieves the most similar images within the predicted class using the features customized to distinguish subclasses within that class needing to find the customized feature subset for each class led us to investigate feature selection for unsupervised learning as result we developed new algorithm called fssem feature subset selection using expectation maximization clustering we applied our approach to database of high resolution computed tomography lung images and show that cqa radically improves the retrieval precision over the single feature vector approach to determine whether our cbir system is helpful to physicians we conducted an evaluation trial with eight radiologists the results show that our system using cqa retrieval doubled the doctors diagnostic accuracy
blind scheduling policies schedule tasks without knowledge of the tasks remaining processing times existing blind policies such as fcfs ps and las have proven useful in network and operating system applications but each policy has separate vastly differing description leading to separate and distinct implementations this paper presents the design and implementation of configurable blind scheduler that contains continuous tunable parameter by merely changing the value of this parameter the scheduler's policy exactly emulates or closely approximates several existing standard policies other settings enable policies whose behavior is hybrid of these standards we demonstrate the practical benefits of such configurable scheduler by implementing it into the linux operating system we show that we can emulate the behavior of linux's existing more complex scheduler with single hybrid setting of the parameter we also show using synthetic workloads that the best value for the tunable parameter is not unique but depends on distribution of the size of tasks arriving to the system finally we use our formulation of the configurable scheduler to contrast the behavior of various blind schedulers by exploring how various properties of the scheduler change as we vary our scheduler's tunable parameter
in this paper we investigate generic methods for placing photos uploaded to flickr on the world map as primary input for our methods we use the textual annotations provided by the users to predict the single most probable location where the image was taken central to our approach is language model based entirely on the annotations provided by users we define extensions to improve over the language model using tag based smoothing and cell based smoothing and leveraging spatial ambiguity further we demonstrate how to incorporate geonames footnote http wwwgeonamesorg visited may large external database of locations for varying levels of granularity we are able to place images on map with at least twice the precision of the state of the art reported in the literature
as massively parallel computers proliferate there is growing interest in finding ways by which performance of massively parallel codes can be efficiently predicted this problem arises in diverse contexts such as parallelizing compilers parallel performance monitoring and parallel algorithm development in this paper we describe one solution where one directly executes the application code but uses discrete event simulator to model details of the presumed parallel machine such as operating system and communication network behavior because this approach is computationally expensive we are interested in its own parallelization specifically the parallelization of the discrete event simulator we describe methods suitable for parallelized direct execution simulation of message passing parallel programs and report on the performance of such system lapse large application parallel simulation environment we have built on the intel paragon on all codes measured to date lapse predicts performance well typically within relative error depending on the nature of the application code we have observed low slowdowns relative to natively executing code and high relative speedups using up to processors
formal verification techniques are not yet widely used in the software industry perhaps because software tends to be more complex than hardware and the penalty for bugs is often lower software can be patched after the release instead large amount of time and money is being spent on software testing which misses many subtle errors especially in concurrent programs increased use of concurrency eg due to the popularity of web services and the surge of complex viruses which exploit security vulnerabilities of software make the problem of creating verifying compiler for production quality code essential and urgent
multimedia and network processing applications make extensive use of subword data since registers are capable of holding full data word when subword variable is assigned register only part of the register is used new embedded processors have started supporting instruction sets that allow direct referencing of bit sections within registers and therefore multiple subword variables can be made to simultaneously reside in the same register without hindering accesses to these variables however new register allocation algorithm is needed that is aware of the bitwidths of program variables and is capable of packing multiple subword variables into single register this paper presents one such algorithmthe algorithm we propose has two key steps first combination of forward and backward data flow analyses are developed to determine the bitwidths of program variables throughout the program this analysis is required because the declared bitwidths of variables are often larger than their true bitwidths and moreover the minimal bitwidths of program variable can vary from one program point to another second novel interference graph representation is designed to enable support for fast and highly accurate algorithm for packing of subword variables into single register packing is carried out by node coalescing phase that precedes the conventional graph coloring phase of register allocation in contrast to traditional node coalescing packing coalesces set of interfering nodes our experiments show that our bitwidth aware register allocation algorithm reduces the register requirements by to over traditional register allocation algorithm that assigns separate registers to simultaneously live subword variables
the core task of sponsored search is to retrieve relevant ads for the user's query ads can be retrieved either by exact match when their bid term is identical to the query or by advanced match which indexes ads as documents and is similar to standard information retrieval ir recently there has been great deal of research into developing advanced match ranking algorithms however no previous research has addressed the ad indexing problem unlike most traditional search problems the ad corpus is defined hierarchically in terms of advertiser accounts campaigns and ad groups which further consist of creatives and bid terms this hierarchical structure makes indexing highly non trivial as naively indexing all possible displayable ads leads to prohibitively large and ineffective index we show that ad retrieval using such an index is not only slow but its precision is suboptimal as well we investigate various strategies for compact hierarchy aware indexing of sponsored search ads through adaptation of standard ir indexing techniques we also propose new ad retrieval method that yields more relevant ads by exploiting the structured nature of the ad corpus experiments carried out over large ad test collection from commercial search engine show that our proposed methods are highly effective and efficient compared to more standard indexing and retrieval approaches
existing methods for measuring the quality of search algorithms use static collection of documents set of queries and mapping from the queries to the relevant documents allow the experimenter to see how well different search engines or engine configurations retrieve the correct answers this methodology assumes that the document set and thus the set of relevant documents are unchanging in this paper we abandon the static collection requirement we begin with recent trec collection created from web crawl and analyze how the documents in that collection have changed over time we determine how decay of the document collection affects trec systems and present the results of an experiment using the decayed collection to measure live web search system we employ novel measures of search effectiveness that are robust despite incomplete relevance information lastly we propose methodology of collection maintenance which supports measuring search performance both for single system and between systems run at different points in time
xml has tree structured data model which is used to uniformly represent structured as well as semi structured data and also enable concise query specification in xquery via the use of its xpath twig patterns this in turn can leverage the recently developed technology of structural join algorithms to evaluate the query efficiently in this paper we identify fundamental tension in xml data modeling data represented as deep trees which can make effective use of twig patterns are often un normalized leading to update anomalies while ii normalized data tends to be shallow resulting in heavy use of expensive value based joins in queriesour solution to this data modeling problem is novel multi colored trees mct logical data model which is an evolutionary extension of the xml data model and permits trees with multi colored nodes to signify their participation in multiple hierarchies this adds significant semantic structure to individual data nodes we extend xquery expressions to navigate between structurally related nodes taking color into account and also to create new colored trees as restructurings of an mct database while mct serves as significant evolutionary extension to xml as logical data model one of the key roles of xml is for information exchange to enable exchange of mct information we develop algorithms for optimally serializing an mct database as xml we discuss alternative physical representations for mct databases using relational and native xml databases and describe an implementation on top of the timber native xml database experimental evaluation using our prototype implementation shows that not only are mct queries updates more succinct and easier to express than equivalent shallow tree xml queries but they can also be significantly more efficient to evaluate than equivalent deep and shallow tree xml queries updates
the large address space needs of many current applications have pushed processor designs toward bit word widths although full bit addresses and operations are indeed sometimes needed arithmetic operations on much smaller quantities are still more common in fact another instruction set trend has been the introduction of instructions geared toward subword operations on bit quantities for examples most major processors now include instruction set support for multimedia operations allowing parallel execution of several subword operations in the same alu this article presents our observations demonstrating that operations on ldquo narrow width rdquo quantities are common not only in multimedia codes but also in more general workloads in fact across the specint benchmarks over half the integer operation executions require bits or less based on this data we propose two hardware mechanisms that dynamically recognize and capitalize on these narrow width operations the first power oriented optimization reduces processor power consumption by using operand value based clock gating to turn off portions of arithmetic units that will be unused by narrow width operations this optimization results in reduction in the integer unit's power consumption for the specint and mediabench benchmark suites applying this optimization to specfp benchmarks results in slightly smaller power reductions but still seems warranted these reductions in integer unit power consumption equate to full chip power savings our second performance oriented optimization improves processor performance by packing together narrow width operations so that they share single arithmetic unit conceptually similar to dynamic form of mmx this optimization offers speedups of for specint and for mediabench overall these optimizations highlight an increasing opportunity for value based optimizations to improve both power and performance in current microprocessors
we describe novel method for controlling physics based fluid simulations through gradient based nonlinear optimization using technique known as the adjoint method derivatives can be computed efficiently even for large simulations with millions of control parameters in addition we introduce the first method for the full control of free surface liquids we show how to compute adjoint derivatives through each step of the simulation including the fast marching algorithm and describe new set of control parameters specifically designed for liquids
context sensitive points to analysis maintains separate points to relationships for each possible abstract calling context of method previous work has shown that large number of equivalence classes exists in the representation of calling contexts such equivalent contexts provide opportunities for context sensitive analyses based on binary decision diagrams bdds in which bdds automatically merge equivalent points to relationships however the use of bdd black box introduces additional overhead for analysis running time furthermore with heap cloning ie using context sensitive object allocation sites bdds are not as effective because the number of equivalence classes increases significantly further step must be taken to look inside the bdd black box to investigate where the equivalence comes from and what tradeoffs can be employed to enable practical large scale heap cloning this paper presents an analysis for java that exploits equivalence classes in context representation for particular pointer variable or heap object all abstract contexts within an equivalence class can be merged this technique naturally results in new non bdd context sensitive points to analysis based on these equivalence classes the analysis employs last substring merging approach to define scalability and precision tradeoffs we show that small values for can enable scalable heap cloning for large java programs the proposed analysis has been implemented and evaluated on large set of java programs the experimental results show improvements over an existing object sensitive analysis with heap cloning which is the most precise scalable analysis implemented in the state of the art paddle analysis framework for computing points to solution for an entire program our approach is an order of magnitude faster compared to this bdd based analysis and to related non bdd refinement based analysis
consequence of ilp systems being implemented in prolog or using prolog libraries is that usually these systems use prolog internal database to store and manipulate data however in real world problems the original data is rarely in prolog format in fact the data is often kept in relational database management systems rdbms and then converted to format acceptable by the ilp system therefore more interesting approach is to link the ilp system to the rdbms and manipulate the data without converting it this scheme has the advantage of being more scalable since the whole data does not need to be loaded into memory by the ilp system in this paper we study several approaches of coupling ilp systems with rdbms systems and evaluate their impact on performance we propose to use deductive database ddb system to transparently translate the hypotheses to relational algebra expressions the empirical evaluation performed shows that the execution time of ilp algorithms can be effectively reduced using ddb and that the size of the problems can be increased due to non memory storage of the data
deployments of wireless lans consisting of hundreds of access points with large number of users have been reported in enterprises as well as college campuses however due to the unreliable nature of wireless links users frequently encounter degraded performance and lack of coverage this problem is even worse in unplanned networks such as the numerous access points deployed by homeowners existing approaches that aim to diagnose these problems are inefficient because they troubleshoot at too high level and are unable to distinguish among the root causes of degradation this paper designs implements and tests fine grained detection algorithms that are capable of distinguishing between root causes of wireless anomalies at the depth of the physical layer an important property that emerges from our system is that diagnostic observations are combined from multiple sources over multiple time instances for improved accuracy and efficiency
the objective of this research is to apply markerless augmented reality ar techniques to aid in the visualisation of robotic helicopter related tasks conventional robotic ar applications work well with markers in prepared environments but are infeasible in outdoor settings in this paper we present preliminary results from real time markerless ar system for tracking natural features in an agricultural scene by constructing virtual marker under known initial configuration of the robotic helicopter camera and the ground plane the camera pose can be continuously tracked using the natural features from the image sequence to perform augmentation of virtual objects the experiments are simulated on mock up model of an agricultural farm and the results show that the current ar system is capable of tracking the camera pose accurately for translational motions and roll rotations future work includes reducing jitter in the virtual marker vertices to improve camera pose estimation accuracy for pitch and yaw rotations and implementing feature recovery algorithms
two key steps in the compilation of strict functional languages are the conversion of higher order functions to data structures closures and the transformation to tail recursive style we show how to perform both steps at once by applying first order offline partial evaluation to suitable interpreter the resulting code is easy to transliterate to low level or native code we have implemented the compilation to it yields performance comparable to that of other modern scheme to compilers in addition we have integrated various optimizations such as constant propagation higher order removal and arity raising simply by modifying the underlying interpreter purely first order methods suffice to achieve the transformations our approach is an instance of semantics directed compiler generation
this paper gives short introduction into parameterized complexity theory aimed towards database theorists interested in this area the main results presented here classify the evaluation of first order queries and conjunctive queries as hard parameterized problems
image restoration is an important and widely studied problem in computer vision and image processing various image filtering strategies have been effective but invariably make strong assumptions about the properties of the signal and or degradation hence these methods lack the generality to be easily applied to new applications or diverse image collections this paper describes novel unsupervised information theoretic adaptive filter uinta that improves the predictability of pixel intensities from their neighborhoods by decreasing their joint entropy in this way uinta automatically discovers the statistical properties of the signal and can thereby restore wide spectrum of images the paper describes the formulation to minimize the joint entropy measure and presents several important practical considerations in estimating neighborhood statistics it presents series of results on both real and synthetic data along with comparisons with current state of the art techniques including novel applications to medical image processing
given terabyte click log can we build an efficient and effective click model it is commonly believed that web search click logs are gold mine for search business because they reflect users preference over web documents presented by the search engine click models provide principled approach to inferring user perceived relevance of web documents which can be leveraged in numerous applications in search businesses due to the huge volume of click data scalability is must we present the click chain model ccm which is based on solid bayesian framework it is both scalable and incremental perfectly meeting the computational challenges imposed by the voluminous click logs that constantly grow we conduct an extensive experimental study on data set containing million query sessions obtained in july from commercial search engine ccm consistently outperforms two state of the art competitors in number of metrics with over better log likelihood over better click perplexity and much more robust up to prediction of the first and the last clicked position
in nutshell duality for constraint satisfaction problem equates the existence of one homomorphism to the non existence of other homomorphisms in this survey paper we give an overview of logical combinatorial and algebraic aspects of the following forms of duality for constraint satisfaction problems finite duality bounded pathwidth duality and bounded treewidth duality
novel face recognition algorithm based on gabor texture information is proposed in this paper two kinds of strategies to capture it are introduced gabor magnitude based texture representation gmtr and gabor phase based texture representation gptr specifically gmtr is characterized by using the gamma density to model the gabor magnitude distribution while gptr is characterized by using the generalized gaussian density ggd to model the gabor phase distribution the estimated model parameters serve as texture representation experiments are performed on yale orl and feret databases to validate the feasibility of the proposed method the results show that the proposed gmtr based and gptr based nlda both significantly outperform the widely used gabor features based nlda and other existing subspace methods in addition the feature level fusion of these two kinds of texture representations performs better than them individually
we propose novel privacy preserving distributed infrastructure in which data resides only with the publishers owning it the infrastructure disseminates user queries to publishers who answer them at their own discretion the infrastructure enforces publisher anonymity guarantee which prevents leakage of information about which publishers are capable of answering certain query given the virtual nature of the global data collection we study the challenging problem of efficiently locating publishers in the community that contain data items matching specified query we propose distributed index structure uqdt that is organized as union of query dissemination trees qdts and realized on an overlay ie logical network infrastructure each qdt has data publishers as its leaf nodes and overlay network nodes as its internal nodes each internal node routes queries to publishers based on summary of the data advertised by publishers in its subtrees we experimentally evaluate design tradeoffs and demonstrate that uqdt can maximize throughput by preventing any overlay network node from becoming bottleneck
web pages often contain objects created at different times the information about the age of such objects may provide useful context for understanding page content and may serve many potential uses in this paper we describe novel concept for detecting approximate creation dates of content elements in web pages our approach is based on dynamically reconstructing page histories using data extracted from external sources web archives and efficiently searching inside them to detect insertion dates of content elements we discuss various issues involving the proposed approach and demonstrate the example of an application that enhances browsing the web by inserting annotations with temporal metadata into page content on user request
program slicing systematically identifies parts of program relevant to seed statement unfortunately slices of modern programs often grow too large for human consumption we argue that unwieldy slices arise primarily from an overly broad definition of relevance rather than from analysis imprecision while traditional slice includes all statements that may affect point of interest not all such statements appear equally relevant to human as an improved method of finding relevant statements we propose thin slicing thin slice consists only of producer statements for the seed ie those statements that help compute and copy avalue to the seed statements that explain why producers affect the seed are excluded for example for seed that reads value from container object thin slice includes statements that store the value into the container but excludes statements that manipulate pointers to the container itself thin slices can also be hierarchically expanded to include statements explaining how producers affect the seed yielding traditional slice in the limit we evaluated thin slicing for set of debugging and program understanding tasks the evaluation showed that thin slices usually included the desired statements for the tasks eg the buggy statement for debugging task furthermore in simulated use of slicing tool thin slices revealed desired statements after inspecting times fewer statements than traditional slicing for our debugging tasks and times fewer statements for our program understanding tasks finally our thin slicing algorithm scales well to relatively large java benchmarks suggesting that thin slicing represents an attractive option for practical tools
reconfigurable hardware is ideal for use in systems on chip soc as it provides both hardware level performance and post fabrication flexibility however any one architecture is rarely equally optimized for all applications socs targeting specific set of applications can greatly benefit from incorporating customized reconfigurable logic instead of generic field programmable gate array fpga logic unfortunately manually designing domain specific architecture for every soc would require significant design time instead this paper discusses our initial efforts towards creating reconfigurable hardware generator capable of automatically creating flexible yet domain specific designs our tests indicate that our generated architectures are more than smaller than equivalent fpga implementations and nearly as area efficient as standard cell designs we also use novel technique employing synthetic circuit generation to demonstrate the flexibility of our architecture generation techniques
from personal software to advanced systems caching mechanisms have steadfastly been ubiquitous means for reducing workloads it is no surprise then that under the grid and cluster paradigms middlewares and other large scale applications often seek caching solutions among these distributed applications scientific workflow management systems have gained ground towards mitigating the often painstaking process of composing sequences of scientific data sets and services to derive virtual data in the past workflow managers have relied on low level system cache for reuse support but in distributed query intensive environments where high volumes of intermediate virtual data can potentially be stored anywhere on the grid novel cache structure is needed to efficiently facilitate workflow planning in this paper we describe an approach to combat the challenges of maintaining large fast virtual data caches for workflow composition hierarchical structure is proposed for indexing scientific data with spatiotemporal annotations across grid nodes our experimental results show that our hierarchical index is scalable and outperforms centralized indexing scheme by an exponential factor in query intensive environments
at the highest level of formal certification the current research trend consists in providing evaluators with formal checkable proof produced by automatic verification tools the aim is to reduce the certification process to verifying the provided proof using proof checker however to date no certified proof checker has emerged in addition checkable proofs do not eliminate the need to validate the formalization of the verification problem in this paper we consider the point of view of evaluators we elaborate criteria that must be fulfilled by formal proof in order to convince skeptical evaluators then we present methodology based on this notion of convincing proofs that requires simple formalizations to reach the level of confidence of formal certification the key idea is to build certified proof checker in collaboration with the evaluators which is finally used to validate the proof provided by developers we illustrate our approach on the correctness proof of buffering protocol written in that manages the data exchanges between concurrent tasks in avionics control systems
nowadays grid computing has been widely recognized as the next big thing in distributed software development grid technologies allow developers to implement massively distributed applications with enormous demands for resources such as processing power data and network bandwidth despite the important benefits grid computing offers contemporary approaches for grid enabling applications still force developers to invest much effort into manually providing code to discover and access grid resources and services moreover the outcome of this task is usually software that is polluted by grid aware code as result of which the maintainability suffers in previous article we presented jgrim novel approach to easily gridify java applications in this paper we report detailed evaluation of jgrim that was conducted by comparing it with ibis and proactive two platforms for grid development specifically we used these three platforms for gridifying the nearest neighbor algorithm and an application for restoring panoramic images the results show that jgrim simplifies gridification without resigning performance for these applications
while touch screen displays are becoming increasingly popular many factors affect user experience and performance surface quality parallax input resolution and robustness for instance can vary with sensing technology hardware configurations and environmental conditions we have developed framework for exploring how we could overcome some of these dependencies by leveraging the higher visual and input resolution of small coarsely tracked mobile devices for direct precise and rapid interaction on large digital displays the results from formal user study show no significant differences in performance when comparing four techniques we developed for tracked mobile device where two existing touch screen techniques served as baselines the mobile techniques however had more consistent performance and smaller variations among participants and an overall higher user preference in our setup our results show the potential of spatially aware handhelds as an interesting complement or substitute for direct touch interaction on large displays
we present set of time efficient approaches to index objects moving on the plane to efficiently answer range queries about their future positions our algorithms are based on previously described solutions as well as on the employment of efficient access methods finally an experimental evaluation is included that shows the performance scalability and efficiency of our methods
the design of complex systems requires powerful mechanisms for modeling data state communication and real time behaviour as well as for structuring and decomposing systems in order to control local complexity timed communicating object tcoz builds on object z's strengths in modeling complex data and state and on timed csp's strengths in modeling process control and real time interactions in this paper we demonstrate the tcoz approach to the design and verification of the teleservices and remote medical care system
recent user interface concepts such as multimedia multimodal wearable ubiquitous tangible or augmented reality based ar interfaces each cover different approaches that are all needed to support complex human computer interaction increasingly an overarching approach towards building what we call ubiquitous augmented reality uar user interfaces that include all of the just mentioned concepts will be required to this end we present user interface architecture that can form sound basis for combining several of these concepts into complex systems we explain in this paper the fundamentals of dwarf user interface framework dwarf standing for distributed wearable augmented reality framework and an implementation of this architecture finally we present several examples that show how the framework can form the basis of prototypical applications
in wireless sensor networks one of the main design challenges is to save severely constrained energy resources and obtain long system lifetime low cost of sensors enables us to randomly deploy large number of sensor nodes thus potential approach to solve lifetime problem arises that is to let sensors work alternatively by identifying redundant nodes in high density networks and assigning them an off duty operation mode that has lower energy consumption than the normal on duty mode in single wireless sensor network sensors are performing two operations sensing and communication therefore there might exist two kinds of redundancy in the network most of the previous work addressed only one kind of redundancy sensing or communication alone wang et al intergrated coverage and connectivity configuration in wireless sensor networks in proceedings of the first acm conference on embedded networked sensor systems sensys los angeles november and zhang and hou maintaining sensing coverage and connectivity in large sensor networks technical report uiucdcs june first discussed how to combine consideration of coverage and connectivity maintenance in single activity scheduling they provided sufficient condition for safe scheduling integration in those fully covered networks however random node deployment often makes initial sensing holes inside the deployed area inevitable even in an extremely high density network therefore in this paper we enhance their work to support general wireless sensor networks by proving another conclusion the communication range is twice of the sensing range is the sufficient condition and the tight lower bound to ensure that complete coverage preservation implies connectivity among active nodes if the original network topology consisting of all the deployed nodes is connected also we extend the result to degree network connectivity and degree coverage preservation
geospatial data play key role in wide spectrum of critical data management applications such as disaster and emergency management environmental monitoring land and city planning and military operations often requiring the coordination among diverse organizations their data repositories and users with different responsibilities although variety of models and techniques are available to manage access and share geospatial data very little attention has been paid to addressing security concerns such as access control security and privacy policies and the development of secure and in particular interoperable gis applications the objective of this paper is to discuss the technical challenges raised by the unique requirements of secure geospatial data management and to suggest comprehensive framework for security and privacy for geospatial data and gis such framework is the first coherent architectural approach to the problem of security and privacy for geospatial data
in virtual machines for embedded devices that use just in time compilation the management of the code cache can significantly impact performance in terms of both memory usage and start up time although improving memory usage has been common focus for system designers start up time is often overlooked in systems with constrained resources however these two performance metrics are often at odds and must be considered together in this paper we present an adaptive self adjusting code cache manager to improve performance with respect to both start up time and memory usage it balances these concerns by detecting changes in method compilation rates resizing the cache after each pitching event we conduct experiments to validate our proposed system and quantify the impacts that different code cache management techniques have on memory usage and start up time through two oracle systems our results show that the proposed algorithm yields nearly the same start up times as hand tuned oracle and shorter execution times than those of the sscli in eight out of ten applications it also has lower memory usage over time in all but one application
structural testing techniques such as statement and branch coverage play an important role in improving dependability of software systems however finding set of tests which guarantees high coverage is time consuming task in this paper we present technique for structural testing based on kernel computation kernel satisfies the property that any set of tests which executes all vertices edges of the kernel executes all vertices edges of the program's flowgraph we present linear time algorithm for computing minimum kernels based on pre and post dominator relations of flowgraph
the user interfaces of the most popular search engines are largely the same typically users are presented with an ordered list of documents which provide limited help if users are having trouble finding the information they need this article presents an interface called the venn diagram interface vdi that offers users improved search transparency the vdi allows users to see how each term or group of terms in query contributes to the entire result set of search furthermore it allows users to browse the result sets generated by each of these terms in test with participants the vdi was compared against standard web search interface google with the vdi users were able to find more documents of higher relevance and were more inclined to continue searching their level of interactivity was higher the quality of the answers they found was perceived to be better eight out of users preferred the vdi
suffix trees are indexing structures that enhance the performance of numerous string processing algorithms in this paper we propose cache conscious suffix tree construction algorithms that are tailored to cmp architectures the proposed algorithms utilize novel sample based cache partitioning algorithm to improve cache performance and exploit on chip parallelism on cmps furthermore several compression techniques are applied to effectively trade space for cache performance through an extensive experimental evaluation using real text data from different domains we demonstrate that the algorithms proposed herein exhibit better cache performance than their cache unaware counterparts and effectively utilize all processing elements achieving satisfactory speedup
online transaction processing oltp databases include suite of features disk resident trees and heap files locking based concurrency control support for multi threading that were optimized for computer technology of the late advances in modern processors memories and networks mean that today's computers are vastly different from those of years ago such that many oltp databases will now fit in main memory and most oltp transactions can be processed in milliseconds or less yet database architecture has changed little based on this observation we look at some interesting variants of conventional database systems that one might build that exploit recent hardware trends and speculate on their performance through detailed instruction level breakdown of the major components involved in transaction processing database system shore running subset of tpc rather than simply profiling shore we progressively modified it so that after every feature removal or optimization we had faster working system that fully ran our workload overall we identify overheads and optimizations that explain total difference of about factor of in raw performance we also show that there is no single high pole in the tent in modern memory resident database systems but that substantial time is spent in logging latching locking tree and buffer management operations
traditionally clock network layout is performed after cell placement such methodology is facing serious problem in nanometer ic designs where people tend to use huge clock buffers for robustness against variations that is clock buffers are often placed far from ideal locations to avoid overlap with logic cells as result both power dissipation and timing are degraded in order to solve this problem we propose low power clock buffer planning methodology which is integrated with cell placement bin divided grouping algorithm is developed to construct virtual buffer tree which can explicitly model the clock buffers in placement the virtual buffer tree is dynamically updated during the placement to reflect the changes of latch locations to reduce power dissipation latch clumping is incorporated with the clock buffer planning the experimental results show that our method can reduce clock power significantly by on average
with recent advances in computing and communication technologies enabling mobile devices more powerful the scope of grid computing has been broadened to include mobile and pervasive devices energy has become critical resource in such devices so battery energy limitation is the main challenge towards enabling persistent mobile grid computing in this paper we address the problem of energy constrained scheduling scheme for the grid environment there is limited energy budget for grid applications the paper investigates both energy minimization for mobile devices and grid utility optimization problem we formalize energy aware scheduling using nonlinear optimization theory under constraints of energy budget and deadline the paper also proposes distributed pricing based algorithm that is used to tradeoff energy and deadline to achieve system wide optimization based on the preference of the grid user the simulations reveal that the proposed energy constrained scheduling algorithms can obtain better performance than the previous approach that considers both energy consumption and deadline
the increasing use of computers for saving valuable data imposes stringent reliability constraints on storage systems reliability improvement via use of redundancy is common practice as the disk capacity improves advanced techniques such as disk scrubbing are being employed to proactively fix latent sector errors these techniques utilize the disk idle time for reliability improvement however the idle time is key to dynamic energy management that detects such idle periods and turns off the disks to save energy in this paper we are concerned with the distribution of the disk idle periods between reliability and energy management tasks for this purpose we define new metric energy reliability product erp to capture the effect of one technique on the other our initial investigation using trace driven simulations of typical enterprise applications shows that the erp is suitable metric for identifying efficient idle period utilization thus erp can facilitate development of systems that provide both reliability and energy managements
this article deals with the design of on chip architectures for testing large system chips socs for manufacturing defects in modular fashion these architectures consist of wrappers and test access mechanisms tams for an soc with specified parameters of modules and their tests we design an architecture that minimizes the required tester vector memory depth and test application time in this article we formulate the test architecture design problems for both modules with fixed and flexible length scan chains assuming the relevant module parameters and maximal soc tam width are given subsequently we derive formulation for an architecture independent lower bound for the soc test time we analyze three types of tam under utilization that make the theoretical lower bound unachievable in most practical architecture instances we present novel architecture independent heuristic algorithm that effectively optimizes the test architecture for given soc the algorithm efficiently determines the number of tams and their widths the assignment of modules to tams and the wrapper design per module we show how this algorithm can be used for optimizing both test bus and testrail architectures with either serial or parallel test schedules experimental results for the itc soc test benchmarks show that compared to manual best effort engineering approaches we can save up to percnt in test times while compared to previously published algorithms we obtain comparable or better test times at negligible compute time
in publish subscribe paradigm user service discovery requires matching user preferences to available published services eg user may want to find if there is chinese restaurant close by this is difficult problem when users are mobile wirelessly connected to network and dynamically roaming in different environments the magnitude of the problem increases with respect to the number of attributes for each users preference criteria as matches must be done in real time we present an algorithm that uses singular value decomposition to encode each service properties in few values users preference criteria are matched by using the same encoding to produce value that can be rapidly compared to those of the services we show that reasonable matches can be found in time log where is the number of publications and the number of attributes in the preference criteria subscription this is in contrast to approximate nearest neighbor techniques which require either time or storage exponential in
the growing popularity of augmented reality ar games in both research and more recently commercial context has led for need to take closer look at design related issues which impact on player experience while issues relating to this area have been considered to date most of the emphasis has been on the technology aspects furthermore it is almost always assumed that the augmented reality element in itself will provide sufficient experience for the player this has led to need to evaluate what makes successful augmented reality game in this paper we present set of design guidelines which are drawn from experiences of three mixed reality games the guidelines provide specific guidance on relationships between real and virtual space social interaction use of ar technologies maintaining consistent themes and implicitly address higher level aspects such as presence within particular augmented reality place
content based retrieval of the similar motions for the human joints has significant impact in the fields of physical medicine biomedicine rehabilitation and motion therapy in this paper we propose an efficient indexing approach for human motion capture data supporting queries involving both subbody motions as well as whole body motions
abductive logic programming offers formalism to declaratively express and solve problems in areas such as diagnosis planning belief revision and hypothetical reasoning tabled logic programming offers computational mechanism that provides level of declarativity superior to that of prolog and which has supported successful applications in fields such as parsing program analysis and model checking in this paper we show how to use tabled logic programming to evaluate queries to abductive frameworks with integrity constraints when these frameworks contain both default and explicit negation the result is the ability to compute abduction over well founded semantics with explicit negation and answer sets our approach consists of transformation and an evaluation method the transformation adjoins to each objective literal in program an objective literal hbox it not along with rules that ensure that hbox it not will be true if and only if is false we call the resulting program dual program the evaluation method abdual then operates on the dual program abdual is sound and complete for evaluating queries to abductive frameworks whose entailment method is based on either the well founded semantics with explicit negation or on answer sets further abdual is asymptotically as efficient as any known method for either class of problems in addition when abduction is not desired abdual operating on dual program provides novel tabling method for evaluating queries to ground extended programs whose complexity and termination properties are similar to those of the best tabling methods for the well founded semantics publicly available meta interpreter has been developed for abdual using the xsb system
in molecular biology dna sequence matching is one of the most crucial operations since dna databases contain huge volume of sequences fast indexes are essential for efficient processing of dna sequence matching in this paper we first point out the problems of the suffix tree an index structure widely used for dna sequence matching in respect of storage overhead search performance and difficulty in seamless integration with dbms then we propose new index structure that resolves such problems the proposed index structure consists of two parts the primary part realizes the trie as binary bit string representation without any pointers and the secondary part helps fast access to the trie's leaf nodes that need to be accessed for post processing we also suggest efficient algorithms based on that index for dna sequence matching to verify the superiority of the proposed approach we conduct performance evaluation via series of experiments the results reveal that the proposed approach which requires smaller storage space can be few orders of magnitude faster than the suffix tree
we study the effects of node mobility on the wireless links and protocol performance in mobile ad hoc networks manets first we examine the behavior of links through an analytical framework and develop statistical models to accurately characterize the distribution of lifetime of such wireless links in manets we compute the lifetimes of links through two state markov model and use these results to model multi hop paths and topology changes we show that the analytical solution follows closely the results obtained through discrete event simulations for two mobility models namely random direction and random waypoint mobility models finally we present comprehensive simulation study that combines the results from the findings in simulations with the analytical results to bring further insight on how different types of mobility translate into protocol performance
workflow views abstract groups of tasks in workflow into high level composite tasks in order to reuse sub workflows and facilitate provenance analysis however unless view is carefully designed it may not preserve the dataflow between tasks in the workflow ie it may not be sound unsound views can be misleading and cause incorrect provenance analysis this paper studies the problem of efficiently identifying and correcting unsound workflow views with minimal changes in particular given workflow view we wish to split each unsound composite task into the minimal number of tasks such that the resulting view is sound we prove that this problem is np hard by reduction from independent set we then propose two local optimality conditions weak and strong and design polynomial time algorithms for correcting unsound views to meet these conditions experiments show that our proposed algorithms are effective and efficient and that the strong local optimality algorithm produces better solutions than the weak local optimality algorithm with little processing overhead
wireless sensor networks consist of many nodes that collect real world data process them and transmit the data by radio wireless sensor networks represent new rapidly developing direction in the field of organization of computer networks of free configuration sensor networks are used for monitoring parameter field where it is often required to fix time of an event with high accuracy high accuracy of local clocks is also necessary for operation of network protocols for energy saving purposes the nodes spend most of the time in the sleeping mode and communicate only occasionally in the paper base techniques used in the existing time synchronization schemes are analyzed models of local clock behavior and models of interaction of the network devices are described classification of the synchronization problems is presented and survey of the existing approaches to synchronization of time in sensor networks is given
in this paper we introduce new branch predictor that predicts the outcome of branches by predicting the value of their inputs and performing an early computation of their results according to the predicted values the design of hybrid predictor comprising the above branch predictor and correlating branch predictor is presented we also propose new selector that chooses the most reliable prediction for each branch this selector is based on the path followed to reach the branch results for immediate updates show significant misprediction rate reductions with respect to conventional hybrid predictor for different size configurations in addition the proposed hybrid predictor with size of kb achieves the same accuracy as conventional one of kb performance evaluation for dynamically scheduled superscalar processor with realistic updates shows speed up of percent despite its higher latency up to four cycles
databases are increasingly being used to store multi media objects such as maps images audio and video storage and retrieval of these objects is accomplished using multi dimensional index structures such as trees and ss trees as dimensionality increases query performance in these index structures degrades this phenomenon generally referred to as the dimensionality curse can be circumvented by reducing the dimensionality of the data such reduction is however accompanied by loss of precision of query results current techniques such as qbic use svd transform based dimensionality reduction to ensure high query precision the drawback of this approach is that svd is expensive to compute and therefore not readily applicable to dynamic databases in this paper we propose novel techniques for performing svd based dimensionality reduction in dynamic databases when the data distribution changes considerably so as to degrade query precision we recompute the svd transform and incorporate it in the existing index structure for recomputing the svd transform we propose novel technique that uses aggregate data from the existing index rather than the entire data this technique reduces the svd computation time without compromising query precision we then explore efficient ways to incorporate the recomputed svd transform in the existing index structure without degrading subsequent query response times these techniques reduce the computation time by factor of in experiments on color and texture image vectors the error due to approximate computation of svd is less than
this paper presents case study of globally distributed work group's use of an online environment called loops loops is web based persistent chat system whose aim is to support collaboration amongst corporate work groups we describe the ways in which the group turned the system's features to its own ends and the unusual usage rhythm that corresponded with the team's varying needs for communication as it moved through its work cycle we conclude with discussion of design implications and suggestion that community may not always be the best way to think about groups use of online systems
we present simple image based method of generating novel visual appearance in which new image is synthesized by stitching together small patches of existing images we call this process image quilting first we use quilting as fast and very simple texture synthesis algorithm which produces surprisingly good results for wide range of textures second we extend the algorithm to perform texture transfer mdash rendering an object with texture taken from different object more generally we demonstrate how an image can be re rendered in the style of different image the method works directly on the images and does not require information
organizations increasingly coordinate their product and service development processes to deliver their products and services as fast as possible and to involve employees customers suppliers and business partners seamlessly in different stages of the processes these processes have to consider that their participants are increasingly on the move or distributed while they are working expertise needs to be shared across locations and different mobile devices this paper describes framework for distributed and mobile collaboration defines set of requirements for virtual communities and discusses mobile teamwork support software architecture that has been developed in the eu project motion the framework together with the architecture enables to enhance current collaboration approaches to include the dimension of mobile participants and virtual communities for distributed product development this is achieved by integrating process and workspace management requirements with peer to peer middleware publish subscribe and community and user management components
schapire and singer's improved version of adaboost for handling weak hypotheses with confidence rated predictions represents an important advance in the theory and practice of boosting its success results from more efficient use of information in weak hypotheses during updating instead of simple binary voting weak hypothesis is allowed to vote for or against classification with variable strength or confidence the pool adjacent violators pav algorithm is method for converting score into probability we show how pav may be applied to weak hypothesis to yield new weak hypothesis which is in sense an ideal confidence rated prediction and that this leads to an optimal updating for adaboost the result is new algorithm which we term pav adaboost we give several examples illustrating problems for which this new algorithm provides advantages in performance
predicting the worst case execution time wcet and best case execution time bcet of real time program is challenging task though much progress has been made in obtaining tighter timing predictions by using techniques that model the architectural features of machine significant overestimations of wcet and underestimations of bcet can still occur even with perfect architectural modeling dependencies on data values can constrain the outcome of conditional branches and the corresponding set of paths that can be taken in program while branch constraint information has been used in the past by some timing analyzers it has typically been specified manually which is both tedious and error prone this paper describes efficient techniques for automatically detecting branch constraints by compiler and automatically exploiting these constraints within timing analyzer the result is significantly tighter timing analysis predictions without requiring additional interaction with user
previous schemes for implementing full tail recursion when compiling into have required some form of trampoline to pop the stack we propose solving the tail recursion problem in the same manner as standard ml of new jersey by allocating all frames in the garbage collected heap the scheme program is translated into continuation passing style so the target functions never return the stack pointer then becomes the allocation pointer for cheney style copying garbage collection scheme our scheme can use function calls arguments variable arity functions and separate compilation without requiring complex block compilation of entire programs our version of the boyer benchmark is available at ftp ftpnetcomcom pub hb hbaker cboyer
we study whether when restricted to using polylogarithmic memory and polylogarithmic passes we can achieve qualitatively better data compression with multiple read write streams than we can with only one we first show how we can achieve universal compression using only one pass over one stream we then show that one stream is not sufficient for us to achieve good grammar based compression finally we show that two streams are necessary and sufficient for us to achieve entropy only bounds the acm portal is published by the association for computing machinery copyright acm inc terms of usage privacy policy code of ethics contact us useful downloads adobe acrobat quicktime windows media player real player
this paper describes new framework of register allocation based on chaitin style coloring our focus is on maximizing the chances for live ranges to be allocated to the most preferred registers while not destroying the colorability obtained by graph simplification our coloring algorithm uses graph representation of preferences called register preference graph which helps find good register selection we then try to relax the register selection order created by the graph simplification the relaxed order is defined as partial order represented using graph called coloring precedence graph our algorithm utilizes such partial order for the register selection instead of using the traditional simplification driven order so that the chances of honoring the preferences are effectively increased experimental results show that our coloring algorithm is powerful to simultaneously handle spill decisions register coalescing and preference resolutions
statistical language models have been successfully applied to many information retrieval tasks including expert finding the process of identifying experts given particular topic in this paper we introduce and detail language modeling approaches that integrate the representation association and search of experts using various textual data sources into generative probabilistic framework this provides simple intuitive and extensible theoretical framework to underpin research into expertise search to demonstrate the flexibility of the framework two search strategies to find experts are modeled that incorporate different types of evidence extracted from the data before being extended to also incorporate co occurrence information the models proposed are evaluated in the context of enterprise search systems within an intranet environment where it is reasonable to assume that the list of experts is known and that data to be mined is publicly accessible our experiments show that excellent performance can be achieved by using these models in such environments and that this theoretical and empirical work paves the way for future principled extensions
bag of tasks applications are parallel applications composed of independent tasks examples of bag of tasks bot applications include monte carlo simulations massive searches such as key breaking image manipulation applications and data mining algorithms this paper analyzes the scalability of bag of tasks applications running on master slave platforms and proposes scalability related measure dubbed input file affinity in this work we also illustrate how the input file affinity which is characteristic of an application can be used to improve the scalability of bag of tasks applications running on master slave platforms the input file affinity was considered in new scheduling algorithm dubbed dynamic clustering which is oblivious to task execution times we compare the scalability of the dynamic clustering algorithm to several other algorithms oblivious and non oblivious to task execution times proposed in the literature we show in this paper that in several situations the oblivious algorithm dynamic clustering has scalability performance comparable to non oblivious algorithms which is remarkable considering that our oblivious algorithm uses much less information to schedule tasks
flow directed inlining strategy uses information derived from control flow analysis to specialize and inline procedures for functional and object oriented languages since it uses control flow analysis to identify candidate call sites flow directed inlining can inline procedures whose relationships to their call sites are not apparent for instance procedures defined in other modules passed as arguments returned as values or extracted from data structures can all be inlined flow directed inlining specializes procedures for particular call sites and can selectively inline particular procedure at some call sites but not at others finally flow directed inlining encourages modular implementations control flow analysis inlining and post inlining optimizations are all orthogonal components results from prototype implementation indicate that this strategy effectively reduces procedure call overhead and leads to significant reduction in execution time
this position paper addresses the question of integrating grid andmas multi agent systems models by means of service oriented approachservice oriented computing soc tries to address many challenges in the worldof computing with services the concept of service is clearly at the intersectionof grid and mas and their integration allows to address one of these keychallenges the implementation of dynamically generated services based on conversationsin our approach services are exchanged ie provided and used byagents through grid mechanisms and infrastructure integration goes beyondthe simple interoperation of applications and standards it has to be intrinsic tothe underpinning model we introduce here an quite unique integration modelfor grid and mas this model is formalized and represented by graphicaldescription language called agent grid integration language agil this integrationis based on two main ideas the representation of agent capabilitiesas grid services in service containers ii the assimilation of the service instantiationmechanism from grid with the creation of new conversation context from mas the integrated model may be seen as formalization of agent interactionfor service exchange
major difficulty of text categorization problems is the high dimensionality of the feature space thus feature selection is often performed in order to increase both the efficiency and effectiveness of the classification in this paper we propose feature selection method based on testor theory this criterion takes into account inter feature relationships we experimentally compared our method with the widely used information gain using two well known classification algorithms nearest neighbour and support vector machine two benchmark text collections were chosen as the testbeds reuters and reuters corpus version rcv we found that our method consistently outperformed information gain for both classifiers and both data collections especially when aggressive feature selection is carried out
when trying to apply recently developed approaches for updating description logic aboxes in the context of an action programming language one encounters two problems first updates generate so called boolean aboxes which cannot be handled by traditional description logic reasoners second iterated update operations result in very large boolean aboxes which however contain huge amount of redundant information in this paper we address both issues from practical point of view
job scheduling in data centers can be considered from cyber physical point of view as it affects the data center's computing performance ie the cyber aspect and energy efficiency the physical aspect driven by the growing needs to green contemporary data centers this paper uses recent technological advances in data center virtualization and proposes cyber physical spatio temporal ie start time and servers assigned thermal aware job scheduling algorithms that minimize the energy consumption of the data center under performance constraints ie deadlines savings are possible by being able to temporally spread the workload assign it to energy efficient computing equipment and further reduce the heat recirculation and therefore the load on the cooling systems this paper provides three categories of thermal aware energy saving scheduling techniques fcfs backfill xint and fcfs backfill lrh thermal aware job placement enhancements to the popular first come first serve with back filling fcfs backfill scheduling policy edf lrh an online earliest deadline first scheduling algorithm with thermal aware placement and an offline genetic algorithm for scheduling to minimize thermal cross interference scint which is suited for batch scheduling of backlogs simulation results based on real job logs from the asu fulton hpc data center show that the thermal aware enhancements to fcfs backfill achieve up to savings compared to fcfs backfill with first fit placement depending on the intensity of the incoming workload while scint achieves up to savings the performance of edf lrh nears that of the offline scint for low loads and it degrades to the performance of fcfs backfill for high loads however edf lrh requires milliseconds of operation which is significantly faster than scint the latter requiring up to hours of runtime depending upon the number and size of submitted jobs similarly fcfs backfill lrh is much faster than fcfs backfill xint but it achieves only part of fcfs backfill xint's savings
structured overlay networks have recently received much attention due to their self properties under dynamic and decentralized settings the number of nodes in an overlay fluctuates all the time due to churn since knowledge of the size of the overlay is core requirement for many systems estimating the size in decentralized manner is challenge taken up by recent research activities gossip based aggregation has been shown to give accurate estimates for the network size but previous work done is highly sensitive to node failures in this paper we present gossip based aggregation style network size estimation algorithm we discuss shortcomings of existing aggregation based size estimation algorithms and give solution that is highly robust to node failures and is adaptive to network delays we examine our solution in various scenarios to demonstrate its effectiveness
the scopedmemory class of the rtsj enables the organization of objects into regions this ensures time predictable management of dynamic memory using scopes forces the programmer to reason in terms of locality to comply with rtsj restrictions the programmer is also faced with the problem of providing upper bounds for regions without appropriate compile time support scoped memory management may lead to unexpected runtime errors this work presents the integration of series of compile time analysis techniques to help identifying memory regions their sizes and overall memory usage first the tool synthesizes scoped based memory organization where regions are associated with methods second it infers their sizes in parametric forms in terms of relevant program variables third it exhibits parametric upper bound on the total amount of memory required to run method we present some preliminary results showing that semi automatic tool assisted generation of scoped based code is both helpful and doable
there is large consensus on the need for middleware to efficiently support adaptation in pervasive and mobile computing advanced forms of adaptation require the aggregation of context data and the evaluation of policy rules that are typically provided by multiple sources this paper addresses the problem of designing the reasoning core of middleware that supports these tasks while guaranteeing very low response times as required by mobile applications technically the paper presents strategies to deal with conflicting rules algorithms that implement the strategies and algorithms that detect and solve potential rule cycles detailed experimental analysis supports the theoretical results and shows the applicability of the resulting middleware in large scale applications
we report an exploratory study of the impacts of design planning on end users asked to develop simple interactive web application some participants were asked to create conceptual map to plan their projects and others to write scenarios third group was asked to do whatever they found useful we describe the planning that each group underwent how they approached the web development task and their reactions to the experience afterwards we also discuss how participants gender and experience was related to their web development activities
the ability to understand and manage social signals of person we are communicating with is the core of social intelligence social intelligence is facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life this paper argues that next generation computing needs to include the essence of social intelligence the ability to recognize human social signals and social behaviours like politeness and disagreement in order to become more effective and more efficient although each one of us understands the importance of social signals in everyday life situations and in spite of recent advances in machine analysis of relevant behavioural cues like blinks smiles crossed arms laughter and similar design and development of automated systems for social signal processing ssp are rather difficult this paper surveys the past efforts in solving these problems by computer it summarizes the relevant findings in social psychology and it proposes set of recommendations for enabling the development of the next generation of socially aware computing
system area networks sans which usually accept arbitrary topologies have been used to connect hosts in pc clusters although deadlock free routing is often employed for low latency communications using wormhole or virtual cut through switching the interconnection adaptivity introduces difficulties in establishing deadlock free paths an up down routing algorithm which has been widely used to avoid deadlocks in irregular networks tends to make unbalanced paths as it employs one dimensional directed graph the current study introduces two dimensional directed graph on which adaptive routings called left up first turn turn routings and right down last turn turn routings are proposed to make the paths as uniformly distributed as possible this scheme guarantees deadlock freedom because it uses the turn model approach and the extra degree of freedom in the two dimensional graph helps to ensure that the prohibited turns are well distributed simulation results show that better throughput and latency results from uniformly distributing the prohibited turns by which the traffic would be more distributed toward the leaf nodes the turn routings which meet this condition improve throughput by up to percent compared with two up down based routings and also reduce latency
optimal data placement on clv constant linear velocity format optical discs has an objective the minimization of the expected access cost of data retrievals from the disc when the probabilities of access of data items may be different the problem of optimal data placement for optical discs is both important and more difficult than the corresponding problem on magnetic discs good data placement on optical discs is more important because data sets on optical discs such as worm and cd rom cannot be modified or moved once they are placed on disc currently even rewritable optical discs are best suited for applications that are archival in nature the problem of optimal data placement on clv format optical discs is more difficult mainly because the useful storage space is not uniformly distributed across the disc surface along the radius this leads to complicated positional performance trade off not present for magnetic disks we present model that encompasses all the important aspects of the placement problem on clv format optical discs the model takes into account the nonuniform distribution of useful storage the dependency of the rotational delay on disc position parameterized seek cost function for optical discs and the varying access probabilities of data items we show that the optimal placement of high probability blocks satisfies unimodality property based on this observation we solve the optimal placement problem we then study the impact of the relative weights of the problem parameters and show that the optimal data placement may be very different from the optimal data placement on magnetic disks we also validate our model and analysis and give an algorithm for computing the placement of disc sectors
in this paper we propose notion of question utility for studying usefulness of questions and show how question utility can be integrated into question search as static ranking to measure question utility we examine three methods method of employing the language model to estimate the probability that question is generated from question collection and then using the probability as question utility method of using the lexrank algorithm to evaluate centrality of questions and then using the centrality as question utility and the combination of and to use question utility in question search we employ log linear model for combining relevance score in question search and utility score regarding question utility our experimental results with the questions about travel from yahoo answers show that question utility can be effective in boosting up ranks of generally useful questions
this paper presents an approach to multi sensory and multi modal fusion in which computer vision information obtained from calibrated cameras is integrated with large scale sentient computing system known as spirit the spirit system employs an ultrasonic location infrastructure to track people and devices in an office building and model their state vision techniques include background and object appearance modelling face detection segmentation and tracking modules integration is achieved at the system level through the metaphor of shared perceptions in the sense that the different modalities are guided by and provide updates to shared world model this model incorporates aspects of both the static eg positions of office walls and doors and the dynamic eg location and appearance of devices and people environmentfusion and inference are performed by bayesian networks that model the probabilistic dependencies and reliabilities of different sources of information over time it is shown that the fusion process significantly enhances the capabilities and robustness of both sensory modalities thus enabling the system to maintain richer and more accurate world model
we present angie system that can answer user queries by combining knowledge from local database with knowledge retrieved from web services if user poses query that cannot be answered by the local database alone angie calls the appropriate web services to retrieve the missing information this information is integrated seamlessly and transparently into the local database so that the user can query and browse the knowledge base while appropriate web services are called automatically in the background
filter method of feature selection based on mutual information called normalized mutual information feature selection nmifs is presented nmifs is an enhancement over battiti's mifs mifs and mrmr methods the average normalized mutual information is proposed as measure of redundancy among features nmifs outperformed mifs mifs and mrmr on several artificial and benchmark data sets without requiring user defined parameter in addition nmifs is combined with genetic algorithm to form hybrid filter wrapper method called gamifs this includes an initialization procedure and mutation operator based on nmifs to speed up the convergence of the genetic algorithm gamifs overcomes the limitations of incremental search algorithms that are unable to find dependencies between groups of features
the serious bugs and security vulnerabilities facilitated by lack of bounds checking are well known yet and remain in widespread use unfortunately c's arbitrary pointer arithmetic conflation of pointers and arrays and programmer visible memory layout make retrofitting with spatial safety guarantees extremely challenging existing approaches suffer from incompleteness have high runtime overhead or require non trivial changes to the source code thus far these deficiencies have prevented widespread adoption of such techniques this paper proposes softbound compile time transformation for enforcing spatial safety of inspired by hardbound previously proposed hardware assisted approach softbound similarly records base and bound information for every pointer as disjoint metadata this decoupling enables softbound to provide spatial safety without requiring changes to source code unlike hardbound softbound is software only approach and performs metadata manipulation only when loading or storing pointer values formal proof shows that this is sufficient to provide spatial safety even in the presence of arbitrary casts softbound's full checking mode provides complete spatial violation detection with runtime overhead on average to further reduce overheads softbound has store only checking mode that successfully detects all the security vulnerabilities in test suite at the cost of only runtime overhead on average
the design issues related to routing in wireless sensor networks wsns are inherently different from those encountered in traditional mobile ad hoc networks routing protocols for adhoc networks usually impose prohibitive demands on scares resources of sensor node such as memory bandwidth and energy therefore they are not suitable for wsns in this paper we present novel energy adaptive data forwarding protocol referred to as ring band based energy adaptive protocol reap in which nodes self organise into virtual ring bands centred at the base station bs packets are automatically delivered to the bs along path with decreasing ring band number furthermore the proposed probabilistic forwarding mechanism also balances the workload among neighbouring nodes within the same ring band simulation study showed reap exhibits good performance in various network settings even when nodes are in rapid motion
the most expressive way humans display emotions is through facial expressions in this work we report on several advances we have made in building system for classification of facial expressions from continuous video input we introduce and test different bayesian network classifiers for classifying expressions from video focusing on changes in distribution assumptions and feature dependency structures in particular we use naive bayes classifiers and change the distribution from gaussian to cauchy and use gaussian tree augmented naive bayes tan classifiers to learn the dependencies among different facial motion features we also introduce facial expression recognition from live video input using temporal cues we exploit the existing methods and propose new architecture of hidden markov models hmms for automatically segmenting and recognizing human facial expression from video sequences the architecture performs both segmentation and recognition of the facial expressions automatically using multi level architecture composed of an hmm layer and markov model layer we explore both person dependent and person independent recognition of expressions and compare the different methods
we study the size of memory of mobile agents that permits to solve deterministically the rendezvous problem ie the task of meeting at some node for two identical agents moving from node to node along the edges of an unknown anonymous connected graph the rendezvous problem is unsolvable in the class of arbitrary connected graphs as witnessed by the example of the cycle hence we restrict attention to rendezvous in trees where rendezvous is feasible if and only if the initial positions of the agents are not symmetric we prove that the minimum memory size guaranteeing rendezvous in all trees of size at most is logn bits the upper bound is provided by an algorithm for abstract state machines accomplishing rendezvous in all trees and using logn bits of memory in trees of size at most the lower bound is consequence of the need to distinguish between up to links incident to node thus in the second part of the paper we focus on the potential existence of pairs of finite agents ie finite automata capable of accomplishing rendezvous in all bounded degree trees we show that as opposed to what has been proved for the graph exploration problem there are no finite agents capable of accomplishing rendezvous in all bounded degree trees
the compare and swap register cas is synchronization primitive for lock free algorithms most uses of it however suffer from the so called aba problem the simplest and most efficient solution to the aba problem is to include tag with the memory location such that the tag is incremented with each update of the target location this solution however is theoretically unsound and has limited applicability this paper presents general lock free pattern that is based on the synchronization primitive cas without causing aba problem or problems with wrap around it can be used to provide lock free functionality for any data type our algorithm is cas variation of herlihy's ll sc methodology for lock free transformation the basis of our techniques is to poll different locations on reading and writing objects in such way that the consistency of an object can be checked by its location instead of its tag it consists of simple code that can be easily implemented using like languages true problem of lock free algorithms is that they are hard to design correctly which even holds for apparently straightforward algorithms we therefore develop reduction theorem that enables us to reason about the general lock free algorithm to be designed on higher level than the synchronization primitives the reduction theorem is based on lamport's refinement mappings and has been verified with the higher order interactive theorem prover pvs using the reduction theorem fewer invariants are required and some invariants are easier to discover and formulate without considering the internal structure of the final implementation
sentence extraction is widely adopted text summarization technique where the most important sentences are extracted from document and presented as summary the first step towards sentence extraction is to rank sentences in order of importance as in the summary this paper proposes novel graph based ranking method ispreadrank to perform this task ispreadrank models set of topic related documents into sentence similarity network based on such network model ispreadrank exploits the spreading activation theory to formulate general concept from social network analysis the importance of node in network ie sentence in this paper is determined not only by the number of nodes to which it connects but also by the importance of its connected nodes the algorithm recursively re weights the importance of sentences by spreading their sentence specific feature scores throughout the network to adjust the importance of other sentences consequently ranking of sentences indicating the relative importance of sentences is reasoned this paper also develops an approach to produce generic extractive summary according to the inferred sentence ranking the proposed summarization method is evaluated using the duc data set and found to perform well experimental results show that the proposed method obtains rouge score of which represents slight difference of when compared with the best participant in the duc evaluation
embedded system security is often compromised when trusted software is subverted to result in unintended behavior such as leakage of sensitive data or execution of malicious code several countermeasures have been proposed in the literature to counteract these intrusions common underlying theme in most of them is to define security policies at the system level in an application independent manner and check for security violations either statically or at run time in this paper we present methodology that addresses this issue from different perspective it defines correct execution as synonymous with the way the program was intended to run and employs dedicated hardware monitor to detect and prevent unintended program behavior specifically we extract properties of an embedded program through static program analysis and use them as the bases for enforcing permissible program behavior at run time the processor architecture is augmented with hardware monitor that observes the program's dynamic execution trace checks whether it falls within the allowed program behavior and flags any deviations from expected behavior to trigger appropriate response mechanisms we present properties that capture permissible program behavior at different levels of granularity namely inter procedural control flow intra procedural control flow and instruction stream integrity we outline systematic methodology to design application specific hardware monitors for any given embedded program hardware implementations using commercial design flow and cycle accurate performance simulations indicate that the proposed technique can thwart several common software and physical attacks facilitating secure program execution with minimal overheads
we present progressive encoding technique specifically designed for complex isosurfaces it achieves better rate distortion performance than all standard mesh coders and even improves on all previous single rate isosurface coders our novel algorithm handles isosurfaces with or without sharp features and deals gracefully with high topologic and geometric complexity the inside outside function of the volume data is progressively transmitted through the use of an adaptive octree while local frame based encoding is used for the fine level placement of surface samples local patterns in topology and local smoothness in geometry are exploited by context based arithmetic encoding allowing us to achieve an average of bits per vertex at very low distortion of this rate only are dedicated to connectivity data this improves by over the best previous single rate isosurface encoder
we explore how the placement of control widgets such as menus affects collaboration and usability for co located tabletop groupware applications we evaluated two design alternatives centralized set of controls shared by all users and separate per user controls replicated around the borders of the shared tabletop we conducted this evaluation in the context of teamtag system for collective annotation of digital photos our comparison of the two design alternatives found that users preferred replicated over shared controls we discuss the cause of this preference and also present data on the impact of these interface design variants on collaboration as well as the role that orientation co touching and the use of different regions of the table played in shaping users behavior and preferences
bpql is novel query language for querying business process specifications introduced recently in it is based on an intuitive model of business processes as rewriting systems an abstraction of the emerging bpel business process execution language standard bpql allows users to query business processes visually in manner very analogous to the language used to specify the processes the goal of the present paper is to study the formal model underlying bpql and investigate its properties as well as the complexity of query evaluation we also study its relationship to previously suggested formalisms for process modeling and querying in particular we propose query evaluation algorithm of polynomial data complexity that can be applied uniformly to queries on the structure of the process specification as well as on the potential behavior of the defined process we show that unless np the efficiency of our algorithm is asymptotically optimal
objects often define usage protocols that clients must follow inorder for these objects to work properly aliasing makes itnotoriously difficult to check whether clients and implementations are compliant with such protocols accordingly existing approaches either operate globally or severely restrict aliasing we have developed sound modular protocol checking approach based on typestates that allows great deal of flexibility in aliasing while guaranteeing the absence of protocol violations at runtime the main technical contribution is novel abstraction access permissions that combines typestate and object aliasing information in our methodology developers express their protocol design intent through annotations based on access permissions our checking approach then tracks permissions through method implementations for each object reference the checker keeps track of the degree of possible aliasing and is appropriately conservativein reasoning about that reference this helps developers account for object manipulations that may occur through aliases the checking approach handles inheritance in novel way giving subclasses more flexibility in method overriding case studies on java iterators and streams provide evidence that access permissions can model realistic protocols and protocol checking based on access permissions can be used to reason precisely about the protocols that arise in practice
the goal of data integration is to provide uniform access to set of heterogeneous data sources freeing the user from the knowledge about where the data are how they are stored and how they can be accessed one of the outcomes of the research work carried out on data integration in the last years is clear architecture comprising global schema the source schema and the mapping between the source and the global schema although in many research works and commercial tools the global schema is simply data structure integrating the data at the sources we argue that the global schema should represent instead the conceptual model of the domain however to fully pursue such an approach several challenging issues are to be addressed the main goal of this paper is to analyze one of them namely how to express the conceptual model representing the global schema we start our analysis with the case where such schema is expressed in terms of uml class diagram and we end up with proposal of particular description logic called textit dl lite mathcal id we show that the data integration framework based on such logic has several interesting properties including the fact that both reasoning at design time and answering queries at run time can be done efficiently
with business emerging as key enabler to drive supply chains the focus of supply chain management has been shifted from production efficiency to customer driven and partnership synchronization approaches this strategic shift depends on the match between the demands and offerings that deliver the services to achieve this we need to coordinate the flow of information among the services and link their business processes under various constraints existing approaches to this problem have relied on complete information of services and resources and have failed to adequately address the dynamics and uncertainties of the operating environments the real world situation is complicated as result of undetermined requirements of services involved in the chain unpredictable solutions contributed by service providers and dynamic selection and aggregation of solutions to services this paper examines an agent mediated approach to on demand business supply chain integration each agent works as service broker exploring individual service decisions as well as interacting with each other for achieving compatibility and coherence among the decisions of all services based on the framework prototype has been implemented with simulated experiments highlighting the effectiveness of the approach
wireless network wlan provides unique challenges to system design wlan uses shared and highly unreliable medium where protocols must rely on precise timing of requests and responses to detect submission errors and priorities among network nodes in addition wlan stations are often embedded and have tight constraints on power costs and performance to design wlan nodes precise estimations on performance and resource usage are needed for the complete network system to explore the design space and assess the quality of solutions our systemclick framework combines systemc with resource models and performance annotations derived from actual implementations based on click model generation is automated and the performance of systemclick model is obtained depending on actual activation patterns of different functional blocks in the model case study demonstrates our approach for the analysis of real time critical systems
unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest in this paper we study three important issues that have been previously ignored in mining unexpected rules first the unexpectedness of rule depends on how the user prefers to apply the prior knowledge to given scenario in addition to the knowledge itself second the prior knowledge should be considered right from the start to focus the search on unexpected rules third the unexpectedness of rule depends on what other rules the user has seen so far thus only rules that remain unexpected given what the user has seen should be considered interesting we develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules
developing profiles to describe user or system behaviour is useful technique employed in computer forensic investigations information found in data obtained by investigators can often be used to establish view of regular usage patterns which can then be examined for unusual occurrences this paper describes one such method based on details provided by events found within computer forensic evidence events compiled from potentially numerous sources are grouped according to some criteria and frequently occurring event sequences are established the methodology and techniques to extract and contrast these sequences are then described and discussed along with similar prior work in the same domain
on line analytical processing olap has become an important component in most data warehouse systems and decision support systems in recent years in order to deal with the huge amount of data highly complex queries and increasingly strict response time requirements approximate query processing has been deemed viable solution most works in this area however focus on the space efficiency and are unable to provide quality guaranteed answers to queries to remedy this in this paper we propose an efficient framework of dct for data with error estimation called dawn which focuses on answering range sum queries from compressed op cubes transformed by dct specifically utilizing the techniques of geometric series and euler's formula we devise robust summation function called the ge function to answer range queries in constant time regardless of the number of data cells involved note that the ge function can estimate the summation of cosine functions precisely thus the quality of the answers is superior to that of previous works furthermore an estimator of errors based on the brown noise assumption bna is devised to provide tight bounds for answering range sum queries our experiment results show that the dawn framework is scalable to the selectivity of queries and the available storage space with ge functions and the bna method the dawn framework not only delivers high quality answers for range sum queries but also leads to shorter query response time due to its effectiveness in error estimation
temperature is becoming first rate design criterion in asics due to its negative impact on leakage power reliability performance and packaging cost incorporating awareness of such lower level physical phenomenon in high level synthesis algorithms will help to achieve better designs in this work we developed temperature aware binding algorithm switching power of module correlates with its operating temperature the goal of our binding algorithm is to distribute the activity evenly across functional units this approach avoids steep temperature differences between modules on chip hence the occurrence of hot spots starting with switching optimal binding solution our algorithm iteratively minimizes the maximum temperature reached by the hottest functional unit our algorithm does not change the number of resources used in the original binding we have used hotspot temperature modeling tool to simulate temperature of number asic designs our binding algorithm reduces temperature reached by the hottest resource by on average reducing the peak temperature has positive impact on leakage as well our binding technique improves leakage power by and overall power by on average at nm technology node compared to switching optimal binding
this research investigates the work practices of system administrators using semi structured interviews and an analysis of existing system administrator literature we theorize that system administrators act as technical brokers who bridge two communities the end users they support and their own technical community we also show that system administrators like other technical workers rely on contextual knowledge this knowledge is largely acquired through practice and less through formal education and certification through discussion of common reactive and proactive system administrator duties we present system administrators as broker technicians who must mediate between the end users they support and their technical community we end with discussion of the changing role of sysadmins as their tools and users get more sophisticated
asymmetric broadband connections in the home provide limited upstream pipe to the internet this limitation makes various applications such as remote backup and sharing high definition video impractical however homes in neighborhood often have high bandwidth wireless networks whose bandwidth exceeds that of single wired uplink moreover most wired and wireless connections are idle most of the time in this paper we examine the fundamental requirements of system that aggregates upstream broadband connections in neighborhood using wireless communication between homes scheme addressing this problem must operate efficiently in an environment that is highly lossy ii broadcast in nature and iii half duplex we propose novel scheme link alike that addresses those three challenges using opportunistic wireless reception novel wireless broadcast rate control scheme and preferential use of the wired downlink through analytical and experimental evaluation we demonstrate that our approach provides significantly better throughput than previous solutions based on tcp or udp unicast
entity resolution er is the problem of identifying which records in database refer to the same real world entity an exhaustive er process involves computing the similarities between pairs of records which can be very expensive for large datasets various blocking techniques can be used to enhance the performance of er by dividing the records into blocks in multiple ways and only comparing records within the same block however most blocking techniques process blocks separately and do not exploit the results of other blocks in this paper we propose an iterative blocking framework where the er results of blocks are reflected to subsequently processed blocks blocks are now iteratively processed until no block contains any more matching records compared to simple blocking iterative blocking may achieve higher accuracy because reflecting the er results of blocks to other blocks may generate additional record matches iterative blocking may also be more efficient because processing block now saves the processing time for other blocks we implement scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets
wikipedia has grown to be the world largest and busiest free encyclopedia in which articles are collaboratively written and maintained by volunteers online despite its success as means of knowledge sharing and collaboration the public has never stopped criticizing the quality of wikipedia articles edited by non experts and inexperienced contributors in this paper we investigate the problem of assessing the quality of articles in collaborative authoring of wikipedia we propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history our scp asic scp model is designed based on the mutual dependency between article quality and their author authority the scp eer scp scp eview scp model introduces the review behavior into measuring article quality finally our scp rob scp scp eview scp models extend scp eer scp scp eview scp with partial reviewership of contributors as they edit various portions of the articles we conduct experiments on set of well labeled wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement
on account of the enormous amounts of rules that can be produced by data mining algorithms knowledge post processing is difficult stage in an association rule discovery process in order to find relevant knowledge for decision making the user decision maker specialized in the data studied needs to rummage through the rules to assist him her in this task we here propose the rule focusing methodology an interactive methodology for the visual post processing of association rules it allows the user to explore large sets of rules freely by focusing his her attention on limited subsets this new approach relies on rule interestingness measures on visual representation and on interactive navigation among the rules we have implemented the rule focusing methodology in prototype system called arvis it exploits the user's focus to guide the generation of the rules by means of specific constraint based rule mining algorithm
for the last few years considerable number of efforts have been devoted into integrating security issues into information systems development practices this has led to number of languages methods methodologies and techniques for considering security issues during the developmental stages of an information system however these approaches mainly focus on security requirements elicitation analysis and design issues and neglect testing this paper presents the security attack testing sat approach novel scenario based approach that tests the security of an information system at the design time the approach is illustrated with the aid of real life case study involving the development of health and social care information system
how to exploit application semantics to improve the performance of real time data intensive application has been an active research topic in the past few years weaker correctness criteria and semantics based concurrency control algorithms were proposed to provide more flexibility in reordering read and write events distinct from past work this paper exploits the trade off between data consistency and system workload the definition of similarity is combined with the idea of transaction skipping to provide theoretical foundation for reducing the workload of transaction system we also propose guidelines to adjust the execution frequencies of static set of transactions and prove their correctness the strengths of this work were verified by simulation experiments on an air traffic control example
we investigate the feasibility of reconstructing an arbitrarily shaped specular scene refractive or mirror like from one or more viewpoints by reducing shape recovery to the problem of reconstructing individual light paths that cross the image plane we obtain three key results first we show how to compute the depth map of specular scene from single viewpoint when the scene redirects incoming light just once second for scenes where incoming light undergoes two refractions or reflections we show that three viewpoints are sufficient to enable reconstruction in the general case third we show that it is impossible to reconstruct individual light paths when light is redirected more than twice our analysis assumes that for every point on the image plane we know at least one point on its light path this leads to reconstruction algorithms that rely on an environment matting procedure to establish pixel to point correspondences along light path preliminary results for variety of scenes mirror glass etc are also presented
the well accepted wisdom is that tcp's exponential backoff mechanism introduced by jacobson years ago is essential for preserving the stability of the internet in this paper we show that removing exponential backoff from tcp altogether can be done without inducing any stability side effects we introduce the implicit packet conservation principle and show that as long as the endpoints uphold this principle they can only improve their end to end performance relative to the exponential backoff case by conducting large scale simulations modeling and network experiments in emulab and the internet using kernel level freebsd tcp implementation realistic traffic distributions and complex network topologies we demonstrate that tcp's binary exponential backoff mechanism can be safely removed moreover we show that insuitability of tcp's exponential backoff is fundamental ie independent from the currently dominant internet traffic properties or bottleneck capacities surprisingly our results indicate that path to incrementally deploying the change does exist
in this paper we investigate whether it is possible to develop measure that quantifies the naturalness of human motion as defined by large database such measure might prove useful in verifying that motion editing operation had not destroyed the naturalness of motion capture clip or that synthetic motion transition was within the space of those seen in natural human motion we explore the performance of mixture of gaussians mog hidden markov models hmm and switching linear dynamic systems slds on this problem we use each of these statistical models alone and as part of an ensemble of smaller statistical models we also implement naive bayes nb model for baseline comparison we test these techniques on motion capture data held out from database keyframed motions edited motions motions with noise added and synthetic motion transitions we present the results as receiver operating characteristic roc curves and compare the results to the judgments made by subjects in user study
in these lecture notes we present the itasks system set of combinators to specify work flows in pure functional language at very high level of abstraction work flow systems are automated systems in which tasks are coordinated that have to be executed by either humans or computers the combinators that we propose support work flow patterns commonly found in commercial work flow systems in addition we introduce novel work flow patterns that capture real world requirements but that can not be dealt with by current systems compared with most of these commercial systems the itasks system offers several further advantages tasks are statically typed tasks can be higher order the combinators are fully compositional dynamic and recursive work flows can be specified and last but not least the specification is used to generate an executable web based multi user work flow application with the itasks system useful work flows can be defined which cannot be expressed in other systems work can be interrupted and subsequently directed to other workers for further processing the itasks system has been constructed in the programming language clean making use of its generic programming facilities and its idata toolkit with which interactive thin client form based web applications can be created in all itasks are an excellent case of the expressive power of functional and generic programming
in this paper we present perceptual experiment whose results aid the creation of non photorealistic rendering npr styles which can positively affect user task performance in real time scenes in visual search task designed to test people's perception of abstracted scenes different types of stylisation are used to investigate how reaction times can be affected we show how npr techniques compare against non stylised renderings compare the effectiveness of different styles determine how varying each style can affect performance and investigate how these styles perform with objects of varying complexity the results show that npr can be useful tool for increasing the saliency of target objects while reducing the visual impact of the rest of the scene however it is also shown that the success of each style depends largely on the scene context and also on the level of stylisation used we believe the results from this study can help in the creation of effective npr styles in the future supplementary material can be found at http isgcstcdie skrbal redmond
applications written in unsafe languages like and are vulnerable to memory errors such as buffer overflows dangling pointers and reads of uninitialized data such errors can lead to program crashes security vulnerabilities and unpredictable behavior we present diehard runtime system that tolerates these errors while probabilistically maintaining soundness diehard uses randomization and replication to achieve probabilistic memory safety by approximating an infinite sized heap diehard's memory manager randomizes the location of objects in heap that is at least twice as large as required this algorithm prevents heap corruption and provides probabilistic guarantee of avoiding memory errors for additional safety diehard can operate in replicated mode where multiple replicas of the same application are run simultaneously by initializing each replica with different random seed and requiring agreement on output the replicated version of die hard increases the likelihood of correct execution because errors are unlikely to have the same effect across all replicas we present analytical and experimental results that show diehard's resilience to wide range of memory errors including heap based buffer overflow in an actual application
in this paper we show how to reduce downtime of jee applications by rapidly and automatically recovering from transient and intermittent software failures without requiring application modifications our prototype combines three application agnostic techniques macroanalysis for fault detection and localization microrebooting for rapid recovery and external management of recovery actions the individual techniques are autonomous and work across wide range of componentized internet applications making them well suited to the rapidly changing software of internet services the proposed framework has been integrated with jboss an open source jee application server our prototype provides an execution platform that can automatically recover jee applications within seconds of the manifestation of fault our system can provide subset of system's active end users with the illusion of continuous uptime in spite of failures occurring behind the scenes even when there is no functional redundancy in the system
we present method for browsing videos by directly dragging their content this method brings the benefits of direct manipulation to an activity typically mediated by widgets we support this new type of interactivity by automatically extracting motion data from videos and new technique called relative flow dragging that lets users control video playback by moving objects of interest along their visual trajectory we show that this method can outperform the traditional seeker bar in video browsing tasks that focus on visual content rather than time
mobile agents are promising technology to face the problems raised by the increasing complexity and size of today's networks in particular in the area of network management mobile agents can lead to fully distributed paradigm to overcome the limits of traditional centralized approaches basic requirement for the management of complex network is the definition of high level and flexible models to coordinate the accesses to the resources mdash data and services mdash provided by the network nodes on this basis this paper describes the mars coordination architecture for mobile agents mars is based on the definition of programmable tuple spaces associated with the network nodes mobile agents can access the local resources and services via the tuple space thus adopting standard and high level interface the network administrator mdash via mobile agents mdash can dynamically program the behavior of the tuple space in reaction to the agents access to the tuple space thus leading to flexible network model several examples show the effectiveness of the mars approach in supporting network management activities
this paper studies the performance and security aspects of the iscsi protocol in network storage based system ethernet speeds have been improving rapidly and network throughput is no longer considered bottleneck when compared to fibre channel based storage area networks however when security of the data traffic is taken into consideration existing protocols like ipsec prove to be major hindrance to the overall throughput in this paper we evaluate the performance of iscsi when deployed over standard security protocols and suggest lazy crypto approaches to alleviate the processing needs at the server the testbed consists of cluster of linux machines directly connected to the server through gigabit ethernet network micro and application benchmarks like btio and dbench were used to analyze the performance and scalability of the different approaches our proposed lazy approaches improved through put by as much as for microbenchmarks and for application benchmarks in comparison to the ipsec based approaches
this paper presents novel independent component analysis ica color space method for pattern recognition the novelty of the ica color space method is twofold deriving effective color image representation based on ica and implementing efficient color image classification using the independent color image representation and an enhanced fisher model efm first the ica color space method assumes that each color image is defined by three independent source images which can be derived by means of blind source separation procedure such as ica unlike the color rgb space where the and component images are correlated the new ica color space method derives three component images and that are independent and hence uncorrelated second the three independent color component images are concatenated to form an augmented pattern vector whose dimensionality is reduced by principal component analysis pca an efm then derives the discriminating features of the reduced pattern vector for pattern recognition the effectiveness of the proposed ica color space method is demonstrated using complex grand challenge pattern recognition problem and large scale database in particular the face recognition grand challenge frgc and the biometric experimentation environment bee reveal that for the most challenging frgc version experiment which contains training images controlled target images and uncontrolled query images the ica color space method achieves the face verification rate roc iii of at the false accept rate far of compared to the face verification rate fvr of of the rgb color space using the same efm and of the frgc baseline algorithm at the same far
in this paper we introduce the method tagging substitute complement attributes on miscellaneous recommending relations and elaborate how this step contributes to electronic merchandising there are already decades of works in building recommender systems steadily outperforming previous algorithms is difficult under the conventional framework however in real merchandising scenarios we find describing the weight of recommendation simply as scalar number is hardly expressive which hinders the further progress of recommender systems we study large log of user browsing data revealing the typical substitute complement relations among items that can further extend recommender systems in enriching the presentation and improving the practical quality finally we provide an experimental analysis and sketch an online prototype to show that tagging attributes can grant more intelligence to recommender systems by differentiating recommended candidates to fit respective scenarios
despite the importance of pointing device movement to efficiency in interfaces little is known on how target shape impacts speed acceleration and other kinematic properties of motion in this paper we examine which kinematic characteristics of motion are impacted by amplitude and directional target constraints in fitts style pointing tasks our results show that instantaneous speed acceleration and jerk are most affected by target constraint results also show that the effects of target constraint are concentrated in the first of movement distance we demonstrate that we can discriminate between the two classes of target constraint using machine learning with accuracy greater than chance finally we highlight future work in designing techniques that make use of target constraint to improve pointing efficiency in computer interfaces
this paper presents an innovative approach to solve the problem of missing transparency of competencies within virtual organizations we based our work on empirical studies to cope with the problem of competence finding in distributed organizations former studies have shown that central storage of expertise profiles is inappropriate due to missing flexibility and high costs of maintenance the focus of our approach is to support peripheral awareness to become aware of the available competences in organizations our approach runs along two lines making expertise related communication visible for all members of an organization and visualizing competence indicating events in collaboration infrastructures we verified this approach by the evaluation of prototypical implementation
with the advent of home networking and widespread deployment of broadband connectivity to homes wealth of new services with real time quality of service qos requirements have emerged eg video on demand vod ip telephony which have to co exist with traditional non real time services such as web browsing and file downloading over the transmission control protocol tcp the co existence of such real time and non real time services demands the residential gateway rg to employ bandwidth management algorithms to control the amount of non real time tcp traffic on the broadband access link from the internet service provider isp to the rg so that the bandwidth requirements of the real time traffic are satisfied in this paper we propose an algorithm to control the aggregate bandwidth of the incoming non real time tcp traffic at the rg so that qos requirements of the real time traffic can be guaranteed the idea is to limit the maximum data rates of active tcp connections by dynamically manipulating their flow control window sizes based on the total available bandwidth for the non real time traffic we show by simulation results that our algorithm limits the aggregate bandwidth of the non real time tcp traffic thus granting the real time traffic the required bandwidth
we give generic construction for universal designated verifier signature schemes from large class of signature schemes the resulting schemes are efficient and have two important properties firstly they are provably dv unforgeable non transferable and also non delegatable secondly the signer and the designated verifier can independently choose their cryptographic settings we also propose generic construction for identity based signature schemes from any signature scheme in and prove that the construction is secure against adaptive chosen message and identity attacks we discuss possible extensions of our constructions to universal multi designated verifier signatures hierarchical identity based signatures identity based universal designated verifier signatures and identity based ring signatures from any signature in
encrypting data in unprotected memory has gained much interest lately for digital rights protection and security reasons counter mode is well known encryption scheme it is symmetric key encryption scheme based on any block cipher eg aes the scheme√Ωs encryption algorithm uses block cipher secret key and counter or sequence number to generate an encryption pad which is xored with the data stored in memory like other memory encryption schemes this method suffers from the inherent latency of decrypting encrypted data when loading them into the on chip cache one solution that parallelizes data fetching and encryption pad generation requires the sequence numbers of evicted cache lines to be cached on chip on chip sequence number caching can be successful in reducing the latency at the cost of large area overhead in this paper we present novel technique to hide the latency overhead of decrypting counter mode encrypted memory by predicting the sequence number and pre computing the encryption pad that we call one time pad or otp in contrast to the prior techniques of sequence number caching our mechanism solves the latency issue by using idle decryption engine cycles to speculatively predict and pre compute otps before the corresponding sequence number is loaded this technique incurs very little area overhead in addition novel adaptive otp prediction technique is also presented to further improve our regular otp prediction and precomputation mechanism this adaptive scheme is not only able to predict encryption pads associated with static and infrequently updated cache lines but also those frequently updated ones as well experimental results using spec benchmark show an prediction rate moreover we also explore several optimization techniques for improving the prediction accuracy two specific techniques two level prediction and context based prediction are presented and evaluated for the two level prediction the prediction rate was improved from to with the context based prediction the prediction rate approaches context based otp prediction outperforms very large kb sequence number cache for many memory bound spec programs ipc results show an overall to performance improvement using our prediction and precomputation and another improvement when context based prediction techniques is used
in order to fulfill the complex resource requirements of some users in grid environments support for coallocation between different resource providers is needed here it is quite difficult to coordinate these different services from different resource providers because grid scheduler has to cope with different policies and objectives of the different resource providers and of the users agreement based resource management is considered feasible solution to solve many of these problems as it supports the reliable interaction between different providers and users however most current models do not well support co allocation here negotiation is needed to create such bi lateral agreements between several grid parties such negotiation process should be automated with no or minimal human interaction considering the potential scale of grid systems and the amount of necessary transactions therefore strategic negotiation models play an important role in this paper negotiation models which supports the co allocation between different resource providers are proposed and examined first simulations have been conducted to evaluate the presented system the results demonstrate that the proposed negotiation model are suitable and effective for grid environments
syndromic surveillance can play an important role in protecting the public's health against infectious diseases infectious disease outbreaks can have devastating effect on society as well as the economy and global awareness is therefore critical to protecting against major outbreaks by monitoring online news sources and developing an accurate news classification system for syndromic surveillance public health personnel can be apprised of outbreaks and potential outbreak situations in this study we have developed framework for automatic online news monitoring and classification for syndromic surveillance the framework is unique and none of the techniques adopted in this study have been previously used in the context of syndromic surveillance on infectious diseases in recent classification experiments we compared the performance of different feature subsets on different machine learning algorithms the results showed that the combined feature subsets including bag of words noun phrases and named entities features outperformed the bag of words feature subsets furthermore feature selection improved the performance of feature subsets in online news classification the highest classification performance was achieved when using svm upon the selected combination feature subset
the self organizing knowledge representation aspects in heterogeneous information environments involving object oriented databases relational databases and rulebases are investigated the authors consider facet of self organizability which sustains the structural semantic integrity of an integrated schemea regardless of the dynamic nature of local schemata to achieve this objective they propose an overall scheme for schema translation and schema integration with an object oriented data model as common data model and it is shown that integrated schemata can be maintained effortlessly by propagating updates in local schemata to integrated schemata unambiguously
the increasing number of functionally similar services requires the existence of non functional properties selection process based on the quality of service qos thus in this article authors focus on the provision of qos model an architecture and an implementation which enhance the selection process by the annotation of service level agreement sla templates with semantic qos metrics this qos model is composed by specification for annotating sla templates files qos conceptual model formed as qos ontology and selection algorithm this approach which is backward compatible provides interoperability among customer providers and lightweight alternative finally its applicability and benefits are shown by using examples of infrastructure services
we present new technique for verifying correspondences in security protocols in particular correspondences can be used to formalize authentication our technique is fully automatic it can handle an unbounded number of sessions of the protocol and it is efficient in practice it significantly extends previous technique for the verification of secrecy the protocol is represented in an extension of the pi calculus with fairly arbitrary cryptographic primitives this protocol representation includes the specification of the correspondence to be verified but no other annotation this representation is then translated into an abstract representation by horn clauses which is used to prove the desired correspondence our technique has been proved correct and implemented we have tested it on various protocols from the literature the experimental results show that these protocols can be verified by our technique in less than
wireless sensor networks are resource constrained self organizing systems that are often deployed in inaccessible and inhospitable environments in order to collect data about some outside world phenomenon for most sensor network applications point to point reliability is not the main objective instead reliable event of interest delivery to the server needs to be guaranteed possibly with certain probability the nature of communication in sensor networks is unpredictable and failure prone even more so than in regular wireless ad hoc networks therefore it is essential to provide fault tolerant techniques for distributed sensor applications many recent studies in this area take drastically different approaches to addressing the fault tolerance issue in routing transport and or application layers in this paper we summarize and compare existing fault tolerant techniques to support sensor applications we also discuss several interesting open research directions
this paper presents an algorithm for maximizing the lifetime of sensor network while guaranteeing an upper bound on the end to end delay we prove that the proposed algorithm is optimal and requires simple computing operations that can be implemented by simple devices to the best of our knowledge this is the first paper to propose sensor wake up frequency that depends on the sensor's location in the routing paths using simulations we show that the proposed algorithm significantly increases the lifetime of the network while guaranteeing maximum on the end to end delay
real time eyegaze selection interface was implemented using tobii eyegaze tracking monitor hierarchical button menu was displayed on the screen and specified selections were made by eyegaze fixations and glances on the menu widgets the initial version tested three different spatial layouts of the menu widgets and employed dwell glance method of selection results from the pilot interface led to usability improvements in the second version of the interface selections were activated using glance dwell method the usability of the second study interface received positive response from all participants each selection gained more than speed increase using the revised interface more intuitive selection interface in the second study allowed us to test users selection accuracy at faster dwell selection thresholds users quickly learned to achieve accurate selections in ms but made errors when selections occurred in ms
the turn model routing algorithms for mesh interconnection network achieve partial adaptivity without any virtual channels however the routing performance measured by simulations is worse than with the simple deterministic routing algorithm authors have explained these results simply by uneven dynamic load through the network however this phenomenon has not been studied furtherthis paper investigates performance degradation with turn model and drawbacks of partially adaptive routing in comparison with the deterministic routing and it introduces some new concepts our simulations deal with individual channels and results are presented by graphs rather than by commonly used averages an additional parameter channel occupation which is consistent with queuing theory commonly used in many proposed analytical models is introduced we also propose new structure the channel directions dependency graph cddg it provides new approach in analysis helps in understanding of dynamic routing behaviour and it can be generalized in other routing algorithms
although tremendous success has been achieved for interactive object cutout in still images accurately extracting dynamic objects in video remains very challenging problem previous video cutout systems present two major limitations reliance on global statistics thus lacking the ability to deal with complex and diverse scenes and treating segmentation as global optimization thus lacking practical workflow that can guarantee the convergence of the systems to the desired results we present video snapcut robust video object cutout system that significantly advances the state of the art in our system segmentation is achieved by the collaboration of set of local classifiers each adaptively integrating multiple local image features we show how this segmentation paradigm naturally supports local user editing and propagates them across time the object cutout system is completed with novel coherent video matting technique comprehensive evaluation and comparison is presented demonstrating the effectiveness of the proposed system at achieving high quality results as well as the robustness of the system against various types of inputs
to efficiently deliver streaming media researchers have developed technical solutions that fall into three categories each of which has its merits and limitations infrastructure based cdns with dedicated network bandwidths and hardware supports can provide high quality streaming services but at high cost server based proxies are cost effective but not scalable due to the limited proxy capacity in storage and bandwidth and its centralized control also brings single point of failure client based pp networks are scalable but do not guarantee high quality streaming service due to the transient nature of peers to address these limitations we present novel and efficient design of scalable and reliable media proxy system assisted by pp networks called prop in the prop system the clients machines in an intranet are self organized into structured pp system to provide large media storage and to actively participate in the streaming media delivery where the proxy is also embedded as an important member to ensure the quality of streaming service the coordination and collaboration in the system are efficiently conducted by our pp management structure and replacement policies our system has the following merits it addresses both the scalability problem in centralized proxy systems and the unreliable service concern by only relying on the pp sharing of clients the proposed content locating scheme can timely serve the demanded media data and fairly dispatch media streaming tasks in appropriate granularity across the system based on the modeling and analysis we propose global replacement policies for proxy and clients which well balance the demand and supply of streaming data in the system achieving high utilization of peers cache we have comparatively evaluated our system through trace driven simulations with synthetic workloads and with real life workload extracted from the media server logs in an enterprise network which shows our design significantly improves the quality of media streaming and the system scalability
message passing interface mpi is widely used standard for managing coarse grained concurrency on distributed computers debugging parallel mpi applications however has always been particularly challenging task due to their high degree of concurrent execution and non deterministic behavior deterministic replay is potentially powerful technique for addressing these challenges with existing mpi replay tools adopting either data replay or order replay approaches unfortunately each approach has its tradeoffs data replay generates substantial log sizes by recording every communication message order replay generates small logs but requires all processes to be replayed together we believe that these drawbacks are the primary reasons that inhibit the wide adoption of deterministic replay as the critical enabler of cyclic debugging of mpi applications this paper describes subgroup reproducible replay srr hybrid deterministic replay method that provides the benefits of both data replay and order replay while balancing their trade offs srr divides all processes into disjoint groups it records the contents of messages crossing group boundaries as in data replay but records just message orderings for communication within group as in order replay in this way srr can exploit the communication locality of traffic patterns in mpi applications during replay developers can then replay each group individually srr reduces recording overhead by not recording intra group communication and reduces replay overhead by limiting the size of each replay group exposing these tradeoffs gives the user the necessary control for making deterministic replay practical for mpi applications we have implemented prototype mpiwiz to demonstrate and evaluate srr mpiwiz employs replay framework that allows transparent binary instrumentation of both library and system calls as result mpiwiz replays mpi applications with no source code modification and relinking and handles non determinism in both mpi and os system calls our preliminary results show that mpiwiz can reduce recording overhead by over factor of four relative to data replay yet without requiring the entire application to be replayed as in order replay recording increases execution time by while the application can be replayed in just of its base execution time
the smoke and mirrors file system smfs mirrors files at geographically remote datacenter locations with negligible impact on file system performance at the primary site and minimal degradation as function of link latency it accomplishes this goal using wide area links that run at extremely high speeds but have long round trip time latencies combination of properties that poses problems for traditional mirroring solutions in addition to its raw speed smfs maintains good synchronization should the primary site become completely unavailable the system minimizes loss of work even for applications that simultaneously update groups of files we present the smfs design then evaluate the system on emulab and the cornell national lambda rail nlr ring testbed intended applications include wide area file sharing and remote backup for disaster recovery
we discuss approaches to incrementally construct an ensemble the first constructs an ensemble of classifiers choosing subset from larger set and the second constructs an ensemble of discriminants where classifier is used for some classes only we investigate criteria including accuracy significant improvement diversity correlation and the role of search direction for discriminant ensembles we test subset selection and trees fusion is by voting or by linear model using classifiers on data sets incremental search finds small accurate ensembles in polynomial time the discriminant ensemble uses subset of discriminants and is simpler interpretable and accurate we see that an incremental ensemble has higher accuracy than bagging and random subspace method and it has comparable accuracy to adaboost but fewer classifiers
using automated reasoning techniques we tackle the niche activity of proving that program is free from run time exceptions such property is particularly valuable in high integrity software for example safety or security critical applications the context for our work is the spark approach for the development of high integrity software the spark approach provides significant degree of automation in proving exception freedom where this automation fails however the programmer is burdened with the task of interactively constructing proof and possibly also having to supply auxiliary program annotations we minimize this burden by increasing the automation through an integration of proof planning and program analysis oracle we advocate cooperative integration where proof failure analysis directly constrains the search for auxiliary program annotations the approach has been successfully tested on industrial data
to provide scalable communication infrastructure for systems on chips socs networks on chips nocs communication centric design paradigm is needed to be cost effective socs are often programmable and integrate several different applications or use cases on to the same chip for the soc platform to support the different use cases the noc architecture should satisfy the performance constraints of each individual use case in this work we motivate the need to consider multiple use cases during the noc design process we present method to efficiently map the applications on to the noc architecture satisfying the design constraints of each individual use case we also present novel ways to dynamically reconfigure the network across the different use cases and explore the possibility of integrating dynamic voltage and frequency scaling dvs dfs techniques with the use case centric noc design methodology we validate the performance of the design methodology on several soc applications the dynamic reconfiguration of the noc integrated with dvs dfs schemes results in large power savings for the resulting noc systems
tools and analyses that find bugs in software are becoming increasingly prevalent however even after the potential false alarms raised by such tools are dealt with many real reported errors may go unfixed in such cases the programmers have judged the benefit of fixing the bug to be less than the time cost of understanding and fixing itthe true utility of bug finding tool lies not in the number of bugs it finds but in the number of bugs it causes to be fixedanalyses that find safety policy violations typically give error reports as annotated backtraces or counterexamples we propose that bug reports additionally contain specially constructed patch describing an example way in which the program could be modified to avoid the reported policy violation programmers viewing the analysis output can use such patches as guides starting points or as an additional way of understanding what went wrongwe present an algorithm for automatically constructing such patches given model checking and policy information typically already produced by most such analyses we are not aware of any previous automatic techniques for generating patches in response to safety policy violations our patches can suggest additional code not present in the original program and can thus help to explain bugs related to missing program elements in addition our patches do not introduce any new violations of the given safety policyto evaluate our method we performed software engineering experiment applying our algorithm to over bug reports produced by two off the shelf bug finding tools running on large java programs bug reports also accompanied by patches were three times as likely to be addressed as standard bug reportsthis work represents an early step toward developing new ways to report bugs and to make it easier for programmers to fix them even minor increase in our ability to fix bugs would be great increase for the quality of software
randomized response techniques have been investigated in privacy preserving categorical data analysis however the released distortion parameters can be exploited by attackers to breach privacy in this paper we investigate whether data mining or statistical analysis tasks can still be conducted on randomized data when distortion parameters are not disclosed to data miners we first examine how various objective association measures between two variables may be affected by randomization we then extend to multiple variables by examining the feasibility of hierarchical loglinear modeling finally we show some classic data mining tasks that cannot be applied on the randomized data directly
digital television will bring significant increase in the amount of channels and programs available to end users with many more difficulties to find contents appealing to them among myriad of irrelevant information thus automatic content recommenders should receive special attention in the following years to improve their assistance to users the current content recommenders have important deficiencies that hamper their wide acceptance in this paper we present new approach for automatic content recommendation that significantly reduces those deficiencies this approach based on semantic web technologies has been implemented in the advanced telematic search of audiovisual contents by semantic reasoning tool hybrid content recommender that makes extensive use of well known standards such as multimedia home platform tv anytime and owl also we have carried out an experimental evaluation the results of which show that our proposal performs better than other existing approaches copyright copy john wiley sons ltd
distributing single global clock across chip while meeting the power requirements of the design is troublesome task due to shrinking technology nodes associated with high clock frequencies to deal with this network on chip noc architectures partitioned into several voltage frequency islands vfis have been proposed to interface the islands on chip operating at different frequencies complex bi synchronous fifo design is inevitable however these fifos are not needed if adjacent switches belong to the same clock domain in this paper reconfigurable synchronous bi synchronous rsbs fifo is proposed which can adapt its operation to either synchronous or bi synchronous mode the fifo is presented by three different scalable and synthesizable design styles and in addition some techniques are suggested to show how the fifo could be utilized in vfi based noc our analysis reveal that the rsbs fifos can help to achieve up to savings in the average power consumption of noc switches and improvement in the total average packet latency in the case of mpeg encoder application when compared to non reconfigurable architecture
in this paper we discuss some of our recent research work designing tabletop interfaces for co located photo sharing we draw particular attention to specific feature of an interface design which we have observed over an extensive number of uses as facilitating an under reported but none the less intriguing aspect of the photo sharing experience namely the process of getting sidetracked through series of vignettes of interaction during photo sharing sessions we demonstrate how users of our tabletop photoware system used peripheral presentation of topically incoherent photos to artfully initiate new photo talk sequences in on going discourse from this we draw implications for the design of tabletop photo applications and for the experiential analysis of such devices
the high performance computing domain is enriching with the inclusion of networks on chip nocs as key component of many core cmps or mpsocs architectures nocs face the communication scalability challenge while meeting tight power area and latency constraints designers must address new challenges that were not present before defective components the enhancement of application level parallelism or power aware techniques may break topology regularity thus efficient routing becomes challengein this paper ulbdr universal logic based distributed routing is proposed as an efficient logic based mechanism that adapts to any irregular topology derived from meshes being an alternative to the use of routing tables either at routers or at end nodes ulbdr requires small set of configuration bits thus being more practical than large routing tables implemented in memories several implementations of ulbdr are presented highlighting the trade off between routing cost and coverage the alternatives span from the previously proposed lbdr approach with of coverage to the ulbdr mechanism achieving full coverage this comes with small performance cost thus exhibiting the trade off between fault tolerance and performance
automatic facial expression analysis is an interesting and challenging problem and impacts important applications in many areas such as human computer interaction and data driven animation deriving an effective facial representation from original face images is vital step for successful facial expression recognition in this paper we empirically evaluate facial representation based on statistical local features local binary patterns for person independent facial expression recognition different machine learning methods are systematically examined on several databases extensive experiments illustrate that lbp features are effective and efficient for facial expression recognition we further formulate boosted lbp to extract the most discriminant lbp features and the best recognition performance is obtained by using support vector machine classifiers with boosted lbp features moreover we investigate lbp features for low resolution facial expression recognition which is critical problem but seldom addressed in the existing work we observe in our experiments that lbp features perform stably and robustly over useful range of low resolutions of face images and yield promising performance in compressed low resolution video sequences captured in real world environments
web personalization is the process of customizing web site to the needs of specific users taking advantage of the knowledge acquired from the analysis of the user's navigational behavior usage data in correlation with other information collected in the web context namely structure content and user profile data due to the explosive growth of the web the domain of web personalization has gained great momentum both in the research and commercial areas in this article we present survey of the use of web mining for web personalization more specifically we introduce the modules that comprise web personalization system emphasizing the web usage mining module review of the most common methods that are used as well as technical issues that occur is given along with brief overview of the most popular tools and applications available from software vendors moreover the most important research initiatives in the web usage mining and personalization areas are presented
an abstract business process contains description the protocol that business process engages in without revealing the internal computation of the process this description provides the information necessary to compose the process with other web services bpel supports this by providing distinct dialects for specifying abstract and executable processes unfortunately bpel does not prevent complex computations from being included in an abstract process this complicates the protocol description unnecessarily reveals implementation details and makes it difficult to analyze correctness we propose some restrictions on the data manipulation constructs that can be used in an abstract bpel process the restrictions permit full description of protocol while hiding computation restricted abstract process can easily be converted into an abstract bpel process or expanded into an executable bpel process based on these restrictions we propose formal model for business process and use it as the basis of an algorithm for demonstrating the correctness of protocol described by restricted abstract process we then sketch an algorithm for synthesizing protocol based on formal specification of its outcome and the tasks available for its construction
to meet the high demand for powerful embedded processors vliw architectures are increasingly complex eg multiple clusters and moreover they now run increasinglysophisticated control intensive applications as result developingarchitecture specific compiler optimizations is becomingboth increasingly critical and complex while time to market constraints remain very tightin this article we present novel program optimizationapproach called the virtual hardware compiler vhc that can perform as well as static compiler optimizations but which requires far less compiler development effort even for complex vliw architectures and complex targetapplications the principle is to augment the target processorsimulator with superscalar like features observe howthe target program is dynamically optimized during execution and deduce an optimized binary for the static vliwarchitecture developing an architecture specific optimizerthen amounts to modifying the processor simulator whichis very fast compared to adapting static compiler optimizationsto an architecture we also show that vhc optimizedbinary trained on number of data sets performs as wellas statically optimized binary on other test data sets theonly drawback of the approach is largely increased compilationtime which is often acceptable for embedded applicationsand devices using the texas instruments vliwprocessor and the associated compiler we experimentallyshow that this approach performs as well as static compileroptimizations for much lower research and developmenteffort using single core and dual core clusteredc processors we also show that the same approach canbe used for efficiently retargeting binary programs within afamily of processors
in this paper we present the results of research project concerning the temporal management of normative texts in xml format in particular four temporal dimensions publication validity efficacy and transaction times are used to correctly represent the evolution of norms in time and their resulting versioning hence we introduce multiversion data model based on xml schema and define basic mechanisms for the maintenance and retrieval of multiversion norm texts finally we describe prototype management system which has been implemented and evaluated
with increasing connectivity between computers the need to keep networks secure progressively becomes more vital intrusion detection systems ids have become an essential component of computer security to supplement existing defenses this paper proposes multiple level hybrid classifier novel intrusion detection system which combines the supervised tree classifiers and unsupervised bayesian clustering to detect intrusions performance of this new approach is measured using the kddcup dataset and is shown to have high detection and low false alarm rates
fault tolerance is an essential requirement for real time systems due to potentially catastrophic consequences of faults in this paper we investigate an efficient off line scheduling algorithm generating schedules in which real time tasks with precedence constraints can tolerate one processor's permanent failure in heterogeneous system with fully connected network the tasks are assumed to be non preemptable and each task has two copies scheduled on different processors and mutually excluded in time in the literature in recent years the quality of schedule has been previously improved by allowing backup copy to overlap with other backup copies on the same processor however this approach assumes that tasks are independent of one other to meet the needs of real time systems where tasks have precedence constraints new overlapping scheme is proposed we show that given two tasks the necessary conditions for their backup copies to safely overlap in time with each other are their corresponding primary copies are scheduled on two different processors they are independent tasks and the execution of their backup copies implies the failures of the processors on which their primary copies are scheduled for tasks with precedence constraints the new overlapping scheme allows the backup copy of task to overlap with its successors primary copies thereby further reducing schedule length based on proposed reliability model tasks are judiciously allocated to processors so as to maximize the reliability of heterogeneous systems additionally times for detecting and handling of permanent fault are incorporated into the scheduling scheme we have performed experiments using synthetic workloads as well as real world application simulation results show that compared with existing scheduling algorithms in the literature our scheduling algorithm improves reliability by up to with an average of and achieves an improvement in performability measure that combines reliability and schedulability by up to with an average of
network throughput can be increased by allowing multipath adaptive routing adaptive routing allows more freedom in the paths taken by messages spreading load over physical channels more evenly the flexibility of adaptive routing introduces new possibilities of deadlock previous deadlock avoidance schemes in ary cubes require an exponential number of virtual channels independent of network size and dimension planar adaptive routing algorithms reduce the complexity of deadlock prevention by reducing the number of choices at each routing step in the fault free case planar adaptive networks are guaranteed to be deadlock free in the presence of network faults the planar adaptive router can be extended with misrouting to produce working network which remains provably deadlock free and is provably livelock free in addition planar adaptive networks can simultaneously support both in order and adaptive out of order packet delivery planar adaptive routing is of practical significance it provides the simplest known support for deadlock free adaptive routing in ary cubes of more than two dimensions with restricting adaptivity reduces the hardware complexity improving router speed or allowing additional performance enhancing network features the structure of planar adaptive routers is amenable to efficient implementation
in the distributed test architecture system with multiple ports is tested using tester at each port interface these testers cannot communicate with one another and there is no global clock recent work has defined an implementation relation for testing against an input output transition system in the distributed test architecture however this framework placed no restrictions on the test cases and in particular allowed them to produce some kind of nondeterminism in addition it did not consider the test generation problem this paper explores the class of controllable test cases for the distributed test architecture defining new implementation relation and test generation algorithm
emulators have long been valuable tool in teaching particularly in the os course emulators have allowed students to experiment meaningfully with different machine architectures furthermore many such tools run in user mode allowing students to operate as system administrators without the concomitant security risks virtual distributed ethernet vde is system which emulates in user mode all aspects of an internet including switches routers communication lines etc in completely realistic manner consistent with the operation of such artifacts in the real world vde's can be implemented on single computer spread over several machines on the same lan or scattered across the internet vde can interoperate with both real systems via standard virtual interface connectivity tools and several virtual machine environments support encryption and actually run fast enough to support real applications furthermore vde can interface interoperate with real networks vdn's have proven highly effective in supporting both undergraduate and graduate networking courses and wide range of student experiments and projects
ownership is relationship that pervades many aspects of our lives from the personal to the economic and is particularly important in the realm of the emerging electronic economy as it is understood on an intuitive level ownership exhibits great deal of complexity and carries rich semantics with respect both to the owner and the possession formal model of an ownership relationship that inherently captures varied ownership semantics is presented this ownership relationship expands the repertoire of available conceptual data modeling primitives it is built up from set of characteristic dimensions namely exclusiveness dependency documentation transferability and inheritance each of which focuses on specific aspect of ownership semantics the data modeler has the ability to make variety of choices along these five dimensions and thus has access to wide range of available ownership features in declarative fashion these choices ultimately impose various constraints specified in ocl on the states of data objects and their respective ownership activities including transactions such as acquiring and relinquishing ownership to complement the formal aspects of the ownership model and enhance its usability we present graphical ownership notation that augments the unified modeling language uml class diagram formalism an implementation of the ownership relationship in commercial object oriented database system is discussed
in this paper we present hybrid placer called ntuplace which integrates both the partitioning and the analytical quadratic programming placement techniques for large scale mixed size designs unlike most existing placers that minimize wirelength alone we also control the cell density to optimize routability while minimizing the total wirelength ntuplace consists of three major stages multilevel global placement legalization and detailed placement to handle mixed size designs in particular we present linear programming based legalization algorithm to remove overlaps between macros during global placement various other techniques are integrated to improve the solution quality at every stage
multimedia applications are fast becoming one of the dominating workloads for modern computer systems since these applications normally have large data sets and little data reuse many researchers believe that they have poor memory behavior compared to traditional programs and that current cache architectures cannot handle them well it is therefore important to quantitatively characterize the memory behavior of these applications in order to provide insights for future design and research of memory systems however very few results on this topic have been published this paper presents comprehensive research on the memory requirements of group of programs that are representative of multimedia applications these programs include subset of the popular mediabench suite and several large multimedia programs running on the linux windows nt and tru unix operating systems we performed extensive measurement and trace driven simulation experiments we then compared the memory utilization of these programs to that of specint applications we found that multimedia applications actually have better memory behavior than specint programs the high cache hit rates of multimedia applications can be contributed to the following three factors most multimedia applications apply block partitioning algorithms to the input data and work on small blocks of data that easily fit into the cache secondly within these blocks there is significant data reuse as well as spatial locality the third reason is that large number of references generated by multimedia applications are to their internal data structures which are relatively small and can also easily fit into reasonably sized caches
this paper studies how to enable an effective ranked retrieval over data with categorical attributes in particular by supporting personalized ranked retrieval of highly relevant data while ranked retrieval has been actively studied lately existing efforts have focused only on supporting ranking over numerical or text data however many real life data contain large amount of categorical attributes in combination with numerical and text attributes which cannot be efficiently supported unlike numerical attributes where natural ordering is inherent the existence of categorical attributes with no such ordering complicates both the formulation and processing of ranking this paper studies the efficient and effective support of ranking over categorical data as well as uniform support with other types of attributes
global routing for modern large scale circuit designs has attracted much attention in the recent literature most of the state of the art academic global routers just work on simplified routing congestion model that ignores the essential via capacity for routing through multiple metal layers such simplified model would easily cause fatal routability problems in subsequent detailed routing to remedy this deficiency we present in this paper more effective congestion metric that considers both the in tile nets and the residual via capacity for global routing with this congestion metric we develop new global router that features two novel routing algorithms for congestion optimization namely least flexibility first routing and multi source multi sink escaping point routing the least flexibility first routing processes the nets with the least flexibility first facilitating quick prediction of congestion hot spots for the subsequent nets enjoying lower time complexity than traditional maze and search routing in particular the linear time escaping point routing guarantees to find the optimal solution and achieves the theoretical lower bound time complexity experimental results show that our global router can achieve very high quality routing solutions with more reasonable via usage which can benefit and correctly guide subsequent detailed routing
in large scale multimedia storage system lmss where the user requests for different multimedia objects may have different demands placement and replication of the objects is an important factor as it may result in an imbalance in loading across the system since replica management and load balancing is crucial issue in multimedia systems normally this problem is handled by centralized servers eg metadata servers mds in distributed file systems each object based storage device osd responds to the requests coming from the centralized servers independently and has no communication with other osds among the system in this paper we design novel distributed architecture of lmss in which the osds have some kind of intelligences and can cooperate to achieve high performance such an osd named as autonomous object based storage device aosd can replicate the objects to and balance the requests among other aosds and handle fail over and recovery autonomously in the proposed architecture we move the request balancing from centralized mds to aosds and make the system more scalable flexible and robust based on the proposed architecture we propose two different object replication and load balancing algorithms named as minimum average waiting time mawt and one of the best two choices obtc respectively we validate the performance of the algorithms via rigorous simulations with respect to several influencing factors our findings conclusively demonstrate that the proposed architecture minimizes the average waiting time and at the same time carries out load balancing across servers
we present an internal language with equivalent expressive power to standard ml and discuss its formalization in lf and the machine checked verification of its type safety in twelf the internal language is intended to serve as the target of elaboration in an elaborative semantics for standard ml in the style of harper and stone therefore it includes all the programming mechanisms necessary to implement standard ml including translucent modules abstraction polymorphism higher kinds references exceptions recursive types and recursive functions our successful formalization of the proof involved careful interplay between the precise formulations of the various mechanisms and required the invention of new representation and proof techniques of general interest
desktop client applications interact with both local and remote resources this is both benefit in terms of the rich features desktop clients can provide but also security risk due to their high connectivity desktop clients can leave user's machine vulnerable to viruses malicious plug ins and scripts aspect oriented software development can be used to address security concerns in software in modular fashion however most existing research focuses on the protection of server side resources in this paper we introduce an aspect oriented mechanism authority aspects to enforce the principle of least privilege on desktop clients this helps to ensure that legitimate resource access is allowed and illegitimate access is blocked we present case study applying our approach on two desktop applications an rss feed aggregator and web browser
in this paper we present multi stream approach for off line handwritten word recognition the proposed approach combines low level feature streams namely density based features extracted from different sliding windows with different widths and contour based features extracted from upper and lower contours the multi stream paradigm provides an interesting framework for the integration of multiple sources of information and is compared to the standard combination strategies namely fusion of representations and fusion of decisions we investigate the extension of stream approach to streams and analyze the improvement in the recognition performance the computational cost of this extension is discussed significant experiments have been carried out on two publicly available word databases ifn enit benchmark database arabic script and ironoff database latin script the multi stream framework improves the recognition performance in both cases using stream approach the best recognition performance is in the case of the arabic script on word lexicon consisting of tunisian town village names in the case of the latin script the proposed approach achieves recognition rate of using lexicon of words
this paper presents new density based clustering algorithm st dbscan which is based on dbscan we propose three marginal extensions to dbscan related with the identification of core objects ii noise objects and iii adjacent clusters in contrast to the existing density based clustering algorithms our algorithm has the ability of discovering clusters according to non spatial spatial and temporal values of the objects in this paper we also present spatial temporal data warehouse system designed for storing and clustering wide range of spatial temporal data we show an implementation of our algorithm by using this data warehouse and present the data mining results
the semantic web augments the current www by giving information well defined meaning better enabling computers and people to work in cooperation this is done by adding machine understandable content to web resources such added content is called metadata whose semantics is provided by referring to an ontology domain's conceptualization agreed upon by community the semantic web relies on the complex interaction of several technologies involving ontologies therefore sophisticated semantic web applications typically comprise more than one software module instead of coming up with proprietary solutions developers should be able to rely on generic infrastructure for application development in this context we call such an infrastructure application server for the semantic web whose design and development are based on existing application servers however we apply and augment their underlying concepts for use in the semantic web and integrate semantic technology within the server itself the article discusses requirements and design issues of such server presents our implementation kaon server and demonstrates its usefulness by detailed scenario
this short paper argues that multi relational data mining has key role to play in the growth of kdd and briefly surveys some of the main drivers research problems and opportunities in this emerging field
for almost decade we have been working at developing and using template based models for coarse grained parallel computing our initial system frameworks was positively received but had number of shortcomings the enterprise parallel programming environment evolved out of this work and now after several years of experience with the system its shortcomings are becoming evident this paper outlines our experiences in developing and using the two parallel programming systems many of our observations are relevant to other parallel programming systems even though they may be based on different assumptions although template base models have the potential for simplifying the complexities of parallel programming they have yet to realize these expectations for high performance applications
jockey is an execution record replay tool for debugging linux programs it records invocations of system calls and cpu instructions with timing dependent effects and later replays them deterministically it supports process checkpointing to diagnose long running programs efficiently jockey is implemented as shared object file that runs as part of the target process while this design is the key for achieving jockey's goal of safety and ease of use it also poses challenges this paper discusses some of the practical issues we needed to overcome in such environments including low overhead system call interception techniques for segregating resource usage between jockey and the target process and an interface for fine grain control of jockey's behavior
this paper presents the results of an empirical study on the subjective evaluation of code smells that identify poorly evolvable structures in software we propose use of the term software evolvability to describe the ease of further developing piece of software and outline the research area based on four different viewpoints furthermore we describe the differences between human evaluations and automatic program analysis based on software evolvability metrics the empirical component is based on case study in finnish software product company in which we studied two topics first we looked at the effect of the evaluator when subjectively evaluating the existence of smells in code modules we found that the use of smells for code evaluation purposes can be difficult due to conflicting perceptions of different evaluators however the demographics of the evaluators partly explain the variation second we applied selected source code metrics for identifying four smells and compared these results to the subjective evaluations the metrics based on automatic program analysis and the human based smell evaluations did not fully correlate based upon our results we suggest that organizations should make decisions regarding software evolvability improvement based on combination of subjective evaluations and code metrics due to the limitations of the study we also recognize the need for conducting more refined studies and experiments in the area of software evolvability
we define and study bisimulation for proving contextual equivalence in an aspect extension of the untyped lambda calculus to our knowledge this is the first study of coinductive reasoning principles aimed at proving equality of aspect programs the language we study is very small yet powerful enough to encode mutable references and range of temporal pointcuts including cflow and regular event patterns examples suggest that our bisimulation principle is useful for an encoding of higher order programs with state our methods suffice to establish well known and well studied subtle examples involving higher order functions with stateeven in the presence of first class dynamic advice and expressive pointcuts our reasoning principles show that aspect aware interfaces can aid in ensuring that clients of component are unaffected by changes to an implementation our paper generalizes existing results given for open modules to also include variety of history sensitive pointcuts such as cflow and regular event patternsour formal techniques and results suggest that aspects are amenable to the formal techniques developed for stateful higher order programs
we explore the ways in which interfaces can be designed to deceive users so as to create the illusion of magic we present study of an experimental performance in which magician used computer vision system to conduct series of illusions based on the well known three cups magic trick we explain our findings in terms of the two broad strategies of misdirecting attention and setting false expectations articulating specific tactics that were employed in each case we draw on existing theories of collaborative and spectator interfaces ambiguity and interpretation and trajectories through experiences to explain our findings in broader hci terms we also extend and integrate current theory to provide refined sensitising concepts for analysing deceptive interactions
animators today have started using motion captured mocap sequences to drive characters mocap allows rapid acquisition of highly realistic animation data consequently animators have at their disposal an enormous amount of mocap sequences which ironically has created new retrieval problem thus while working with mocap databases an animator often needs to work with subset of useful clips once the animator selects candidate working set of motion clips she then needs to identify appropriate transition points amongst these clips for maximal reusein this paper we describe methods for querying mocap databases and identifying transitions for given set of clips we preprocess clips and clip subsequences and precompute frame locations to allow interactive stitching in contrast with existing methods that view each individual clips as nodes for optimal reuse we reduce the granularity
there has been significant increase in the number of cctv cameras in public and private places worldwide the cost of monitoring these cameras manually and of reviewing recorded video is prohibitive and therefore manual systems tend to be used mainly reactively with only small fraction of the cameras being monitored at any given time there is need to automate at least simple observation tasks through computer vision functionality that has become known popularly as video analytics the large size of cctv systems and the requirement of high detection rates and low false alarms are major challenges this paper illustrates some of the recent efforts reported in the literature highlighting advances and pointing out important limitations
articulated hand tracking systems have been widely used in virtual reality but are rarely deployed in consumer applications due to their price and complexity in this paper we propose an easy to use and inexpensive system that facilitates articulated user input using the hands our approach uses single camera to track hand wearing an ordinary cloth glove that is imprinted with custom pattern the pattern is designed to simplify the pose estimation problem allowing us to employ nearest neighbor approach to track hands at interactive rates we describe several proof of concept applications enabled by our system that we hope will provide foundation for new interactions in modeling animation control and augmented reality
pervasive devices are becoming popular and smaller those mobile systems should be able to adapt to changing requirements and execution environments but it requires the ability to reconfigure deployed codes which is considerably simplified if applications are component oriented rather than monolithic blocks of codes so we propose middleware called wcomp which federates an event driven component oriented approach to compose distributed services for devices this approach is coupled with adaptation mechanisms dealing with separation of concerns in such mechanisms aspects called aspects of assembly are selected either by the user or by self adaptive process and composed by weaver with logical merging of high level specifications the result of the weaver is then projected in terms of pure elementary modifications of components assemblies with respect to blackbox properties of cots components our approach is validated by analyzing the results of different experiments drawn from sets of application configurations randomly generated and by showing its advantages while evaluating the additional costs on the reaction time to context changing
we introduce method for segmentation of materials segmented in volumetric models of mechanical parts created by ray ct scanning for the purpose of generating their boundary surfaces when the volumetric model is composed of two materials one for the object and the other for the background air these boundary surfaces can be extracted as isosurfaces using surface contouring method for volumetric model composed of more than two materials we need to classify the voxel types into segments by material and then use surface contouring method that can deal with both ct values and material types here we propose method for precisely classifying the volumetric model into its component materials using modified and combined method of two well known algorithms in image segmentation region growing and graph cut we then apply our non manifold iso contouring method to generate triangulated mesh surfaces in addition we demonstrate the effectiveness of our method by constructing high quality triangular mesh models of the segmented parts
users often want to find entities instead of just documents ie finding documents entirely about specific real world entities rather than general documents where the entities are merely mentioned searching for entities on web scale repositories is still an open challenge as the effectiveness of ranking is usually not satisfactory semantics can be used in this context to improve the results leveraging on entity driven ontologies in this paper we propose three categories of algorithms for query adaptation using semantic information nlp techniques and link structure to rank entities in wikipedia our approaches focus on constructing queries using not only keywords but also additional syntactic information while semantically relaxing the query relying on highly accurate ontology the results show that our approaches perform effectively and that the combination of simple nlp link analysis and semantic techniques improves the retrieval performance of entity search
this paper presents tangible interaction techniques for fine tuning one to one scale nurbs curves on large display for automotive design we developed new graspable handle with transparent groove that allows designers to manipulate virtual curves on display screen directly the use of the proposed handle leads naturally to rich vocabulary of terms describing interaction techniques that reflect existing shape styling methods user test raised various issues related to the graspable user interface two handed input and large display interaction
as fundamental problem in distributed hash table dht based systems load balancing is important to avoid performance degradation and guarantee system fairness among existing migration based load balancing strategies there are two main categories rendezvous directory strategy rds and independent searching strategy iss however none of them can achieve resilience and efficiency at the same time in this paper we propose group multicast strategy gms for load balancing in dht systems which attempts to achieve the benefits of both rds and iss gms does not rely on few static rendezvous directories to perform load balancing instead load information is disseminated within the formed groups via multicast protocol thus each peer has enough information to act as the rendezvous directory and perform load balancing within its group besides intra group load balancing inter group load balancing and emergent load balancing are also supported by gms in gms the position of the rendezvous directory is randomized in each round which further improves system resilience in order to have better understanding of gms we also perform analytical studies on gms in terms of its scalability and efficiency under churn finally the effectiveness of gms is evaluated by extensive simulation under different workload and churn levels
the family of trees is suitable for indexing various kinds of multidimensional objects tpr trees are tree based structures that have been proposed for indexing moving object database eg data base of moving boats region quadtrees are suitable for indexing dimensional regional data and their linear variant linear region quadtrees is used in many geographical information systems gis for this purpose eg for the representation of stormy or sunny regions although both are tree structures the organization of data space the types of spatial data stored and the search algorithms applied on them are different in trees and region quadtrees in this paper we examine spatio temporal problem that appears in many practical applications processing of predictive joins between moving objects and regions eg discovering the boats that will enter storm using these two families of data structures as storage and indexing mechanisms and taking into account their similarities and differences with thorough experimental study we show that the use of synchronous depth first traversal order has the best performance balance on average taking into account the activity and response time as performance measurements
the development of grid and workflow technologies has enabled complex loosely coupled scientific applications to be executed on distributed resources many of these applications consist of large numbers of short duration tasks whose runtimes are heavily influenced by delays in the execution environment such applications often perform poorly on the grid because of the large scheduling overheads commonly found in grids in this paper we present provisioning system based on multi level scheduling that improves workflow runtime by reducing scheduling overheads the system reserves resources for the exclusive use of the application and gives applications control over scheduling policies we describe our experiences with the system when running suite of real workflow based applications including in astronomy earthquake science and genomics provisioning resources with corral ahead of the workflow execution reduced the runtime of the astronomy application by up to on average and of genome mapping application by an order of magnitude when compared to traditional methods we also show how provisioning can benefit applications both on small local cluster as well as large scale campus resource
quality aspects become increasingly important when business process modeling is used in large scale enterprise setting in order to facilitate storage without redundancy and an efficient retrieval of relevant process models in model databases it is required to develop theoretical understanding of how degree of behavioral similarity can be defined in this paper we address this challenge in novel way we use causal footprints as an abstract representation of the behavior captured by process model since they allow us to compare models defined in both formal modeling languages like petri nets and informal ones like epcs based on the causal footprint derived from two models we calculate their similarity based on the established vector space model from information retrieval we validate this concept with an experiment using the sap reference model and an implementation in the prom framework
in this article we propose deniable electronic voting authentication protocol for mobile ad hoc networks which meets the essential requirements of secure voting system due to the characteristics constraints and security requirements of mobile ad hoc networks our protocol does not require the aid of any centralized administration and mobile nodes could cooperate with each other to securely facilitate voting finally the proposed protocol provides the ability to deniable authentication when legal voter casts vote with this voting system he she can deny that he she has voted for third party and the third party cannot judge who votes
this paper presents two new hardware assisted rendering techniques developed for interactive visualization of the terascale data generated from numerical modeling of next generation accelerator designs the first technique based on hybrid rendering approach makes possible interactive exploration of large scale particle data from particle beam dynamics modeling the second technique based on compact texture enhanced representation exploits the advanced features of commodity graphics cards to achieve perceptually effective visualization of the very dense and complex electromagnetic fields produced from the modeling of reflection and transmission properties of open structures in an accelerator design because of the collaborative nature of the overall accelerator modeling project the visualization technology developed is for both desktop and remote visualization settings we have tested the techniques using both time varying particle data sets containing up to one billion particles per time step and electromagnetic field data sets with millions of mesh elements
dynamic instruction scheduling logic is quite complex and dissipates significant energy in microprocessors that support superscalar and out of order execution we propose novel microarchitectural technique to reduce the complexity and energy consumption of the dynamic instruction scheduling logic the proposed method groups several instructions as single issue unit and reduces the required number of ports and the size of the structure for dispatch wakeup select and issue the present paper describes the microarchitecture mechanisms and shows evaluation results for energy savings and performance these results reveal that the proposed technique can greatly reduce energy with almost no performance degradation compared to the conventional dynamic instruction scheduling logic
an important feature of information systems is the ability to inform users about changes of the stored information therefore systems have to know what changes user wants to be informed about this is well known from the field of publish subscribe architectures in this paper we propose solution for information system designers of how to extend their information model in way that the notification mechanism can consider semantic knowledge when determining which parties to inform two different kinds of implementations are introduced and evaluated one based on aspect oriented programming aop the other one based on traditional database triggers the evaluation of both approaches leads to combined approach preserving the advantages of both techniques using model driven architecture mda to create the triggers from uml model enhanced with stereotypes
the problem of object recognition has been considered here color descriptions from distinct regions covering multiple segments are considered for object representation distinct multicolored regions are detected using edge maps and clustering performance of the proposed methodologies has been evaluated on three data sets and the results are found to be better than existing methods when small number of training views is considered
dynamic class loading during program execution in the java programming language is an impediment for generating code that is as efficient as code generated using static whole program analysis and optimization whole program analysis and optimization is possible for languages such as that do not allow new classes and or methods to be loaded during program execution one solution for performing whole program analysis and avoiding incorrect execution after new class is loaded is to invalidate and recompile affected methods runtime invalidation and recompilation mechanisms can be expensive in both space and time and therefore generally restrict optimization to address these drawbacks we propose new framework called the extant analysis framework for interprocedural optimization of programs that support dynamic class or method loading given set of classes comprising the closed world we perform an offline static analysis which partitions references into two categories unconditionally extant references which point only to objects whose runtime type is guaranteed to be in the closed world and conditionally extant references which point to objects whose runtime type is not guaranteed to be in the closed world optimizations solely dependent on the first categorycan be statically performed and are guaranteed to be correct even with any future class method loading optimizations dependent on the second category are guarded by dynamic tests called extant safety tests for correct execution behaviorwe describe the properties for extant safety tests and provide algorithms for their generation and placement
this paper proposes learning approach for the merging process in multilingual information retrieval mlir to conduct the learning approach we also present large number of features that may influence the mlir merging process these features are mainly extracted from three levels query document and translation after the feature extraction we then use the frank ranking algorithm to construct merge model to our knowledge this practice is the first attempt to use learning based ranking algorithm to construct merge model for mlir merging in our experiments three test collections for the task of crosslingual information retrieval clir in ntcir and are employed to assess the performance of our proposed method moreover several merging methods are also carried out for comparison including traditional merging methods the step merging strategy and the merging method based on logistic regression the experimental results show that our method can significantly improve merging quality on two different types of datasets in addition to the effectiveness through the merge model generated by frank our method can further identify key factors that influence the merging process this information might provide us more insight and understanding into mlir merging
in today's multitiered application architectures clients do not access data stored in the databases directly instead they use applications which in turn invoke the dbms to generate the relevant content since executing application programs may require significant time and other resources it is more advantageous to cache application results in result cache various view materialization and update management techniques have been proposed to deal with updates to the underlying data these techniques guarantee that the cached results are always consistent with the underlying data several applications including commerce sites on the other hand do not require the caches be consistent all the time instead they require that all outdated pages in the caches are invalidated in timely fashion in this paper we show that invalidation is inherently different from view maintenance we develop algorithms that benefit from this difference in reducing the cost of update management in certain applications and we present an invalidation framework that benefits from these algorithms
in this paper we propose novel framework for the web based retrieval of chinese calligraphic manuscript images which includes two main components shape similarity ss based method which is to effectively support retrieval over large chinese calligraphic manuscript databases in this retrieval method shapes of calligraphic characters are represented by their approximate contour points extracted from the character images to speed up the retrieval efficiency we then propose composite distance tree cd tree based high dimensional indexing scheme for it comprehensive experiments are conducted to testify the effectiveness and efficiency of our proposed retrieval and indexing methods respectively
we propose in this paper an algorithm for off line selection of the contents of on chip memories the algorithm supports two types of on chip memories namely locked caches and scratchpad memories the contents of on chip memory although selected off line is changed at run time for the sake of scalability with respect to task size experimental results show that the algorithm yields to good ratios of on chip memory accesses on the worst case execution path with tolerable reload overhead for both types of on chip memories furthermore we highlight the circumstances under which one type of on chip memory is more appropriate than the other depending of architectural parameters cache block size and application characteristics basic block size
most existing dynamic voltage scaling dvs schemes for multiple tasks assume an energy cost function energy consumption versus execution time that is independent of the task characteristics in practice the actual energy cost functions vary significantly from task to task different tasks running on the same hardware platform can exhibit different memory and peripheral access patterns cache miss rates etc these effects results in distinct energy cost function for each taskwe present new formulation and solution to the problem of minimizing the total dynamic and static system energy while executing set of tasks under dvs first we demonstrate and quantify the dependence of the energy cost function on task characteristics by direct measurements on real hardware platform the ti omap processor using real application programs next we present simple analytical solutions to the problem of determining energy optimal voltage scale factors for each task while allowing each task to be preempted and to have its own energy cost function based on these solutions we present simple and efficient algorithms for implementing dvs with multiple tasks we consider two cases all tasks have single deadline and each task has its own deadline experiments on real hardware platform using real applications demonstrate additional saving in total system energy compared to previous leakage aware dvs schemes
the order and arrangement of dimensions variates is crucial for the effectiveness of large number of visualization techniques such as parallel coordinates scatterplots recursive pattern and many others in this paper we describe systematic approach to arrange the dimensions according to their similarity the basic idea is to rearrange the data dimensions such that dimensions showing similar behavior are positioned next to each other for the similarity clustering of dimensions we need to define similarity measures which determine the partial or global similarity of dimensions we then consider the problem of finding an optimal one or two dimensional arrangement of the dimensions based on their similarity theoretical considerations show that both the one and the two dimensional arrangement problem are surprisingly hard problems ie they are np complete our solution of the problem is therefore based on heuristic algorithms an empirical evaluation using number of different visualization techniques shows the high impact of our similarity clustering of dimensions on the visualization results
nowadays enormous amounts of data are continuously generated not only in massive scale but also from different sometimes conflicting views therefore it is important to consolidate different concepts for intelligent decision making for example to predict the research areas of some people the best results are usually achieved by combining and consolidating predictions obtained from the publication network co authorship network and the textual content of their publications multiple supervised and unsupervised hypotheses can be drawn from these information sources and negotiating their differences and consolidating decisions usually yields much more accurate model due to the diversity and heterogeneity of these models in this paper we address the problem of consensus learning among competing hypotheses which either rely on outside knowledge supervised learning or internal structure unsupervised clustering we argue that consensus learning is an np hard problem and thus propose to solve it by an efficient heuristic method we construct belief graph to first propagate predictions from supervised models to the unsupervised and then negotiate and reach consensus among them their final decision is further consolidated by calculating each model's weight based on its degree of consistency with other models experiments are conducted on newsgroups data cora research papers dblp author conference network and yahoo movies datasets and the results show that the proposed method improves the classification accuracy and the clustering quality measure nmi over the best base model by up to furthermore it runs in time proportional to the number of instances which is very efficient for large scale data sets
business process modeling is expensive and time consuming it largely depends on the elicitation method and the person in charge the model needs to be shared in order to promote multiple perspectives this paper describes group storytelling approach as an alternative to the traditional individual interviews to elicitate processes information gathering is proposed to be done through capturing the stories told by process performers who describe their work difficulties and suggestions process to abstract and transform stories into business process representations is also part of the method tool to support storytelling and this transformation is described as well
in this paper new algorithm is proposed to improve the efficiency and robustness of random sampling consensus ransac without prior information about the error scale three techniques are developed in an iterative hypothesis and evaluation framework firstly we propose consensus sampling technique to increase the probability of sampling inliers by exploiting the feedback information obtained from the evaluation procedure secondly the preemptive multiple th order approximation pmka is developed for efficient model evaluation with unknown error scale furthermore we propose coarse to fine strategy for the robust standard deviation estimation to determine the unknown error scale experimental results of the fundamental matrix computation on both simulated and real data are shown to demonstrate the superiority of the proposed algorithm over the previous methods
cooperative checkpointing increases the performance and robustness of system by allowing checkpoints requested by applications to be dynamically skipped at runtime robust system must be more than merely resilient to failures it must be adaptable and flexible in the face of new and evolving challenges simulation based experimental analysis using both probabilistic and harvested failure distributions reveals that cooperative checkpointing enables an application to make progress under wide variety of failure distributions that periodic checkpointing lacks the flexibility to handle cooperative checkpointing can be easily implemented on top of existing application initiated checkpointing mechanisms and may be used to enhance other reliability techniques like qos guarantees and fault aware job scheduling the simulations also support number of theoretical predictions related to cooperative checkpointing including the non competitiveness of periodic checkpointing
mallet multi agent logic language for encoding teamwork is intended to enable expression of teamwork emulating human teamwork allowing experimentation with different levels and forms of inferred team intelligence consequence of this goal is that the actual teamwork behavior is determined by the level of intelligence built into the underlying system as well as the semantics of the language in this paper we give the design objectives the syntax and an operational semantics for mallet in terms of transition system we show how the semantics can be used to reason about the behaviors of team based agents the semantics can also be used to guide the implementation of various mallet interpreters emulating different forms of team intelligence as well as formally study the properties of team based agents specified in mallet we have explored various forms of proactive information exchange behavior embodied in human teamwork using the cast system which implements built in mallet interpreter
superimposition is composition technique that has been applied successfully in several areas of software development in order to unify several languages and tools that rely on superimposition we present an underlying language independent model that is based on feature structure trees fsts furthermore we offer tool called fstcomposer that composes software components represented by fsts currently the tool supports the composition of components written in java jak xml and plain text three nontrivial case studies demonstrate the practicality of our approach
beaconless georouting algorithms are fully reactive and work without prior knowledge of their neighbors however existing approaches can either not guarantee delivery or they require the exchange of complete neighborhood information we describe two general methods for completely reactive face routing with guaranteed delivery the beaconless forwarder planarization bfp scheme determines correct edges of local planar subgraph without hearing from all neighbors face routing then continues properly angular relaying determines directly the next hop of face traversal both schemes are based on the select and protest principle neighbors respond according to delay function but only if they do not violate planar subgraph condition protest messages are used to remove falsely selected neighbors that are not in the planar subgraph we show that correct beaconless planar subgraph construction is not possible without protests we also show the impact of the chosen planar subgraph on the message complexity with the new circlunar neighborhood graph cng we can bound the worst case message complexity of bfp which is not possible when using the gabriel graph gg for planarization simulation results show similar message complexities in the average case when using cng and gg angular relaying uses delay function that is based on the angular distance to the previous hop we develop theoretical framework for delay functions and show both theoretically and in simulations that with function of angle and distance we can reduce the number of protests by factor of compared to simple angle based delay function
broadcasting is commonly used communication primitive needed by many applications and protocols in mobile ad hoc networks manet unfortunately most broadcast solutions are tailored to one class of manets with respect to node density and node mobility and are unlikely to operate well in other classes in this paper we introduce hypergossiping novel adaptive broadcast algorithm that combines two strategies hypergossiping uses adaptive gossiping to efficiently distribute messages within single network partitions and implements an efficient heuristic to distribute them across partitions simulation results in ns show that hypergossiping operates well for broad range of manets with respect to node densities mobility levels and network loads
in this paper we solve the classic problem of computing the average number of decomposed quadtree blocks quadrants nodes or pieces in quadtree decomposition for an arbitrary hyperrectangle aligned with the axes we derive closed form formula for general cases the previously known state of the art solution provided closed form solution for special cases and utilized these formulas to derive linearly interpolated formulas for general cases individually however there is no exact and uniform closed form formula that fits all cases contrary to the top down counting approach used by most prior solutions we employ bottom up enumeration approach to transform the problem into one that involves the cartesian product of multisets of successive powers classic combinatorial enumeration techniques are applied to obtain an exact and uniform closed form formula the result is of theoretical interest since it is the first exact closed form formula for arbitrary cases practically it is nice to have uniform formula for estimating the average number since simple program can be conveniently constructed taking side lengths as inputs
the problem of performing tasks on asynchronous or undependable processors is basic problem in parallel and distributed computing we consider an abstraction of this problem called the write all problem mdash using processors write into all locations of an array of size the most efficient known deterministic asynchronous algorithms for this problem are due to anderson and woll the first class of algorithms has work complexity of ogr egr for nle ty and any egr and they are the best known for the full range of processors to schedule the work of the processors the algorithms use sets of permutations on nle that have certain combinatorial properties instantiating such an algorithm for specific egr either requires substantial pre processing exponential in egr to find the requisite permutations or imposes prohibitive constant exponential in egr hidden by the asymptotic analysis the second class deals with the specific case of nu nle and these algorithms have work complexity of ogr log they also use sets of permutations with the same combinatorial properties however instantiating these algorithms requires exponential in preprocessing to find the permutations to alleviate this costly instantiation kanellakis and shvartsman proposed simple way of computing the permutation schedules they conjectured that their construction has the desired properties but they provided no analysis in this paper we show for the first time an analysis of the properties of the set of permutations proposed by kanellakis and shvartsman our result is hybrid as it includes analytical and empirical parts the analytical result covers subset of the possible adversarial patterns of asynchrony the empirical results provide strong evidence that our analysis covers the worst case scenario and we formally state it as conjecture we use these results to analyze an algorithm for nu gne tasks that takes advantage of processor slackness and that has work ogr log conditioned on our conjecture this algorithm requires only ogr log time to instantiate it next we study the case for the full range of processors nle we define family of deterministic asynchronous write all algorithms with work ogr egr contingent upon our conjecture we show that our method yields faster construction of ogr egr write all algorithms than the method developed by anderson and woll finally we show that our approach yields more efficient write all algorithms as compared to the algorithms induced by the constructions of naor and roth for the same asymptotic work complexity
microprocessor vendors have provided special purpose instructions such as psadbw and pdist to accelerate the sum of absolute differences sad similarity measurement the usefulness of these special purpose instructions is limited except for the motion estimation kernel this has several drawbacks first if the sad becomes obsolete because different similarity metric is going to be employed then those special purpose instructions are no longer useful second these special instructions process bit subwords only this precision is not su cient for some kernels such as motion estimation in the transform domain in addition when employing other way parallel simd instructions to implement the sad and sum of squared differences ssd the obtained speedup is much less than this is because there is mismatch between the storage and the computational format in this paper we design and evaluate variety of simd instructions for different data types we synthesize special purpose instructions using few general purpose simd instructions in addition we employ the extended subwords technique to avoid conversion overhead and to increase parallelism in this technique there are four extra bits for every byte of register the results show that using different simd instructions and extended subwords achieve speedup ranging from to over performance for sad ssd with interpolation and ssd functions in the motion estimation kernel while mmx achieves speedup ranging from to additionally the proposed simd instructions improve the performance of similarity measurement for image histograms by factor ranging from way to way over cwhile for mmx speedup is between way and way
component based software development cbsd is an attractive way to deliver generic executable pieces of program ready to be reused in many different contexts component reuse is based on black box model that frees component consumers from diving into implementation details adapting generic component to particular context of use is then based on parameterized interface that becomes specific component wrapper at runtime this shallow adaptation which keeps the component implementation unchanged is major source of inefficiency by building on top of well known specialization techniques it is possible to take advantage of the genericity of components and adapt their implementation to their usage context without breaking the black box model we illustrate these ideas on simple component model considering dual specialization techniques partial evaluation and slicing key to not breaking encapsulation is to use specialization scenarios extended with assumptions on the required services and to package components as component generators
web spam is behavior that attempts to deceive search engine ranking algorithms trustrank is recent algorithm that can combat web spam however trustrank is vulnerable in the sense that the seed set used by trustrank may not be sufficiently representative to cover well the different topics on the web also for given seed set trustrank has bias towards larger communities we propose the use of topical information to partition the seed set and calculate trust scores for each topic separately to address the above issues combination of these trust scores for page is used to determine its ranking experimental results on two large datasets show that our topical trustrank has better performance than trustrank in demoting spam sites or pages compared to trustrank our best technique can decrease spam from the top ranked sites by as much as
building hardware prototypes for computer architecture research is challenging unfortunately development of the required software tools compilers debuggers runtime is even more challenging which means these systems rarely run real applications to overcome this issue when developing our prototype platform we used the tensilica processor generator to produce customized processor and corresponding software tools and libraries while this base processor was very different from the streamlined custom processor we initially imagined it allowed us to focus on our main objective the design of reconfigurable cmp memory system and to successfully tape out an core cmp chip with only small group of designers one person was able to handle processor configuration and hardware generation support of complete software tool chain as well as developing the custom runtime software to support three different programming models having sophisticated software tool chain not only allowed us to run more applications on our machine it once again pointed out the need to use optimized code to get an accurate evaluation of architectural features
interpreters are widely used to implement portable language runtime environments programs written in these languages may benefit from performance beyond that obtainable by optimizing interpretation alone modern high performance mixed mode virtual machine vm includes amethod based just in time jit compiler method based jit however requires the up front development of complex compilation infrastructure before any performance benefits are realizedideally the architecture for mixed mode vm could detect and execute variety of shapes of hot regions of virtual program our vm architecture is based on context threading it supports powerful efficient instrumentation and simple framework for dynamic code generation it has the potential to directly support full spectrum of mixed mode execution from interpreted bytecode bodies to specialized bytecode generated at runtime to traces to compiled methods further it provides the necessary tools to detect these regions at runtimewe extended two vms sablevm and the ocaml interpreter with our infrastructure on both the and ppc to demonstrate the power and flexibility of our infrastructure we compare the selection and dispatch effectiveness for three common region shapes whole methods partial methods and specl traces we report results for preliminary version of our code generator which compiles region into sequence of direct calls to bytecode bodies
this tutorial introduces dynamic web services as solution to cope with the dynamism and flexibility required by many modern software systems current technologies wsdl ws bpel etc have proven insufficient in addressing these issues however they remain good starting point for the analysis of the current situation and for building for the futurethe core part of the tutorial analyzes by looking at available technologies and prominent research proposals the deployment and execution of these applications within three separate phases composition phase to discover available services and implement the desired behavior monitoring phase to understand if given service is behaving correctly with respect to both functional and non functional requirements and recovery phase to react to anomalies by means of suitable replanning or recovery strategiesin conclusion the tutorial summarizes the main topics presents list of still to be solved problems and highlights possible directions for future research
defeasible logic is promising representation for legal knowledge that appears to overcome many of the deficiencies of previous approaches to representing legal knowledge unfortunately an immediate application of technology to the challenges of generating theories in the legal domain is an expensive and computationally intractable problem so in light of the potential benefits we seek to find practical algorithm that uses heuristics to discover an approximate solution as an outcome of this work we have developed an algorithm that integrates defeasible logic into decision support system by automatically deriving its knowledge from databases of precedents experiments with the new algorithm are very promising delivering results comparable to and exceeding other approaches
mobile ad hoc networks are subject to some unique security issues that could delay their diffusion several solutions have already been proposed to enforce specific security properties however mobility pattern nodes obey to can on one hand severely affect the quality of the security solutions that have been tested over synthesized mobility pattern on the other hand specific mobility patterns could be leveraged to design specific protocols that could outperform existing solutionsin this work we investigate the influence of realistic mobility scenario over benchmark mobility model random waypoint mobility model using as underlying protocol recent solution introduced for the detection of compromised nodes extensive simulations show the quality of the underlying protocol however the main contribution is to show the relevance of the mobility model over the achieved performances stressing out that in mobile ad hoc networks the quality of the solution provided is satisfactory only when it can be adapted to the nodes underlying mobility model
most existing transfer learning techniques are limited to problems of knowledge transfer across tasks sharing the same set of class labels in this paper however we relax this constraint and propose spectral based solution that aims at unveiling the intrinsic structure of the data and generating partition of the target data by transferring the eigenspace that well separates the source data furthermore clustering based kl divergence is proposed to automatically adjust how much to transfer we evaluate the proposed model on text and image datasets where class categories of the source and target data are explicitly different eg classes transfer to classes and show that the proposed approach improves other baselines by an average of in accuracy the source code and datasets are available from the authors
in this paper we propose and experimentally evaluate the use of the client server database paradigm for real time processing to date the study of transaction processing with time constraints has mostly been restricted to centralized or single node systems recently client server databases have exploited locality of data accesses in real world applications to successfully provide reduced transaction response times our objective is to investigate the feasibility of real time processing in data shipping client server database architecture we compare the efficiency of the proposed architecture with that of centralized real time database system we discuss transaction scheduling issues in the two architectures and propose new policy for scheduling transactions in the client server environment this policy assigns higher priorities to transactions that have greater potential for successful completion through the use of locally available data through detailed performance scalability study we investigate the effects of client data access locality and various updating workloads on transaction completion rates our experimental results show that real time client server databases can provide significant performance gains over their centralized counterparts these gains become evident when large numbers of clients more than are attached per server even in the presence of high data contention
we introduce novel measure called four pointscondition pc which assigns value to every metric space quantifying how close the metric is to tree metric data sets taken from real internet measurements indicate remarkable closeness of internet latencies to tree metrics based on this condition we study embeddings of pc metric spaces into trees and prove tight upper and lower bounds specifically we show that there are constants and such that every metric which satisfies the pc can be embedded into tree with distortion clog and for every and any number of nodes there is metric space satisfying the pc that does not embed into tree with distortion less than clog in addition we prove lower bound on approximate distance labelings of pc metrics and give tight bounds for tree embeddings with additive error guarantees
the usage of interactive applications increases in handheld systems in this paper we describe system level dynamic power management scheme that considers interaction between the cpu and the wnic and interactive applications to reduce the energy consumption of handheld systems previous research efforts considered the cpu and the wnic separately to reduce energy consumption the proposed scheme reduces the energy consumption of handheld systems by using the information gathered from the wnic to control the cpu voltage and frequency when interactive applications are executed experimental results show that on average the proposed scheme reduces energy consumption by when compared to dvfs dynamic voltage and frequency scaling for the cpu and dpm dynamic power management for the wnic
program restructuring is key method for improving the quality of ill structured programs thereby increasing the understandability and reducing the maintenance cost it is challenging task and great deal of research is still ongoing this paper presents an approach to program restructuring inside of function based on clustering techniques with cohesion as the major concern clustering has been widely used to group related entities together the approach focuses on automated support for identifying ill structured or low cohesive functions and providing heuristic advice in both the development and evolution phases new similarity measure is defined and studied intensively specifically from the function perspective comparative study on three different hierarchical agglomerative clustering algorithms is also conducted the best algorithm is applied to restructuring of functions of real industrial system the empirical observations show that the heuristic advice provided by the approach can help software designers make better decision of why and how to restructure program specific source code level software metrics are presented to demonstrate the value of the approach
the total amount of stored information on disks has increased tremendously in recent years with storage devices getting cheaper and government agencies requiring strict data retention policies it is clear that this trend will continue for several years to come this progression creates challenge for system administrators who must determine several aspects of storage policy with respect to provisioning backups retention redundancy security performance etc these decisions are made for an entire file system logical volume or storage pool however this granularity is too large and can sacrifice storage efficiency and performance particularly since different files have different requirements in this paper we advocate that storage policy decisions be made on finer granularity we describe attest an extendable stackable storage architecture that allows storage policy decisions to be made at file granularity and at all levels of the storage stack through the use of attributes that enable plugin policy modules and application aware storage functionality we present an implementation of attest that shows minimal impact on overall performance
we present method to certify subset of the java bytecode with respect to security the method is based on abstract interpretation of the operational semantics of the language we define concrete small step enhanced semantics of the language able to keep information on the flow of data and control during execution main point of this semantics is the handling of the influence of the information flow on the operand stack we then define an abstract semantics keeping only the security information and forgetting the actual values this semantics can be used as static analysis tool to check security of programs the use of abstract interpretation allows on one side being semantics based to accept as secure wide class of programs and on the other side being rule based to be fully automated
advanced typing matching and evaluation strategy features as well as very general conditional rules are routinely used in equational programming languages such as for example asf sdf obj cafeobj maude and equational subsets of elan and casl proving termination of equational programs having such expressive features is important but nontrivial because some of those features may not be supported by standard termination methods and tools such as muterm cime aprove ttt termptation etc yet use of the features may be essential to ensure termination we present sequence of theory transformations that can be used to bridge the gap between expressive equational programs and termination tools prove the correctness of such transformations and discuss prototype tool performing the transformations on maude equational programs and sending the resulting transformed theories to some of the aforementioned tools
as greater volume of information becomes increasingly available across all disciplines many approaches such as document clustering and information visualization have been proposed to help users manage information easily however most of these methods do not directly extract key concepts and their semantic relationships from document corpora which could help better illuminate the conceptual structures within given information to address this issue we propose an approach called ldquo clonto rdquo to process document corpus identify the key concepts and automatically generate ontologies based on these concepts for the purpose of conceptualization for given document corpus clonto applies latent semantic analysis to identify key concepts allocates documents based on these concepts and utilizes wordnet to automatically generate corpus related ontology the documents are linked to the ontology through the key concepts based on two test collections the experimental results show that clonto is able to identify key concepts and outperforms four other clustering algorithms moreover the ontologies generated by clonto show significant informative conceptual structures copy wiley periodicals inc
multithreaded programs are prone to errors caused by unintended interference between concurrent threads this paper focuses on verifying that deterministically parallel code is free of such thread interference errors deterministically parallel code may create and use new threads via fork and join and coordinate their behavior with synchronization primitives such as barriers and semaphores such code does not satisfy the traditional non interference property of atomicity or serializability however and so existing atomicity tools are inadequate for checking deterministically parallel code we introduce new non interference specification for deterministically parallel code and we present dynamic analysis to enforce it we also describe singletrack prototype implementation of this analysis singletrack's performance is competitive with prior atomicity checkers but it produces many fewer spurious warnings because it enforces more general non interference property that is applicable to more software
with suitable algorithm for ranking the expertise of user in collaborative tagging system we will be able to identify experts and discover useful and relevant resources through them we propose that the level of expertise of user with respect to particular topic is mainly determined by two factors firstly an expert should possess high quality collection of resources while the quality of web resource depends on the expertise of the users who have assigned tags to it secondly an expert should be one who tends to identify interesting or useful resources before other users do we propose graph based algorithm spear spamming resistant expertise analysis and ranking which implements these ideas for ranking users in folksonomy we evaluate our method with experiments on data sets collected from deliciouscom comprising over web documents million users and million shared bookmarks we also show that the algorithm is more resistant to spammers than other methods such as the original hits algorithm and simple statistical measures
as computer systems are essential components of many critical commercial services the need for secure online transactions is now becoming evident the demand for such applications as the market grows exceeds the capacity of individual businesses to provide fast and reliable services making outsourcing technologies key player in alleviating issues of scale consider stock broker that needs to provide real time stock trading monitoring service to clients since the cost of multicasting this information to large audience might become prohibitive the broker could outsource the stock feed to third party providers who are in turn responsible for forwarding the appropriate sub feed to clients evidently in critical applications the integrity of the third party should not be taken for granted in this work we study variety of authentication algorithms for selection and aggregation queries over sliding windows our algorithms enable the end users to prove that the results provided by the third party are correct ie equal to the results that would have been computed by the original provider our solutions are based on merkle hash trees over forest of space partitioning data structures and try to leverage key features like update query signing and authentication costs we present detailed theoretical analysis for our solutions and empirically evaluate the proposed techniques
we study the incorporation of generic types in aspect languages since advice acts like method update such study has to accommodate the subtleties of the interaction of classes polymorphism and aspects indeed simple examples demonstrate that current aspect compiling techniques do not avoid runtime type errorswe explore type systems with polymorphism for two models of parametric polymorphism the type erasure semantics of generic java and the type carrying semantics of designs such as generic our main contribution is the design and exploration of source level type system for parametric oo language with aspects we prove progress and preservation propertieswe believe our work is the first source level typing scheme for an aspect based extension of parametric object oriented language
the scaling of microchip technologies has enabled large scale systems on chip soc network on chip noc research addresses global communication in soc involving move from computation centric to communication centric design and ii the implementation of scalable communication structures this survey presents perspective on existing noc research we define the following abstractions system network adapter network and link to explain and structure the fundamental concepts first research relating to the actual network design is reviewed then system level design and modeling are discussed we also evaluate performance analysis techniques the research shows that noc constitutes unification of current trends of intrachip communication rather than an explicit new alternative
online social media draws heavily on active reader participation such as voting or rating of news stories articles or responses to question this user feedback is invaluable for ranking filtering and retrieving high quality content tasks that are crucial with the explosive amount of social content on the web unfortunately as social media moves into the mainstream and gains in popularity the quality of the user feedback degrades some of this is due to noise but increasingly small fraction of malicious users are trying to game the system by selectively promoting or demoting content for profit or fun hence an effective ranking of social media content must be robust to noise in the user interactions and in particular to vote spam we describe machine learning based ranking framework for social media that integrates user interactions and content relevance and demonstrate its effec tiveness for answer retrieval in popular community question answering portal we consider several vote spam attacks and introduce method of training our ranker to increase its robustness to some common forms of vote spam attacks the results of our large scale experimental evaluation show that our ranker is signifcicantly more robust to vote spam compared to state of the art baseline as well as the ranker not explicitly trained to handle malicious interactions
the star graph interconnection network has been recognized as an attractive alternative to the popular hypercube network in this paper we present multipath based multicast routing model for wormholerouted star graph networks propose two efficient multipath routing schemes and contrast the performance of the proposed schemes with the performance of the scheme presented in our previous work both of the two proposed schemes have been proven to be deadlock free the first scheme simple multipath routing uses multiple independent paths for concurrent multicasting the second scheme two phase multipath routing includes two phases source to relay and relay to destination for each phase multicasting is carried out using simple multipath routing experimental results show that for short and medium messages with small message startup latencies the proposed schemes reduce multicast latency more efficiently than other schemes
we show that there exist translations between polymorphic calculus and subsystem of minimal logic with existential types which form galois insertion embedding the translation from polymorphic calculus into the existential type system is the so called call by name cps translation that can be expounded as an adjoint from the neat connection the construction of an inverse translation is investigated from viewpoint of residuated mappings the duality appears not only in the reduction relations but also in the proof structures such as paths between the source and the target calculi from programming point of view this result means that abstract data types can interpret polymorphic functions under the cps translation we may regard abstract data types as dual notion of polymorphic functions
place names are often used to describe and to enquire about geographical information it is common for users to employ vernacular names that have vague spatial extent and which do not correspond to the official and administrative place name terminology recorded within typical gazetteers there is need therefore to enrich gazetteers with knowledge of such vague places and hence improve the quality of place name based information retrieval here we describe method for modelling vague places using knowledge harvested from web pages it is found that vague place names are frequently accompanied in text by the names of more precise co located places that lie within the extent of the target vague place density surface modelling of the frequency of co occurrence of such names provides an effective method of representing the inherent uncertainty of the extent of the vague place while also enabling approximate crisp boundaries to be derived from contours if required the method is evaluated using both precise and vague places the use of the resulting approximate boundaries is demonstrated using an experimental geographical search engine
this paper outlines the history of the programming language from the early days of its iso standardization through the iso standard to the later stages of the revision of that standard the emphasis is on the ideals constraints programming techniques and people that shaped the language rather than the minutiae of language features among the major themes are the emergence of generic programming and the stl the standard library's algorithms and containers specific topics include separate compilation of templates exception handling and support for embedded systems programming during most of the period covered here was mature language with millions of users consequently this paper discusses various uses of and the technical and commercial pressures that provided the background for its continuing evolution
the analysis of data usage in large set of real traces from high energy physics collaboration revealed the existence of an emergent grouping of files that we coined filecules this paper presents the benefits of using this file grouping for prestaging data and compares it with previously proposed file grouping techniques along range of performance metrics our experiments with real workloads demonstrate that filecule grouping is reliable and useful abstraction for data management in science grids that preserving time locality for data prestaging is highly recommended that job reordering with respect to data availability has significant impact on throughput and finally that relatively short history of traces is good predictor for filecule grouping our experimental results provide lessons for workload modeling and suggest design guidelines for data management in data intensive resource sharing environments
underwater sensor networks are attracting increasing interest from researchers in terrestrial radio based sensor networks there are important physical technological and economic differences between terrestrial and underwater sensor networks in this survey we highlight number of important practical issues that have not been emphasized in recent surveys of underwater networks with an intended audience of researchers who are moving from radio based terrestrial networks into underwater networks
over the last years game theory has provided great insights into the behavior of distributed systems by modeling the players as utility maximizing agents in particular it has been shown that selfishness causes many systems to perform in globally suboptimal fashion such systems are said to have large price of anarchy in this paper we extend this active field of research by allowing some players to be malicious or byzantine rather than selfish we ask what is the impact of byzantine players on the system's efficiency compared to purely selfish environments or compared to the social optimum in particular we introduce the price of malice which captures this efficiency degradation as an example we analyze the price of malice of game which models the containment of the spread of viruses in this game each node can choose whether or not to install anti virus software then virus starts from random node and iteratively infects all neighboring nodes which are not inoculated we establish various results about this game for instance we quantify how much the presence of byzantine players can deteriorate or in case of highly risk averse selfish players improve the social welfare of the distributed system
networks on chip noc have emerged as the design paradigm for scalable system on chip soc communication infrastructure due to convergence growing number of applications are integrated on the same chip when combined these applications result in use cases with different communication requirements the noc is configured per use case and traditionally all running applications are disrupted during use case transitions even those continuing operation in this paper we present model that enables partial reconfiguration of nocs and mapping algorithm that uses the model to map multiple applications onto noc with undisrupted quality of service during reconfiguration the performance of the methodology is verified by comparison with existing solutions for several soc designs we apply the algorithm to mobile phone soc with telecom multimedia and gaming applications reducing noc area by more than and power consumption by compared to state of the art approach
the realization of semantic web reasoning is central to substantiating the semantic web vision however current mainstream research on this topic faces serious challenges which force us to question established lines of research and to rethink the underlying approaches the acm portal is published by the association for computing machinery copyright acm inc terms of usage privacy policy code of ethics contact us useful downloads adobe acrobat quicktime windows media player real player
persistently saturated links are abnormal conditions that indicate bottlenecks in internet traffic network operators are interested in detecting such links for troubleshooting to improve capacity planning and traffic estimation and to detect denial of service attacks currently bottleneck links can be detected either locally through snmp information or remotely through active probing or passive flow based analysis however local snmp information may not be available due to administrative restrictions and existing remote approaches are not used systematically because of their network or computation overhead this paper proposes new approach to remotely detect the presence of bottleneck links using spectral and statistical analysis of traffic our approach is passive operates on aggregate traffic without flow separation and supports remote detection of bottlenecks addressing some of the major limitations of existing approaches our technique assumes that traffic through the bottleneck is dominated by packets with common size typically the maximum transfer unit for reasons discussed in section with this assumption we observe that bottlenecks imprint periodicities on packet transmissions based on the packet size and link bandwidth such periodicities manifest themselves as strong frequencies in the spectral representation of the aggregate traffic observed at downstream monitoring point we propose detection algorithm based on rigorous statistical methods to detect the presence of bottleneck links by examining strong frequencies in aggregate traffic we use data from live internet traces to evaluate the performance of our algorithm under various network conditions results show that with proper parameters our algorithm can provide excellent accuracy up to even if the traffic through the bottleneck link accounts for less than of the aggregate traffic
interaction patterns with handheld mobile devices are constantly evolving researchers observed that users prefer to interact with mobile device using one hand however only few interaction techniques support this mode of operation we show that one handed operations can be enhanced with coordinated interaction using for input the front and back of mobile device which we term as dual surface interaction we present some of the design rationale for introducing coordinated dual surface interactions we demonstrate that several tasks including target selection benefit from dual surface input which allows users to rapidly select small targets in locations that are less accessible when interacting using the thumb with one handed input furthermore we demonstrate the benefits of virtual enhancements that are possible with behind the display relative input to perform complex tasks such as steering our results show that dual surface interactions offer numerous benefits that are not available with input on the front or the back alone
this paper formulates set of minimal requirements for the platform independent model pim of the model driven architecture mda it then defines the use case responsibility driven analysis and design methodology urdad which provides simple algorithmic design methodology generating pim satisfying the specified pim requirements
energy consumption can be reduced by scaling down frequency when peak performance is not needed lower frequency permits slower circuits and hence lower supply voltage energy reduc tion comes from voltage reduction technique called dynamic voltage scaling dvs this paper makes the case that the useful frequency range of dvs is limited because there is lower bound on voltage lowering fre quency permits voltage reduction until the lowest voltage is reached beyond that point lowering frequency further does not save energy because voltage is constanthowever there is still opportunity for energy reduction outside the influence of dvs if frequency is lowered enough pairs of pipe line stages can be merged to form shallower pipeline the shal low pipeline has better instructions per cycle ipc than the deep pipeline since energy also depends on ipc energy is reduced for given frequency accordingly we propose dynamic pipeline scaling dps dps enabled deep pipeline can merge adjacent pairs of stages by making the intermediate latches transparent and disabling corresponding feedback paths thus dps enabled pipeline has deep mode for higher frequencies within the influ ence of dvs and shallow mode for lower frequencies shallow mode extends the frequency range for which energy reduction is possible for frequencies outside the influence of dvs dps enabled deep pipeline consumes from to less energy than rigid deep pipeline
the popularity of object oriented programming has led to the wide use of container libraries it is important for the reliability of these containers that they are tested adequately we describe techniques for automated test input generation of java container classes test inputs are sequences of method calls from the container interface the techniques rely on state matching to avoid generation of redundant tests exhaustive techniques use model checking with explicit or symbolic execution to explore all the possible test sequences up to predefined input sizes lossy techniques rely on abstraction mappings to compute and store abstract versions of the concrete states they explore underapproximations of all the possible test sequenceswe have implemented the techniques on top of the java pathfinder model checker and we evaluate them using four java container classes we compare state matching based techniques and random selection for generating test inputs in terms of testing coverage we consider basic block coverage and form of predicate coverage that measures whether all combinations of predetermined set of predicates are covered at each basic block the exhaustive techniques can easily obtain basic block coverage but cannot obtain good predicate coverage before running out of memory on the other hand abstract matching turns out to be powerful approach for generating test inputs to obtain high predicate coverage random selection performed well except on the examples that contained complex input spaces where the lossy abstraction techniques performed better
hashing schemes are common technique to improve the performance in mining not only association rules but also sequential patterns or traversal patterns however the collision problem in hash schemes may result in severe performance degradation in this paper we propose perfect hashing schemes for mining traversal patterns to avoid collisions in the hash table the main idea is to transform each large itemsets into one large itemset by employing delicate encoding scheme then perfect hash schemes designed only for itemsets of length two rather than varied lengths are applied the experimental results show that our method is more than twice as faster than fs algorithm the results also show our method is scalable to database sizes one variant of our perfect hash scheme called partial hash is proposed to cope with the enormous memory space required by typical perfect hash functions we also give comparison of the performances of different perfect hash variants and investigate their properties
recent web searching and mining tools are combining text and link analysis to improve ranking and crawling algorithms the central assumption behind such approaches is that there is correlation between the graph structure of the web and the text and meaning of pages here formalize and empirically evaluate two general conjectures drawing connections from link information to lexical and semantic web content the link content conjecture states that page is similar to the pages that link to it and the link cluster conjecture that pages about the same topic are clustered together these conjectures are often simply assumed to hold and web search tools are built on such assumptions the present quantitative confirmation sheds light on the connection between the success of the latest web mining techniques and the small world topology of the web with encouraging implications for the design of better crawling algorithms
nested transactional memory tm facilitates software composition by letting one module invoke another without either knowing whether the other uses transactions closed nested transactions extend isolation of an inner transaction until the toplevel transaction commits implementations may flatten nested transactions into the top level one resulting in complete abort on conflict or allow partial abort of inner transactions open nested transactions allow committing inner transaction to immediately release isolation which increases parallelism and expressiveness at the cost of both software and hardware complexitythis paper extends the recently proposed flat log based transactional memory logtm with nested transactions flat logtm saves pre transaction values in log detects conflicts with read and write bits per cache block and on abort invokes software handler to unroll the log nested logtm supports nesting by segmenting the log into stack of activation records and modestly replicating bits to facilitate composition with nontransactional code such as language runtime and operating system services we propose escape actions that allow trusted code to run outside the confines of the transactional memory system
the valued constraint satisfaction problem vcsp is generic optimization problem defined by network of local cost functions defined over discrete variables it has applications in artificial intelligence operations research bioinformatics and has been used to tackle optimization problems in other graphical models including discrete markov random fields and bayesian networks the incremental lower bounds produced by local consistency filtering are used for pruning inside branch and bound search in this paper we extend the notion of arc consistency by allowing fractional weights and by allowing several arc consistency operations to be applied simultaneously over the rationals and allowing simultaneous operations we show that an optimal arc consistency closure can theoretically be determined in polynomial time by reduction to linear programming this defines optimal soft arc consistency osac to reach more practical algorithm we show that the existence of sequence of arc consistency operations which increases the lower bound can be detected by establishing arc consistency in classical constraint satisfaction problem csp derived from the original cost function network this leads to new soft arc consistency method called virtual arc consistency which produces improved lower bounds compared with previous techniques and which can solve submodular cost functions these algorithms have been implemented and evaluated on variety of problems including two difficult frequency assignment problems which are solved to optimality for the first time our implementation is available in the open source toulbar platform
gossip based communication protocols are often touted as being robust not surprisingly such claim relies on assumptions under which gossip protocols are supposed to operate in this paper we discuss and in some cases expose some of these assumptions and discuss how sensitive the robustness of gossip is to these assumptions this analysis gives rise to collection of new research challenges
electronic business business enables employees in organizations to complete jobs more efficiently if they can rely on the dependable functions and services delivered by business unfortunately only limited amount of research has explored the topic of employee perceived dependability of business and the concept lacks measurement tools hence this research develops validated instrument for measuring business dependability ebd from an employee perspective based on survey of employees in six large semiconductor manufacturing companies in the hsinchu science based industrial park in taiwan this research conceptualizes and operationalizes the construct of ebd and then creates and refines an business dependability instrument ebdi after rigorous purification procedures the item ebd instrument with good reliability and validity is presented finally this paper concludes with discussion of potential applications of this ebd instrument the business dependability instrument and its potential implications will not only aid researchers interested in designing and testing related business theories but also provide direction for managers in improving the quality and performance of business
flooding based querying and broadcasting schemes have low hop delays of to reach any node that is unit distance away where is the transmission range of any sensor node however in sensor networks with large radio ranges flooding based broadcasting schemes cause many redundant transmissions leading to broadcast storm problem in this paper we study the role of geographic information and state information ie memory of previous messages or transmissions in reducing the redundant transmissions in the network we consider three broadcasting schemes with varying levels of local information where nodes have no geographic or state information ii coarse geographic information about the origin of the broadcast and iii no geographic information but remember previously received messages for each of these network models we demonstrate localized forwarding algorithms for broadcast based on geography or state information that achieve significant reductions in the transmission overheads while maintaining hop delays comparable to flooding based schemes we also consider the related problem of broadcasting to set of spatially uniform points in the network lattice points in the regime where all nodes have only local sense of direction and demonstrate an efficient sparse broadcast scheme based on branching random walk that has low number of packet transmissions thus our results show that even with very little local information it is possible to make broadcast schemes significantly more efficient
hot potato routing is form of synchronous routing which makes no use of buffers at intermediate nodes packets must move at every time step until they reach their destination if contention prevents packet from taking its preferred outgoing edge it is deflected on different edge two simple design principles for hot potato routing algorithms are minimum advance that advances at least one packet towards its destination from every nonempty node and possibly deflects all other packets and maximum advance that advances the maximum possible number of packets livelock is situation in which packets keep moving indefinitely in the network without any packet ever reaching its destination it is known that even maximum advance algorithms might livelock on some networks we show that minimum advance algorithms never livelock on tree networks and that maximum advance algorithms never livelock on triangulated networks
secabstractnak autonomic computing self configuring self healing self managing applications systems and networks is promising solution to ever increasing system complexity and the spiraling costs of human management as systems scale to global proportions most results to date however suggest ways to architect new software designed from the ground up as autonomic systems whereas in the real world organizations continue to use stovepipe legacy systems and or build systems of systems that draw from gamut of disparate technologies from numerous vendors our goal is to retrofit autonomic computing onto such systems externally without any need to understand modify or even recompile the target system's code we present an autonomic infrastructure that operates similarly to active middleware to explicitly add autonomic services to pre existing systems via continual monitoring and feedback loop that performs reconfiguration and or repair as needed our lightweight design and separation of concerns enables easy adoption of individual components for use with variety of target systems independent of the rest of the full infrastructure this work has been validated by several case studies spanning multiple real world application domains
the windows and doorways that connect offices to public spaces are site for people to gather awareness information and initiate interaction however these portals often reveal more information to the public area than the office occupant would like as result people often keep doors and window blinds closed which means that nobody can gather awareness information even those with whom the occupant would be willing to share one solution to this problem is co present media space computer mediated video connection at the boundary between an office and public area these systems can provide both greater privacy control to the occupant and greater overall awareness information to observers to see how co present media spaces would work in real world settings we built what we believe are the first ever co present media spaces and deployed them in two offices from observations gathered over fifteen months it is clear that the systems can do better job of balancing the occupant's need for privacy and the observers need for awareness better than standard window however we also identified number of issues that affected the use and the success of the systems the existence of alternate information sources confusion with existing social norms disparities between effort and need and reduced interactional subtlety for observers in the public area our work contributes both novel arrangement of media space for co present collaborators and the first investigation into the design factors that affect the use and acceptance of these systems
due to the uncertainty of software processes statistic based schedule estimation and stochastic project scheduling both play significant roles in software project management however most current work investigates them independently without an integrated process to achieve on time delivery for software development organisations for such an issue this paper proposes two stage probabilistic scheduling strategy which aims to decrease schedule overruns specifically probability based temporal consistency model is employed at the first pre scheduling stage to support negotiation between customers and project managers for setting balanced deadlines of individual software processes at the second scheduling stage an innovative genetic algorithm based scheduling strategy is proposed to minimise the overall completion time of multiple software processes with individual deadlines the effectiveness of our strategy in achieving on time delivery is verified with large scale simulation experiments
creating user profiles is an important step in personalization many methods for user profile creation have been developed to date using different representations such as term vectors and concepts from an ontology like dmoz in this paper we propose and evaluate different methods for creating user profiles using wikipedia as the representation the key idea in our approach is to map documents to wikipedia concepts at different levels of resolution words key phrases sentences paragraphs the document summary and the entire document itself we suggest method for evaluating profile recall by pooling the relevant results from the different methods and evaluate our results for both precision and recall we also suggest novel method for profile evaluation by assessing the recall over known ontological profile drawn from dmoz
we present new system for robustly performing boolean operations on linear polyhedra our system is exact meaning that all internal numeric predicates are exactly decided in the sense of exact geometric computation our bsp tree based system is faster at performing iterative computations than cgal's nef polyhedra based system the current best practice in robust boolean operations while being only twice as slow as the non robust modeler maya meanwhile we achieve much smaller substrate of geometric subroutines than previous work comprised of only predicates convex polygon constructor and convex polygon splitting routine the use of bsp tree based boolean algorithm atop this substrate allows us to explicitly handle all geometric degeneracies without treating large number of cases
in distributed system broadcasting is an efficient way to dispense data in certain highly dynamic environments while there are several well known on line broadcast scheduling strategies that minimize wait time there has been little research that considers on demand broadcasting with timing constraints one application which could benefit from strategy for on demand broadcast with timing constraints is real time database system scheduling strategies are needed in real time databases that identify which data item to broadcast next in order to minimize missed deadlines the scheduling decisions required in real time broadcast system allow the system to be modeled as markov decision process mdp in this paper we analyze the mdp model and determine that finding an optimal solution is hard problem in pspace we propose scheduling approach called aggregated critical requests acr which is based on the mdp formulation and present two algorithms based on this approach acr is designed for timely delivery of data to clients in order to maximize the reward by minimizing the deadlines missed results from trace driven experiments indicate the acr approach provides flexible strategy that can outperform existing strategies under variety of factors
multi label learning deals with the problem where each instance is associated with multiple labels simultaneously the task of this learning paradigm is to predict the label set for each unseen instance through analyzing training instances with known label sets in this paper neural network based multi label learning algorithm named ml rbf is proposed which is derived from the traditional radial basis function rbf methods briefly the first layer of an ml rbf neural network is formed by conducting clustering analysis on instances of each possible class where the centroid of each clustered groups is regarded as the prototype vector of basis function after that second layer weights of the ml rbf neural network are learned by minimizing sum of squares error function specifically information encoded in the prototype vectors corresponding to all classes are fully exploited to optimize the weights corresponding to each specific class experiments on three real world multi label data sets show that ml rbf achieves highly competitive performance to other well established multi label learning algorithms
privacy is an important issue in data publishing many organizations distribute non aggregate personal data for research and they must take steps to ensure that an adversary cannot predict sensitive information pertaining to individuals with high confidence this problem is further complicated by the fact that in addition to the published data the adversary may also have access to other resources eg public records and social networks relating individuals which we call adversarial knowledge robust privacy framework should allow publishing organizations to analyze data privacy by means of not only data dimensions data that publishing organization has but also adversarial knowledge dimensions information not in the data in this paper we first describe general framework for reasoning about privacy in the presence of adversarial knowledge within this framework we propose novel multidimensional approach to quantifying adversarial knowledge this approach allows the publishing organization to investigate privacy threats and enforce privacy requirements in the presence of various types and amounts of adversarial knowledge our main technical contributions include multidimensional privacy criterion that is more intuitive and flexible than previous approaches to modeling background knowledge in addition we identify an important congregation property of the adversarial knowledge dimensions based on this property we provide algorithms for measuring disclosure and sanitizing data that improve computational efficiency several orders of magnitude over the best known techniques
while symmetry reduction has been established to be an important technique for reducing the search space in model checking its application in concurrent software verification is still limited due to the difficulty of specifying symmetry in realistic software we propose an algorithm for automatically discovering and applying transition symmetry in multithreaded programs during dynamic model checking our main idea is using dynamic program analysis to identify permutation of variables and labels of the program that entails syntactic equivalence among the residual code of threads and to check whether the local states of threads are equivalent under the permutation the new transition symmetry discovery algorithm can bring substantial state space savings during dynamic verification of concurrent programs we have implemented the new algorithm in the dynamic model checker inspect our preliminary experiments show that this algorithm can successfully discover transition symmetries that are hard or otherwise cumbersome to identify manually and can significantly reduce the model checking time while using inspect to examine realistic multithreaded applications
spatio temporal geo referenced datasets are growing rapidly and will be more in the near future due to both technological and social commercial reasons from the data mining viewpoint spatio temporal trajectory data introduce new dimensions and correspondingly novel issues in performing the analysis tasks in this paper we consider the clustering problem applied to the trajectory data domain in particular we propose an adaptation of density based clustering algorithm to trajectory data based on simple notion of distance between trajectories then set of experiments on synthesized data is performed in order to test the algorithm and to compare it with other standard clustering approaches finally new approach to the trajectory clustering problem called temporal focussing is sketched having the aim of exploiting the intrinsic semantics of the temporal dimension to improve the quality of trajectory clustering
this paper evaluates managing the processor's datapath width at the compiler level by means of exploiting dynamic narrow width operands we capitalize on the large occurrence of these operands in multimedia programs to build static narrow width regions that may be directly exposed to the compiler we propose to augment the isa with instructions directly exposing the datapath and the register widths to the compiler simple exception management allows this exposition to be only speculative in this way we permit the software to speculatively accommodate the execution of program on narrower datapath width in order to save energy for this purpose we introduce novel register file organization the byte slice register file which allows the width of the register file to be dynamically reconfigured providing both static and dynamic energy savings we show that by combining the advantages of the byte slice register file with the advantages provided by clock gating the datapath on per region basis up to of the datapath dynamic energy can be saved while reduction of the register file static energy is achieved
networks on chip nocs emerge as the solution for the problem of interconnecting cores or ips in systems on chip socs which require reusable and scalable communication architectures the building block of noc is its router or switch whose architecture has great impact on the costs and on the performance of the network this work presents parameterizable router architecture for nocs which is based on canonical template and on library of building components offering different alternatives and implementations for the circuits used for packet forwarding in noc such features allow to explore the noc design space in order to obtain router configuration which best fits the performance requirements of target application at lower silicon costs we describe the router architecture and present some synthesis results which demonstrate the feasibility of this new router
today many workers spend too much of their time translating their co workers requests into structures that information systems can understand this paper presents the novel interaction design and evaluation of vio an agent that helps workers trans late request vio monitors requests and makes suggestions to speed up the translation vio allows users to quickly correct agent errors these corrections are used to improve agent performance as it learns to automate work our evaluations demonstrate that this type of agent can significantly reduce task completion time freeing workers from mundane tasks
this paper presents boosting based algorithm for learning bipartite ranking function brf with partially labeled data until now different attempts had been made to build brf in transductive setting in which the test points are given to the methods in advance as unlabeled data the proposed approach is semi supervised inductive ranking algorithm which as opposed to transductive algorithms is able to infer an ordering on new examples that were not used for its training we evaluate our approach using the trec ohsumed and the reuters data collections comparing against two semi supervised classification algorithms for rocarea auc uninterpolated average precision aup mean precision tp and precision recall pr curves in the most interesting cases where there are an unbalanced number of irrelevant examples over relevant ones we show our method to produce statistically significant improvements with respect to these ranking measures
the mpi programming model hides network type and topology from developers but also allows them to seamlessly distribute computational job across multiple cores in both an intra and inter node fashion this provides for high locality performance when the cores are either on the same node or on nodes closely connected by the same network type the streaming model splits computational job into linear chain of decoupled units this decoupling allows the placement of job units on optimal nodes according to network topology furthermore the links between these units can be of varying protocols when the application is distributed across heterogeneous network in this paper we study how to integrate the mpi and stream programming models in order to exploit network locality and topology we present hybrid mpi stream framework that aims to take advantage of each model's strengths we test our framework with financial application this application simulates an electronic market for single financial instrument stream of buy and sell orders is fed into price matching engine the matching engine creates stream of order confirmations trade confirmations and quotes based on its attempts to match buyers with sellers our results show that the hybrid mpi stream framework can deliver performance improvement at certain order transmission rates
information extraction ie is the task of extracting knowledge from unstructured text we present novel unsupervised approach for information extraction based on graph mutual reinforcement the proposed approach does not require any seed patterns or examples instead it depends on redundancy in large data sets and graph based mutual reinforcement to induce generalized extraction patterns the proposed approach has been used to acquire extraction patterns for the ace automatic content extraction relation detection and characterization rdc task ace rdc is considered hard task in information extraction due to the absence of large amounts of training data and inconsistencies in the available data the proposed approach achieves superior performance which could be compared to supervised techniques with reasonable training data
constraints on the memory size of embedded systems require reducing the image size of executing programs common techniques include code compression and reduced instruction sets we propose novel technique that eliminates large portions of the executable image without compromising execution time due to decompression or code generation due to reduced instruction sets frozen code and data portions are identified using profiling techniques and removed from the loadable image they are replaced with branches to code stubs that load them in the unlikely case that they are accessed the executable is sustained in runnable modeanalysis of the frozen portions reveals that most are error and uncommon input handlers only minority of the code less than that was identified as frozen during training run is also accessed with production datasetsthe technique was applied on three benchmark suites spec cint spec cfp and mediabench and results in image size reductions of up to and per suite the average reductions are and per suite
in this paper we propose and evaluate cluster based network server called press the server relies on locality conscious request distribution and standard for user level communication to achieve high performance and portability we evaluate press by first isolating the performance benefits of three key features of user level communication low processor overhead remote memory accesses and zero copy transfers next we compare press to servers that involve less intercluster communication but are not as easily portable our results for an node server cluster and five www traces demonstrate that user level communication can improve performance by as much as percent compared to kernel level protocol low processor overhead remote memory writes and zero copy all make nontrivial contributions toward this overall gain our results also show that portability in press causes no throughput degradation when we exploit user level communication extensively
citations to publication venues in the form of journal conference and workshop contain spelling variants acronyms abbreviated forms and misspellings all of which make more difficult to retrieve the item of interest the task of discovering and reconciling these variant forms of bibliographic references is known as authority work the key goal is to create the so called authority files which maintain for any given bibliographic item list of variant labels ie variant strings used as reference to it in this paper we propose to use information available on the web to create high quality publication venue authority files our idea is to recognize and extract references to publication venues in the text snippets of the answers returned by search engine references to same publication venue are then reconciled in an authority file each entry in this file is composed of canonical name for the venue an acronym the venue type ie journal conference or workshop and mapping to various forms of writing its name in bibliographic citations experimental results show that our web based method for creating authority files is superior to previous work based on straight string matching techniques considering the average precision in finding correct venue canonical names we observe gains up to
in this paper we present new program analysis method which we call storage use analysis this analysis deduces how objects are used by the program and allows the optimization of their allocation this analysis can be applied to both statically typed languages eg ml and latently typed languages eg scheme it handles side effects higher order functions separate compilation and does not require cps transformation we show the application of our analysis to two important optimizations stack allocation and unboxing the first optimization replaces some heap allocations by stack allocations for user and system data storage eg lists vectors procedures the second optimization avoids boxing some objects this analysis and associated optimitations have been implemented in the bigloo scheme ml compiler experimental results show that for many allocation intensive programs we get significant speedup in particular numerically intensive programs are almost times faster because floating point numbers are unboxed and no longer heap allocated
extensive research efforts have been devoted to implement group of type safe mutually recursive classes recently proposals for separating each member of the group as reusable and composable programming unit have also been presented one problem of these proposals is verbosity of the source programs we have to declare recursive type parameter to parameterize each mutually recursive class within each class declaration and we have to declare fixed point class with empty class body for each parameterized class therefore even though the underlying type system is simple programs written in these languages tend to be rather complex and hard to understand in this paper we propose language with lightweight dependent classes that forms simple type system built on top of generic java in this language we can implement each member of type safe mutually recursive classes in separate source file without writing lot of complex boilerplate code to carefully investigate type soundness of our proposal we develop xfgj simple extension of fgj supporting lightweight dependent classes this type system is proved to be sound
as data streams are gaining prominence in growing number of emerging application domains classification on data streams is becoming an active research area currently the typical approach to this problem is based on ensemble learning which learns basic classifiers from training data stream and forms the global predictor by organizing these basic ones while this approach seems successful to some extent its performance usually suffers from two contradictory elements existing naturally within many application scenarios firstly the need for gathering sufficient training data for basic classifiers and engaging enough basic learners in voting for bias variance reduction and secondly the requirement for significant sensitivity to concept drifts which places emphasis on using recent training data and up to date individual classifiers it results in such dilemma that some algorithms are not sensitive enough to concept drifts while others although sensitive enough suffer from unsatisfactory classification accuracy in this paper we propose an ensemble learning algorithm which furnishes training data for basic classifiers starting from the up to date data chunk and searching for complement from past chunks while ruling out the data inconsistent with current concept provides effective voting by adaptively distinguishing sensible classifiers from the else and engaging sensible ones as voters experimental results justify the superiority of this strategy in terms of both accuracy and sensitivity especially in severe circumstances where training data is extremely insufficient or concepts are evolving frequently and significantly
this paper presents the design implementation and performance evaluation of suite of resource policing mechanisms that allow guest processes to efficiently and unobtrusively exploit otherwise idle workstation resources unlike traditional policies that harvest cycles only from unused machines we employ fine grained cycle stealing to exploit resources even from machines that have active users we developed suite of kernel extensions that enable these policies to operate without significantly impacting host processes new starvation level cpu priority for guest jobs new page replacement policy that imposes hard bounds on physical memory usage by guest processes and new scheduling mechanism called rate windows that throttle guest processes usage of and network bandwidth we evaluate both the individual impacts of each mechanism and their utility for our fine grain cycle stealing
the reconfigurable devices such as cpld and fpga become more popular for its great potential on accelerating applications they are widely used as an application specified hardware accelerator many run time reconfigurable platforms are introduced such as the intel quickassist technology however it's time consuming to design hardware accelerator while the performance is hard to determine because of the extra overheads it involved in order to estimate the efficiency of the accelerator theoretical analysis of such platforms was done in our paper three factors which impact the performance of the accelerator were concluded as well speed up ratio reconfiguration overhead and communication overhead furthermore performance model was established and an experiment on bzip was done to verify the model the results showed that the model's estimation is very close to the real world and the average error on the efficiency's threshold is less than
software engineer can use meaning preserving program restructuring tool during maintenance to change program's structure to ease modification one common restructuring action is to create new abstract data type by encapsulating an existing data structure data encapsulation simplifies modification by isolating changes to the implementation and behavior of an abstract data type to perform encapsulation programmer must understand how the data structure is used in the code identify abstract operations performed on the data structure and choose concrete expressions to be made into functions we provide manipulable program visualization called the star diagram that both highlights information partinent to encapsulation and supports the application of meaning preserving restructuring transformations on the program through direct manipulation user interface the visualization graphically and compactly presents all statements in the program that use the given global data structure helping the programmer to choose the functions that completely encapsulate it additionally the visualization elides code unrelated to the data structure and to the task and collapses similar expressions to allow the programmer to identify frequently occurring code fragments and manipulate them together the visualization is mapped directly to the program text so manipulation of the visualization also restructures the program we describe the design implementation and application of the star diagram and evaluate its ability to assist data encapsulation in large programs
in this paper we address stereo matching in the presence of class of non lambertian effects where image formation can be modeled as the additive superposition of layers at different depths the presence of such effects makes it impossible for traditional stereo vision algorithms to recover depths using direct color matching based methods we develop several techniques to estimate both depths and colors of the component layers depth hypotheses are enumerated in pairs one from each layer in nested plane sweep for each pair of depth hypotheses matching is accomplished using spatial temporal differencing we then use graph cut optimization to solve for the depths of both layers this is followed by an iterative color update algorithm which we proved to be convergent our algorithm recovers depth and color estimates for both synthetic and real image sequences
this research investigates ways of predicting which files would be most likely to contain large numbers of faults in the next release of large industrial software system previous work involved making predictions using several different models ranging from simple fully automatable model the loc model to several different variants of negative binomial regression model that were customized for the particular software system under study not surprisingly the custom models invariably predicted faults more accurately than the simple model however development of customized models requires substantial time and analytic effort as well as statistical expertise we now introduce new more sophisticated models that yield more accurate predictions than the earlier loc model but which nonetheless can be fully automated we also extend our earlier research by presenting another large scale empirical study of the value of these prediction models using new industrial software system over nine year period
this paper presents new approach to dynamically monitoring operating system kernel integrity based on property called state based control flow integrity sbcfi violations of sbcfi signal persistent unexpected modification of the kernel's control flow graph we performed thorough analysis of linux rootkits and found that employ persistent control flow modifications an informal study of windows rootkits yielded similar results we have implemented sbcfi enforcement as part of the xen and vmware virtual machine monitors our implementation detected all the control flow modifying rootkits we could install while imposing unnoticeable overhead for both typical web server workload and cpu intensive workloads when operating at second intervals
per core local scratchpad memories allow direct inter core communication with latency and energy advantages over coherent cache based communication especially as cmp architectures become more distributed we have designed cache integrated network interfaces nis appropriate for scalable multicores that combine the best of two worlds the flexibility of caches and the efficiency of scratchpad memories on chip sram is configurably shared among caching scratchpad and virtualized ni functions this paper presents our architecture which provides local and remote scratchpad access to either individual words or multi word blocks through rdma copy furthermore we introduce event responses as mechanism for software configurable synchronization primitives we present three event response mechanisms that expose ni functionality to software for multiword transfer initiation memory barriers for explicitly selected accesses of arbitrary size and multi party synchronization queues we implemented these mechanisms in four core fpga prototype and evaluated the on chip communication performance on the prototype as well as on cmp simulator with up to cores we demonstrate efficient synchronization low overhead communication and amortized overhead bulk transfers which allow parallelization gains for fine grain tasks and efficient exploitation of the hardware bandwidth
query expansion of named entities can be employed in order to increase the retrieval effectiveness peculiarity of named entities compared to other vocabulary terms is that they are very dynamic in appearance and synonym relationships between terms change with time in this paper we present an approach to extracting synonyms of named entities over time from the whole history of wikipedia in addition we will use their temporal patterns as feature in ranking and classifying them into two types ie time independent or time dependent time independent synonyms are invariant to time while time dependent synonyms are relevant to particular time period ie the synonym relationships change over time further we describe how to make use of both types of synonyms to increase the retrieval effectiveness ie query expansion with time independent synonyms for an ordinary search and query expansion with time dependent synonyms for search wrt temporal criteria finally through an evaluation based on trec collections we demonstrate how retrieval performance of queries consisting of named entities can be improved using our approach
load instructions diminish processor performance in two ways first due to the continuously widening gap between cpu and memory speed the relative latency of load instructions grows constantly and already slows program execution second memory reads limit the available instruction level parallelism because instructions that use the result of load must wait for the memory access to complete before they can start executing load value predictors alleviate both problems by allowing the cpu to speculatively continue processing without having to wait for load instructions which can significantly improve the execution speed while several hybrid load value predictors have been proposed and found to work well no systematic study of such predictors exists in this paper we investigate the performance of all hybrids that can be built out of register value last value stride delta last four value and finite context method predictor our analysis shows that hybrids can deliver percent more speedup than the best single component predictors an examination of the individual components of hybrids revealed that predictors with poor standalone performance sometimes make excellent components in hybrid while combining well performing individual predictors often does not result in an effective hybrid our hybridization study identified the register value stride delta predictor as one of the best two component hybrids it matches or exceeds the speedup of two component hybrids from the literature in spite of its substantially smaller and simpler design of all the predictors we studied the register value stride delta last four value hybrid performs best it yields harmonic mean speedup over the eight specint programs of percent
this paper presents lambda language for dynamic tracking of information flow across multiple interdependent dimensions of information typical dimensions of interest are integrity and confidentiality lambda supports arbitrary domain specific policies that can be developed independently lambda treats information flow metadata as first class entity and tracks information flow on the metadata itself integrity on integrity integrity on confidentiality etc this paper also introduces impolite novel class of information flow policies for lambda unlike many systems which only allow for absolute security relations impolite can model more realistic security policies based on relative security relations impolite demonstrates how policies on interdependent dimensions of information can be simultaneously enforced within lambda i's unified framework
reputation systems have been popular in estimating the trustworthiness and predicting the future behavior of nodes in large scale distributed system where nodes may transact with one another without prior knowledge or experience one of the fundamental challenges in distributed reputation management is to understand vulnerabilities and develop mechanisms that can minimize the potential damages to system by malicious nodes in this paper we identify three vulnerabilities that are detrimental to decentralized reputation management and propose trustguard safeguard framework for providing highly dependable and yet efficient reputation system first we provide dependable trust model and set of formal methods to handle strategic malicious nodes that continuously change their behavior to gain unfair advantages in the system second transaction based reputation system must cope with the vulnerability that malicious nodes may misuse the system by flooding feedbacks with fake transactions third but not least we identify the importance of filtering out dishonest feedbacks when computing reputation based trust of node including the feedbacks filed by malicious nodes through collusion our experiments show that comparing with existing reputation systems our framework is highly dependable and effective in countering malicious nodes regarding strategic oscillating behavior flooding malevolent feedbacks with fake transactions and dishonest feedbacks
question classification is an important step in factual question answering qa and other dialog systems several attempts have been made to apply statistical machine learning approaches including support vector machines svms with sophisticated features and kernels curiously the payoff beyond simple bag of words representation has been small we show that most questions reveal their class through short contiguous token subsequence which we call its informer span perfect knowledge of informer spans can enhance accuracy from to using linear svms on standard benchmarks in contrast standard heuristics based on shallow pattern matching give only improvement showing that the notion of an informer is non trivial using novel multi resolution encoding of the question's parse tree we induce conditional random field crf to identify informer spans with about accuracy then we build meta classifier using linear svm on the crf output enhancing accuracy to which is better than all published numbers
content filtering based intrusion detection systems have been widely deployed in enterprise networks and have become standard measure to protect networks and network users from cyber attacks although several solutions have been proposed recently finding an efficient solution is considered as difficult problem due to the limitations in resources such as small memory size as well as the growing link speed in this paper we present novel content filtering technique called table driven bottom up tree tbt which was designed to fully exploit hardware parallelism to achieve real time packet inspection ii to require small memory for storing signatures iii to be flexible in modifying the signature database and iv to support complex signature representation such as regular expressions we configured tbt considering the hardware specifications and limitations and implemented it using fpga simulation based performance evaluations showed that the proposed technique used only kilobytes of memory for storing the latest version of snort rule consisting of signatures in addition unlike many other hardware based solutions modification to signature database does not require hardware re compilation in tbt
the goal of this article is to review the state of the art tracking methods classify them into different categories and identify new trends object tracking in general is challenging problem difficulties in tracking objects can arise due to abrupt object motion changing appearance patterns of both the object and the scene nonrigid object structures object to object and object to scene occlusions and camera motion tracking is usually performed in the context of higher level applications that require the location and or shape of the object in every frame typically assumptions are made to constrain the tracking problem in the context of particular application in this survey we categorize the tracking methods on the basis of the object and motion representations used provide detailed descriptions of representative methods in each category and examine their pros and cons moreover we discuss the important issues related to tracking including the use of appropriate image features selection of motion models and detection of objects
ensuring model quality is key success factor in many computer science areas and becomes crucial in recent software engineering paradigms like the one proposed by model driven software development tool support for measurements and redesigns becomes essential to help developers improve the quality of their models however developing such helper tools for the wide variety of frequently domain specific visual notations used by software engineers is hard and repetitive task that does not take advantage from previous developments thus being frequently forgotten in this paper we present our approach for the visual specification of measurements and redesigns for domain specific visual languages dsvls with this purpose we introduce novel dsvl called slammer that contains generalisations of some of the more used types of internal product measurements and redesigns the goal is to facilitate the task of defining measurements and redesigns for any dsvl as well as the generation of tools from such specification reducing or eliminating the necessity of coding we rely on the use of visual patterns for the specification of the relevant elements for each measurement and redesign type in addition slammer allows the specification of redesigns either procedurally or by means of graph transformation rules these redesigns can be triggered when the measurements reach certain threshold these concepts have been implemented in the meta modelling tool atom in this way when dsvl is designed it is possible to specify measurements and redesigns that will become available in the final modelling environment generated for the language as an example we show case study in the web modelling domain
low latency anonymity systems are susceptive to traffic analysis attacks in this paper we propose dependent link padding scheme to protect anonymity systems from traffic analysis attacks while providing strict delay bound the covering traffic generated by our scheme uses the minimum sending rate to provide full anonymity for given set of flows the relationship between user anonymity and the minimum covering traffic rate is then studied via analysis and simulation when user flows are poisson processes with the same sending rate the minimum covering traffic rate to provide full anonymity to users is log for pareto traffic we show that the rate of the covering traffic converges to constant when the number of flows goes to infinity finally we use real internet trace files to study the behavior of our algorithm when user flows have different rates
virtual machine monitors vmms have been hailed as the basis for an increasing number of reliable or trusted computing systems the xen vmm is relatively small piece of software hypervisor that runs at lower level than conventional operating system in order to provide isolation between virtual machines its size is offered as an argument for its trustworthiness however the management of xen based system requires privileged full blown operating system to be included in the trusted computing base tcb in this paper we introduce our work to disaggregate the management virtual machine in xen based system we begin by analysing the xen architecture and explaining why the status quo results in large tcb we then describe our implementation which moves the domain builder the most important privileged component into minimal trusted compartment we illustrate how this approach may be used to implement trusted virtualisation and improve the security of virtual tpm implementations finally we evaluate our approach in terms of the reduction in tcb size and by performing security analysis of the disaggregated system
java is high productivity object oriented programming language that is rapidly gaining popularity in high performance application development one major obstacle to its broad acceptance is its mediocre performance when compared with fortran or especially if the developers use object oriented features of the language extensively previous work in improving the performance of object oriented high performance scientific java applications consisted of high level compiler optimization and analysis strategies such as class specialization and object inlining this paper extends prior work on object inlining by improving the analysis and developing new code transformation techniques to further improve the performance of high performance applications written in high productivity object oriented style two major impediments to effective object inlining are object and array aliasing and binary method invocations this paper implements object and array alias strategies to address the aliasing problem while utilizing an idea from telescoping languages to address the binary method invocation problem application runtime gains of up to result from employing these techniques these improvements should further increase the scientific community's acceptance of the java programming language in the development of high performance high productivity scientific applications
as part of an architectural modeling project this paper investigates the problem of understanding and manipulating images of buildings our primary motivation is to automatically detect and seamlessly remove unwanted foreground elements from urban scenes without explicit handling these objects will appear pasted as artifacts on the model recovering the building facade in video sequence is relatively simple because parallax induces foreground background depth layers but here we consider static images only we develop series of methods that enable foreground removal from images of buildings or brick walls the key insight is to use priori knowledge about grid patterns on building facades that can be modeled as near regular textures nrt we describe markov random field mrf model for such textures and introduce markov chain monte carlo mcmc optimization procedure for discovering them this simple spatial rule is then used as starting point for inference of missing windows facade segmentation outlier identification and foreground removal
large scientific parallel applications demand large amounts of memory space current parallel computing platforms schedule jobs without fully knowing their memory requirements this leads to uneven memory allocation in which some nodes are overloaded this in turn leads to disk paging which is extremely expensive in the context of scientific parallel computing to solve this problem we propose new peer to peer solution called parallel network ram this approach avoids the use of disk better utilizes available ram resources and will allow larger problems to be solved while reducing the computational communication and synchronization overhead typically involved in parallel applications we proposed several different parallel network ram designs and evaluated the performance of each under different conditions we discovered that different designs are appropriate in different situations
we show the practical feasibility of monitoring complex security properties using runtime monitoring approach for metric first order temporal logic in particular we show how wide variety of security policies can be naturally formalized in this expressive logic ranging from traditional policies like chinese wall and separation of duty to more specialized usage control and compliance requirements we also explain how these formalizations can be directly used for monitoring and experimentally evaluate the performance of the resulting monitors
the continuing trend toward greater processing power larger storage and in particular increased display surface by using multiple monitor supports increased multi tasking by the computer user the concomitant increase in desktop complexity has the potential to push the overhead of window management to frustrating and counterproductive new levels it is difficult to adequately design for multiple monitor systems without understanding how multiple monitor users differ from or are similar to single monitor users therefore we deployed tool to group of single monitor and multiple monitor users to log window management activity analysis of the data collected from this tool revealed that usage of interaction components may change with an increase in number of monitors and window visibility can be useful measure of user display space management activity especially for multiple monitor users the results from this analysis begin to fill gap in research about real world window management practices
graph data are subject to uncertainties in many applications due to incompleteness and imprecision of data mining uncertain graph data is semantically different from and computationally more challenging than mining exact graph data this paper investigates the problem of mining frequent subgraph patterns from uncertain graph data the frequent subgraph pattern mining problem is formalized by designing new measure called expected support an approximate mining algorithm is proposed to find an approximate set of frequent subgraph patterns by allowing an error tolerance on the expected supports of the discovered subgraph patterns the algorithm uses an efficient approximation algorithm to determine whether subgraph pattern can be output or not the analytical and experimental results show that the algorithm is very efficient accurate and scalable for large uncertain graph databases
hierarchical multi label classification hmc is variant of classification where instances may belong to multiple classes at the same time and these classes are organized in hierarchy this article presents several approaches to the induction of decision trees for hmc as well as an empirical study of their use in functional genomics we compare learning single hmc tree which makes predictions for all classes together to two approaches that learn set of regular classification trees one for each class the first approach defines an independent single label classification task for each class sc obviously the hierarchy introduces dependencies between the classes while they are ignored by the first approach they are exploited by the second approach named hierarchical single label classification hsc depending on the application at hand the hierarchy of classes can be such that each class has at most one parent tree structure or such that classes may have multiple parents dag structure the latter case has not been considered before and we show how the hmc and hsc approaches can be modified to support this setting we compare the three approaches on yeast data sets using as classification schemes mips's funcat tree structure and the gene ontology dag structure we show that hmc trees outperform hsc and sc trees along three dimensions predictive accuracy model size and induction time we conclude that hmc trees should definitely be considered in hmc tasks where interpretable models are desired
time dependant models have been intensively studied for many reasons among others because of their applications in software verification and due to the development of embedded platforms where reliability and safety depend to large extent on the time features many of the time dependant models were suggested as real time extensions of several well known untimed models the most studied formalisms include networks of timed automata which extend the model of communicating finite state machines with finite number of real valued clocks and timed extensions of petri nets where the added time constructs include eg time intervals that are assigned to the transitions time petri nets or to the arcs timed arc petri nets in this paper we shall semi formally introduce these models discuss their strengths and weaknesses and provide an overview of the known results about the relationships among the models
spline joints are novel class of joints that can model general scleronomic constraints for multibody dynamics based on the minimal coordinates formulation the main idea is to introduce spline curves and surfaces in the modeling of joints we model dof joints using splines on se and construct multi dof joints as the product of exponentials of splines in euclidean space we present efficient recursive algorithms to compute the derivatives of the spline joint as well as geometric algorithms to determine optimal parameters in order to achieve the desired joint motion our spline joints can be used to create interesting new simulated mechanisms for computer animation and they can more accurately model complex biomechanical joints such as the knee and shoulder
the iceberg cube mining computes all cells corresponding to group by partitions that satisfy given constraint on aggregated behaviors of the tuples in group by partition the number of cells often is so large that the result cannot be realistically searched without pushing the constraint into the search previous works have pushed antimonotone and monotone constraints however many useful constraints are neither antimonotone nor monotone we consider general class of aggregate constraints of the form theta sigma where is an arithmetic function of sql like aggregates and theta is one of we propose novel pushing technique called divide and approximate to push such constraints the idea is to recursively divide the search space and approximate the given constraint using antimonotone or monotone constraints in subspaces this technique applies to class called separable constraints which properly contains all constraints built by an arithmetic function of all sql aggregates
finite state verification eg model checking provides powerful means to detect errors that are often subtle and difficult to reproduce nevertheless the transition of this technology from research to practice has been slow while there are number of potential causes for reluctance in adopting such formal methods in practice we believe that primary cause rests with the fact that practitioners are unfamiliar with specification processes notations and strategies recent years have seen growing success in leveraging experience with design and coding patterns we propose pattern based approach to the presentation codification and reuse of property specifications for finite state verification
structured documents eg sgml can benefit lot from database support and more specifically from object oriented database oodb management systems this paper describes natural mapping from sgml documents into oodb's and formal extension of two oodb query languages one sql like and the other calculus in order to deal with sgml document retrievalalthough motivated by structured documents the extensions of query languages that we present are general and useful for variety of other oodb applications key element is the introduction of paths as first class citizens the new features allow to query data and to some extent schema without exact knowledge of the schema in simple and homogeneous fashion
devices are increasingly vulnerable to soft errors as their feature sizes shrink previously soft error rates were significant primarily in space and high atmospheric computing modern architectures now use features so small at sufficiently low voltages that soft errors are becoming important even at terrestrial altitudes due to their large number of components supercomputers are particularly susceptible to soft errors since many large scale parallel scientific applications use iterative linear algebra methods the soft error vulnerability of these methods constitutes large fraction of the applications overall vulnerability many users consider these methods invulnerable to most soft errors since they converge from an imprecise solution to precise one however we show in this paper that iterative methods are vulnerable to soft errors exhibiting both silent data corruptions and poor ability to detect errors further we evaluate variety of soft error detection and tolerance techniques including checkpointing linear matrix encodings and residual tracking techniques
the world wide web has now become humongous archive of various contents the inordinate amount of information found on the web presents challenge to deliver right information to the right users on one hand the abundant information is freely accessible to all web denizens on the other hand much of such information may be irrelevant or even deleterious to some users for example some control and filtering mechanisms are desired to prevent inappropriate or offensive materials such as pornographic websites from reaching children ways of accessing websites are termed as access scenarios an access scenario can include using search engines eg image search that has very little textual content url redirection to some websites or directly typing porn website urls in this paper we propose framework to analyze website from several different aspects or information sources and generate classification model aiming to accurately classify such content irrespective of access scenarios extensive experiments are performed to evaluate the resulting system which illustrates the promise of the proposed approach
the possibility for spontaneous ad hoc networks between mobile devices has been increasing as small devices become more capable of hosting useful networked applications these applications face the challenges of frequent disconnections highly dynamic network topologies and varying communication patterns combination unique to mobile ad hoc networks this is the first survey to examine current manet programming approaches including tuple spaces remote objects publish subscribe and code migration through analysis and experimental results we suggest that these approaches are essentially extensions to existing distributed and parallel computing concepts and new abstractions may be necessary to fully handle the programming issues presented by manets
many techniques have been developed to perform indoor location each strategy has its own advantages and drawbacks with the application demanding location information the main determinant of the system to be used in this paper system is presented that serves location to innovative services for elderly and disabled people ranging from alarm and monitoring to support for navigation and leisure the system uses zigbee and ultrasound to fulfill the application requirements differing in this respect from all other existing systems zups zigbee and ultrasound positioning system provides wide multicell coverage easy extension robustness even in crowded scenarios different levels of precision depending on the user's profile and service requirements from few centimeters to meters limited infrastructure requirements simple calibration and cost effectiveness the system has been evaluated from the technical functional and usability standpoints with satisfactory results and its suitability has also been demonstrated in residence for people with disabilities located in zaragoza spain
we present technique for implementing visual language compilers through standard compiler generation platforms the technique exploits extended positional grammars xpgs for short for modeling the visual languages in natural way and uses set of mapping rules to translate an xpg specification into translation schema this lets us generate visual language parsers through standard compiler compiler techniques and tools like yacc the generated parser accepts exactly the same set of visual sentences derivable through the application of xpg productions the technique represents an important achievement since it enables us to perform visual language compiler construction through standard compiler compilers rather than specific compiler generation tools this makes our approach particularly appealing since compiler compilers are widely used and rely on well founded theory moreover the approach provides the basis for the unification of traditional textual language technologies and visual language compiler technologies
program analysis is the heart of modern compilers most control flow analyses are reduced to the problem of finding fixed point in certain transition system and such fixed point is commonly computed through an iterative procedure that repeats tracing until convergencethis paper proposes new method to analyze programs through recursive graph traversals instead of iterative procedures based on the fact that most programs without spaghetti goto have well structured control flow graphs graphs with bounded tree width our main techniques are an algebraic construction of control flow graph called sp term which enables control flow analysis to be defined in natural recursive form and the optimization theorem which enables us to compute optimal solution by dynamic programmingwe illustrate our method with two examples dead code detection and register allocation different from the traditional standard iterative solution our dead code detection is described as simple combination of bottom up and top down traversals on sp term register allocation is more interesting as it further requires optimality of the result we show how the optimization theorem on sp terms works to find an optimal register allocation as certain dynamic programming
data dominated signal processing applications are typically described using large and multi dimensional arrays and loop nests the order of production and consumption of array elements in these loop nests has huge impact on the amount of memory required during execution this is essential since the size and complexity of the memory hierarchy is the dominating factor for power performance and chip size in these applications this paper presents number of guiding principles for the ordering of the dimensions in the loop nests they enable the designer or design tools to find the optimal ordering of loop nest dimensions for individual data dependencies in the code we prove the validity of the guiding principles when no prior restrictions are given regarding fixation of dimensions if some dimensions are already fixed at given nest levels this is taken into account when fixing the remaining dimensions in most cases an optimal ordering is found for this situation as well the guiding principles can be used in the early design phases in order to enable minimization of the memory requirement through in place mapping we use real life examples to show how they can be applied to reach cost optimized end product the results show orders of magnitude improvement in memory requirement compared to using the declared array sizes and similar penalties for choosing the suboptimal ordering of loops when in place mapping is exploited
in this paper we depart from tcp probing tsaoussidis and badr and propose an experimental transport protocol that achieves energy and throughput performance gains in mixed wired and wireless environments our approach decouples error recovery from contention estimation and focuses on the way these two mechanisms can feed the probing decision process and ii implement the protocol strategy by shaping traffic according to detected conditions we use validation mechanism that uncovers previous possibly wrong estimations our analysis matches well our simulation results that are very promising
the medial axis is classical representation of digital objects widely used in many applications however such set of balls may not be optimal subsets of the medial axis may exist without changing the reversivility of the input shape representation in this article we first prove that finding minimum medial axis is an np hard problem for the euclidean distance then we compare two algorithms which compute an approximation of the minimum medial axis one of them providing bounded approximation results
we present graph based intermediate representation ir with simple semantics and low memory cost implementation the ir uses directed graph with labeled vertices and ordered inputs but unordered outputs vertices are labeled with opcodes edges are unlabeled we represent the cfg and basic blocks with the same vertex and edge structures each opcode is defined by class that encapsulates opcode specific data and behavior we use inheritance to abstract common opcode behavior allowing new opcodes to be easily defined from old ones the resulting ir is simple fast and easy to use
the emergence of location aware services calls for new real time spatio temporal query processing algorithms that deal with large numbers of mobile objects and queries online query response is an important characterization of location aware services delay in the answer to query gives invalid and obsolete results simply because moving objects can change their locations before the query responds to handle large numbers of spatio temporal queries efficiently we propose the idea of sharing as means to achieve scalability in this paper we introduce several types of sharing in the context of continuous spatio temporal queries examples of sharing in the context of real time spatio temporal database systems include sharing the execution sharing the underlying space sharing the sliding time windows and sharing the objects of interest we demonstrate how sharing can be integrated into query predicates eg selection and spatial join processing the goal of this paper is to outline research directions and approaches that will lead to scalable and efficient location aware services
we discuss information retrieval methods that aim at serving diverse stream of user queries such as those submitted to commercial search engines we propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions we formulate the problem as multi task learning problem using risk minimization framework in particular we show how to calibrate the empirical risk to incorporate query difference in terms of introducing nuisance parameters in the statistical models and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameters we work out the details for both and regularization cases and provide convergence analysis for the alternating optimization method for the special case when the retrieval functions belong to reproducing kernel hilbert space we illustrate the effectiveness of the proposed methods using modeling data extracted from commercial search engine we also point out how the current framework can be extended in future research
the current research progress and the existing problems of uncertain or imprecise knowledge representation and reasoning in description logics are analyzed in this paper approximate concepts are introduced to description logics based on rough set theory and kind of new rough description logic rdl rough description logic based on approximate concepts is proposed based on approximate concepts the syntax semantics and properties of the rdl are given it is proved that the approximate concept satisfiability definitely satisfiability and possibly satisfiability reasoning problem and approximate concepts rough subsumption reasoning problem wrt rough tbox in rdl may be reduced to the concept satisfiability reasoning problem in almost standard alc the description logic that provides the boolean concept constructors plus the existential and universal restriction constructors the works of this paper provide logic foundations for approximate ontologies and theoretical foundations for reasoning algorithms of more expressive rough description logics including approximate concepts number restrictions nominals inverse roles and role hierarchies
the main contribution of this paper is compiler based cache topology aware code optimization scheme for emerging multicore systems this scheme distributes the iterations of loop to be executed in parallel across the cores of target multicore machine and schedules the iterations assigned to each core our goal is to improve the utilization of the on chip multi layer cache hierarchy and to maximize overall application performance we evaluate our cache topology aware approach using set of twelve applications and three different commercial multicore machines in addition to study some of our experimental parameters in detail and to explore future multicore machines with higher core counts and deeper on chip cache hierarchies we also conduct simulation based study the results collected from our experiments with three intel multicore machines show that the proposed compiler based approach is very effective in enhancing performance in addition our simulation results indicate that optimizing for the on chip cache hierarchy will be even more important in future multicores with increasing numbers of cores and cache levels
design patterns have become widely acknowledged software engineering practice and therefore have been incorporated in the curricula of most computer science departments this paper presents an observational study on students ability to understand and apply design patterns within the context of postgraduate software engineering course students had to deliver two versions of software system one without and one with design patterns the former served as poorly designed system suffering from architectural problems while the latter served as an improved system where design problems had been solved by appropriate patterns the experiment allowed the quantitative evaluation of students preference to patterns moreover it was possible to assess students ability in relating design problems with patterns and interpreting the impact of patterns on software metrics the overall goal was to empirically identify ways in which course on design patterns could be improved
biometric based personal authentication is an effective method for automatically recognizing with high confidence person's identity by observing that the texture pattern produced by bending the finger knuckle is highly distinctive in this paper we present new biometric authentication system using finger knuckle print fkp imaging specific data acquisition device is constructed to capture the fkp images and then an efficient fkp recognition algorithm is presented to process the acquired data in real time the local convex direction map of the fkp image is extracted based on which local coordinate system is established to align the images and region of interest is cropped for feature extraction for matching two fkps feature extraction scheme which combines orientation and magnitude information extracted by gabor filtering is proposed an fkp database which consists of images from different fingers is established to verify the efficacy of the proposed system and promising results are obtained compared with the other existing finger back surface based biometric systems the proposed fkp system achieves much higher recognition rate and it works in real time it provides practical solution to finger back surface based biometric systems and has great potentials for commercial applications
whereas the remote procedure call rpc abstraction including its derivates such as remote method invocation has proven to be an adequate programming paradigm for client server applications over lans type based publish subscribe tps is an appealing candidate programming abstraction for decoupled and completely decentralized applications that run over large scale and mobile networks tps enforces type safety and encapsulation just like rpc while providing decoupling and scalability properties unlike rpc two tps implementations in java demonstrate this approach's potential the first is seminal approach relying on specific primitives added to the java language the second is library implementation based on more general recent java mechanisms avoiding any specific compilation
in this paper we present task assignment policy suited to environments such as high volume web serving clusters where local centralised dispatchers are utilised to distribute tasks amongst back end hosts offering mirrored services with negligible cost work conserving migration available between hosts the taptf wc task assignment based on prioritising traffic flows with work conserving migration policy was specifically created to exploit such environments as such taptf wc exhibits consistently good performance over wide range of task distribution scenarios due to its flexible nature spreading the work over multiple hosts when prudent and separating short task flows from large task flows via the use of dual queues tasks are migrated in work conserving manner reducing the penalty associated with task migration found in many existing policies such as tags and taptf which restart tasks upon migration we find that the taptf wc policy is well suited for load distribution under wide range of different workloads in environments where task sizes are not known priori and negligible cost work conserving migration is available
previous research has shown that rotation and orientation of items plays three major roles during collaboration comprehension coordination and communication based on these roles of orientation and advice from kinesiology research we have designed the rotate'n translate rnt interaction mechanism which provides integrated control of rotation and translation using only single touch point for input we present an empirical evaluation comparing rnt to common rotation mechanism that separates control of rotation and translation results of this study indicate rnt is more efficient than the separate mechanism and better supports the comprehension coordination and communication roles of orientation
the integration of knowledge from multiple sources is an important aspect in several areas such as data warehousing database integration automated reasoning systems active reactive databases and others thus central topic in databases is the construction of integration systems designed for retrieving and querying uniform data stored in multiple information sources this chapter illustrates recent techniques for computing repairs as well as consistent answers over inconsistent databases often databases may be inconsistent with respect to set of integrity constraints that is one or more integrity constraints are not satisfied most of the techniques for computing repairs and queries over inconsistent databases work for restricted cases and only recently there have been proposals to consider more general constraints in this chapter we give an informal description of the main techniques proposed in the literature
although graphical user interfaces guis constitute large part of the software being developed today and are typically created using rapid prototyping there are no effective regression testing techniques for guis the needs of gui regression testing differ from those of traditional software when the structure of gui is modified test cases from the original gui are either reusable or unusable on the modified gui since gui test case generation is expensive our goal is to make the unusable test cases usable the idea of reusing these unusable aka obsolete test cases has not been explored before in this paper we show that for guis the unusability of large number of test cases is serious problem we present novel gui regression testing technique that first automatically determines the usable and unusable test cases from test suite after gui modification it then determines which of the unusable test cases can be repaired so they can execute on the modified gui the last step is to repair the test cases our technique is integrated into gui testing framework that given test case automatically executes it on the gui we implemented our regression testing technique and demonstrate for two case studies that our approach is effective in that many of the test cases can be repaired and is practical in terms of its time performance
existing methods for white balancing photographs tend to rely on skilled interaction from the user which is prohibitive for most amateur photographers we propose minimal interaction system for white balancing photographs that contain humans many of the pictures taken by amateur photographers fall into this category our system matches user selected patch of skin in photograph to an entry in skin reflectance function database the estimate of the illuminant that emerges from the skin matching can be used to white balance the photograph allowing users to compensate for biased illumination in an image with single click we compare the quality of our results to output from three other low interaction methods including commercial approaches such as google picasa's one click relighting whitepoint based algorithm and ebner's localized gray world algorithm the comparisons indicate that our approach offers several advantages for amateur photographers
most prolog implementations are implemented in low level languages such as and are based on variation of the wam instruction set which enhances their performance but makes them hard to write in addition many of the more dynamic features of prolog like assert despite their popularity are not well supported we present high level continuation based prolog interpreter based on the pypy project the pypy project makes it possible to easily and efficiently implement dynamic languages it provides tools that automatically generate just in time compiler for given interpreter of the target language by using partial evaluation techniques the resulting prolog implementation is surprisingly efficient it clearly outperforms existing interpreters of prolog in high level languages such as java moreover on some benchmarks our system outperforms state of the art wam based prolog implementations our paper aims to show that declarative languages such as prolog can indeed benefit from having just in time compiler and that pypy can form the basis for implementing programming languages other than python
abstract prefetching is an important technique to reduce the average web access latency existing prefetching methods are based mostly on url graphs they use the graphical nature of http links to determine the possible paths through hypertext system although the url graph based approaches are effective in prefetching of frequently accessed documents few of them can prefetch those urls that are rarely visited this paper presents keyword based semantic prefetching approach to overcome the limitation it predicts future requests based on semantic preferences of past retrieved web documents we apply this technique to internet news services and implement client side personalized prefetching system newsagent the system exploits semantic preferences by analyzing keywords in url anchor text of previously accessed documents in different news categories it employs neural network model over the keyword set to predict future requests the system features self learning capability and good adaptability to the change of client surfing interest newsagent does not exploit keyword synonymy for conservativeness in prefetching however it alleviates the impact of keyword polysemy by taking into account server provided categorical information in decision making and hence captures more semantic knowledge than term document literal matching methods experimental results from daily browsing of abc news cnn and msnbc news sites for period of three months show an achievement of up to percent hit ratio due to prefetching
this paper studies the problem of pruning an ensemble of classifiers from reinforcement learning perspective it contributes new pruning approach that uses the learning algorithm in order to approximate an optimal policy of choosing whether to include or exclude each classifier from the ensemble extensive experimental comparisons of the proposed approach against state of the art pruning and combination methods show very promising results additionally we present an extension that allows the improvement of the solutions returned by the proposed approach over time which is very useful in certain performance critical domains
we introduce modular framework for distributed abstract argumentation where the argumentation context that is information about preferences among arguments values validity reasoning mode skeptical vs credulous and even the chosen semantics can be explicitly represented the framework consists of collection of abstract argument systems connected via mediators each mediator integrates information coming from connected argument systems thereby handling conflicts within this information and provides the context used in particular argumentation module the framework can be used in different directions eg for hierarchic argumentation as typically found in legal reasoning or to model group argumentation processes
in this paper is presented an improved model of ambient intelligent based on non traditional grids it is studied the fault tolerance of the proposed model taking into account the redundancy at link level
we propose to exploit three valued abstraction to stochastic systems in compositional way this combines the strengths of an aggressive state based abstraction technique with compositional modeling applying this principle to interactive markov chains yields abstract models that combine interval markov chains and modal transition systems in natural and orthogonal way we prove the correctness of our technique for parallel and symmetric composition and show that it yields lower bounds for minimal and upper bounds for maximal timed reachability probabilities
innovation in the fields of wireless data communications mobile devices and biosensor technology enables the development of new types of monitoring systems that provide people with assistance anywhere and at any time in this paper we present an architecture useful to build those kind of systems that monitor data streams generated by biological sensors attached to mobile users we pay special attention to three aspects related to the system efficiency selection of the optimal granularity that is the selection of the size of the input data stream package that has to be acquired in order to start new processing cycle the possible use of compression techniques to store and send the acquired input data stream and finally the performance of local analysis versus remote one moreover we introduce two particular real systems to illustrate the suitability and applicability of our proposal an anywhere and at any time monitoring system of heart arrhythmias and an apnea monitoring system
to carry out work assignments small groups distributed within larger enterprise often need to share documents among themselves while shielding those documents from others eyes in this situation users need an indexing facility that can quickly locate relevant documents that they are allowed to access without leaking information about the remaining documents imposing large management burden as users groups and documents evolve or requiring users to agree on central completely trusted authority to address this problem we propose the concept of confidentiality which captures the degree of information leakage from an index about the terms contained in inaccessible documents then we propose the confidential zerber indexing facility for sensitive documents which uses secret splitting and term merging to provide tunable limits on information leakage even under statistical attacks requires only limited trust in central indexing authority and is extremely easy to use and administer experiments with real world data show that zerber offers excellent performance for index insertions and lookups while requiring only modest amount of storage space and network bandwidth
this research aims to study the development of augmented reality of rare books or manuscripts of special collections in the libraries augmented reality has the ability to enhance users perception of and interaction with the real world libraries has to ensure that this special collection is well handled as these rare books and manuscripts are priceless as they represent the inheritance of each nation the use of augmented reality will be able to model these valuable manuscripts and rare books and appear as augmented reality to ensure that the collection can be better maintained users will be able to open the augmented rare book and flip the pages as well as read the contents of the rare books and manuscripts using the peripheral equipment such as the hmd or the marker the ar rare bm developed is modeled as an augmented reality that allows users to put the augmented rare book on his palm or table and manipulate it while reading users can also leave bookmark in the ar rare bm after reading so that they can read their favourite sections again at later date
requirements related scenarios capture typical examples of system behaviors through sequences of desired interactions between the software to be and its environment their concrete narrative style of expression makes them very effective for eliciting software requirements and for validating behavior models however scenarios raise coverage problems as they only capture partial histories of interaction among system component instances moreover they often leave the actual requirements implicit numerous efforts have therefore been made recently to synthesize requirements or behavior models inductively from scenarios two problems arise from those efforts on the one hand the scenarios must be complemented with additional input such as state assertions along episodes or flowcharts on such episodes this makes such techniques difficult to use by the nonexpert end users who provide the scenarios on the other hand the generated state machines may be hard to understand as their nodes generally convey no domain specific properties their validation by analysts complementary to model checking and animation by tools may therefore be quite difficult this paper describes tool supported techniques that overcome those two problems our tool generates labeled transition system lts for each system component from simple forms of message sequence charts msc taken as examples or counterexamples of desired behavior no additional input is required global lts for the entire system is synthesized first this lts covers all scenario examples and excludes all counterexamples it is inductively generated through an interactive procedure that extends known learning techniques for grammar induction the procedure is incremental on training examples it interactively produces additional scenarios that the end user has to classify as examples or counterexamples of desired behavior the lts synthesis procedure may thus also be used independently for requirements elicitation through scenario questions generated by the tool the synthesized system lts is then projected on local lts for each system component for model validation by analysts the tool generates state invariants that decorate the nodes of the local lts
some tasks in dataspace loose collection of heterogeneous data sources require integration of fine grained data from diverse sources this work is often done by end users knowledgeable about the domain who copy and paste data into spreadsheet or other existing application inspired by this kind of work in this paper we define data curation setting characterized by data that are explicitly selected copied and then pasted into target dataset where they can be confirmed or replaced rows and columns in the target may also be combined for example when redundant each of these actions is an integration decision often of high quality that when taken together comprise the provenance of data value in the target in this paper we define conceptual model for data and provenance for these user actions and we show how questions about data provenance can be answered we note that our model can be used in automated data curation as well as in setting with the manual activity we emphasize in our examples
in this paper we present novel algorithm for reconstructing scenes from set of images the user defines set of polygonal regions with corresponding labels in each image using familiar photo editing tools our reconstruction algorithm computes the model with maximum volume that is consistent with the set of regions in the input images the algorithm is fast uses only intersection operations and directly computes polygonal model we implemented user assisted system for scene reconstruction and show results on scenes that are difficult or impossible to reconstruct with other methods
in this paper we present the infocious web search engine our goal in creating infocious is to improve the way people find information on the web by resolving ambiguities present in natural language text this is achieved by performing linguistic analysis on the content of the web pages we index which is departure from existing web search engines that return results mainly based on keyword matching this additional step of linguistic processing gives infocious two main advantages first infocious gains deeper understanding of the content of web pages so it can better match users queries with indexed documents and therefore can improve relevancy of the returned results second based on its linguistic processing infocious can organize and present the results to the user in more intuitive ways in this paper we present the linguistic processing technologies that we incorporated in infocious and how they are applied in helping users find information on the web more efficiently we discuss the various components in the architecture of infocious and how each of them benefits from the added linguistic processing finally we experimentally evaluate the performance of component which leverages linguistic information in order to categorize web pages
the problem of building scalable shared memory multiprocessor can be reduced to that of building scalable memory hierarchy assuming interprocessor communication is handled by the memory system in this paper we describe the vmp mc design distributed parallel multi computer based on the vmp multiprocessor design that is intended to provide set of building blocks for configuring machines from one to several thousand processors vmp mc uses memory hierarchy based on shared caches ranging from on chip caches to board level caches connected by busses to at the bottom high speed fiber optic ring in addition to describing the building block components of this architecture we identify the key performance issues associated with the design and provide performance evaluation of these issues using trace drive simulation and measurements from the vmp this work was sponsored in part by the defense advanced research projects agency under contract
we propose in this paper very fast feature selection technique based on conditional mutual information by picking features which maximize their mutual information with the class to predict conditional to any feature already picked it ensures the selection of features which are both individually informative and two by two weakly dependant we show that this feature selection method outperforms other classical algorithms and that naive bayesian classifier built with features selected that way achieves error rates similar to those of state of the art methods such as boosting or svms the implementation we propose selects features among based on training set of examples in tenth of second on standard ghz pc
this paper examines the usability issues involved in ticketless travelling with an airport train the main contribution of this paper is that it describes actual use situations in detail we show how users intentions are difficult to anticipate unless in explicit communication eg with people whose job it is to help out with using the system being conspicuously assisted however only aggravates situation where users usually prefer anonymity given private in public type of design users had little chance of learning from watching others moreover users were quickly annoyed when they struggled with the machine they seemed to treat it as an agent for the provider rather than an assistant or tool for themselves at the end of this paper we outline and illustrate some new design ideas which we think ought to be considered for future designs of it in public spaces
heterogeneous architectures that integrate mix of big and small cores are very attractive because they can achieve high single threaded performance while enabling high performance thread level parallelism with lower energy costs despite their benefits they pose significant challenges to the operating system software thread scheduling is one of the most critical challenges in this paper we propose bias scheduling for heterogeneous systems with cores that have different microarchitectures and performancewe identify key metrics that characterize an application bias namely the core type that best suits its resource needs by dynamically monitoring application bias the operating system is able to match threads to the core type that can maximize system throughput bias scheduling takes advantage of this by influencing the existing scheduler to select the core type that bests suits the application when performing load balancing operations bias scheduling can be implemented on top of most existing schedulers since its impact is limited to changes in the load balancing code in particular we implemented it over the linux scheduler on real system that models microarchitectural differences accurately and found that it can improve system performance significantly and in proportion to the application bias diversity present in the workload unlike previous work bias scheduling does not require sampling of cpi on all core types or offline profiling we also expose the limits of dynamic voltage frequency scaling as an evaluation vehicle for heterogeneous systems
simulation of an application is popular and reliable approach to find the optimal configuration of level one cache memory for an application specific embedded system processor however long simulation time is one of the main disadvantages of simulation based approaches in this paper we propose new and fast simulation method super set simulator susesim while previous methods use top down searching strategy susesim utilizes bottom up search strategy along with new elaborate data structure to reduce the search space to determine cache hit or miss susesim can simulate hundreds of cache configurations simultaneously by reading an application's memory request trace just once total number of cache hits and misses are accurately recorded depending on different cache block sizes and benchmark applications susesim can reduce the number of tags to be checked by up to compared to the existing fastest simulation approach the crcb algorithm with the help of faster search and an easy to maintain data structure susesim can be up to faster in simulating memory requests compared to the crcb algorithm
we address the development of normalization theory for object oriented data models that have common features to support objects we first provide an extension of functional dependencies to cope with the richer semantics of relationships between objects called path dependency local dependency and global dependency constraints using these dependency constraints we provide normal forms for object oriented data models based on the notions of user interpretation user specified dependency constraints and object model in constrast to conventional data models in which normalized object has unique interpretation in object oriented data models an object may have many multiple interpretations that form the model for that object an object will then be in normal form if and only if the user's interpretation is derivable from the model of the object our normalization process is by nature iiterative in which objects are restructured until their models reflect the user's interpretation
in this paper we propose an efficient text classification method using term projection firstly we use modified statistic to project terms into predefined categories which is more efficient compared to other clustering methods afterwards we utilize the generated clusters as features to represent the documents the classification is then performed in rule based manner or via svm experiment results show that our modified statistic feature selection method outperforms traditional statistic especially at lower dimensionalities and our method is also more efficient than latent semantic analysis lsa on homogeneous dataset meanwhile we can reduce the feature dimensionality by three orders of magnitude to save training and testing cost and maintain comparable accuracy moreover we could use small training set to gain an approximately improvement on heterogeneous dataset as compared to traditional method which indicates that our method has better generalization capability
rough sets are widely used in feature subset selection and attribute reduction in most of the existing algorithms the dependency function is employed to evaluate the quality of feature subset the disadvantages of using dependency are discussed in this paper and the problem of forward greedy search algorithm based on dependency is presented we introduce the consistency measure to deal with the problems the relationship between dependency and consistency is analyzed it is shown that consistency measure can reflects not only the size of decision positive region like dependency but also the sample distribution in the boundary region therefore it can more finely describe the distinguishing power of an attribute set based on consistency we redefine the redundancy and reduct of decision system we construct forward greedy search algorithm to find reducts based on consistency what's more we employ cross validation to test the selected features and reduce the overfitting features in reduct the experimental results with uci data show that the proposed algorithm is effective and efficient
aligning structures often referred to as docking or registration is frequently required in fields such as computer science robotics and structural biology the task of aligning the structures is usually automated but due to noise and imprecision the user often needs to evaluate the results before final decision can be made the solutions involved are of multidimensional nature and normally densely populated therefore some form of visualization is necessary especially if users want to achieve higher level understanding such as solution symmetry or clustering from the data we have developed system that provides two views of the data one view places focus on the orientation of the solutions and the other focuses on translations solutions within the views are crosslinked using various visual cues users are also able to apply various filters intelligently reducing the solution set we applied the visualization to data generated by the automated cryo em process of docking molecular structures into electron density maps current systems in this field only allow for visual representation of single solution or numerical list of the data we evaluated the system through multi phase user study and found that the users were able to gain better high level understanding of the data even in cases of relatively small solution sets
in collaborative software development projects tasks are often used as mechanism to coordinate and track shared development work modern development environments provide explicit support for task management where tasks are typically organized and managed through predefined categories although there have been many studies that analyze data available from task management systems there has been relatively little work on the design of task management tools in this paper we explore how tagging with freely assigned keywords provides developers with lightweight mechanism to further categorize and annotate development tasks we investigate how tags that are frequently used over long period of time reveal the need for additional predefined categories of keywords in task management tool support finally we suggest future work to explore how integrated lightweight tool features in development environment may improve software development practices
let be set of points in dimensional lp metric space let epsilon and let be any real number an bounded leg path from to is an ordered set of points which connects to such that the leg between any two consecutive points in the set is at most the minimal path among all these paths is the bounded leg shortest path from to in the bounded leg shortest path stblsp problem we are given two points and and real number and are required to compute an bounded leg shortest path from to in the all pairs bounded leg shortest path apblsp problem we are required to build data structure that given any two query points from and any real number outputs the length of the bounded leg shortest path distance query or the path itself path query in this paper present first an algorithm for the apblsp problem in any lp metric which for any fixed epsilon computes in log middot epsilon time data structure which approximates any bounded leg shortest path within multiplicative error of epsilon it requires nlog space and distance queries are answered in log log time this improves on an algorithm with running time of given by bose et al in we present also an algorithm for the stblsp problem that given isin and real number computes in middot polylog the exact bounded shortest path from to this algorithm works in and infin metrics in the euclidean metric we also obtain an exact algorithm but with running time of epsilon for any epsilon we end by showing that for any weighted directed graph there is data structure of size nlog which is capable of answering path queries with multiplicative error of epsilon in log log ell time where ell is the length of the reported path our results improve upon the results given by bose et al our algorithms incorporate several new ideas along with an interesting observation made on geometric spanners which is of an independent interest
we propose processor virtualization architecture virtus to provide dedicated domain for preinstalled applications and virtualized domains for downloaded native applications with it security oriented next generation mobile terminals can provide any number of domains for native applications virtus features three new technologies namely vmm asymmetrization dynamic interdomain communication idc and virtualization assist logic and it is first in the world to virtualize an arm based multiprocessor evaluations have shown that vmm asymmetrization results in significantly less performance degradation and loc increase than do other vmms further dynamic idc overhead is low enough and virtualization assist logic can be implemented in sufficiently small area
we present minimum bayes risk mbr decoding over translation lattices that compactly encode huge number of translation hypotheses we describe conditions on the loss function that will enable efficient implementation of mbr decoders on lattices we introduce an approximation to the bleu score papineni et al that satisfies these conditions the mbr decoding under this approximate bleu is realized using weighted finite state automata our experiments show that the lattice mbr decoder yields moderate consistent gains in translation performance over best mbr decoding on arabic to english chinese to english and english to chinese translation tasks we conduct range of experiments to understand why lattice mbr improves upon best mbr and study the impact of various parameters on mbr performance
concurrent programs are notorious for containing errors that are difficult to reproduce and diagnose two common kinds of concurrency errors are data races and atomicity violations informally atomicity means that executing methods concurrently is equivalent to executing them serially several static and dynamic run time analysis techniques exist to detect potential races and atomicity violations run time checking may miss errors in unexecuted code and incurs significant run time overhead on the other hand run time checking generally produces fewer false alarms than static analysis this is significant practical advantage since diagnosing all of the warnings from static analysis of large codebases may be prohibitively expensivethis paper explores the use of static analysis to significantly decrease the overhead of run time checking our approach is based on type system for analyzing data races and atomicity type discovery algorithm is used to obtain types for as much of the program as possible complete type inference for this type system is np hard and parts of the program might be untypable warnings from the typechecker are used to identify parts of the program from which run time checking can safely be omitted the approach is completely automatic scalable to very large programs and significantly reduces the overhead of run time checking for data races and atomicity violations
there are three common challenges in real world classification applications ie how to use domain knowledge how to resist noisy samples and how to use unlabeled data to address these problems novel classification framework called mutually beneficial learning mbl is proposed in this paper mbl integrates two learning steps together in the first step the underlying local structures of feature space are discovered through learning process the result provides necessary capability to resist noisy samples and prepare better input for the second step where consecutive classification process is further applied to the result these two steps are iteratively performed until stop condition is met different from traditional classifiers the output of mbl consists of two components common classifier and set of rules corresponding to local structures in application test sample is first matched with the discovered rules if matched rule is found the label of the rule is assigned to the sample otherwise the common classifier will be utilized to classify the sample we applied the mbl to online news classification and our experimental results showed that mbl is significantly better than na√Øve bayes and svm even when the data is noisy or partially labeled
in state based testing it is common to include verdicts within test cases the result of the test case being the verdict reached by the test run in addition approaches that reason about test effectiveness or produce tests that are guaranteed to find certain classes of faults are often based on either fault domain or set of test hypotheses this article considers how the presence of fault domain or test hypotheses affects our notion of test verdict the analysis reveals the need for new verdicts that provide more information than the current verdicts and for verdict functions that return verdict based on set of test runs rather than single test run the concepts are illustrated in the contexts of testing from nondeterministic finite state machine and the testing of datatype specified using an algebraic specification language but are potentially relevant whenever fault domains or test hypotheses are used
program dependence information is useful for variety of software testing and maintenance tasks properly defined control and data dependencies can be used to identify semantic dependencies to function effectively on whole programs tools that utilize dependence information require information about interprocedural dependencies dependencies that exist because of interactions among procedures many techniques for computing data and control dependencies exist however in our search of the literature we find only one attempt to define and compute interprocedural control dependencies unfortunately that approach can omit important control dependencies and incorrectly identifies control dependencies for large class of programs this paper presents definition of interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence an efficient algorithm for calculating interprocedural control dependencies and empirical results obtained by our implementation of the algorithm
there has been much recent interest in on line data mining existing mining algorithms designed for stored data are either not applicable or not effective on data streams where real time response is often needed and data characteristics change frequently therefore researchers have been focusing on designing new and improved algorithms for on line mining tasks such as classification clustering frequent itemsets mining pattern matching etc relatively little attention has been paid to designing dsmss which facilitate and integrate the task of mining data streams ie stream systems that provide inductive functionalities analogous to those provided by weka and ms ole db for stored data in this paper we propose the notion of an inductive dsms system that besides providing rich library of inter operable functions to support the whole mining process also supports the essentials of dsms including optimization of continuous queries load shedding synoptic constructs and non stop computing ease of use and extensibility are additional desiderata for the proposed inductive dsms we first review the many challenges involved in realizing such system and then present our approach of extending the stream mill dsms toward that goal our system features powerful query language where mining methods are expressed via aggregates for generic streams and arbitrary windows ii library of fast and light mining algorithms and iii an architecture that makes it easy to customize and extend existing mining methods and introduce new ones
we study selectivity estimation techniques for set similarity queries wide variety of similarity measures for sets have been proposed in the past in this work we concentrate on the class of weighted similarity measures eg tf idf and bm cosine similarity and variants and design selectivity estimators based on priori constructed samples first we study the pitfalls associated with straightforward applications of random sampling and argue that care needs to be taken in how the samples are constructed uniform random sampling yields very low accuracy while query sensitive realtime sampling is more expensive than exact solutions both in cpu and cost we show how to build robust samples priori based on existing synopses for distinct value estimation we prove the accuracy of our technique theoretically and verify its performance experimentally our algorithm is orders of magnitude faster than exact solutions and has very small space overhead
web applications support many of our daily activities but they often have security problems and their accessibility makes them easy to exploit in cross site scripting xss an attacker exploits the trust web client browser has for trusted server and executes injected script on the browser with the server's privileges in xss constituted the largest class of newly reported vulnerabilities making it the most prevalent class of attacks today web applications have xss vulnerabilities because the validation they perform on untrusted input does not suffice to prevent that input from invoking browser's javascript interpreter and this validation is particularly difficult to get right if it must admit some html mark up most existing approaches to finding xss vulnerabilities are taint based and assume input validation functions to be adequate so they either miss real vulnerabilities or report many false positives this paper presents static analysis for finding xss vulnerabilities that directly addresses weak or absent input validation our approach combines work on tainted information flow with string analysis proper input validation is difficult largely because of the many ways to invoke the javascript interpreter we face the same obstacle checking for vulnerabilities statically and we address it by formalizing policy based on the wc recommendation the firefox source code and online tutorials about closed source browsers we provide effective checking algorithms based on our policy we implement our approach and provide an extensive evaluation that finds both known and unknown vulnerabilities in real world web applications
an important but very expensive primitive operation of high dimensional databases is the nearest neighbor knn similarity join the operation combines each point of one dataset with its knns in the other dataset and it provides more meaningful query results than the range similarity join such an operation is useful for data mining and similarity search in this paper we propose novel knn join algorithm called the gorder or the ordering knn join method gorder is block nested loop join method that exploits sorting join scheduling and distance computation filtering and reduction to reduce both and cpu costs it sorts input datasets into the order and applied the scheduled block nested loop join on the ordered data the distance computation reduction is employed to further reduce cpu cost it is simple and yet efficient and handles high dimensional data efficiently extensive experiments on both synthetic cluster and real life datasets were conducted and the results illustrate that gorder is an efficient knn join method and outperforms existing methods by wide margin
social tagging systems have become increasingly popular for sharing and organizing web resources tag prediction is common feature of social tagging systems social tagging by nature is an incremental process meaning that once user has saved web page with tags the tagging system can provide more accurate predictions for the user based on user's incremental behaviors however existing tag prediction methods do not consider this important factor in which their training and test datasets are either split by fixed time stamp or randomly sampled from larger corpus in our temporal experiments we perform time sensitive sampling on an existing public dataset resulting in new scenario which is much closer to real world in this paper we address the problem of tag prediction by proposing probabilistic model for personalized tag prediction the model is bayesian approach and integrates three factors ego centric effect environmental effects and web page content two methods both intuitive calculation and learning optimization are provided for parameter estimation pure graphbased methods which may have significant constraints such as every user every item and every tag has to occur in at least posts cannot make prediction in most of real world cases while our model improves the measure by over compared to leading algorithm in our real world use case
we envision new better together mobile application paradigm where multiple mobile devices are placed in close proximity and study specific together viewing video application in which higher resolution video isplayed back across screens of two mobile devices placed side by side this new scenario imposes real time synchronous decoding and rendering requirements which are difficult to achieve because of the intrinsic complexity of video andthe resource constraints such as processing power and battery life of mobile devices we develop novel efficient collaborative half frame decoding schemeand design tightly coupled collaborative system architecture that aggregates resources of both devices to achieve the task we have implemented the system and conducted experimental evaluation results confirm that our proposed collaborative and resource aggregation techniques can achieve our vision of better together mobile experiences
exceptions in safety critical systems must be addressed during conceptual design and risk analysis we developed conceptual model of exceptions methodology for eliciting and modeling exceptions and templates for modeling them in an extension of the object process methodology opm system analysis and design methodology and language that uses single graphical model for describing systems including their timing exceptions which has been shown to be an effective modeling methodology using an antibiotics treatment guideline as case study we demonstrate the value of our approach in eliciting and modeling exceptions that occur in clinical care systems
we consider the problem of sparse interpolation of an approximate multivariate black box polynomial in floating point arithmetic that is both the inputs and outputs of the black box polynomial have some error and all numbers are represented in standard fixed precision floating point arithmetic by interpolating the black box evaluated at random primitive roots of unity we give efficient and numerically robust solutions we note the similarity between the exact ben or tiwari sparse interpolation algorithm and the classical prony's method for interpolating sum of exponential functions and exploit the generalized eigenvalue reformulation of prony's method we analyse the numerical stability of our algorithms and the sensitivity of the solutions as well as the expected conditioning achieved through randomization finally we demonstrate the effectiveness of our techniques in practice through numerical experiments and applications
wireless sensor networks consist of system of distributed sensors embedded in the physical world and promise to allow observation of previously unobservable phenomena since they are exposed to unpredictable environments sensor network applications must handle wide variety of faults software errors node and link failures and network partitions the code to manually detect and recover from faults crosscuts the entire application is tedious to implement correctly and efficiently and is fragile in the face of program modifications we investigate language support for modularly managing faults our insight is that such support can be naturally provided as an extension to existing macroprogramming systems for sensor networks in such system programmer describes sensor network application as centralized program compiler then produces equivalent node level programs we describe simple checkpoint api for macroprograms which can be automatically implemented in distributed fashion across the network we also describe declarative annotations that allow programmers to specify checkpointing strategies at higher level of abstraction we have implemented our approach in the kairos macroprogramming system experiments show it to improve application availability by an order of magnitude and incur low messaging overhead
many authors have proposed power management techniques for general purpose processors at the cost of degraded performance such as lower ipc or longer delay some proposals have focused on cache memories because they consume significant fraction of total microprocessor power we propose reconfigurable and adaptive cache microarchitecture based on field programmable technology that is intended to deliver high performance at low energy consumption in this paper we evaluate the performance and energy consumption of run time algorithm when used to manage field programmable data cache the adaptation strategy is based on two techniques learning process provides the best cache configuration for each program phase and recognition process detects program phase changes by using data working set signatures to activate low overhead reconfiguration mechanism our proposals achieve performance improvement and cache energy saving at the same time considering design scenario driven by performance constraints we show that processor execution time and cache energy consumption can be reduced on average by and compared to non adaptive high performance microarchitecture alternatively when energy saving is prioritized and considering non adaptive energy efficient microarchitecture as baseline cache energy and processor execution time are reduced on average by and respectively in addition to comparing to conventional microarchitectures we show that the proposed microarchitecture achieves better performance and more cache energy reduction than other configurable caches
this paper investigates how the vision of the semantic web can be carried over to the realm of email we introduce general notion of semantic email in which an email message consists of structured query or update coupled with corresponding explanatory text semantic email opens the door to wide range of automated email mediated applications with formally guaranteed properties in particular this paper introduces broad class of semantic email processes for example consider the process of sending an email to program committee asking who will attend the pc dinner automatically collecting the responses and tallying them up we define both logical and decision theoretic models where an email process is modeled as set of updates to data set on which we specify goals via certain constraints or utilities we then describe set of inference problems that arise while trying to satisfy these goals and analyze their computational tractability in particular we show that for the logical model it is possible to automatically infer which email responses are acceptable wrt set of constraints in polynomial time and for the decision theoretic model it is possible to compute the optimal message handling policy in polynomial time in addition we show how to automatically generate explanations for process's actions and identify cases where such explanations can be generated in polynomial time finally we discuss our publicly available implementation of semantic email and outline research challenges in this realm
exception handling in workflow management systems wfmss is very important problem since it is not possible to specify all possible outcomes and alternatives effective reuse of existing exception handlers can greatly help in dealing with workflow exceptions on the other hand cooperative support for user driven resolution of unexpected exceptions and workflow evolution at run time is vital for an adaptive wfms we have developed adome wfms via meta modeling approach as comprehensive framework in which the problem of workflow exception handling can be adequately addressed in this chapter we present an overview of exception handling in adome wfms with procedures for supporting the following reuse of exception handlers thorough and automated resolution of expected exceptions effective management of problem solving agents cooperative exception handling user driven computer supported resolution of unexpected exceptions and workflow evolution
constraint programming holds many promises for model driven software development mdsd up to now constraints have only started to appear in mdsd modeling languages but have not been properly reflected in model transformation this paper introduces constraint programming in model transformation shows how constraint programming integrates with qvt relations as pathway to wide spread use of our approach and describes the corresponding model transformation engine in particular the paper will illustrate the use of constraint programming for the specification of attribute values in target models and provide qualitative evaluation of the benefit drawn from constraints integrated with qvt relations
we investigate generalizations of the all subtrees dop approach to unsupervised parsing unsupervised dop models assign all possible binary trees to set of sentences and next use large random subset of all subtrees from these binary trees to compute the most probable parse trees we will test both relative frequency estimator for unsupervised dop and maximum likelihood estimator which is known to be statistically consistent we report state of the art results on english wsj german negra and chinese ctb data to the best of our knowledge this is the first paper which tests maximum likelihood estimator for dop on the wall street journal leading to the surprising result that an unsupervised parsing model beats widely used supervised model treebank pcfg
drawback of structured prediction methods is that parameter estimation requires repeated inference which is intractable for general structures in this paper we present an approximate training algorithm called piecewise training pw that divides the factors into tractable subgraphs which we call pieces that are trained independently piecewise training can be interpreted as approximating the exact likelihood using belief propagation and different ways of making this interpretation yield different insights into the method we also present an extension to piecewise training called piecewise pseudolikelihood pwpl designed for when variables have large cardinality on several real world natural language processing tasks piecewise training performs superior to besag's pseudolikelihood and sometimes comparably to exact maximum likelihood in addition pwpl performs similarly to pw and superior to standard pseudolikelihood but is five to ten times more computationally efficient than batch maximum likelihood training
this paper describes randomized algorithm for approximate counting that preserves the same modest memory requirements of log log bits per counter as the approximate counting algorithm introduced in the seminal paper of morris and in addition is characterized by lower expected number of memory accesses and ii lower standard error on more than percent of its counting range an exact analysis of the relevant statistical properties of the algorithm is carried out performance evaluation via simulations is also provided to validate the presented theory given its properties the presented algorithm is suitable as basic building block of data streaming applications having large number of simultaneous counters and or operating at very high speeds as such it is applicable to wide range of measurement and monitoring operations including performance monitoring of communication hardware measurements for optimization in large database systems and gathering statistics for data compression
path queries have been extensively used to query semistructured data such as the web and xml documents in this paper we introduce weighted path queries an extension of path queries enabling several classes of optimization problems such as the computation of shortest paths to be easily expressed weighted path queries are based on the notion of weighted regular expression ie regular expression whose symbols are associated to weight we characterize the problem of answering weighted path queries and provide an algorithm for computing their answer we also show how weighted path queries can be effectively embedded into query languages for xml data to express in simple and compact form several meaningful research problems
database security research aims to protect database from unintended activities such as authenticated misuse malicious attacks in recent years surviving dbms from an attack is becoming even more crucial because networks have become more open and the increasingly critical role that database servers are playing nowadays unlike the traditional database failure attack recovery mechanisms in this paper we propose light weight dynamic data damage tracking quarantine and recovery dtqr solution we built the dtqr scheme into the kernel of postgresql we comprehensively study this approach from few aspects eg system overhead impact of the intrusion detection system and the experimental results demonstrated that our dtqr can sustain an excellent data service while healing the database server when it is under malicious attack
this paper presents type based solution to the long standing problem of object initialization constructors the conventional mechanism for object initialization have semantics that are surprising to programmers and that lead to bugs they also contribute to the problem of null pointer exceptions which make software less reliable masked types are new type state mechanism that explicitly tracks the initialization state of objects and prevents reading from uninitialized fields in the resulting language constructors are ordinary methods that operate on uninitialized objects and no special default value null is needed in the language initialization of cyclic data structures is achieved with the use of conditionally masked types masked types are modular and compatible with data abstraction the type system is presented in simplified object calculus and is proved to soundly prevent reading from uninitialized fields masked types have been implemented as an extension to java in which compilation simply erases extra type information experience using the extended language suggests that masked types work well on real code
dynamic peer group dpg applications such as video conferencing and multi user gaming environments have proliferated the internet and group key agreement protocols are critical in securing dpg communication identity based cryptosystems avoid certificate management complications inherent to traditional public key systems but introduce key escrow which is highly undesirable in most applications we design scalable technique to eliminate key escrow and develop distributed escrow less identity based group key agreement protocol for securing resource constrained dpg communication the proposed protocol is provably secure against passive adversaries resilient to active attacks scalable and more cost effective than any such protocol we provide detailed theoretical analysis to support the claims
we show that the accessibility problem the common descendant problem the termination problem and the uniform termination problem are undecidable for rules semi thue systems as corollary we obtain the undecidability of the post correspondence problem for rules
this paper introduces the concept of virtual access points vaps for wireless vehicular ad hoc networks vanets this new technique allows data dissemination among vehicles thus extending the reach of roadside access points to uncovered road areas each vehicle that receives message from an access point ap stores this message and rebroadcasts it into non covered areas this extends the network coverage for non time critical messages the vap role is transparent to the connected nodes and designed to avoid interference since each operates on bounded region outside any ap the experiments show the presented mechanism of store and forward at specific positions present gain in term of all the evaluated parameters
we show new camera based interaction solution where an ordinary camera can detect small optical tags from relatively large distance current optical tags such as barcodes must be read within short range and the codes occupy valuable physical space on products we present new low cost optical design so that the tags can be shrunk to mm visible diameter and unmodified ordinary cameras several meters away can be set up to decode the identity plus the relative distance and angle the design exploits the bokeh effect of ordinary cameras lenses which maps rays exiting from an out of focus scene point into disk like blur on the camera sensor this bokeh code or bokode is barcode design with simple lenslet over the pattern we show that code with mu features can be read using an off the shelf camera from distances of up to meters we use intelligent binary coding to estimate the relative distance and angle to the camera and show potential for applications in augmented reality and motion capture we analyze the constraints and performance of the optical system and discuss several plausible application scenarios
informal interactions an important subject of study in cscw are an essential resource in hospital work they are used as means to collaborate and to coordinate the way in which the work is performed as well as to locate and gather the artifacts and human resources necessary for patient care among others results from an observational study of work in public hospital show that significant amount of informal interactions happen face to face due to opportunistic encounters that is due to hospital work being mainly characterized by intense mobility task fragmentation collaboration and coordination this encouraged us to develop an architecture and system tool aimed at supporting mobile co located collaboration based on the findings of our study this paper presents set of design insights for developing collaborative applications that support co located interactions in hospital work as well as the implementation of these design insights in collaborative tool additionally we generalized the characteristic that must be fulfilled by tools that support mobile informal co located collaboration through the design of generic architecture that includes the characteristics of this type of tools
the multiple expected sources of traffic skewness in next generation sensornets ngsn will trigger the need for load balanced point to point routing protocols driven by this fact we present in this paper load balancing primitive namely traffic oblivious load balancing tolb to be used on top of any point to point routing protocol tolb obliviously load balances traffic by pushing the decision making responsibility to the source of any packet without depending on the energy status of the network sensors or on previously taken decisions for similar packets we present theoretical bounds on tolb's performance for special network types such as mesh networks additionally we ran simulations to evaluate tolb's performance on general networks our experimental results show the high benefit in terms of network lifetime and throughput of applying tolb on top of routing schemes to deal with various traffic skewness levels in different sensor deployment scenarios
in this paper we propose an object oriented model for designing hypermedia applications as the object oriented paradigm allows complex and user defined types nonconventional and nonatomic attributes we can take advantage of these capabilities not only for information modelling but also for providing alternative ways for accessing informationa query language is then presented it is based on an object oriented database system query language it combines features of object oriented databases queries and primitives for hypermedia navigation the language offers the possibility of querying both the application domain information and allowing the designers to obtain information about the schema of the applicationwe present some examples of the use of the object oriented model and the query language
this paper reports on study of professional web designers and developers we provide detailed characterization of their knowledge of fundamental programming concepts elicited through card sorting additionally we present qualitative findings regarding their motivation to learn new concepts and the learning strategies they employ we find high level of recognition of basic concepts but we identify number of concepts that they do not fully understand consider difficult to learn and use infrequently we also note that their learning process is motivated by work projects and often follows pattern of trial and error we conclude with implications for end user programming researchers
although researchers have begun to explicitly support end user programmers debugging by providing information to help them find bugs there is little research addressing the right content to communicate to these users the specific semantic content of these debugging communications matters because if the users are not actually seeking the information the system is providing they are not likely to attend to it this paper reports formative empirical study that sheds light on what end users actually want to know in the course of debugging spreadsheet given the availability of set of interactive visual testing and debugging features our results provide in sights into end user debuggers information gaps and further suggest opportunities to improve end user debugging systems support for the things end user debuggers actually want to know
debugging long running multithreaded programs is very challenging problem when using tracing based analyses since such programs are non deterministic reproducing the bug is non trivial and generating and inspecting traces for long running programs can be prohibitively expensive we propose framework in which to overcome the problem of bug reproducibility lightweight logging technique is used to log the events during the original execution when bug is encountered it is reproduced using the generated log and during the replay fine grained tracing technique is employed to collect control flow dependence traces that are then used to locate the root cause of the bug in this paper we address the key challenges resulting due to tracing that is the prohibitively high expense of collecting traces and the significant burden on the user who must examine the large amount of trace information to locate the bug in long running multithreaded program these challenges are addressed through execution reduction that realizes combination of logging and tracing such that traces collected contain only the execution information from those regions of threads that are relevant to the fault this approach is highly effective because we observe that for long running multithreaded programs many threads that execute are irrelevant to the fault hence these threads need not be replayed and traced when trying to reproduce the bug we develop novel lightweight scheme that identifies such threads by observing all the interthread data dependences and removes their execution footprint in the replay run in addition we identify regions of thread executions that need not be replayed or if they must be replayed we determine if they need not be traced following execution reduction the replayed execution takes lesser time to run and it produces much smaller trace than the original execution thus the cost of collecting traces and the effort of examining the traces to locate the fault are greatly reduced
current approaches to modeling the structure and semantics of video recordings restrict its reuse this is because these approaches are either too rigidly structured or too generally structured and so do not represent the structural and semantic regularities of classes of video recordings this paper proposes framework which tackles the problem of reuse by supporting the definition of wide range of models of video recordings and supporting reuse between them examples of the framework's use are presented and examined with respect to different kinds of reuse of video current research and the development of toolset to support the framework
adaptive programs compute with objects just like object oriented programs each task to be accomplished is specified by so called propagation pattern which traverses the receiver object the object traversal is recursive descent via the instance variables where information is collected or propagated along the way propagation pattern consists of name for the task succinct specification of the parts of the receiver object that should be traversed and code fragments to be executed when specific object types are encountered the propagation patterns need to be complemented by class graph which defines the detailed object structure the separation of structure and behavior yields degree of flexibility and understandability not present in traditional object oriented languages for example the class graph can be changed without changing the adaptive program at all we present an efficient implementation of adaptive programs given an adaptive program and class graph we generate an efficient object oriented program for example in moreover we prove the correctness of the core of this translation key assumption in the theorem is that the traversal specifications are consistent with the class graph we prove the soundness of proof system for conservatively checking consistency and we show how to implement it efficiently
we study online social networks in which relationships can be either positive indicating relations such as friendship or negative indicating relations such as opposition or antagonism such mix of positive and negative links arise in variety of online settings we study datasets from epinions slashdot and wikipedia we find that the signs of links in the underlying social networks can be predicted with high accuracy using models that generalize across this diverse range of sites these models provide insight into some of the fundamental principles that drive the formation of signed links in networks shedding light on theories of balance and status from social psychology they also suggest social computing applications by which the attitude of one user toward another can be estimated from evidence provided by their relationships with other members of the surrounding social network
memory based methods for collaborative filtering predict new ratings by averaging weighted ratings between respectively pairs of similar users or items in practice large number of ratings from similar users or similar items are not available due to the sparsity inherent to rating data consequently prediction quality can be poor this paper re formulates the memory based collaborative filtering problem in generative probabilistic framework treating individual user item ratings as predictors of missing ratings the final rating is estimated by fusing predictions from three sources predictions based on ratings of the same item by other users predictions based on different item ratings made by the same user and third ratings predicted based on data from other but similar users rating other but similar items existing user based and item based approaches correspond to the two simple cases of our framework the complete model is however more robust to data sparsity because the different types of ratings are used in concert while additional ratings from similar users towards similar items are employed as background model to smooth the predictions experiments demonstrate that the proposed methods are indeed more robust against data sparsity and give better recommendations
ontologies have proven to be powerful tool for many tasks such as natural language processing and information filtering and retrieval however their development is an error prone and expensive task one approach for this problem is to provide automatic or semi automatic support for ontology construction this work presents the probabilistic relational hierarchy extraction prehe technique an approach for extracting concept hierarchies from text that uses statistical relational learning and natural language processing for combining cues from many state of the art techniques markov logic network has been developed for this task and is described here preliminary evaluation of the proposed approach is also outlined
in color spatial retrieval technique the color information is integrated with the knowledge of the colors spatial distribution to facilitate content based image retrieval several techniques have been proposed in the literature but these works have been developed independently without much comparison in this paper we present an experimental evaluation of three color spatial retrieval techniques mdash the signature based technique the partition based algorithm and the cluster based method we implemented these techniques and compare them on their retrieval effectiveness and retrieval efficiency the experimental study is performed on an image database consisting of images with the proliferation of image retrieval mechanisms and the lack of extensive performance study the experimental results can serve as guidelines in selecting suitable technique and designing new technique
existing studies on time series are based on two categories of distance functions the first category consists of the lp norms they are metric distance functions but cannot support local time shifting the second category consists of distance functions which are capable of handling local time shifting but are nonmetric the first contribution of this paper is the proposal of new distance function which we call erp edit distance with real penalty representing marriage of norm and the edit distance erp can support local time shifting and is metric the second contribution of the paper is the development of pruning strategies for large time series databases given that erp is metric one way to prune is to apply the triangle inequality another way to prune is to develop lower bound on the erp distance we propose such lower bound which has the nice computational property that it can be efficiently indexed with standard tree moreover we show that these two ways of pruning can be used simultaneously for erp distances specifically the false positives obtained from the tree can be further minimized by applying the triangle inequality based on extensive experimentation with existing benchmarks and techniques we show that this combination delivers superb pruning power and search time performance and dominates all existing strategies
people are thirsty for medical information existing web search engines often cannot handle medical search well because they do not consider its special requirements often medical information searcher is uncertain about his exact questions and unfamiliar with medical terminology therefore he sometimes prefers to pose long queries describing his symptoms and situation in plain english and receive comprehensive relevant information from search results this paper presents medsearch specialized medical web search engine to address these challenges medsearch uses several key techniques to improve its usability and the quality of search results first it accepts queries of extended length and reforms long queries into shorter queries by extracting subset of important and representative words this not only significantly increases the query processing speed but also improves the quality of search results second it provides diversified search results lastly it suggests related medical phrases to help the user quickly digest search results and refine the query we evaluated medsearch using medical questions posted on medical discussion forums the results show that medsearch can handle various medical queries effectively and efficiently
information extraction by text segmentation iets applies to cases in which data values of interest are organized in implicit semi structured records available in textual sources eg postal addresses bibliographic information ads it is an important practical problem that has been frequently addressed in the recent literature in this paper we introduce ondux on demand unsupervised information extraction new unsupervised probabilistic approach for iets as other unsupervised iets approaches ondux relies on information available on pre existing data to associate segments in the input string with attributes of given domain unlike other approaches we rely on very effective matching strategies instead of explicit learning strategies the effectiveness of this matching strategy is also exploited to disambiguate the extraction of certain attributes through reinforcement step that explores sequencing and positioning of attribute values directly learned on demand from test data with no previous human driven training feature unique to ondux this assigns to ondux high degree of flexibility and results in superior effectiveness as demonstrated by the experimental evaluation we report with textual sources from different domains in which ondux is compared with state of art iets approach
this paper presents technique called register value prediction rvp which uses type of locality called register value reuse by predicting that an instruction will produce the value that is already stored in the destination register we eliminate the need for large value buffers to enable value prediction even without the large buffers register value prediction can be made as or more effective than last value prediction particularly with the aid of compiler management of values in the register fileboth static and dynamic register value prediction techniques are demonstrated to exploit register value reuse the former requiring minimal instruction set architecture changes and the latter requiring set of small confidence counters we show an average gain of with dynamic rvp and moderate compiler assistance on next generation processor and on wide processor
we continue to investigate the direct style transformation by extending it to programs requiring call with current continuation aka call cc the direct style ds and the continuation passing style cps transformations form galois connection this pair of functions has place in the programmer's toolbox mdash yet we are not aware of the existence of any other ds transformer starting from our ds transformer towards pure call by value functional terms scheme we extend it with counting analysis to detect non canonical occurrences of continuation the declaration of such continuation is translated into call cc and its application into the application of the corresponding first class continuation we also present staged versions of the ds and of the cps transformations where administrative reductions are separated from the actual translation and where the actual translations are carried out by local structure preserving rewriting rules these staged transformations are used to prove the galois continuation together the cps and the ds transformations enlarge the class of programs that can be manipulated on semantic basis we illustrate this point with partial evaluation by specializing scheme program with respect to static part of its input the program uses coroutines this illustration achieves first static coroutine is executed statically and its computational content is inlined in the residual program
this paper describes the results of research project aimed at implementing realistic embodied agent that can be animated in real time and is believable and expressive that is able to coherently communicate complex information through the combination and the tight synchronisation of verbal and nonverbal signals we describe in particular how we animate this agent that we called greta so as to enable her to manifest the affective states that are dynamically activated and de activated in her mind during the dialog with the user the system is made up of three tightly interrelated components representation of the agent mind this includes long and short term affective components personality and emotions and simulates how emotions are triggered and decay over time according to the agent's personality and to the context and how several emotions may overlap dynamic belief networks with weighting of goals is the formalism we employ to this purpose mark up language to denote the communicative meanings that may be associated with dialog moves performed by the agent translation of the agent's tagged move into face expression that combines appropriately the available channels gaze direction eyebrow shape head direction and movement etc the final output is facial model that respects the mpeg standard and uses mpeg facial animation parameters to produce facial expressions throughout the paper we illustrate the results obtained with an example of dialog in the domain of advice about eating disorders the paper concludes with an analysis of advantages of our cognitive model of emotion triggering and of the problems found in testing it although we did not yet complete formal evaluation of our system we briefly describe how we plan to assess the agent's believability in terms of consistency of its communicative behaviour
the increasing complexity of applications on handheld devices requires the development of rich new interaction methods specifically designed for resource limited mobile use contexts one appealingly convenient approach to this problem is to use device motions as input paradigm in which the currently dominant interaction metaphors are gesture recognition and visually mediated scrolling however neither is ideal the former suffers from fundamental problems in the learning and communication of gestural patterns while the latter requires continual visual monitoring of the mobile device task that is undesirable in many mobile contexts and also inherently in conflict with the act of moving device to control it this paper proposes an alternate approach gestural menu technique inspired by marking menus and designed specifically for the characteristics of motion input it uses rotations between targets occupying large portions of angular space and emphasizes kinesthetic eyes free interaction three evaluations are presented two featuring an abstract user interface ui and focusing on how user performance changes when the basic system parameters of number size and depth of targets are manipulated these studies show that version of the menu system containing commands yields optimal performance compares well against data from the previous literature and can be used effectively eyes free without graphical feedback the final study uses full graphical ui and untrained users to demonstrate that the system can be rapidly learnt together these three studies rigorously validate the system design and suggest promising new directions for handheld motion based uis
efficiently building and maintaining resilient regular graphs is important for many applications such graphs must be easy to build and maintain in the presence of node additions and deletions they must also have high resilience connectivity typically algorithms use offline techniques to build regular graphs with strict bounds on resilience and such techniques are not designed to maintain these properties in the presence of online additions deletions and failures on the other hand random regular graphs are easy to construct and maintain and provide good properties with high probability but without strict guarantees in this paper we introduce new class of graphs that we call resilient random regular graphs and present technique to create and maintain graphs the graphs meld the desirable properties of random regular graphs and regular graphs with strict structural properties they are efficient to create and maintain and additionally are highly connected ie node and edge connected in the worst case we present the graph building and maintenance techniques present proofs for graph connectedness and various properties of graphs we believe that graphs will be useful in many communication applications
this paper proposes novel interaction techniques for parallel search tasks the system displays multiple search results returned by search engine side by side for each search query the system enables the user to rerank search results using reranking algorithm based on vertical and horizontal propagation of his her intention method of recommending operations for specific keywords is also proposed supporting operations such as shift to parallel search with an alternate term upgrading or downgrading results in terms of specific viewpoint and so on applications for the proposed system are also discussed
an important correctness criterion for software running on embedded microcontrollers is stack safety guarantee that the call stack does not overflow our first contribution is method for statically guaranteeing stack safety of interrupt driven embedded software using an approach based on context sensitive dataflow analysis of object code we have implemented prototype stack analysis tool that targets software for atmel avr microcontrollers and tested it on embedded applications compiled from up to lines of we experimentally validate the accuracy of the tool which runs in under sec on the largest programs that we tested the second contribution of this paper is the development of two novel ways to reduce stack memory requirements of embedded software
with more computing platforms connected to the internet each day computer system security has become critical issue one of the major security problems is execution of malicious injected code in this paper we propose new processor extensions that allow execution of trusted instructions only the proposed extensions verify instruction block signatures in run time signatures are generated during trusted installation process using multiple input signature register misr and stored in an encrypted form the coefficients of the misr and the key used for signature encryption are based on hidden processor key signature verification is done in the background concurrently with program execution thus reducing negative impact on performance the preliminary results indicate that the proposed processor extensions will prevent execution of any unauthorized code at relatively small increase in system complexity and execution time
radio frequency identification is gaining broader adoption in many areas one of the challenges in implementing an rfid based system is dealing with anomalies in rfid reads small number of anomalies can translate into large errors in analytical results conventional eager approaches cleanse all data upfront and then apply queries on cleaned data however this approach is not feasible when several applications define anomalies and corrections on the same data set differently and not all anomalies can be defined beforehand this necessitates anomaly handling at query time we introduce deferred approach for detecting and correcting rfid data anomalies each application specifies the detection and the correction of relevant anomalies using declarative sequence based rules an application query is then automatically rewritten based on the cleansing rules that the application has specified to provide answers over cleaned data we show that naive approach to deferred cleansing that applies rules without leveraging query information can be prohibitive we develop two novel rewrite methods both of which reduce the amount of data to be cleaned by exploiting predicates in application queries while guaranteeing correct answers we leverage standardized sql olap functionality to implement rules specified in declarative sequence based language this allows efficient evaluation of cleansing rules using existing query processing capabilities of dbms our experimental results show that deferred cleansing is affordable for typical analytic queries over rfid data
modern window based user interface systems generate user interface events as natural products of their normal operation because such events can be automatically captured and because they indicate user behavior with respect to an application's user interface they have long been regarded as potentially fruitful source of information regarding application usage and usability however because user interface events are typically voluminos and rich in detail automated support is generally required to extract information at level of abstraction that is useful to investigators interested in analyzing application usage or evaluating usability this survey examines computer aided techniques used by hci practitioners and researchers to extract usability related information from user interface events framework is presented to help hci practitioners and researchers categorize and compare the approaches that have been or might fruitfully be applied to this problem because many of the techniques in the research literature have not been evaluated in practice this survey provides conceptual evaluation to help identify some of the relative merits and drawbacks of the various classes of approaches ideas for future research in this area are also presented this survey addresses the following questions how might user interface events be used in evaluating usability how are user interface events related to other forms of usability data what are the key challenges faced by investigators wishing to exploit this data what approaches have been brought to bear on this problem and how do they compare to one another what are some of the important open research questions in this area
in recent years there has been tremendous growth of online text information related to the explosive growth of the web which provides very useful information resource to all types of users who access the internet for various purposes the major demand of these users is to get required information within the stipulated time humans can recognise subjects of document fields by reading only some relevant specific words called field association words in the field this paper presents method of relevant estimation among fields by using field association words two methods are proposed in this paper first is method of extraction of co occurrence among fields and the second is method of judgment of similarity among fields as the methods of relevant estimation among fields from experimental results precision of the first method is high when relevance among fields is very high and considering direction of fields preferable results are obtained in the second method
in order to obtain machine understandable semantics for web resources research on the semantic web tries to annotate web resources with concepts and relations from explicitly defined formal ontologies this kind of formal annotation is usually done manually or semi automatically in this paper we explore complement approach that focuses on the social annotations of the web which are annotations manually made by normal web users without pre defined formal ontology compared to the formal annotations although social annotations are coarse grained informal and vague they are also more accessible to more people and better reflect the web resources meaning from the users point of views during their actual usage of the web resources using social bookmark service as an example we show how emergent semantics can be statistically derived from the social annotations furthermore we apply the derived emergent semantics to discover and search shared web bookmarks the initial evaluation on our implementation shows that our method can effectively discover semantically related web bookmarks that current social bookmark service can not discover easily
miss rate curves mrcs are useful in number of contexts in our research online cache mrcs enable us to dynamically identify optimal cache sizes when cache partitioning shared cache multicore processor obtaining mrcs has generally been assumed to be expensive when done in software and consequently their usage for online optimizations has been limited to address these problems and opportunities we have developed low overhead software technique to obtain mrcs online on current processors exploiting features available in their performance monitoring units so that no changes to the application source code or binaries are required our technique called rapidmrc requires single probing period of roughly million processor cycles ms and subsequently million cycles ms to process the data we demonstrate its accuracy by comparing the obtained mrcs to the actual mrcs of applications taken from speccpu speccpu and specjbb we show that rapidmrc can be applied to sizing cache partitions helping to achieve performance improvements of up to
in this paper we present framework for mining diverging patterns new type of contrast patterns whose frequency changes significantly differently in two data sets eg it changes from relatively low to relatively high value in one dataset but from high to low in the other in this framework measure called diverging ratio is defined and used to discover diverging patterns we use four dimensional vector to represent pattern and define the pattern's diverging ratio based on the angular difference between its vectors in two datasets an algorithm is proposed to mine diverging patterns from pair of datasets which makes use of standard frequent pattern mining algorithm to compute vector components efficiently we demonstrate the effectiveness of our approach on real world datasets showing that the method can reveal novel knowledge from large databases
we present formal framework to specify and test systems presenting both soft and hard deadlines while hard deadlines must be always met on time soft deadlines can be sometimes met in different time usually higher from the specified one it is this characteristic to formally define sometimes what produces several reasonable alternatives to define appropriate implementation relations that is relations to decide wether an implementation is correct with respect to specification in addition to introduce these relations we define testing framework to test implementations
information extraction ie systems are prone to false hits for variety of reasons and we observed that many of these false hi ts occur in sentences that contain subjective language eg opinions emotions and sentiments motivated by these observations we explore the idea of using subjectivity analysis to improve the precision of information extraction systems in this paper we describe an ie system that uses subjective sentence classifier to filter its extractions we experimented with several different strategies for using the subjectivity classifications including an aggressive strategy that discards all extractions found in subjective sentences and more complex strategies that selectively discard extractions we evaluated the performance of these different approaches on the muc terrorism data set we found that indiscriminately filtering extractions from subjective sentences was overly aggressive but more selective filtering strategies improved ie precision with minimal recall loss
we present novel counting network construction where the number of input wires is smaller than or equal to the number of output wires the depth of our network is lg which depends only on in contrast the amortized contention of the network depends on the number of concurrent processes and the parameters and this offers more flexibility than all previously known networks with the same number of input and output wires whose contention depends only on two parameters and in case wlgw by choosing wlgw the contention of our network is nlgw which improves by logarithmic factor of over all previously known networks with wires
lot of recent work has focussed on bulk loading of data into multidimensional index structures in order to efficiently construct such structures for large datasets in this paper we address this problem with particular focus on trees mdash which are an important class of index structures used widely in commercial database systems we propose new technique which as opposed to the current technique of inserting data one by one bulk inserts entire new datasets into an active tree this technique called stlt for small tree large tree considers the new dataset as an tree itself small tree identifies and prepares suitable location in the original tree large tree for insertion and lastly performs the insert of the small tree into the large tree besides an analytical cost model of stlt extensive experimental studies both on synthetic and real gis data sets are also reported these experiments not only compare stlt against the conventional technique but also evaluate the suitability and limitations of stlt under different conditions such as varying buffer sizes ratio between existing and new data sizes and skewness of new data with respect to the whole spatial region we find that stlt does much better in average about than the existing technique for skewed datasets as well for large sizes of both the large tree and the small tree in terms of insertion time while keeping comparable query tree quality stlt consistently outperforms the alternate technique in all other circumstances in terms of bulk insertion time especially even up to for the cases when the area of new data sets covers up to of the global region covered by the existing index tree however at the cost of deteriorating resulting tree quality
the advent of the internet and the web and their subsequent ubiquity have brought forth opportunities to connect information sources across all types of boundaries local regional organizational etc examples of such information sources include databases xml documents and other unstructured sources uniformly querying those information sources has been extensively investigated major challenge relates to query optimization indeed querying multiple information sources scattered on the web raises several barriers for achieving efficiency this is due to the characteristics of web information sources that include volatility heterogeneity and autonomy those characteristics impede straightforward application of classical query optimization techniques they add new dimensions to the optimization problem such as the choice of objective function selection of relevant information sources limited query capabilities and unpredictable events in this paper we survey the current research on fundamental problems to efficiently process queries over web data integration systems we also outline classification for optimization techniques and framework for evaluating them
garbage first is server style garbage collector targeted for multi processors with large memories that meets soft real time goal with high probability while achieving high throughput whole heap operations such as global marking are performed concurrently with mutation to prevent interruptions proportional to heap or live data size concurrent marking both provides collection completeness and identifies regions ripe for reclamation via compacting evacuation this evacuation is performed in parallel on multiprocessors to increase throughput
this article discusses potential application of radio frequency identification rfid and collaborative filtering for targeted advertising in grocery stores every day hundreds of items in grocery stores are marked down for promotional purposes whether these promotions are effective or not depends primarily on whether the customers are aware of them or not and secondarily whether the customers are interested in the products or not currently the companies are incapable of influencing the customers decisionmaking process while they are shopping however the capabilities of rfid technology enable us to transfer the recommendation systems of commerce to grocery stores in our model using rfid technology we get real time information about the products placed in the cart during the shopping process based on that information we inform the customer about those promotions in which the customer is likely to be interested in the selection of the product advertised is dynamic decision making process since it is based on the information of the products placed inside the cart while customer is shopping collaborative filtering will be used for the identification of the advertised product and bayesian networks will be used for the application of collaborative filtering we are assuming scenario where all products have rfid tags and grocery carts are equipped with rfid readers and screens that would display the relevant promotions
modern computer systems permit users to access protected information from remote locations in certain secure environments it would be desirable to restrict this access to particular computer or set of computers existing solutions of machine level authentication are undesirable for two reasons first they do not allow fine grained application layer access decisions second they are vulnerable to insider attacks in which trusted administrator acts maliciously in this work we describe novel approach using secure hardware that solves these problems in our design multiple administrators are required for installation of system after installation the authentication privileges are physically linked to that machine and no administrator can bypass these controls we define an administrative model and detail the requirements for an authentication protocol to be compatible with our methodology our design presents some challenges for large scale systems in addition to the benefit of reduced maintenance
wi fi clients can obtain much better performance at some commercial hotspots than at others unfortunately there is currently no way for users to determine which hotspot access points aps will be sufficient to run their applications before purchasing access to address this problem this paper presents wifi reports collaborative service that provides wi fi clients with historical information about ap performance and application support the key research challenge in wifi reports is to obtain accurate user submitted reports this is challenging because two conflicting goals must be addressed in practical system preserving the privacy of users reports and limiting fraudulent reports we introduce practical cryptographic protocol that achieves both goals and we address the important engineering challenges in building wifi reports using measurement study of commercial aps in seattle we show that wifi reports would improve performance over previous ap selection approaches in of locations
in this paper we present an approach to design of command tables in aircraft cockpits to date there is no common standard for designing this kind of command tables command tables impose high load on human visual senses for displaying flight information such as altitude attitude vertical speed airspeed heading and engine power heavy visual workload and physical conditions significantly influence cognitive processes of an operator in an aircraft cockpit proposed solution formalizes the design process describing instruments in terms of estimated effects they produce on flight operators in this way we can predict effects and constraints of particular type of flight instrument and avoid unexpected effects early in the design process
there has been flurry of recent work on the design of high performance software and hybrid hardware software transactional memories stms and hytms this paper reexamines the design decisions behind several of these stateof the art algorithms adopting some ideas rejecting others all in an attempt to make stms faster we created the transactional locking tl framework of stm algorithms and used it to conduct range of comparisons of the performance of non blocking lock based and hybrid stm algorithms versus fine grained hand crafted ones we were able to make several illuminating observations regarding lock acquisition order the interaction of stms with memory management schemes and the role of overheads and abort rates in stm performance
the analysis of movement of people vehicles and other objects is important for carrying out research in social and scientific domains the study of movement behavior of spatiotemporal entities helps enhance the quality of service in decision making in real applications however the spread of certain entities such as diseases or rumor is difficult to observe compared to the movement of people vehicles or animals we can only infer their locations in certain region of space time on the basis of observable events in this paper we propose new model called as moving phenomenon to represent time varying phenomena over geotime tagged contents on the web the most important feature of this model is the integration of thematic dimension into an event based spatiotemporal data model by using the proposed model user can aggregate relevant contents relating to an interesting phenomenon and perceive its movement behavior further the model also enables user to navigate the spatial temporal and thematic information of the contents along all the three dimensions finally we present an example of typhoons to illustrate moving phenomena and draw comparison between the movement of the moving phenomenon created using information from news articles on the web and that of the actual typhoon
we present detailed study of network evolution by analyzing four large online social networks with full temporal information about node and edge arrivals for the first time at such large scale we study individual node arrival and edge creation processes that collectively lead to macroscopic properties of networks using methodology based on the maximum likelihood principle we investigate wide variety of network formation strategies and show that edge locality plays critical role in evolution of networks our findings supplement earlier network models based on the inherently non local preferential attachment based on our observations we develop complete model of network evolution where nodes arrive at prespecified rate and select their lifetimes each node then independently initiates edges according to gap process selecting destination for each edge according to simple triangle closing model free of any parameters we show analytically that the combination of the gap distribution with the node lifetime leads to power law out degree distribution that accurately reflects the true network in all four cases finally we give model parameter settings that allow automatic evolution and generation of realistic synthetic networks of arbitrary scale
we study complexity issues for interaction systems general model for component based systems that allows for very flexible interaction mechanism we present complexity results for important properties of interaction systems such as local global deadlock freedom progress and availability of components
many distributed monitoring applications of wireless sensor networks wsns require the location information of sensor node in this article we address the problem of enabling nodes of wireless sensor networks to determine their location in an untrusted environment known as the secure localization problem we propose novel range independent localization algorithm called serloc that is well suited to resource constrained environment such as wsn serloc is distributed algorithm based on two tier network architecture that allows sensors to passively determine their location without interacting with other sensors we show that serloc is robust against known attacks on wsns such as the wormhole attack the sybil attack and compromise of network entities and analytically compute the probability of success for each attack we also compare the performance of serloc with state of the art range independent localization schemes and show that serloc has better performance
in recent years network coding has emerged as new communication paradigm that can significantly improve the efficiency of network protocols by requiring intermediate nodes to mix packets before forwarding them recently several real world systems have been proposed to leverage network coding in wireless networks although the theoretical foundations of network coding are well understood real world system needs to solve plethora of practical aspects before network coding can meet its promised potential these practical design choices expose network coding systems to wide range of attacks we identify two general frameworks inter flow and intra flow that encompass several network coding based systems proposed in wireless networks our systematic analysis of the components of these frameworks reveals vulnerabilities to wide range of attacks which may severely degrade system performance then we identify security goals and design challenges in achieving security for network coding systems adequate understanding of both the threats and challenges is essential to effectively design secure practical network coding systems our paper should be viewed as cautionary note pointing out the frailty of current network coding based wireless systems and general guideline in the effort of achieving security for network coding systems
the real world we live in is mostly perceived through an incredibly large collection of views generated by humans machines and other systems this is the view reality the opsis project concentrates its efforts on dealing with the multifaceted form and complexity of data views including data projection views aggregate views summary views synopses and finally web views in particular opsis deals with the generation the storage organization cubetrees the efficient run time management dynamat of materialized views for data warehouse systems and for web servers with dynamic content webviews
the visual vocabulary is an intermediate level representation which has been proved to be very powerful for addressing object categorization problems it is generally built by vector quantizing set of local image descriptors independently of the object model used for categorizing images we propose here to embed the visual vocabulary creation within the object model construction allowing to make it more suited for object class discrimination and therefore for object categorization we also show that the model can be adapted to perform object level segmentation task without needing any shape model making the approach very adapted to high intra class varying objects
traditional web service discovery is strongly related to the use of service directories especially in the case of mobile web services where both service requestors and providers are mobile the dynamics impose the need for directory based discovery context plays an eminent role with mobility as filtering mechanism that enhances service discovery through the selection of the most appropriate service however current service directory specifications do not focus on mobility of services or context awareness in this paper we propose casd context aware service directory envisioned as context based index for services on top of any traditional service directory and design algorithms for construction search update and merge of such directories furthermore we describe the architecture of our system for context aware service discovery we present prototype implementation and discuss the experimental results as well as the overall evaluation the contribution of this work is the proposal for novel enhanced representation model for context aware service directory
we address the pose mismatch problem which can occur in face verification systems that have only single frontal face image available for training in the framework of bayesian classifier based on mixtures of gaussians the problem is tackled through extending each frontal face model with artificially synthesized models for non frontal views the synthesis methods are based on several implementations of maximum likelihood linear regression mllr as well as standard multi variate linear regression linreg all synthesis techniques rely on prior information and learn how face models for the frontal view are related to face models for non frontal views the synthesis and extension approach is evaluated by applying it to two face verification systems holistic system based on pca derived features and local feature system based on dct derived features experiments on the feret database suggest that for the holistic system the linreg based technique is more suited than the mllr based techniques for the local feature system the results show that synthesis via new mllr implementation obtains better performance than synthesis based on traditional mllr the results further suggest that extending frontal models considerably reduces errors it is also shown that the local feature system is less affected by view changes than the holistic system this can be attributed to the parts based representation of the face and due to the classifier based on mixtures of gaussians the lack of constraints on spatial relations between the face parts allowing for deformations and movements of face areas
views over distributed information sources such as data warehouses rely on the stability of the schemas of underlying databases in the event of meta data changes in the sources such as the deletion of table or column such views may become undefined using meta data about information redundancy views can be evolved as necessary to remain well defined after source meta data changesprevious work in view synchronization focused only on deletions of schema elements we now offer an approach that makes use of additions also our algorithm returns view definitions to previous versions by using knowledge about the history of views and meta data this technology enables us to adapt views to temporary meta data changes by canceling out opposite changes it also allows undo redo operations on meta data last in many cases the resulting evolved views even have an improved information quality in this paper we give formal taxonomy of schema and constraint changes and full description of the proposed history driven view synchronization algorithm for this taxonomy we also prove the history driven view synchronization algorithm to be correct our approach falls in the global as view category of data integration solutions but unlike prior solutions in this category it now also deals with changes in the information space rather than requiring source schemas to remain constant over time
this work addresses the problem of trading off the latency in delivering the answer to the sink at the benefit of balancing the spatial dispersion of the energy consumption among the nodes and consequently prolonging the lifetime in sensor networks typically in response to query that pertains to the data from some geographic region tree structure is constructed and when possible some in network aggregation is performed on the other hand in order to increase the robustness and or balance the load multipath routing is employed motivated by earlier work that combined trees and multipaths in this paper we explore the possibility and the impact of combining multiple trees and multiple multipaths for routing when processing query with respect to given region of interest we present and evaluate two approaches that enable load balancing in terms of alternating among collection of routing structures
short tcp flows may suffer significant response time performance degradations during network congestion unfortunately this creates an incentive for misbehavior by clients of interactive applications eg gaming telnet web to send dummy packets into the network at tcp fair rate even when they have no data to send thus improving their performance in moments when they do have data to send even though no law is violated in this way large scale deployment of such an approach has the potential to seriously jeopardize one of the core internet's principles statistical multiplexing we quantify by means of analytical modeling and simulation gains achievable by the above misbehavior our research indicates that easy to implement application level techniques are capable of dramatically reducing incentives for conducting the above transgressions still without compromising the idea of statistical multiplexing
this paper presents the architecture of an asynchronous array of simple processors asap and evaluates its key architectural features as well as its performance and energy efficiency the asap processor calculates dsp applications with high energy efficiency is capable of high performance is easily scalable and is well suited to future fabrication technologies it is composed of two dimensional array of simple single issue programmable processors interconnected by reconfigurable mesh network processors are designed to capture the kernels of many dsp algorithms with very little additional overhead each processor contains its own tunable and haltable clock oscillator and processors operate completely asynchronously with respect to each other in globally asynchronous locally synchronous gals fashion asap array has been designed and fabricated in Œºm cmos technology each processor occupies mm is fully functional at clock rate of mhz at and dissipates an average of mw per processor at mhz under typical conditions while executing applications such as jpeg encoder core and complete ieee wireless lan baseband transmitter most processors operate at over mhz at processors dissipate mw at mhz and single asap processor occupies or less area than single processing element in other multi processor chips compared to several risc processors single issue mips and arm asap achieves performance times greater energy efficiency times greater while using far less area compared to the ti cx high end dsp processor asap achieves performance times greater energy efficiency times greater with an area times smaller compared to asic implementations asap achieves performance within factor of energy efficiency within factor of with area within factor of these data are for varying numbers of asap processors per benchmark
there is growing wealth of data describing networks of various types including social networks physical networks such as transportation or communication networks and biological networks at the same time there is growing interest in analyzing these networks in order to uncover general laws that govern their structure and evolution and patterns and predictive models to develop better policies and practices however fundamental challenge in dealing with this newly available observational data describing networks is that the data is often of dubious quality it is noisy and incomplete and before any analysis method can be applied the data must be cleaned and missing information inferred in this paper we introduce the notion of graph identification which explicitly models the inference of cleaned output network from noisy input graph it is this output network that is appropriate for further analysis we present an illustrative example and use the example to explore the types of inferences involved in graph identification as well as the challenges and issues involved in combining those inferences we then present simple general approach to combining the inferences in graph identification and experimentally show the utility of our combined approach and how the performance of graph identification is sensitive to the inter dependencies among these inferences
this paper studies the memory system behavior of java programs by analyzing memory reference traces of several specjvm applications running with just in time jit compiler trace information is collected by an exception based tracing tool called jtrace without any instrumentation to the java programs or the jit compilerfirst we find that the overall cache miss ratio is increased due to garbage collection which suffers from higher cache misses compared to the application we also note that going beyond way cache associativity improves the cache miss ratio marginally second we observe that java programs generate substantial amount of short lived objects however the size of frequently referenced long lived objects is more important to the cache performance because it tends to determine the application's working set size finally we note that the default heap configuration which starts from small initial heap size is very inefficient since it invokes garbage collector frequently although the direct costs of garbage collection decrease as we increase the available heap size there exists an optimal heap size which minimizes the total execution time due to the interaction with the virtual memory performance
previous studies of non parametric kernel npk learning usually reduce to solving some semi definite programming sdp problem by standard sdp solver however time complexity of standard interior point sdp solvers could be as high as such intensive computation cost prohibits npk learning applicable to real applications even for data sets of moderate size in this paper we propose an efficient approach to npk learning from side information referred to as simplenpkl which can efficiently learn non parametric kernels from large sets of pairwise constraints in particular we show that the proposed simplenpkl with linear loss has closed form solution that can be simply computed by the lanczos algorithm moreover we show that the simplenpkl with square hinge loss can be re formulated as saddle point optimization task which can be further solved by fast iterative algorithm in contrast to the previous approaches our empirical results show that our new technique achieves the same accuracy but is significantly more efficient and scalable
discovering and unlocking the full potential of complex pervasive environments is still approached in application centric ways set of statically deployed applications often defines the possible interactions within the environment however the increasing dynamics of such environments require more versatile and generic approach which allows the end user to inspect configure and control the overall behavior of such an environment meta ui addresses these needs by providing the end user with an interactive view on physical or virtual environment which can then be observed and manipulated at runtime the meta ui bridges the gap between the resource providers and the end users by abstracting resource's features as executable activities that can be assembled at runtime to reach common goal in order to allow software services to automatically integrate with pervasive computing environment the minimal requirements of the environment's meta ui must be identified and agreed on in this paper we present meta stud goal and service oriented reference framework that supports the creation of meta uis for usage in pervasive environments the framework is validated using two independent implementation approaches designed with different technologies and focuses
case based reasoning cbr was firstly introduced into the area of business failure prediction bfp in the conclusion drawn out in its first application in this area is that cbr is not more applicable than multiple discriminant analysis mda and logit on the contrary there are some arguments which claim that cbr with nearest neighbor nn as its heart is not surely outranked by those machine learning techniques in this research we attempt to investigate whether or not cbr is sensitive to the so called optimal feature subsets in bfp since feature subset is an important factor that accounts for cbr's performance when cbr is used to solve such classification problem the retrieval process of its life cycle is mainly used we use the classical euclidean metric technique to calculate case similarity empirical data two years prior to failure are collected from shanghai stock exchange and shenzhen stock exchange in china four filters ie mda stepwise method logit stepwise method one way anova independent samples test and the wrapper approach of genetic algorithm are employed to generate five optimal feature subsets after data normalization thirty times hold out method is used as assessment of predictive performances by combining leave one out cross validation and hold out method the two statistical baseline models ie mda and logit and the new model of support vector machine are employed as comparative models empirical results indicate that cbr is truly sensitive to optimal feature subsets with data for medium term bfp the stepwise method of mda filter approach is the first choice for cbr to select optimal feature subsets followed by the stepwise method of logit and the wrapper the two filter approaches of anova and test are the fourth choice if mda stepwise method is employed to select optimal feature subset for the cbr system there are no significant difference on predictive performance of medium term bfp between cbr and the other three models ie mda logit svm on the contrary cbr is outperformed by the three models at the significant level of if anova or test is used as feature selection method for cbr
understanding the nature of the workloads and system demands created by users of the world wide web is crucial to properly designing and provisioning web services previous measurements of web client workloads have been shown to exhibit number of characteristic features semi however it is not clear how those features may be changing with time in this study we compare two measurements of web client workloads separated in time by three years both captured from the same computing facility at boston university the older dataset obtained in is well known in the research literature and has been the basis for wide variety of studies the newer dataset was captured in and is comparable in size to the older dataset the new dataset has the drawback that the collection of users measured may no longer be representative of general web users semi however using it has the advantage that many comparisons can be drawn more clearly than would be possible using new different source of measurement our results fall into two categories first we compare the statistical and distributional properties of web requests across the two datasets this serves to reinforce and deepen our understanding of the characteristic statistical properties of web client requests we find that the kinds of distributions that best describe document sizes have not changed between and although specific values of the distributional parameters are different second we explore the question of how the observed differences in the properties of web client requests particularly the popularity and temporal locality properties affect the potential for web file caching in the network we find that for the computing facility represented by our traces between and the benefits of using size based caching policies have diminished semi and the potential for caching requested files in the network has declined
component based development is promising way to promote the productivity of large workflow systems development this paper proposes component based workflow systems development approach by investigating the following notions mechanisms and methods workflow component workflow component composition reuse association relationship between workflow components and workflow component repository the proposed approach is supported by set of development strategies and development platform through application and comparison we show the advantages of the component based workflow systems and the effectiveness of the proposed approach
given nodes in social network say authorship network how can we find the node author that is the center piece and has direct or indirect connections to all or most of them for example this node could be the common advisor or someone who started the research area that the nodes belong to isomorphic scenarios appear in law enforcement find the master mind criminal connected to all current suspects gene regulatory networks find the protein that participates in pathways with all or most of the given proteins viral marketing and many moreconnection subgraphs is an important first step handling the case of query nodes then the connection subgraph algorithm finds the intermediate nodes that provide good connection between the two original query nodeshere we generalize the challenge in multiple dimensions first we allow more than two query nodes second we allow whole family of queries ranging from or to and with softand in between finally we design and compare fast approximation and study the quality speed trade offwe also present experiments on the dblp dataset the experiments confirm that our proposed method naturally deals with multi source queries and that the resulting subgraphs agree with our intuition wall clock timing results on the dblp dataset show that our proposed approximation achieve good accuracy for about speedup
animating crowd of characters is an important problem in computer graphics the latest techniques enable highly realistic group motions to be produced in feature animation films and video games however interactive methods have not emerged yet for editing the existing group motion of multiple characters we present an approach to editing group motion as whole while maintaining its neighborhood formation and individual moving trajectories in the original animation as much as possible the user can deform group motion by pinning or dragging individuals multiple group motions can be stitched or merged to form longer or larger group motion while avoiding collisions these editing operations rely on novel graph structure in which vertices represent positions of individuals at specific frames and edges encode neighborhood formations and moving trajectories we employ shape manipulation technique to minimize the distortion of relative arrangements among adjacent vertices while editing the graph structure the usefulness and flexibility of our approach is demonstrated through examples in which the user creates and edits complex crowd animations interactively using collection of group motion clips
many testing and analysis techniques have been developed for inhouse use although they are effective at discovering defects before program is deployed these techniques are often limited due to the complexity of real world code and thus miss program faults it will be the users of the program who eventually experience failures caused by the undetected faults to take advantage of the large number of program runs carried by the users recent work has proposed techniques to collect execution profiles from the users for developers to perform post deployment failure analysis however in order to protect users privacy and to reduce run time overhead such profiles are usually not detailed enough for the developers to identify or fix the root causes of the failures in this paper we propose novel approach to utilize user execution profiles for more effective in house testing and analysis our key insight is that execution profiles for program failures can be used to simplify program while preserving its erroneous behavior by simplifying program and scaling down its complexity according to its profiles in house testing and analysis techniques can be performed more accurately and efficiently and pragmatically program defects that occur more often and are arguably more relevant to users will be given preference during failure analysis specifically we adapt statistical debugging on execution profiles to predict likely failure related code and use syntax directed algorithm to trim failure irrelevant code from program while preserving its erroneous behavior as much as possible we conducted case studies on testing engine cute and software model checker blast to evaluate our technique we used subject programs from the aristotle analysis system and the software artifact infrastructure repository sir our empirical results show that using simplified programs cute and blast find more bugs with improved accuracy and performance they were able to detect and out of more bugs respectively in about half of the time as they took on the original test programs
to offset the effect of read miss penalties on processor utilization in shared memory multiprocessors several software and hardware based data prefetching schemes have been proposed major advantage of hardware techniques is that they need no support from the programmer or compilersequential prefetching is simple hardware controlled prefetching technique which relies on the automatic prefetch of consecutive blocks following the block that misses in the cache thus exploiting spatial locality in its simplest form the number of prefetched blocks on each miss is fixed throughout the execution however since the prefetching efficiency varies during the execution of program we propose to adapt the number of prefetched blocks according to dynamic measure of prefetching effectiveness simulations of this adaptive scheme show reductions of the number of read misses the read penalty and of the execution time by up to and respectively
change detection is an important issue for modern geospatial information systems in this paper we address change detection of areal objects ie objects with closed curve outlines we specifically focus on the detection of movement translation and rotation and or deformation of such objects using aerial imagery the innovative approach we present in this paper combines geometric analysis with our model of differential snakes to support change detection geometric analysis proceeds by comparing the first moments of the two outlines describing the same object in different instances to estimate translation moment information allows us to determine the principal axes and eigenvectors of these outlines and this we can determine object rotation as the angle between these principal axes next we apply polygon clipping techniques to calculate the intersection and difference of these two outlines we use this result to estimate the radial deformation of the object expansion and contraction the results are further refined through the use of our differential snakes model to distinguish true change from the effects of inaccuracy in object determination the aggregation of these tools defines powerful approach for change detection in the paper we present the theoretical background behind these components and experimental results that demonstrate the performance of our approach
coupled transformation occurs when multiple software artifacts must be transformed in such way that they remain consistent with each other for instance when database schema is adapted in the context of system maintenance the persistent data residing in the system's database needs to be migrated to conform to the adapted schema also queries embedded in the application code and any declared referential constraints must be adapted to take the schema changes into account as another example in xml to relational data mapping hierarchical xml schema is mapped to relational sql schema with appropriate referential constraints and the xml documents and queries are converted into relational data and relational queries the lt project is aimed at providing formal basis for coupled transformation this formal basis is found in data refinement theory point free program calculation and strategic term rewriting we formalize the coupled transformation of data type by an algebra of information preserving data refinement steps each witnessed by appropriate data conversion functions refinement steps are modeled by so called two level rewrite rules on type expressions that synthesize conversion functions between redex and reduct while rewriting strategy combinators are used to composed two level rewrite rules into complete rewrite systems point free program calculation is applied to optimized synthesize conversion function to migrate queries and to normalize data type constraints in this paper we provide an overview of the challenges met by the lt project and we give sketch of the solutions offered
in this paper we develop fault tolerant job scheduling strategy in order to tolerate faults gracefully in an economy based grid environment we propose novel adaptive task checkpointing based fault tolerant job scheduling strategy for an economy based grid the proposed strategy maintains fault index of grid resources it dynamically updates the fault index based on successful or unsuccessful completion of an assigned task whenever grid resource broker has tasks to schedule on grid resources it makes use of the fault index from the fault tolerant schedule manager in addition to using time optimization heuristic while scheduling grid job on grid resource the resource broker uses fault index to apply different intensity of task checkpointing inserting checkpoints in task at different intervals to simulate and evaluate the performance of the proposed strategy this paper enhances the gridsim toolkit to exhibit fault tolerance related behavior we also compare checkpointing fault tolerant job scheduling strategy with the well known time optimization heuristic in an economy based grid environment from the measured results we conclude that even in the presence of faults the proposed strategy effectively schedules grid jobs tolerating faults gracefully and executes more jobs successfully within the specified deadline and allotted budget it also improves the overall execution time and minimizes the execution cost of grid jobs
the microprocessor industry is currently struggling with higher development costs and longer design times that arise from exceedingly complex processors that are pushing the limits of instruction level parallelism meanwhile such designs are especially ill suited for important commercial applications such as on line transaction processing oltp which suffer from large memory stall times and exhibit little instruction level parallelism given that commercial applications constitute by far the most important market for high performance servers the above trends emphasize the need to consider alternative processor designs that specifically target such workloads the abundance of explicit thread level parallelism in commercial workloads along with advances in semiconductor integration density identify chip multiprocessing cmp as potentially the most promising approach for designing processors targeted at commercial servers this paper describes the piranha system research prototype being developed at compaq that aggressively exploits chip multi processing by integrating eight simple alpha processor cores along with two level cache hierarchy onto single chip piranha also integrates further on chip functionality to allow for scalable multiprocessor configurations to be built in glueless and modular fashion the use of simple processor cores combined with an industry standard asic design methodology allow us to complete our prototype within short time frame with team size and investment that are an order of magnitude smaller than that of commercial microprocessor our detailed simulation results show that while each piranha processor core is substantially slower than an aggressive next generation processor the integration of eight cores onto single chip allows piranha to outperform next generation processors by up to times on per chip basis on important workloads such as oltp this performance advantage can approach factor of five by using full custom instead of asic logic in addition to exploiting chip multiprocessing the piranha prototype incorporates several other unique design choices including shared second level cache with no inclusion highly optimized cache coherence protocol and novel architecture
we present generalisation of first order rewriting which allows us to deal with terms involving binding operations in an elegant and practical way we use nominal approach to binding in which bound entities are explicitly named rather than using nameless syntax such as de bruijn indices yet we get rewriting formalism which respects conversion and can be directly implemented this is achieved by adapting to the rewriting framework the powerful techniques developed by pitts et al in the freshml projectnominal rewriting can be seen as higher order rewriting with first order syntax and built in conversion we show that standard first order rewriting is particular case of nominal rewriting and that very expressive higher order systems such as klop's combinatory reduction systems can be easily defined as nominal rewriting systems finally we study confluence properties of nominal rewriting
we present novel approach for retrieval of object categories based on novel type of image representation the generalized correlogram gc in our image representation the object is described as constellation of gcs where each one encodes information about some local part and the spatial relations from this part to others ie the part's context we show how such representation can be used with fast procedures that learn the object category with weak supervision and efficiently match the model of the object against large collections of images in the learning stage we show that by integrating our representation with boosting the system is able to obtain compact model that is represented by very few features where each feature conveys key properties about the object's parts and their spatial arrangement in the matching step we propose direct procedures that exploit our representation for efficiently considering spatial coherence between the matching of local parts combined with an appropriate data organization such as inverted files we show that thousands of images can be evaluated efficiently the framework has been applied to different standard databases and we show that our results are favorably compared against state of the art methods in both computational cost and accuracy
we consider an extension of integer linear arithmetic with star operator takes closure under vector addition of the solution set of linear arithmetic subformula we show that the satisfiability problem for this extended language remains in np and therefore np complete our proof uses semilinear set characterization of solutions of integer linear arithmetic formulas as well as generalization of recent result on sparse solutions of integer linear programming problems as consequence of our result we present worst case optimal decision procedures for two np hard problems that were previously not known to be in np the first is the satisfiability problem for logic of sets multisets bags and cardinality constraints which has applications in verification interactive theorem proving and description logics the second is the reachability problem for class of transition systems whose transitions increment the state vector by solutions of integer linear arithmetic formulas
we present compositional program logic for call by value imperative higher order functions with general forms of aliasing which can arise from the use of reference names as function parameters return values content of references and parts of data structures the program logic extends our earlier logic for alias free imperative higher order functions with new modal operators which serve as building blocks for clean structural reasoning about programs and data structures in the presence of aliasing this has been an open issue since the pioneering work by cartwright oppen and morris twenty five years ago we illustrate usage of the logic for description and reasoning through concrete examples including higher order polymorphic quicksort the logical status of the new operators is clarified by translating them into in equalities of reference names the logic is observationally complete in the sense that two programs are observationally indistinguishable if they satisfy the same set of assertions
the input vocabulary for touch screen interaction on handhelds is dramatically limited especially when the thumb must be used to enrich that vocabulary we propose to discriminate among thumb gestures those we call microrolls characterized by zero tangential velocity of the skin relative to the screen surface combining four categories of thumb gestures drags swipes rubbings and microrolls with other classification dimensions we show that at least elemental gestures can be automatically recognized we also report the results of two experiments showing that the roll vs slide distinction facilitates thumb input in realistic copy and paste task relative to existing interaction techniques
the increasing availability and accuracy of eye gaze detection equipment has encouraged its use for both investigation and control in this paper we present novel methods for navigating and inspecting extremely large images solely or primarily using eye gaze control we investigate the relative advantages and comparative properties of four related methods stare to zoom stz in which control of the image position and resolution level is determined solely by the user's gaze position on the screen head to zoom htz and dual to zoom dtz in which gaze control is augmented by head or mouse actions and mouse to zoom mtz using conventional mouse input as an experimental control the need to inspect large images occurs in many disciplines such as mapping medicine astronomy and surveillance here we consider the inspection of very large aerial images of which google earth is both an example and the one employed in our study we perform comparative search and navigation tasks with each of the methods described and record user opinions using the swedish user viewer presence questionnaire we conclude that while gaze methods are effective for image navigation they as yet lag behind more conventional methods and interaction designers may well consider combining these techniques for greatest effect
previous research in real time concurrency control mainly focuses on the schedulability guarantee of hard real time transactions and the reduction of the miss rate of soft real time transactions although many new database applications have significant response time requirements not much work has been done in the joint scheduling of traditional nonreal time transactions and soft real time transactions in this paper we study the concurrency control problems in mixed soft real time database systems in which both non real time and soft real time transactions exist simultaneously the objectives are to identify the cost and the performance tradeoff in the design of cost effective and practical real time concurrency control protocols and to evaluate their performance under different real time and non real time supports in particular we are interested in studying the impacts of different scheduling approaches for soft real time transactions on the performance of non real time transactions instead of proposing yet another completely new real time concurrency control protocol our objective is to design an efficient integrated concurrency control method based on existing techniques we propose several methods to integrate the well known two phase locking and optimistic concurrency control with the aims to meet the deadline requirements of soft real time transactions and at the same time to minimize the impact on the performance of non real time transactions we have conducted series of experiments based on sanitized version of stock trading systems to evaluate the performance of both soft real time and non real time transactions under different real time supports in the system
social networks play important roles in the semantic web knowledge management information retrieval ubiquitous computing and so on we propose social network extraction system called polyphonet which employs several advanced techniques to extract relations of persons detect groups of persons and obtain keywords for person search engines especially google are used to measure co occurrence of information and obtain web documentsseveral studies have used search engines to extract social networks from the web but our research advances the following points first we reduce the related methods into simple pseudocodes using google so that we can build up integrated systems second we develop several new algorithms for social networking mining such as those to classify relations into categories to make extraction scalable and to obtain and utilize person to word relations third every module is implemented in polyphonet which has been used at four academic conferences each with more than participants we overview that system finally novel architecture called super social network mining is proposed it utilizes simple modules using google and is characterized by scalability and relate identify processes identification of each entity and extraction of relations are repeated to obtain more precise social network
for synchronous distributed system of processes with up to potential and actual crash failures where
the specification of constraint languages for access control models has proven to be difficult but remains necessary for safety and for mandatory access control policies while the authorisation relation subject times object rightarrow pow right defines the authorised permissions an authorisation schema defines how the various concepts such as subjects users roles labels are combined to form complete access control modelusing examples drawn from common access control models in the literature we extend the authorisation schema of dtac to define general formalism for describing authorisation schema for any access control modelbased on our generic authorisation schema we define new simpler constraint specification language which is as expressive as our previous graphical constraint languages and no more complex to verify
currently there is an increasing interest in data mining and educational systems making educational data mining as new growing research community this paper surveys the application of data mining to traditional educational systems particular web based courses well known learning content management systems and adaptive and intelligent web based educational systems each of these systems has different data source and objectives for knowledge discovering after preprocessing the available data in each case data mining techniques can be applied statistics and visualization clustering classification and outlier detection association rule mining and pattern mining and text mining the success of the plentiful work needs much more specialized work in order for educational data mining to become mature area
we present formal approach to implement fault tolerance in real time embedded systems the initial fault intolerant system consists of set of independent periodic tasks scheduled onto set of fail silent processors connected by reliable communication network we transform the tasks such that assuming the availability of an additional spare processor the system tolerates one failure at time transient or permanent failure detection is implemented using heartbeating and failure masking using checkpointing and rollback these techniques are described and implemented by automatic program transformations on the tasks programs the proposed formal approach to fault tolerance by program transformations highlights the benefits of separation of concerns it allows us to establish correctness properties and to compute optimal values of parameters to minimize fault tolerance overhead we also present an implementation of our method to demonstrate its feasibility and its efficiency
this paper describes kernel interface that provides an untrusted user level process an executive with protected access to memory management functions including the ability to create manipulate and execute within subservient contexts address spaces page motion callbacks not only give the executive limited control over physical memory management but also shift certain responsibilities out of the kernel greatly reducing kernel state and complexity the executive interface was motivated by the requirements of the wisconsin wind tunnel wwt system for evaluating cache coherent shared memory parallel architectures wwt uses the executive interface to implement fine grain user level extension of li's shared virtual memory on thinking machines cm message passing multicomputer however the interface is sufficiently general that an executive could act as multiprogrammed operating system exporting an alternative interface to the threads running in its subservient contexts the executive interface is currently implemented as an extension to cmost the standard operating system for the cm in cmost policy decisions are made on central distinct control processor cp and broadcast to the processing nodes pns the pns execute minimal kernel sufficient only to implement the cp's policy while this structure efficiently supports some parallel application models the lack of autonomy on the pns restricts its generality adding the executive interface provides limited autonomy to the pns creating structure that supports multiple models of application parallelism this structure with autonomy on top of centralization is in stark contrast to most microkernel based parallel operating systems in which the nodes are fundamentally autonomous
honeypot has been an invaluable tool for the detection and analysis of network based attacks by either human intruders or automated malware in the wild the insights obtained by deploying honeypots especially high interaction ones largely rely on the monitoring capability on the honeypots in practice based on the location of sensors honeypots can be monitored either internally or externally being deployed inside the monitored honeypots internal sensors are able to provide semantic rich view on various aspects of system dynamics eg system calls however their very internal existence makes them visible tangible and even subvertible to attackers after break ins from another perspective existing external honeypot sensors eg network sniffers could be made invisible to the monitored honeypot however they are not able to capture any internal system events such as system calls executed it is desirable to have honeypot monitoring system that is invisible tamper resistant and yet is capable of recording and understanding the honeypot's system internal events such as system calls in this paper we present virtualization based system called vmscope which allows us to view the system internal events of virtual machine vm based honeypots from outside the honeypots particularly by observing and interpreting vm internal system call events at the virtual machine monitor vmm layer vmscope is able to provide the same deep inspection capability as that of traditional inside the honeypot monitoring tools eg sebek while still obtaining similar tamper resistance and invisibility as other external monitoring tools we have built proof of concept prototype by leveraging and extending one key virtualization technique called binary translation our experiments with real world honeypots show that vmscope is robust against advanced countermeasures that can defeat existing internally deployed honeypot monitors and it only incurs moderate run time overhead
recently grids and pervasive systems have been drawing increasing attention in order to coordinate large scale resources enabling access to small and smart devices in this paper we propose caching approach enabling to improve querying on pervasive grids our proposal called semantic pervasive dual cache follows semantics oriented approach it is based on the one hand on clear separation between the analysis and the evaluation process and on the other hand on the cooperation between client caches considering light analysis and proxy caches including evaluation capabilities such an approach helps load balancing making the system more scalable we have validated semantic pervasive dual cache using analytic models and simulations results obtained show that our approach is quite promising
the algorithm selection problem rice seeks to answer the question which algorithm is likely to perform best for my problem quest recognizing the problem as learning task in the early the machine learning community has developed the field of meta learning focused on learning about learning algorithm performance on classification problems but there has been only limited generalization of these ideas beyond classification and many related attempts have been made in other disciplines such as ai and operations research to tackle the algorithm selection problem in different ways introducing different terminology and overlooking the similarities of approaches in this sense there is much to be gained from greater awareness of developments in meta learning and how these ideas can be generalized to learn about the behaviors of other nonlearning algorithms in this article we present unified framework for considering the algorithm selection problem as learning problem and use this framework to tie together the crossdisciplinary developments in tackling the algorithm selection problem we discuss the generalization of meta learning concepts to algorithms focused on tasks including sorting forecasting constraint satisfaction and optimization and the extension of these ideas to bioinformatics cryptography and other fields
behavior analysis of complex distributed systems has led to the search for enhanced reachability analysis techniques which support modularity and which control the state explosion problem while modularity has been achieved state explosion in still problem indeed this problem may even be exacerbated as locally minimized subsystem may contain many states and transitions forbidden by its environment or context context constraints specified as interface processes are restrictions imposed by the environment on subsystem behavior recent research has suggested that the state explosion problem can be effectively controlled if context constraints are incorporated in compositional reachability analysis cra although theoretically very promising the approach has rarely been used in practice because it generally requires more complex computational model and does not contain mechanism to derive context constraints automatically this article presents technique to automate the approach while using similar computational model to that of cra context constraints are derived automatically based on set of sufficient conditions for these constraints to be transparently included when building reachability graphs as result the global reachability graph generated using the derived constraints is shown to be observationally equivalent to that generated by cra without the inclusion of context constraints constraints can also be specified explicitly by users based on their application knowledge erroneous constraints which contravene transparency can be identified together with an indication of the error sources user specified constraints can be combined with those generated automatically the technique is illustrated using clients server system and other examples
developing efficient and automatic testing techniques is one of the major challenges facing software validation community in this paper we show how uniform random generation process of finite automata developed in recent work by bassino and nicaud is relevant for many faces of automatic testing the main contribution is to show how to combine two major testing approaches model based testing and random testing this leads to new testing technique successfully experimented on realistic case study we also illustrate how the power of random testing applied on chinese postman problem implementation points out an error in well known algorithm finally we provide some statistics on model based testing algorithms
we study the problem of querying xml data sources that accept only limited set of queries such as sources accessible by web services which can implement very large potentially infinite families of xpath queries to compactly specify such families of queries we adopt the query set specifications formalism close to context free grammars we say that query is expressible by the specification if it is equivalent to some expansion of is supported by if it has an equivalent rewriting using some finite set of p's expansions we study the complexity of expressibility and support and identify large classes of xpath queries for which there are efficient ptime algorithms our study considers both the case in which the xml nodes in the results of the queries lose their original identity and the one in which the source exposes persistent node ids
industry globalization brings with it inevitable changes to traditional organizational structures the notion of global virtual teams working together across geographical cultural and functional borders is becoming increasingly appealing this paper presents observations of how team of designers negotiate shared understanding in the collaborative design of virtual pedals for volvo car corporation although the team was globally distributed during most of the development process examples are drawn from collocated design sessions since this enables careful examination of the multifaceted ways in which collocated designers use wide variety of artifacts and techniques to create common ground the findings highlight the situational and interactional characteristics of design collaboration and suggest that the addition of shared objects to think with in distributed design environments could greatly facilitate global design teams in their collaborative process of thinking together apart
we consider the class of database programs and address the problem of minimizing the cost of their exchanges with the database server this cost partly consists of query execution at the server side and partly of query submission and network exchanges between the program and the server the natural organization of database programs leads to submit an intensive flow of elementary sql queries to the server and exploits only locally its optimization power in this paper we develop global optimization approach we base this approach on an execution model where queries can be executed asynchronously with respect to the flow of the application program our method aims at choosing an efficient query scheduling which limits the penalty of client server interactions our results show that the technique can improve the execution time of database programs by several orders of magnitude
both integer programming models and heuristic algorithms have been proposed for finding minimum energy broadcast and multicast trees in wireless ad hoc networks among heuristic algorithms the broadcast multicast incremental power bip mip algorithm is most known the theoretical performance of bip mip has been quantified in several studies to assess the empirical performance of bip mip and other heuristic algorithms it is necessary to compute an optimal tree or very good lower bound of the optimum in this paper we present an integer programming approach as well as improved heuristic algorithms our integer programming approach comprises novel integer model and relaxation scheme unlike previously proposed models the continuous relaxation of our model leads to very sharp lower bound of the optimum our relaxation scheme allows for performance evaluation of heuristics without having to compute optimal trees our contributions to heuristic algorithms consist of the power improving algorithm successive power adjustment spa and improved time complexity of some previously suggested algorithms we report extensive numerical experiments algorithm spa finds better solutions in comparison to host of other algorithms moreover the integer programming approach shows that trees found by algorithm spa are optimal or near optimal
the widespread deployment of sensor networks is on the horizon one of the main challenges in sensor networks is to process and aggregate data in the network rather than wasting energy by sending large amounts of raw data to reply to query some efficient data dissemination methods particularly data centric storage and information aggregation rely on efficient routing from one node to another in this paper we introduce gem graph embedding for sensor networks an infrastructure for node to node routing and data centric storage and information processing in sensor networks unlike previous approaches it does not depend on geographic information and it works well even in the face of physical obstacles in gem we construct labeled graph that can be embedded in the original network topology in an efficient and distributed fashion in that graph each node is given label that encodes its position in the original network topology this allows messages to be efficiently routed through the network while each node only needs to know the labels of its neighborsto demonstrate how gem can be applied we have developed concrete graph embedding method vpcs virtual polar coordinate space in vpcs we embed ringed tree into the network topology and label the nodes in such manner as to create virtual polar coordinate space we have also developed vpcr an efficient routing algorithm that uses vpcs vpcr is the first algorithm for node to node routing that guarantees reachability requires each node to keep state only about its immediate neighbors and requires no geographic information our simulation results show that vpcr is robust on dynamic networks works well in the face of voids and obstacles and scales well with network size and density
jxta is an open source initiative that allows to specify set of collaboration and communication protocols which enable the creation and deployment of peer to peer pp applications this paper provides survey on its current state regarding the topic of security the study focuses on the security evaluation of standard peer operations within the jxta network highlighting which issues must be seriously taken into account in those applications sensitive to security
role based access control rbac is supported directly or in closely related form by number of products this article presents formalization of rbac using graph transformations that is graphical specification technique based on generalization of classical string grammars to nonlinear structures the proposed formalization provides an intuitive description for the manipulation of graph structures as they occur in information systems access control and precise specification of static and dynamic consistency conditions on graphs and graph transformations the formalism captures the rbac models published in the literature and also allows uniform treatment of user roles and administrative roles and detailed analysis of the decentralization of administrative roles
the convergence of cs and biology will serve both disciplines providing each with greater power and relevance
while static typing is widely accepted as being necessary for secure program execution dynamic typing is also viewed as being essential in some applications particularly for distributed programming environments dynamics have been proposed as language construct for dynamic typing based on experience with languages such as clu cedar mesa and modula however proposals for incorporating dynamic typing into languages with parametric polymorphism have serious shortcomings new approach is presented to extending polymorphic lnanguages with dynamic typing at the heart of the approach is the use of dynamic type dispatch where polymorphic functions may analyze the structure of their type arguments this approach solves several open problems with the traditional approach to adding dynamic typing to polymorphic languages an explicity typed language xmldyn is presented this language uses refinement kinds to ensure that dynamic type dispatch does not fail at run time safe dynamics are new form of dynamics that use refinement kinds to statically check the use of run time dynamic typing run time errors are isolated to separate construct for performing run time type checks
in this paper we pose novel research problem for machine learning that involves constructing process model from continuous data we claim that casting learned knowledge in terms of processes with associated equations is desirable for scientific and engineering domains where such notations are commonly used we also argue that existing induction methods are not well suited to this task although some techniques hold partial solutions in response we describe an approach to learning process models from time series data and illustrate its behavior in three domains in closing we describe open issues in process model induction and encourage other researchers to tackle this important problem
the internet today offers primarily best effort service research and technology development efforts are currently underway to allow provisioning of better than best effort quality of service qos assurances the residual uncertainty in qos can be managed using pricing strategies in this article we develop spot pricing framework for intra domain expected bandwidth contracts with loss based qos guarantees the framework builds on nonlinear pricing scheme for cost recovery from earlier work and extends it to price risk utility based options pricing approach is developed to account for the uncertainties in delivering loss guarantees application of options pricing techniques in internet services provides mechanism for fair risk sharing between the provider and the customer and may be extended to price other uncertainties in qos guarantees
the widespread adoption of xml holds out the promise that document structure can be exploited to specify precise database queries however the user may have only limited knowledge of the xml structure and hence may be unable to produce correct xquery especially in the context of heterogeneous information collection the default is to use keyword based search and we are all too familiar with how difficult it is to obtain precise answers by these means we seek to address these problems by introducing the notion of meaningful lowest common ancestor structure mlcas for finding related nodes within an xml document by automatically computing mlcas and expanding ambiguous tag names we add new functionality to xquery and enable users to take full advantage of xquery in querying xml data precisely and efficiently without requiring perfect knowledge of the document structure such schema free xquery is potentially of value not just to casual users with partial knowledge of schema but also to experts working in data integration or data evolution context in such context schema free query once written can be applied universally to multiple data sources that supply similar content under different schemas and applied forever as these schemas evolve our experimental evaluation found that it was possible to express wide variety of queries in schema free manner and have them return correct results over broad diversity of schemas furthermore the evaluation of schema free query is not expensive using novel stack based algorithm we develop for computing mlcas from to times the execution time of an equivalent schema aware query
farsite is secure scalable file system that logically functions as centralized file server but is physically distributed among set of untrusted computers farsite provides file availability and reliability through randomized replicated storage it ensures the secrecy of file contents with cryptographic techniques it maintains the integrity of file and directory data with byzantine fault tolerant protocol it is designed to be scalable by using distributed hint mechanism and delegation certificates for pathname translations and it achieves good performance by locally caching file data lazily propagating file updates and varying the duration and granularity of content leases we report on the design of farsite and the lessons we have learned by implementing much of that design
as open source development has evolved differentiation of roles and increased sophistication of collaborative processes has occurred recently we described coordination issues in software development and an interactive visualization tool called the social health overview sho developed to address them this paper presents an empirical evaluation of sho intended to identify its strengths and weaknesses eleven informants in various open source roles were interviewed about their work practices eight of these participated in an evaluation comparing three change management tasks in sho and bugzilla results are discussed with respect to task strategy with each tool and participants roles
we propose definition for frequent approximate patterns in order to model important subgraphs in graph database with incomplete or inaccurate information by our definition frequent approximate patterns possess three main properties possible absence of exact match maximal representation and the apriori property since approximation increases the number of frequent patterns we present novel randomized algorithm called ram using feature retrieval large number of real and synthetic data sets are used to demonstrate the effectiveness and efficiency of the frequent approximate graph pattern model and the ram algorithm
we study the problem of finding efficient equivalent view based rewritings of relational queries focusing on query optimization using materialized views under the assumption that base relations cannot contain duplicate tuples lot of work in the literature addresses the problems of answering queries using views and query optimization however most of it proposes solutions for special cases such as for conjunctive queries cqs or for aggregate queries only in addition most of it addresses the problems separately under set or bag set semantics for query evaluation and some of it proposes heuristics without formal proofs for completeness or soundness in this paper we look at the two problems by considering cq queries that is both pure conjunctive and aggregate queries with aggregation functions sum count min and max the distinct keyword in sql versions of our queries is also allowed we build on past work to provide algorithms that handle this general setting this is possible because recent results on rewritings of cq queries show that there are sound and complete algorithms based on containment tests of cqsour focus is that our algorithms are efficient as well as sound and complete besides the contribution we make in putting and addressing the problems in this general setting we make two additional contributions for bag set and set semantics first we propose efficient sound and complete tests for equivalence of cq queries to rewritings that use overlapping views the algorithms are complete with respect to the language of rewritings these results apply not only to query optimization but to all areas where the goal is to obtain efficient equivalent view based query rewritings second based on these results we propose two sound algorithms bdpv and cdpv that find efficient execution plans for cq queries in terms of materialized views both algorithms extend the cost based query optimization approach of system the efficient sound algorithm bdpv is also complete in some cases whereas cdpv is sound and complete for all cq queries we consider we present study of the completeness efficiency tradeoff in the algorithms and provide experimental results that show the viability of our approach and test the limits of query optimization using overlapping views
this paper addresses the problem of resolving virtual method and interface calls in java bytecode the main focus is on new practical technique that can be used to analyze large applications our fundamental design goal was to develop technique that can be solved with only one iteration and thus scales linearly with the size of the program while at the same time providing more accurate results than two popular existing linear techniques class hierarchy analysis and rapid type analysiswe present two variations of our new technique variable type analysis and coarser grain version called declared type analysis both of these analyses are inexpensive easy to implement and our experimental results show that they scale linearly in the size of the programwe have implemented our new analyses using the soot frame work and we report on empirical results for seven benchmarks we have used our techniques to build accurate call graphs for complete applications including libraries and we show that compared to conservative call graph built using class hierarchy analysis our new variable type analysis can remove significant number of nodes methods and call edges further our results show that we can improve upon the compression obtained using rapid type analysiswe also provide dynamic measurements of monomorphic call sites focusing on the benchmark code excluding libraries we demonstrate that when considering only the benchmark code both rapid type analysis and our new declared type analysis do not add much precision over class hierarchy analysis however our finer grained variable type analysis does resolve significantly more call sites particularly for programs with more complex uses of objects
software model checkers are typically language specific require substantial development efforts and are hard to reuse for other languages adding partial order reduction por capabilities to such tools typically requires sophisticated changes to the tool's model checking algorithms this paper proposes new method to make software model checkers language independent and improving their performance through por getting the por capabilities does not require making any changes to the underlying model checking algorithms for each language they are instead achieved through theory transformationr of l's formal semantics rewrite theory under very minimal assumptions this can be done for any language with relatively little effort our experiments with the jvm promela like language and maude indicate that significant state space reductions and time speedups can be gained for tools generated this way
database sharing db sharing refers to general approach for building distributed high performance transaction system the nodes of db sharing system are locally coupled via high speed interconnect and share common database at the disk level this is also known as ldquo shared disk rdquo approach we compare database sharing with the database partitioning shared nothing approach and discuss the functional dbms components that require new and coordinated solutions for db sharing the performance of db sharing systems critically depends on the protocols used for concurrency and coherency control the frequency of communication required for these functions has to be kept as low as possible in order to achieve high transation rates and short response times trace driven simulation system for db sharing complexes has been developed that allows realistic performance comparison of four different concurrency and coherency control protocols we consider two locking and two optimistic schemes which operate either under central or distributed control for coherency control we investigate so called on request and broadcast invalidation schemes and employ buffer to buffer communication to exchange modified pages directly between different nodes the performance impact of random routing versus affinity based load distribution and different communication costs is also examined in addition we analyze potential performance bottlenecks created by hot spot pages
an important goal of an operating system is to make computing and communication resources available in fair and efficient way to the applications that will run on top of it to achieve this result the operating system implements number of policies for allocating resources to and sharing resources among applications and it implements safety mechanisms to guard against misbehaving applications however for most of these allocation and sharing tasks no single optimal policy exists different applications may prefer different operating system policies to achieve their goals in the best possible way customizable or adaptable operating system is an operating system that allows for flexible modification of important system policies over the past decade wide range of approaches for achieving customizability has been explored in the operating systems research community in this survey an overview of these approaches structured around taxonomy is presented
this paper explores the use of resolution as meta framework for developing various different deduction calculi in this work the focus is on developing deduction calculi for modal dynamic logics dynamic modal logics are pdl like extended modal logics which are closely related to description logics we show how tableau systems modal resolution systems and rasiowa sikorski systems can be developed and studied by using standard principles and methods of first order theorem proving the approach is based on the translation of reasoning problems in modal logic to first order clausal form and using suitable refinement of resolution to construct and mimic derivations of the desired proof method the inference rules of the calculus can then be read off from the clausal form we show how this approach can be used to generate new proof calculi and prove soundness completeness and decidability results this slightly unusual approach allows us to gain new insights and results for familiar and less familiar logics for different proof methods and compare them not only theoretically but also empirically in uniform framework
geometry processing algorithms have traditionally assumed that the input data is entirely in main memory and available for random access this assumption does not scale to large data sets as exhausting the physical memory typically leads to io inefficient thrashing recent works advocate processing geometry in streaming manner where computation and output begin as soon as possible streaming is suitable for tasks that require only local neighbor information and batch process an entire data setwe describe streaming compression scheme for tetrahedral volume meshes that encodes vertices and tetrahedra in the order they are written to keep the memory footprint low the compressor is informed when vertices are referenced for the last time ie are finalized the compression achieved depends on how coherent the input order is and how many tetrahedra are buffered for local reordering for reasonably coherent orderings and buffer of tetrahedra we achieve compression rates that are only to percent above the state of the art while requiring drastically less memory resources and less than half the processing time
it is well known that query is an approximate representation of the user's information needs since it does not provide sufficient specification of the attended results numerous studies addressed this issue using techniques for better eliciting either document or query representations more recent studies investigated the use of search context to better understand the user intent driven by the query in order to deliver personalized information results in this article we propose personalized information retrieval model that leverages the information relevance by its usefulness to both the query and the user's profile expressed by his main topics of interest the model is based on the influence diagram formalism which is an extension of bayesian networks dedicated to decision problems this graphical model offers an intuitive way to represent in the same framework all the basic information terms documents user interests surrounding the user's information need and also quantify their mutual influence on the relevance estimation experimental results demonstrate that our model was successful at eliciting user queries according to dynamic changes of the user interests
this paper presents pioneer vliw architecture of native java processor we show that thanks to the specific stack architecture and to the use of the vliw technique one is able to obtain meaningful reduction of power dissipation with small area overhead when compared to other ways of executing java in hardware the underlying technique is based on the reuse of memory access instructions hence reducing power during memory or cache accesses the architecture is validated for some complex embedded applications like imdct computation and other data processing benchmarks
in the maximum constraint satisfaction problem max csp one is given finite collection of possibly weighted constraints on overlapping sets of variables and the goal is to assign values from given finite domain to the variables so as to maximize the number or the total weight for the weighted case of satisfied constraints this problem is np hard in general and therefore it is natural to study how restricting the allowed types of constraints affects the approximability of the problem in this article we show that any max csp problem with finite set of allowed constraint types which includes all fixed value constraints ie constraints of the form equals is either solvable exactly in polynomial time or else is apx complete even if the number of occurrences of variables in instances is bounded moreover we present simple description of all polynomial time solvable cases of our problem this description relies on the well known algebraic combinatorial property of supermodularity
test suite minimization techniques aim to eliminate redundant test cases from test suite based on some criteria such as coverage or fault detection capability most existing test suite minimization techniques have two main limitations they perform minimization based on single criterion and produce suboptimal solutions in this paper we propose test suite minimization framework that overcomes these limitations by allowing testers to easily encode wide spectrum of test suite minimization problems handle problems that involve any number of criteria and compute optimal solutions by leveraging modern integer linear programming solvers we implemented our framework in tool called mints that is freely available and can be interfaced with number of different state of the art solvers our empirical evaluation shows that mints can be used to instantiate number of different test suite minimization problems and efficiently find an optimal solution for such problems using different solvers
in this paper the authors share their experiences gathered during the design and implementation of the corba persistent object service there are two problems related to design and implementation of the persistence service first omg intentionally leaves the functionality core of the persistence service unspecified second omg encourages reuse of other object services without being specific enough in this respect the paper identifies the key design issues implied both by the intentional lack of omg specification and the limits of the implementation environment characteristics at the same time the paper discusses the benefits and drawbacks of reusing other object services particularly the relationship and externalization services to support the persistence service surprisingly the key lesson learned is that direct reuse of these object services is impossible
human communication involves not only speech but also wide variety of gestures and body motions interactions in virtual environments often lack this multi modal aspect of communication we present method for automatically synthesizing body language animations directly from the participants speech signals without the need for additional input our system generates appropriate body language animations by selecting segments from motion capture data of real people in conversation the synthesis can be performed progressively with no advance knowledge of the utterance making the system suitable for animating characters from live human speech the selection is driven by hidden markov model and uses prosody based features extracted from speech the training phase is fully automatic and does not require hand labeling of input data and the synthesis phase is efficient enough to run in real time on live microphone input user studies confirm that our method is able to produce realistic and compelling body language
software change resulting from new requirements environmental modifications and error detection creates numerous challenges for the maintenance of software products while many software evolution strategies focus on code to modeling language analysis few address software evolution at higher abstraction levels most lack the flexibility to incorporate multiple modeling languages not many consider the integration and reuse of domain knowledge with design knowledge we address these challenges by combining ontologies and model weaving to assist in software evolution of abstract artifacts our goals are to recover high level artifacts such as requirements and design models defined using variety of software modeling languages simplify modification of those models reuse software design and domain knowledge contained within models and integrate those models with enhancements via novel combination of ontological and model weaving concepts additional benefits to design recovery and software evolution include detecting high level dependencies and identifying differences between evolved software and initial specifications
collaborative filtering recommender systems are typically unable to generate adequate recommendations for newcomers empirical evidence suggests that the incorporation of trust network among the users of recommender system can significantly help to alleviate this problem hence users are highly encouraged to connect to other users to expand the trust network but choosing whom to connect to is often difficult task given the impact this choice has on the delivered recommendations it is critical to guide newcomers through this early stage connection process in this paper we identify several classes of key figures in the trust network namely mavens frequent raters and connectors furthermore we introduce measures to assess the influence of these users on the amount and the quality of the recommendations delivered by trust enhanced collaborative filtering recommender system experiments on dataset from epinionscom support the claim that generated recommendations for new users are more beneficial if they connect to an identified key figure compared to random user
fault tolerance in distributed computing is wide area with significant body of literature that is vastly diverse in methodology and terminology this paper aims at structuring the area and thus guiding readers into this interesting field we use formal approach to define important terms like fault fault tolerance and redundancy this leads to four distinct forms of fault tolerance and to two main phases in achieving them detection and correction we show that this can help to reveal inherently fundamental structures that contribute to understanding and unifying methods and terminology by doing this we survey many existing methodologies and discuss their relations the underlying system model is the close to reality asynchronous message passing model of distributed computing
cache misses due to coherence actions are often the major source for performance degradation in cache coherent multiprocessors it is often difficult for the programmer to take cache coherence into account when writing the program since the resulting access pattern is not apparent until the program is executedsm prof is performance analysis tool that addresses this problem by visualising the shared data access pattern in diagram with links to the source code lines causing performance degrading access patterns the execution of program is divided into time slots and each data block is classified based on the accesses made to the block during time slot this enables the programmer to follow the execution over time and it is possible to track the exact position responsible for accesses causing many cache misses related to coherence actionsmatrix multiplication and the mpd application from splash are used to illustrate the use of sm prof for mpd sm prof revealed performance limitations that resulted in performance improvement of over the current implementation is based on program driven simulation in order to achieve non intrusive profiling if small perturbation of the program execution is acceptable it is also possible to use software tracing techniques given that data address can be related to the originating instruction
many web sites have begun allowing users to submit items to collection and tag them with keywords the folksonomies built from these tags are an interesting topic that has seen little empirical research this study compared the search information retrieval ir performance of folksonomies from social bookmarking web sites against search engines and subject directories thirty four participants created queries for various information needs results from each ir system were collected and participants judged relevance folksonomy search results overlapped with those from the other systems and documents found by both search engines and folksonomies were significantly more likely to be judged relevant than those returned by any single ir system type the search engines in the study had the highest precision and recall but the folksonomies fared surprisingly well delicious was statistically indistinguishable from the directories in many cases overall the directories were more precise than the folksonomies but they had similar recall scores better query handling may enhance folksonomy ir performance further the folksonomies studied were promising and may be able to improve web search performance
the emergence of mobile computing provides the ability to access information at any time and place however as mobile computing environments have inherent factors like power storage asymmetric communication cost and bandwidth limitations efficient query processing and minimum query response time are definitely of great interest this survey groups variety of query optimization and processing mechanisms in mobile databases into two main categories namely query processing strategy and ii caching management strategy query processing includes both pull and push operations broadcast mechanisms we further classify push operation into on demand broadcast and periodic broadcast push operation on demand broadcast relates to designing techniques that enable the server to accommodate multiple requests so that the request can be processed efficiently push operation periodic broadcast corresponds to data dissemination strategies in this scheme several techniques to improve the query performance by broadcasting data to population of mobile users are described caching management strategy defines number of methods for maintaining cached data items in clients local storage this strategy considers critical caching issues such as caching granularity caching coherence strategy and caching replacement policy finally this survey concludes with several open issues relating to mobile query optimization and processing strategy
kernel functions have become an extremely popular tool in machine learning with an attractive theory as well this theory views kernel as implicitly mapping data points into possibly very high dimensional space and describes kernel function as being good for given learning problem if data is separable by large margin in that implicit space however while quite elegant this theory does not necessarily correspond to the intuition of good kernel as good measure of similarity and the underlying margin in the implicit space usually is not apparent in natural representations of the data therefore it may be difficult for domain expert to use the theory to help design an appropriate kernel for the learning task at hand moreover the requirement of positive semi definiteness may rule out the most natural pairwise similarity functions for the given problem domainin this work we develop an alternative more general theory of learning with similarity functions ie sufficient conditions for similarity function to allow one to learn well that does not require reference to implicit spaces and does not require the function to be positive semi definite or even symmetric instead our theory talks in terms of more direct properties of how the function behaves as similarity measure our results also generalize the standard theory in the sense that any good kernel function under the usual definition can be shown to also be good similarity function under our definition though with some loss in the parameters in this way we provide the first steps towards theory of kernels and more general similarity functions that describes the effectiveness of given function in terms of natural similarity based properties
safety and security are top concerns in maritime navigation particularly as maritime traffic continues to grow and as crew sizes are reduced the automatic identification system ais plays key role in regard to these concerns this system whose objective is in part to identify and locate vessels transmits location related information from vessels to ground stations that are part of so called vessel traffic service vts thus enabling these to track the movements of the vessels this paper presents techniques that improve the existing ais by offering better and guaranteed tracking accuracies at lower communication costs the techniques employ movement predictions that are shared between vessels and the vts empirical studies with prototype implementation and real vessel data demonstrate that the techniques are capable of significantly improving the ais
mature knowledge allows engineering disciplines the achievement of predictable results unfortunately the type of knowledge used in software engineering can be considered to be of relatively low maturity and developers are guided by intuition fashion or market speak rather than by facts or undisputed statements proper to an engineering discipline testing techniques determine different criteria for selecting the test cases that will be used as input to the system under examination which means that an effective and efficient selection of test cases conditions the success of the tests the knowledge for selecting testing techniques should come from studies that empirically justify the benefits and application conditions of the different techniques this paper analyzes the maturity level of the knowledge about testing techniques by examining existing empirical studies about these techniques we have analyzed their results and obtained testing technique knowledge classification based on their factuality and objectivity according to four parameters
designing and implementing system software so that it scales well on shared memory multiprocessors smmps has proven to be surprisingly challenging to improve scalability most designers to date have focused on concurrency by iteratively eliminating the need for locks and reducing lock contention however our experience indicates that locality is just as if not more important and that focusing on locality ultimately leads to more scalable system in this paper we describe methodology and framework for constructing system software structured for locality exploiting techniques similar to those used in distributed systems specifically we found two techniques to be effective in improving scalability of smmp operating systems an object oriented structure that minimizes sharing by providing natural mapping from independent requests to independent code paths and data structures and ii the selective partitioning distribution and replication of object implementations in order to improve locality we describe concrete examples of distributed objects and our experience implementing them we demonstrate that the distributed implementations improve the scalability of operating system intensive parallel workloads
memory based collaborative filtering cf makes recommendations based on collection of user preferences for items the idea underlying this approach is that the interests of an active user will more likely coincide with those of users who share similar preferences to the active user hence the choice and computation of similarity measure between users is critical to rating items this work proposes similarity update method that uses an iterative message passing procedure additionally this work deals with drawback of using the popular mean absolute error mae for performance evaluation namely that ignores ratings distribution novel modulation method and an accuracy metric are presented in order to minimize the predictive accuracy error and to evenly distribute predicted ratings over true rating scales preliminary results show that the proposed similarity update and prediction modulation techniques significantly improve the predicted rankings
many empirical studies have found that software metrics can predict class error proneness and the prediction can be used to accurately group error prone classes recent empirical studies have used open source systems these studies however focused on the relationship between software metrics and class error proneness during the development phase of software projects whether software metrics can still predict class error proneness in system's post release evolution is still question to be answered this study examined three releases of the eclipse project and found that although some metrics can still predict class error proneness in three error severity categories the accuracy of the prediction decreased from release to release furthermore we found that the prediction cannot be used to build metrics model to identify error prone classes with acceptable accuracy these findings suggest that as system evolves the use of some commonly used metrics to identify which classes are more prone to errors becomes increasingly difficult and we should seek alternative methods to the metric prediction models to locate error prone classes if we want high accuracy
recently with the broad usage of location aware devices applications with moving object management become very popular in order to manage moving objects efficiently many spatial spatial temporal data access methods have been proposed however most of these data access methods are designed for single user environments in multiple user systems frequent updates may cause significant number of read write conflicts using these data access methods in this paper we propose an efficient framework concurrent location management clam for managing moving objects in multiple user environments the proposed concurrency control protocol integrates the efficiency of the link based approach and the flexibility of the lock coupling mechanism based on this protocol concurrent location update and search algorithms are provided we formally analyze and prove the correctness of the proposed concurrent operations experiment results on real datasets validate the efficiency and scalability of the proposed concurrent location management framework
novel adaptive trust based anonymous network atan is proposed the distributed and decentralised network management in atan does not require central authority so that atan alleviates the problem of single point of failure in some existing anonymous networks packets are routed onto intermediate nodes anonymously without knowing whether these nodes are trustworthy on the other hand an intermediate node should ensure that packets which it forwards are not malicious and it will not be allegedly accused of involving in the attack to meet these objectives the intermediate node only forwards packets received from the trusted predecessor which can be either the source or another intermediate node in atan our trust and reputation model aims to enhance anonymity by establishing trust and reputation relationship between the source and the forwarding members the trust and reputation relationship of any two nodes is adaptive to new information learned by these two nodes or recommended from other trust nodes therefore packets are anonymously routed from the trusted source to the destination through trusted intermediate nodes thereby improving anonymity of communications
it is time honored fashion to implement domain specific language dsl by translation to general purpose language such an implementation is more portable but an unidiomatic translation jeopardizes performance because in practice language implementations favor the common cases this tension arises especially when the domain calls for complex control structures we illustrate this tension by revisiting landin's original correspondence between algol and church's lambda notationwe translate domain specific programs with lexically scoped jumps to javascript our translation produces the same block structure and binding structure as in the source program la abdali the target code uses control operator in direct style la landin in fact the control operator used is almost landin's hence our title our translation thus complements continuation passing translation la steele these two extreme translations require javascript implementations to cater either for first class continuations as rhino does or for proper tail recursion less extreme translations should emit more idiomatic control flow instructions such as for break and throwthe present experiment leads us to conclude that translations should preserve not just the data structures and the block structure of source program but also its control structure we thus identify new class of use cases for control structures in javascript namely the idiomatic translation of control structures from dsls
reconfigurable place transition systems are petri nets with initial markings and set of rules which allow the modification of the net during runtime in order to adapt the net to new requirements of the environment in this paper we use transformation rules for place transition systems in the sense of the double pushout approach for graph transformation the main problem in this context is to analyze under which conditions net transformations and token firing can be executed in arbitrary order this problem is solved in the main theorems of this paper reconfigurable place transition systems then are applied in mobile network scenario
the development of complex products such as automobiles involves engineering changes that frequently require redesigning or altering the products although it has been found that efficient management of knowledge and collaboration in engineering changes is crucial for the success of new product development extant systems for engineering changes focus mainly on storing documents related to the engineering changes or simply automating the approval processes while the knowledge that is generated from collaboration and decision making processes may not be captured and managed easily this consequently limits the use of the systems by the participants in engineering change processes this paper describes model for knowledge management and collaboration in engineering change processes and based on the model builds prototype system that demonstrates the model's strengths we studied major korean automobile company to analyze the automobile industry's unique requirements regarding engineering changes we also developed domain ontologies from the case to facilitate knowledge sharing in the design process for achieving efficient retrieval and reuse of past engineering changes we used case based reasoning cbr with concept based similarity measure
in this paper we introduce cacti significant enhancement of cacti cacti adds support for modeling of commodity dram technology and support for main memory dram chip organization cacti enables modeling of the complete memory hierarchy with consistent models all the way from sram based caches through main memory drams on dimms we illustrate the potential applicability of cacti in the design and analysis of future memory hierarchies by carrying out last level cache study for multicore multithreaded architecture at the nm technology node in this study we use cacti to model all components of the memory hierarchy including last level sram logic process based dram or commodity dram caches and main memory dram chips we carry out architectural simulation using benchmarks with large data sets and present results of their execution time breakdown of power in the memory hierarchy and system energy delay product for the different system configurations we find that commodity dram technology is most attractive for stacked last level caches with significantly lower energy delay products
the nosq microarchitecture performs store load communication without store queue and without executing stores in the out of order engine it uses speculative memory bypassing for all in flight store load communication enabled by percent accurate store load communication predictor the result is simple fast core data path containing no dedicated store load forwarding structures
customer relationship management crm is an important concept to maintain competitiveness at commerce thus many organizations hastily implement ecrm and fail to achieve its goal crm concept consists of number of compound components on product designs marketing attributes and consumer behaviors this requires different approaches from traditional ones in developing ecrm requirements engineering is one of the important steps in software development without well defined requirements specification developers do not know how to proceed with requirements analysis this research proposes strategy based process for requirements elicitation this framework contains three steps define customer strategies identify consumer and marketing characteristics and determine system requirements prior literature lacks discussing the important role of customer strategies in ecrm development empirical findings reveal that this strategy based view positively improves the performance of requirements elicitation
we present new rigging and skinning method which uses database of partial rigs extracted from set of source characters given target mesh and set of joint locations our system can automatically scan through the database to find the best fitting body parts tailor them to match the target mesh and transfer their skinning information onto the new character for the cases where our automatic procedure fails we provide an intuitive set of tools to fix the problems when used fully automatically the system can generate results of much higher quality than standard smooth bind and with some user interaction it can create rigs approaching the quality of artist created manual rigs in small fraction of the time
group comparison per se is fundamental task in many scientific endeavours but is also the basis of any classifier contrast sets and emerging patterns contrast between groups of categorical data comparing groups of sequence data is relevant task in many applications we define emerging sequences ess as subsequences that are frequent in sequences of one group and less frequent in the sequences of another and thus distinguishing or contrasting sequences of different classes there are two challenges to distinguish sequence classes the extraction of ess is not trivially efficient and only exact matches of sequences are considered in our work we address those problems by suffix tree based framework and similar matching mechanism we propose classifier based on emerging sequences evaluating against two learning algorithms based on frequent subsequences and exact matching subsequences the experiments on two datasets show that our model outperforms the baseline approaches by up to in prediction accuracy
raid robust and adaptable distributed database system for transaction processing is described raid is message passing system with server processes on each site the servers manage concurrent processing consistent replicated copies during site failures and atomic distributed commitment high level layered communications package provides clean location independent interface between servers the latest design of the communications package delivers messages via shared memory in high performance configuration in which several servers are linked into single process raid provides the infrastructure to experimentally investigate various methods for supporting reliable distributed transaction processing measurements on transaction processing time and server cpu time are presented data and conclusions of experiments in three categories are also presented communications software consistent replicated copy control during site failures and concurrent distributed checkpointing software tool for the evaluation of transaction processing algorithms in an operating system kernel is proposed
this paper introduces framework for long distance face recognition using both dense and sparse stereo reconstruction two methods to determine correspondences of the stereo pair are used in this paper dense global stereo matching using maximum posteriori markov random fields map mrf algorithms and active appearance model aam fitting of both images of the stereo pair and using the fitted aam mesh as the sparse correspondences experiments are performed regarding the use of different features extracted from these vertices for face recognition comparison between the two approaches and are carried out in this paper the cumulative rank curves cmc which are generated using the proposed framework confirms the feasibility of the proposed work for long distance recognition of human faces
chip multi processor cmp architectures have become mainstream for designing processors with large number of cores networks on chip nocs provide scalable communication method for cmp architectures nocs must be carefully designed to meet constraints of power consumption and area and provide ultra low latencies existing nocs mostly use dimension order routing dor to determine the route taken by packet in unicast traffic however with the development of diverse applications in cmps one to many multicast and one to all broadcast traffic are becoming more common current unicast routing cannot support multicast and broadcast traffic efficiently in this paper we propose recursive partitioning multicast rpm routing and detailed multicast wormhole router design for nocs rpm allows routers to select intermediate replication nodes based on the global distribution of destination nodes this provides more path diversities thus achieves more bandwidth efficiency and finally improves the performance of the whole network our simulation results using detailed cycle accurate simulator show that compared with the most recent multicast scheme rpm saves of crossbar and link power and of link utilization with network performance improvement also rpm is more scalable to large networks than the recently proposed vctm
in this paper we proposed an efficient and accurate text chunking system using linear svm kernel and new technique called masked method previous researches indicated that systems combination or external parsers can enhance the chunking performance however the cost of constructing multi classifiers is even higher than developing single processor moreover the use of external resources will complicate the original tagging process to remedy these problems we employ richer features and propose masked based method to solve unknown word problem to enhance system performance in this way no external resources or complex heuristics are required for the chunking system the experiments show that when training with the conll chunking dataset our system achieves in rate with linear furthermore our chunker is quite efficient since it adopts linear kernel svm the turn around tagging time on conll testing data is less than which is about times than polynomial kernel svm
type systems for secure information flow are useful for efficiently checking that programs have secure information flow they are however conservative so that they often reject safe programs as ill typed accordingly users have to check whether the rejected programs indeed have insecure flows to remedy this problem we propose method for automatically finding counterexample of secure information flow input states that actually lead to leakage of secret information our method is novel combination of type based analysis and model checking suspicious execution paths that may cause insecure information flow are first found by using the result of type based information flow analysis and then model checker is used to check whether the paths are indeed unsafe we have formalized and implemented the method the result of preliminary experiments shows that our method can often find counterexamples faster than method using model checker alone
linear constraint databases and query languages are appropriate for spatial database applications not only is the data model suitable for representing large portion of spatial data such as in gis systems but there also exist efficient algorithms for the core operations in the query languages an important limitation of linear constraints however is that they cannot model constructs such as euclidean distance extending such languages to include such constructs without obtaining the full power of polynomial constraints has proven to be quite difficult one approach to this problem by kuijpers kuper paredaens and vandeurzen used the notion of euclidean constructions with ruler and compass as the basis for first order query language while their language had the desired expressive power the semantics are not really natural due to its use of an ad hoc encoding in this paper we define language over similar class of databases with more natural semantics we show that this language captures natural subclass the representation independent queries of the first order language of kuijpers kuper paredaens and vandeurzen
this paper presents mobicausal new protocol to implement causal ordering in mobile computing systems the implementation of causal ordering proposed in this paper uses the new timestamping mechanisms proposed by prakash and singhal for mobile environments dependency sequences and hierarchical clocks our protocol compared with previous proposals is characterized by the elimination of unnecessary inhibition delay in delivering messages while maintaining low message overhead our protocol requires minimal resources on mobile hosts and wireless links the proposed protocol is also scalable and can easily handle dynamic change in the number of participating mobile hosts in the system
many tasks require attention switching for example searching for information on one sheet of paper and then entering this information onto another one with paper we see that people use fingers or objects as placeholders using these simple aids the process of switching attention between displays can be simplified and speeded up with large or multiple visual displays we have many tasks where both attention areas are on the screen and where using finger as placeholder is not suitable one way users deal with this is to use the mouse and highlight their current focus however this also has its limitations in particular in environments where there is no pointing device our approach is to utilize the user's gaze position to provide visual placeholder the last area where user fixated on the screen before moving their attention away is highlighted we call this visual reminder gazemark gazemarks ease orientation and the resumption of the interrupted task when coming back to this display in this paper we report on study where the effectiveness of using gazemarks was investigated in particular we show how they can ease attention switching our results show faster completion times for resumed simple visual search task when using this technique the paper analyzes relevant parameters for the implementation of gazemarks and discusses some further application areas for this approach
method for the automatic generation of test scenarios from the behavioral requirements of system is presented in this paper the generated suite of test scenarios validates the system design or implementation against the requirements the approach proposed here uses requirements model and set of four algorithms the requirements model is an executable model of the proposed system defined in deterministic state based modeling formalism each action in the requirements model that changes the state of the model is identified with unique requirement identifier the scenario generation algorithms perform controlled simulations of the requirements model in order to generate suite of test scenarios applicable for black box testing measurements of several metrics on the scenario generation algorithms have been collected using prototype tools
we consider ad hoc radio networks in which each node knows only its own identity but is unaware of the topology of the network or of any bound on its size or diameter acknowledged broadcasting ab is communication task consisting in transmitting message from distinguished source to all other nodes of the network and making this fact common knowledge among all nodes to do this the underlying directed graph must be strongly connected working in model allowing all nodes to transmit spontaneously even before getting the source message chlebus et al chlebus ga sieniec gibbons pelc rytter deterministic broadcasting in unknown radio networks distrib comput proved that ab is impossible if collision detection is not available and gave an ab algorithm using collision detection that works in time nd where is the number of nodes and is the eccentricity of the source uchida et al uchida chen wada acknowledged broadcasting and gossiping in ad hoc radio networks theoret comput sci showed an ab algorithm without collision detection working in time log for all strongly connected networks of size at least in particular it follows that the impossibility result from chlebus ga sieniec gibbons pelc rytter deterministic broadcasting in unknown radio networks distrib comput is really caused by the singleton network for which ab amounts to realize that the source is alone we improve those two results by presenting two generic ab algorithms using broadcasting algorithm without acknowledgement as procedure for large class of broadcasting algorithms the resulting ab algorithm has the same time complexity using the currently best known broadcasting algorithms we obtain an ab algorithm with collision detection working in time min nlog nlognloglogn for arbitrary strongly connected networks and an ab algorithm without collision detection working in time nlognloglogn for all strongly connected networks of size moreover we show that in the model in which only nodes that already got the source message can transmit ab is infeasible in strong sense for any ab algorithm there exists an infinite family of networks for which this algorithm is incorrect
spoken language generation for dialogue systems requires dictionary of mappings between the semantic representations of concepts that the system wants to express and the realizations of those concepts dictionary creation is costly process it is currently done by hand for each dialogue domain we propose novel unsupervised method for learning such mappings from user reviews in the target domain and test it in the restaurant and hotel domains experimental results show that the acquired mappings achieve high consistency between the semantic representation and the realization and that the naturalness of the realization is significantly higher than the baseline
in this paper we propose mobile terminal mt location registration update model in this model the registration decision is based on two factors the time elapsed since last call arrival and the distance the mt has traveled since last registration it is established that the optimal registration strategy can be represented by curve only when the state of the system reaches this curve is registration performed in order for an mt to calculate its traveled distance an interactive implementation scheme and distance calculation algorithm are developed when the call interarrival times are independent and geometrically distributed the proposed model becomes distance based model and in this case the optimal registration strategy is of threshold structure for the distance based model single sample path based ordinal optimization algorithm is devised in this algorithm without any knowledge about the system parameters the mt observes the system state transitions estimates the ordinal of set of strategies and updates the registration strategy adaptively since only single sample path is used this algorithm can be implemented online several numerical examples are provided to compare the proposed model and the existing ones
in mobile and wireless environments mobile clients can access information with respect to their locations by submitting location dependent spatial queries ldsqs to location based service lbs servers owing to scarce wireless channel bandwidth and limited client battery life frequent ldsq submission from clients must be avoided observing that ldsqs issued from similar client positions would normally return the same results we explore the idea of valid scope that represents spatial area in which set of ldsqs will retrieve exactly the same query results with valid scope derived and an ldsq result cached at the client side client can assert whether the new ldsqs can be answered with the maintained ldsq result thus eliminating the ldsqs sent to the server as such contention on wireless channel and client energy consumed for data transmission can be considerably reduced in this paper we design efficient algorithms to compute the valid scope for common types of ldsqs including nearest neighbor queries and range queries through an extensive set of experiments our proposed valid scope computation algorithms are shown to significantly outperform existing approaches
we present framework for real time animation of explosions that runs completely on the gpu the simulation allows for arbitrary internal boundaries and is governed by combustion process stable fluid solver which includes thermal expansion and turbulence modeling the simulation results are visualised by two particle systems rendered using animated textures the results are physically based non repeating and dynamic real time explosions with high visual quality
in this paper we propose memory reduction as new approach to data locality enhancement under this approach we use the compiler to reduce the size of the data repeatedly referenced in collection of nested loops between their reuses the data will more likely remain in higher speed memory devices such as the cache specifically we present an optimal algorithm to combine loop shifting loop fusion and array contraction to reduce the temporary array storage required to execute collection of loops when applied to benchmark programs our technique reduces the memory requirement counting both the data and the code by on average the transformed programs gain speedup of on average due to the reduced footprint and consequently the improved data locality
domain model which captures the common knowledge and the possible variability allowed among applications in domain may assist in the creation of other valid applications in that domain however to create such domain models is not trivial task it requires expertise in the domain reaching very high level of abstraction and providing flexible yet formal artifacts in this paper an approach called semi automated domain modeling sdm to create draft domain models from applications in those domains is presented sdm takes repository of application models in domain and matches merges and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains the similarity of the different elements is measured with consideration of syntactic semantic and structural aspects unlike ontology and schema integration these models capture both structural and behavioral aspects of the domain running sdm on small repositories of project management applications and scheduling systems we found that the approach may provide reasonable draft domain models whose comprehensibility correctness completeness and consistency levels are satisfactory
this paper tackles the problem of structural integration testing of stateful classes previous work on structural testing of objectoriented software exploits data flow analysis to derive test requirements for class testing and defines contextual def use associations to characterize inter method relations non contextual data flow testing of classes works well for unit testing but not for integration testing since it misses definitions and uses when properly encapsulated contextual data flow analysis approaches investigated so far either do not focus on state dependent behavior or have limited applicability due to high complexity this paper proposes an efficient structural technique based on contextual data flow analysis to test state dependent behavior of classes that aggregate other classes as part of their state
the augmented graph model as introduced in kleinberg stoc is an appealing model for analyzing navigability in social networks informally this model is defined by pair where is graph in which inter node distances are supposed to be easy to compute or at least easy to estimate this graph is augmented by links called long range links that are selected according to the probability distribution the augmented graph model enables the analysis of greedy routing in augmented graphs in greedy routing each intermediate node handling message for target selects among all its neighbors in the one that is the closest to in and forwards the message to it this paper addresses the problem of checking whether given graph is an augmented graph it answers part of the questions raised by kleinberg in his problem int congress of math more precisely given we aim at extracting the base graph and the long range links out of we prove that if has high clustering coefficient and has bounded doubling dimension then simple local maximum likelihood algorithm enables us to partition the edges of into two sets and such that and the edges in are of small stretch ie the map is not perturbed too greatly by undetected long range links remaining in the perturbation is actually so small that we can prove that the expected performances of greedy routing in using the distances in are close to the expected performances of greedy routing using the distances in although this latter result may appear intuitively straightforward since it is not as we also show that routing with map more precise than may actually damage greedy routing significantly finally we show that in the absence of hypothesis regarding the high clustering coefficient any local maximum likelihood algorithm extracting the long range links can miss the detection of logn long range links of stretch for any
we propose novel algorithm terminated ramp support vector machines tr svm for classification and feature ranking purposes in the family of support vector machines the main improvement relies on the fact that the kernel is automatically determined by the training examples it is built as function of simple classifiers generalized terminated ramp functions obtained by separating oppositely labeled pairs of training points the algorithm has meaningful geometrical interpretation and it is derived in the framework of tikhonov regularization theory its unique free parameter is the regularization one representing trade off between empirical error and solution complexity employing the equivalence between the proposed algorithm and two layer networks theoretical bound on the generalization error is also derived together with vapnik chervonenkis dimension performances are tested on number of synthetic and real data sets
verifying properties of large real world programs requires vast quantities of information on aspects such as procedural contexts loop invariants or pointer aliasing it is unimaginable to have all these properties provided to verification tool by annotations from the user static analysis will clearly play key role in the design of future verification engines by automatically discovering the bulk of this information the body of research in static program analysis can be split up in two major areas one probably the larger in terms of publications is concerned with discovering properties of data structures shape analysis pointer analysis the other addresses the inference of numerical invariants for integer or floating point algorithms range analysis propagation of round off errors in numerical algorithms we will call the former symbolic static analysis and the latter numerical static analysis both areas were successful in effectively analyzing large applications however symbolic and numerical static analysis are commonly regarded as entirely orthogonal problems for example pointer analysis usually abstracts away all numerical values that appear in the program whereas the floating point analysis tool astree does not abstract memory at all
in mobile computing environment database servers disseminate information to multiple mobile clients via wireless channels due to the low bandwidth and low reliability of wireless channels it is important for mobile client to cache its frequently accessed database items into its local storage this improves performance of database queries and improves availability of database items for query processing during disconnection in this paper we investigate issues on caching granularity coherence strategy and replacement policy of caching mechanisms for mobile environment utilizing point to point communication paradigmwe first illustrate that page based caching is not suitable in the mobile context due to the lack of locality among database items we propose three different levels of caching granularity attribute caching object caching and hybrid caching hybrid approach of attribute and object caching next we show that existing coherence strategies are inappropriate due to frequent disconnection in mobile environment and propose cache coherence strategy based on the update patterns of database items via detail simulation model we examine the performance of various levels of caching granularity with our cache coherence strategy we observe in general that hybrid caching could achieve better performance finally we propose several cache replacement policies that can adapt to the access patterns of database items for each given caching granularity we discover that our replacement policies outperform conventional ones in most situations
prior work including our own shows that application performance in garbage collected languages is highly dependent upon the application behavior and on underlying resource availability we show that given wide range of diverse garbage collection gc algorithms no single system performs best across programs and heap sizes we present java virtual machine extension for dynamic and automatic switching between diverse widely used gcs for application specific garbage collection selection we describe annotation guided and automatic gc switching we also describe novel extension to extant on stack replacement osr mechanisms for aggressive gc specialization that is readily amenable to compiler optimization
the paper considers the consensus problem in partially synchronous system with byzantine processes in this context the literature distinguishes authenticated byzantine faults where messages can be signed by the sending process with the assumption that the signature cannot be forged by any other process and byzantine faults where there is no mechanism for signatures but the receiver of message knows the identity of the sender the paper proposes an abstraction called weak interactive consistency wic that unifies consensus algorithms with and without signed messages wic can be implemented with and without signaturesthe power of wic is illustrated on two seminal byzantine consensus algorithms the castro liskov pbft algorithm no signatures and the martin alvisi fab paxos algorithms signatures wic allows very concise expression of these two algorithms
we present novel algorithms that optimize the order in which triangles are rendered to improve post transform vertex cache efficiency as well as for view independent overdraw reduction the resulting triangle orders perform on par with previous methods but are orders magnitude faster to compute the improvements in processing speed allow us to perform the optimization right after model is loaded when more information on the host hardware is available this allows our vertex cache optimization to often outperform other methods in fact our algorithms can even be executed interactively allowing for re optimization in case of changes to geometry or topology which happen often in cad cam applications we believe that most real time rendering applications will immediately benefit from these new results
this paper presents simple but effective method to reduce on chip access latency and improve core isolation in cmp non uniform cache architectures nuca the paper introduces feasible way to allocate cache blocks according to the access pattern each bank is dynamically partitioned at set level in private and shared content simply by adjusting the replacement algorithm we can place private data closer to its owner processor in contrast independently of the accessing processor shared data is always placed in the same position this approach is capable of reducing on chip latency without significantly sacrificing hit rates or increasing implementation cost of conventional static nuca additionally most of the unnecessary interference between cores in private accesses is removed to support the architectural decisions adopted and provide comparative study comprehensive evaluation framework is employed the workbench is composed of full system simulator and representative set of multithreaded and multiprogrammed workloads with this infrastructure different alternatives for the coherence protocol replacement policies and cache utilization are analyzed to find the optimal proposal we conclude that the cost for feasible implementation should be closer to conventional static nuca and significantly less than dynamic nuca finally comparison with static and dynamic nuca is presented the simulation results suggest that on average the mechanism proposed could improve system performance of static nuca and idealized dynamic nuca by and respectively
artists often need to import and embellish models coming from cad cam into vector graphics software to produce eg brochures or manuals current automatic solutions tend to result at best in triangle soup and artists often have to trace over renderings we describe method to convert models into layered vector illustrations that respect visibility and facilitate further editing our core contribution is visibility method that can partition mesh into large components that can be layered according to visibility because self occluding objects and objects forming occlusion cycles cannot be represented by layers without being cut we introduce new cut algorithm that uses graph representation of the mesh and curvature aware geodesic distances
class sharing is new language mechanism for building extensible software systems recent work has separately explored two different kinds of extensibility first family inheritance in which an entire family of related classes can be inherited and second adaptation in which existing objects are extended in place with new behavior and state class sharing integrates these two kinds of extensibility mechanisms with little programmer effort objects of one family can be used as members of another while preserving relationships among objects therefore family of classes can be adapted in place with new functionality spanning multiple classes object graphs can evolve from one family to another adding or removing functionality even at run time several new mechanisms support this flexibility while ensuring type safety class sharing has been implemented as an extension to java and its utility for evolving and extending software is demonstrated with realistic systems
spurred by range of potential applications there has been growing body of research in computational models of human emotion to advance the development of these models it is critical that we evaluate them against the phenomena they purport to model in this paper we present one method to evaluate an emotion model that compares the behavior of the model against human behavior using standard clinical instrument for assessing human emotion and coping we use this method to evaluate the emotion and adaptation ema model of emotion gratch and marsella the evaluation highlights strengths of the approach and identifies where the model needs further development
for set of points in ‚Ñùd spanner is sparse graph on the points of such that between any pair of points there is path in the spanner whose total length is at most times the euclidean distance between the points in this paper we show how to construct epsilon spanner with epsilon edges and maximum degree epsilon in time log spanner with similar properties was previously presented in however using our new construction coupled with several other innovations we obtain new results for two fundamental problems for constant doubling dimension metrics the first result is an essentially optimal compact routing scheme in particular we show how to perform routing with stretch of isin where the label size is log and the size of the table stored at each point is only log epsilon this routing problem was first considered by peleg and hassin who presented routing scheme in the plane later chan et al and abraham et al considered this problem for doubling dimension metric spaces abraham et al were the first to present isin routing scheme where the label size depends solely on the number of points in their scheme labels are of size of log and each point stores table of size log epsilon in our routing scheme we achieve routing tables of size log epsilon which is essentially the same size as label up to the factor of epsilon the second and main result of this paper is the first fully dynamic geometric spanner with poly logarithmic update time for both insertions and deletions we present an algorithm that allows points to be inserted into and deleted from with an amortized update time of log
this paper describes the security framework that is to be developed for the generic grid platform created for the project gredia this platform is composed of several components that need to be secured the platform uses the ogsa standards so that the security framework will follow gsi the portion of globus that implements security thus we will show the security features that gsi already provides and we will outline which others need to be created or enhanced
in parallel adaptive applications the computational structure of the applications changes over time leading to load imbalances even though the initial load distributions were balanced to restore balance and to keep communication volume low in further iterations of the applications dynamic load balancing repartitioning of the changed computational structure is required repartitioning differs from static load balancing partitioning due to the additional requirement of minimizing migration cost to move data from an existing partition to new partition in this paper we present novel repartitioning hypergraph model for dynamic load balancing that accounts for both communication volume in the application and migration cost to move data in order to minimize the overall cost the use of hypergraph based model allows us to accurately model communication costs rather than approximate them with graph based models we show that the new model can be realized using hypergraph partitioning with fixed vertices and describe our parallel multilevel implementation within the zoltan load balancing toolkit to the best of our knowledge this is the first implementation for dynamic load balancing based on hypergraph partitioning to demonstrate the effectiveness of our approach we conducted experiments on linux cluster with processors the results show that in terms of reducing total cost our new model compares favorably to the graph based dynamic load balancing approaches and multilevel approaches improve the repartitioning quality significantly
secure broadcasting of web documents is becoming crucial requirement for many web based applications under the broadcast document dissemination strategy web document source periodically broadcasts portions of its documents to potentially large community of users without the need for explicit requests by secure broadcasting we mean that the delivery of information to users must obey the access control policies of the document source traditional access control mechanisms that have been adapted for xml documents however do not address the performance issues inherent in access control in this paper labeling scheme is proposed to support rapid reconstruction of xml documents in the context of well known method called xml pool encryption the proposed labeling scheme supports the speedy inference of structure information in all portions of the document the binary representation of the proposed labeling scheme is also investigated in the experimental results the proposed labeling scheme is efficient in searching for the location of decrypted information
this paper presents the alpha ev conditional branch predictor the alpha ev microprocessor project canceled in june in late phase of development envisioned an aggressive wide issue out of order superscalar microarchitecture featuring very deep pipeline and simultaneous multithreading performance of such processor is highly dependent on the accuracy of its branch predictor and consequently very large silicon area was devoted to branch prediction on ev the alpha ev branch predictor relies on global history and features total of kbitsthe focus of this paper is on the different trade offs performed to overcome various implementation constraints for the ev branch predictor one such instance is the pipelining of the predictor on two cycles to facilitate the prediction of up to branches per cycle from any two dynamically successive instruction fetch blocks this resulted in the use of three fetch block old compressed branch history information for accesing the predictor implementation constraints also restricted the composition of the index functions for the predictor and forced the usage of only single ported memory cellsnevertheless we show that the alpha ev branch predictor achieves prediction accuracy in the same range as the state of the art academic global history branch predictors that do not consider implementation constraints in great detail
the widespread use of internet based services is increasing the amount of information such as user profiles that clients are required to disclose this information demand is necessary for regulating access to services and functionally convenient eg to support service customization but it has raised privacy related concerns which if not addressed may affect the users disposition to use network services at the same time servers need to regulate service access without disclosing entirely the details of their access control policy there is therefore pressing need for privacy aware techniques to regulate access to services open to the networkwe propose an approach for regulating service access and information disclosure on the web the approach consists of uniform formal framework to formulate and reason about both service access and information disclosure constraints it also provides means for parties to communicate their requirements while ensuring that no private information be disclosed and that the communicated requirements are correct with respect to the constraints
mobile is an extension of the net common intermediate language that supports certified in lined reference monitoring mobile programs have the useful property that if they are well typed with respect to declared security policy then they are guaranteed not to violate that security policy when executed thus when an in lined reference monitor irm is expressed in mobile it can be certified by simple type checker to eliminate the need to trust the producer of the irmsecurity policies in mobile are declarative can involve unbounded collections of objects allocated at runtime and can regard infinite length histories of security events exhibited by those objects the prototype mobile implementation enforces properties expressed by finite state security automata one automaton for each security relevant object and can type check mobile programs in the presence of exceptions finalizers concurrency and non termination executing mobile programs requires no change to existing net virtual machine implementations since mobile programs consist of normal managed cil code with extra typing annotations stored in net attributes
the fields of machine learning and mathematical programming are increasingly intertwined optimization problems lie at the heart of most machine learning approaches the special topic on machine learning and large scale optimization examines this interplay machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued the special topic includes models using quadratic linear second order cone semi definite and semi infinite programs we observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different mathematical programming puts premium on accuracy speed and robustness since generalization is the bottom line in machine learning and training is normally done off line accuracy and small speed improvements are of little concern in machine learning machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems reducing machine learning problems to well explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques in turn machine learning presents new challenges to mathematical programming the special issue include papers from two primary themes novel machine learning models and novel optimization approaches for existing models many papers blend both themes making small changes in the underlying core mathematical program that enable the develop of effective new algorithms
we propose xrpc minimal xquery extension that enables distributed yet efficient querying of heterogeneous xquery data sources xrpc enhances the existing concept of xquery functions with the remote procedure call rpc paradigm by calling out of an xquery for loop to multiple destinations and by calling functions that themselves perform xrpc calls complex pp communication patterns can be achieved the xrpc extension is orthogonal to all xquery features including the xquery update facility xquf we provide formal semantics for xrpc that encompasses execution of both read only and update queries xrpc is also network soap sub protocol that integrates seamlessly with web services and service oriented architectures soa and ajax based guis crucial feature of the protocol is bulk rpc that allows remote execution of many different calls to the same procedure using possibly single network round trip the efficiency potential of xrpc is demonstrated via an open source implementation in monetdb xquery we show however that xrpc is not system specific every xquery data source can service xrpc calls using wrapper since xquery is pure functional language we can leverage techniques developed for functional query decomposition to rewrite data shipping queries into xrpc based function shipping queries powerful distributed database techniques such as semi join optimizations directly map on bulk rpc opening up interesting future work opportunities
haptic gestures and sensations through the sense of touch are currently unavailable in remote communication there are two main reasons for this good quality haptic technology has not been widely available and knowledge on the use of this technology is limited to address these challenges we studied how users would like to and managed to create spatial haptic information by gesturing two separate scenario based experiments were carried out an observation study without technological limitations and study on gesturing with functional prototype with haptic actuators the first study found three different use strategies for the device the most common gestures were shaking smoothing and tapping multimodality was requested to create the context for the communication and to aid the interpretation of haptic stimuli the second study showed that users were able to utilize spatiality in haptic messages eg forward backward gesture for agreement however challenges remain in presenting more complex information via remote haptic communication the results give guidance for communication activities that are usable in spatial haptic communication and how to make it possible to enable this form of communication in reality
as parallelism in microprocessors becomes mainstream new prog ramming languages and environments are emerging to meet the challenges of parallel programming to support research on these languages we are developing low level language infrastructure called pillar derived from parallel implementation language although pillar programs are intended to be automatically generated from source programs in each parallel language pillar programs can also be written by expert programmers the language is defined as small set of extensions to as result pillar is familiar to programmers but more importantly it is practical to reuse an existing optimizing compiler like gcc or open to implement pillar compilerpillar's concurrency features include constructs for threading synchronization and explicit data parallel operations the threading constructs focus on creating new threads only when hardware resources are idle and otherwise executing parallel work within existing threads thus minimizing thread creation overhead in addition to the usual synchronization constructs pillar includes transactional memory its sequential features include stack walking second class continuations support for precise garbage collection tail calls and seamless integration of pillar and legacy code this paper describes the design and implementation of the pillar software stack including the language compiler runtime and high level converters that translate high level language programs into pillar programs it also reports on early experience with three high level languages that target pillar
real time java is quickly emerging as platform for building safety critical embedded systems the real time variants of java including are attractive alternatives to ada and since they provide cleaner simpler and safer programming model unfortunately current real time java implementations have trouble scaling down to very hard real time embedded settings where memory is scarce and processing power is limited in this paper we describe the architecture of the fiji vm which enables vanilla java applications to run in very hard environments including booting on bare hardware with only very rudimentary operating system support we also show that our minimalistic approach delivers comparable performance to that of server class production java virtual machine implementations
research on ontologies is becoming widespread in the biomedical informatics community at the same time it has become apparent that the challenges of properly constructing and maintaining ontologies have proven more difficult than many workers in the field initially expected discovering general feasible methods has thus become central activity for many of those hoping to reap the benefits of ontologies this paper reviews current methods in the construction maintenance alignment and evaluation of ontologies
the classification performance of nearest prototype classifiers largely relies on the prototype learning algorithm the minimum classification error mce method and the soft nearest prototype classifier snpc method are two important algorithms using misclassification loss this paper proposes new prototype learning algorithm based on the conditional log likelihood loss cll which is based on the discriminative model called log likelihood of margin logm regularization term is added to avoid over fitting in training as well as to maximize the hypothesis margin the cll in the logm algorithm is convex function of margin and so shows better convergence than the mce in addition we show the effects of distance metric learning with both prototype dependent weighting and prototype independent weighting our empirical study on the benchmark datasets demonstrates that the logm algorithm yields higher classification accuracies than the mce generalized learning vector quantization glvq soft nearest prototype classifier snpc and the robust soft learning vector quantization rslvq and moreover the logm with prototype dependent weighting achieves comparable accuracies to the support vector machine svm classifier
the evolution of multimedia technology and the internet boost the multimedia sharing and searching activities among social networks the requirements of semantic multimedia retrieval goes far beyond those provided by the text based search engines technology here we present an collaborative approach that enables the semantic search of the multimedia objects by the collective discovery and meaningful indexing of their semantic concepts through the successive use of our model semantic concepts can be discovered and incorporated by analyzing the users search queries relevance feedback and selection patterns eventually through the growth and evolution of the index hierarchy the semantic index can be dynamically constructed validated and naturally built up towards the expectation of the social network
in this study we describe methodology to exploit specific type of domain knowledge in order to find tighter error bounds on the performance of classification via support vector machines the domain knowledge we consider is that the input space lies inside of specified convex polytope first we consider prior knowledge about the domain by incorporating upper and lower bounds of attributes we then consider more general framework that allows us to encode prior knowledge in the form of linear constraints formed by attributes by using the ellipsoid method from optimization literature we show that this can be exploited to upper bound the radius of the hyper sphere that contains the input space and enables us to tighten generalization error bounds we provide comparative numerical analysis and show the effectiveness of our approach
common geographic information systems gis require high degree of expertise from its users making them difficult to be operated by laymen this paper describes novel approaches to easily perform typical basic spatial tasks within gis eg pan zoom and selection operations by using multi touch gestures in combination with foot gestures we are interested in understanding how non expert users interact with such multi touch surfaces we provide categorization and framework of multi touch hand gestures for interacting with gis this framework is based on an initial evaluation we present results of more detailed in situ study mainly focusing on multi user multi touch interaction with geospatial data furthermore we extend our framework using combination of multi touch gestures with small set of foot gestures to solve geospatial tasks
we study the problem of continuous monitoring of top queries over multiple non synchronized streams assuming sliding window model this general problem has been well addressed research topic in recent years most approaches however assume synchronized streams where all attributes of an object are known simultaneously to the query processing engine in many streaming scenarios though different attributes of an item are reported in separate non synchronized streams which do not allow for exact score calculations we present how the traditional notion of object dominance changes in this case such that the dominance set still includes all and only those objects which have chance of being among the top results in their life time based on this we propose an exact algorithm which builds on generating multiple instances of the same object in way that enables efficient object pruning we show that even with object pruning the necessary storage for exact evaluation of top queries is linear in the size of the sliding window as data should reside in main memory to provide fast answers in an online fashion and cope with high stream rates storing all this data may not be possible with limited resources we present an approximate algorithm which leverages correlation statistics of pairs of streams to evict more objects while maintaining accuracy we evaluate the efficiency of our proposed algorithms with extensive experiments
this paper studies inductive definitions involving binders in which aliasing between free and bound names is permitted such aliasing occurs in informal specifications of operational semantics but is excluded by the common representation of binding as meta level abstraction drawing upon ideas from functional logic programming we represent such definitions with aliasing as recursively defined functions in higher order typed functional programming language that extends core ml with types for name binding type of semi decidable propositions and existential quantification for types with decidable equality we show that the representation is sound and complete with respect to the language's operational semantics which combines the use of evaluation contexts with constraint programming we also give new and simple proof that the associated constraint problem is np complete
the automated categorization or classification of texts into predefined categories has witnessed booming interest in the last years due to the increased availability of documents in digital form and the ensuing need to organize them in the research community the dominant approach to this problem is based on machine learning techniques general inductive process automatically builds classifier by learning from set of preclassified documents the characteristics of the categories the advantages of this approach over the knowledge engineering approach consisting in the manual definition of classifier by domain experts are very good effectiveness considerable savings in terms of expert labor power and straightforward portability to different domains this survey discusses the main approaches to text categorization that fall within the machine learning paradigm we will discuss in detail issues pertaining to three different problems namely document representation classifier construction and classifier evaluation
this paper describes new approach to indexing time series indexing time series is more problematic than indexing text since in extreme we need to find all possible subsequences in the time series sequence we propose to use signature files to index the time series and we also propose new method to index the signature files to speed up the search of large time series we propose novel index structure the signature tree for time series indexing we implemented the signature tree and we discuss its performance
rapid technological change has had an impact on the nature of software this has led to new exigencies and to demands for software engineering paradigms that pay particular atttention to meeting them we advocate that such demands can be met at least in large parts through the adoption of software engineering processes that are founded on reflective stance to this end we turn our attention to the field of design rationale we analyze and characterize design rationale approaches and show that despite surface differences between different approaches they all tend to be variants of relatively small set of static and dynamic affinities we use the synthesis of static and dynamic affinities to develop generic model for reflective design the model is nonprescriptive and affects minimally the design process it is context independent and is intended to be used as facilitator in participative design supporting group communication and deliberation the potential utility of the model is demonstrated through two examples one from the world of business design and the other from programming language design
publish subscribe systems have demonstrated the ability to scale to large numbers of users and high data rates when providing content based data dissemination services on the internet however their services are limited by the data semantics and query expressiveness that they support on the other hand the recent work on selective dissemination of xml data has made significant progress in moving from xml filtering to the richer functionality of transformation for result customization but in general has ignored the challenges of deploying such xml based services on an internet scale in this paper we address these challenges in the context of incorporating the rich functionality of xml data dissemination in highly scalable system we present the architectural design of onyx system based on an overlay network we identify the salient technical challenges in supporting xml filtering and transformation in this environment and propose techniques for solving them
it is difficult to write programs that behave correctly in the presence of run time errors existing programming language features often provide poor support for executing clean up code and for restoring invariants in such exceptional situations we present dataflow analysis for finding certain class of error handling mistakes those that arise from failure to release resources or to clean up properly along all paths many real world programs violate such resource safety policies because of incorrect error handling our flow sensitive analysis keeps track of outstanding obligations along program paths and does precise modeling of control flow in the presence of exceptions using it we have found over error handling mistakes almost million lines of java code the analysis is unsound and produces false positives but few simple filtering rules suffice to remove them in practice the remaining mistakes were manually verified these mistakes cause sockets files and database handles to be leaked along some paths we present characterization of the most common causes of those errors and discuss the limitations of exception handling finalizers and destructors in addressing them based on those errors we propose programming language feature that keeps track of obligations at run time and ensures that they are discharged finally we present case studies to demonstrate that this feature is natural efficient and can improve reliability for example retrofitting kloc program with it resulted in code size decrease surprising speed increase from correctly deallocating resources in the presence of exceptions and more consistent behavior
increasing the level of spacecraft autonomy is essential for broadening the reach of solar system exploration computer vision has and will continue to play an important role in increasing autonomy of both spacecraft and earth based robotic vehicles this article addresses progress on computer vision for planetary rovers and landers and has four main parts first we review major milestones in the development of computer vision for robotic vehicles over the last four decades since research on applications for earth and space has often been closely intertwined the review includes elements of both second we summarize the design and performance of computer vision algorithms used on mars in the nasa jpl mars exploration rover mer mission which was major step forward in the use of computer vision in space these algorithms did stereo vision and visual odometry for rover navigation and feature tracking for horizontal velocity estimation for the landers third we summarize ongoing research to improve vision systems for planetary rovers which includes various aspects of noise reduction fpga implementation and vision based slip perception finally we briefly survey other opportunities for computer vision to impact rovers landers and orbiters in future solar system exploration missions
improving the precision of information retrieval has been challenging issue on chinese web as exemplified by chinese recipes on the web it is not easy natural for people to use keywords eg recipe names to search recipes since the names can be literally so abstract that they do not bear much if any information on the underlying ingredients or cooking methods in this paper we investigate the underlying features of chinese recipes and based on workflow like cooking procedures we model recipes as graphs we further propose novel similarity measurement based on the frequent patterns and devise an effective filtering algorithm to prune unrelated data so as to support efficient on line searching benefiting from the characteristics of graphs frequent common patterns can be mined from cooking graph database so in our prototype system called recipeview we extend the subgraph mining algorithm fsg to cooking graphs and combine it with our proposed similarity measurement resulting in an approach that well caters for specific users needs our initial experimental studies show that the filtering algorithm can efficiently prune unrelated cooking graphs without affecting the retrieval performance and the similarity measurement gets relatively higher precision recall against its counterparts
we study epidemic schemes in the context of collaborative data delivery in this context multiple chunks of data reside at different nodes and the challenge is to simultaneously deliver all chunks to all nodes here we explore the inter operation between the gossip of multiple simultaneous message chunks in this setting interacting nodes must select which chunk among many to exchange in every communication round we provide an efficient solution that possesses the inherent robustness and scalability of gossip our approach maintains the simplicity of gossip and has low message connections and computation overhead because our approach differs from solutions proposed by network coding we are able to provide insight into the tradeoffs and analysis of the problem of collaborative content distribution we formally analyze the performance of the algorithm demonstrating its efficiency with high probability
in field study conducted at leading fortune company we examined how having development teams reside in their own large room an arrangement called radical collocation affected system development the collocated projects had significantly higher productivity and shorter schedules than both the industry benchmarks and the performance of past similar projects within the firm the teams reported high satisfaction about their process and both customers and project sponsors were similarly highly satisfied the analysis of questionnaire interview and observational data from these teams showed that being at hand both visible and available helped them coordinate their work better and learn from each other radical collocation seems to be one of the factors leading to high productivity in these teams
in this paper we examine the issue of mining association rules among items in large database of sales transactions mining association rules means that given database of sales transactions to discover all associations among items such that the presence of some items in transaction will imply the presence of other items in the same transaction the mining of association rules can be mapped into the problem of discovering large itemsets where large itemset is group of items that appear in sufficient number of transactions the problem of discovering large itemsets can be solved by constructing candidate set of itemsets first and then identifying within this candidate set those itemsets that meet the large itemset requirement generally this is done iteratively for each large itemset in increasing order of where large itemset is large itemset with items to determine large itemsets from huge number of candidate sets in early iterations is usually the dominating factor for the overall data mining performance to address this issue we develop an effective algorithm for the candidate set generation it is hash based algorithm and is especially effective for the generation of candidate set for large itemsets explicitly the number of candidate itemsets generated by the proposed algorithm is in orders of magnitude smaller than that by previous methods thus resolving the performance bottleneck note that the generation of smaller candidate sets enables us to effectively trim the transaction database size at much earlier stage of the iterations thereby reducing the computational cost for later iterations significantly the advantage of the proposed algorithm also provides us the opportunity of reducing the amount of disk required extensive simulation study is conducted to evaluate performance of the proposed algorithm
new principles framework is presented for retrieval evaluation of ranked outputs it applies decision theory to model relevance decision preferences and shows that the probability ranking principle prp specifies optimal ranking it has two new components namely probabilistic evaluation model and general measure of retrieval effectiveness its probabilities may be interpreted as subjective or objective ones its performance measure is the expected weighted rank which is the weighted average rank of retrieval list starting from this measure the expected forward rank and some existing retrieval effectiveness measures eg top precision and discounted cumulative gain are instantiated using suitable weighting schemes after making certain assumptions the significance of these instantiations is that the ranking prescribed by prp is shown to be optimal simultaneously for all these existing performance measures in addition the optimal expected weighted rank may be used to normalize the expected weighted rank of retrieval systems for summary performance comparison across different topics between systems the framework also extends prp and our evaluation model to handle graded relevance thereby generalizing the discussed existing measures eg top precision and probabilistic retrieval models for graded relevance
vision based technology such as motion detection has long been limited to the domain of powerful processor intensive systems such as desktop pc's and specialist hardware solutions with the advent of much faster mobile phone processors and memory we are now seeing plethora of feature rich software being deployed onto the mobile platform since these high powered smart phones are now equipped with cameras it has become feasible to combine their powerful processors and the camera to support new ways of interacting with the phone however it is not clear whether or not these processor intensive visual interactions can in fact be run at an acceptable speed on current mobile handsets in this paper we look at one of the most popular and widespread mobile smart phone systems the symbian and benchmark the speed accuracy and deployability of the three popular mobile languages we test pixel thresholding algorithm in python and java and rank them based on their speed within the context of intensive image based processing
this paper makes two contributions to architectural support for software debugging first it proposes novel statistics based on the fly bug detectionmethod called pc based invariant detection the idea is based on the observation that in most programs given memory location is typically accessed by only few instructions therefore by capturing the invariant of the set of pcs that normally access given variable we can detect accesses by outlier instructions which are often caused by memory corruption buffer overflow stack smashing or other memory related bugs since this method is statistics based it can detect bugs that do not violate any programming rules and that therefore are likely to be missed by many existing tools the second contribution is novel architectural extension called the check look aside buffer clb the clb uses bloom filter to reduce monitoring overheads in the recently proposed iwatcher architectural framework for software debugging the clb significantly reduces the overhead of pc based invariant debugging we demonstrate pc based invariant detection tool called accmon that leverages architectural run time system and compiler support our experimental results with seven buggy applications and total of ten bugs show that accmon can detect all ten bugs with few false alarms for five applications and for two applications and with low overhead times several existing tools evaluated including purify ccured and value based invariant detection tools fail to detect some of the bugs in addition purify's overhead is one order of magnitude higher than accmon's finally we show that the clb is very effective at reducing overhead
texture synthesis is important for many applications in computer graphics vision and image processing however it remains difficult to design an algorithm that is both efficient and capable of generating high quality results in this paper we present an efficient algorithm for realistic texture synthesis the algorithm is easy to use and requires only sample texture as input it generates textures with perceived quality equal to or better than those produced by previous techniques but runs two orders of magnitude faster this permits us to apply texture synthesis to problems where it has traditionally been considered impractical in particular we have applied it to constrained synthesis for image editing and temporal texture generation our algorithm is derived from markov random field texture models and generates textures through deterministic searching process we accelerate this synthesis process using tree structured vector quantization
icooolps was the first edition ofecoop icooolps workshop it intended to bring researchers and practitioners both from academia and industry together with spirit of openness to try and identify and begin to address the numerous and very varied issues of optimization this succeeded as can be seen from the papers the attendance and the liveliness of the discussions that took place during and after the workshop not to mention few new cooperations or postdoctoral contracts the talented people from different groups who participated were unanimous to appreciate this first edition and recommend that icooolps be continued next year community is thus beginning to form and should be reinforced by second edition next year with all the improvements this first edition made emerge
the distance or similarity metric plays an important role in many natural language processing nlp tasks previous studies have demonstrated the effectiveness of number of metrics such as the jaccard coefficient especially in synonym acquisition while the existing metrics perform quite well to further improve performance we propose the use of supervised machine learning algorithm that fine tunes them given the known instances of similar or dissimilar words we estimated the parameters of the mahalanobis distance we compared number of metrics in our experiments and the results show that the proposed metric has higher mean average precision than other metrics
schema mapping is high level declarative specification of the relationship between two schemas it specifies how data structured under one schema called the source schema is to be converted into data structured under possibly different schema called the target schema schema mappings are fundamental components for both data exchange and data integration to date language for specifying or programming schema mappings exists however developmental support for programming schema mappings is still lacking in particular tool for debugging schema mappings has not yet been developed in this paper we propose to build debugger for understanding and exploring schema mappings we present primary feature of our debugger called routes that describes the relationship between source and target data with the schema mapping we present two algorithms for computing all routes or one route for selected target data both algorithms execute in polynomial time in the size of the input in computing all routes our algorithm produces concise representation that factors common steps in the routes furthermore every minimal route for the selected data can essentially be found in this representation our second algorithm is able to produce one route fast if there is one and alternative routes as needed we demonstrate the feasibility of our route algorithms through set of experimental results on both synthetic and real datasets
we generalize the method of constructing windows in subsequence matching by this generalization we can explain earlier subsequence matching methods as special cases of common framework based on the generalization we propose new subsequence matching method general match the earlier work by faloutsos et al called frm for convenience causes lot of false alarms due to lack of point filtering effect dual match recently proposed as dual approach of frm improves performance significantly over frm by exploiting point filtering effect however it has the problem of having smaller allowable window size half that of frm given the minimum query length smaller window increases false alarms due to window size effect general match offers advantages of both methods it can reduce window size effect by using large windows like frm and at the same time can exploit point filtering effect like dual match general match divides data sequences into generalized sliding windows sliding windows and the query sequence into generalized disjoint windows disjoint windows we formally prove that general match is correct ie it incurs no false dismissal we then propose method of estimating the optimal value of the sliding factor that minimizes the number of page accesses experimental results for real stock data show that for low selectivities sim general match improves average performance by over dual match and by over frm for high selectivities sim by over dual match and by over frm the proposed generalization provides an excellent theoretical basis for understanding the underlying mechanisms of subsequence matching
database reverse engineering dbre methods recover conceptual data models from physical databases the bottom up nature of these methods imposes two major limitations first they do not provide an initial high level abstract schema suitable for use as basis for reasoning about the application domain single detailed schema is only produced at the very end of the project second they provide no support for divide and conquer approach the entire database schema must be analysed and processed as unit this paper presents simple solution to overcome both limitations in our proposal relations are grouped based on their primary keys each group can be perceived in two ways as relational schema that can be reversed engineered as standalone dbre project and as an element either an entity or relationship of high level abstract schema that provides initial insight about the application domain we also present examples from actual large database systems
gene expression data are numerical and describe the level of expression of genes in different situations thus featuring behaviour of the genes two methods based on fca formal concept analysis are considered for clustering gene expression data the first one is based on interordinal scaling and can be realized using standard fca algorithms the second method is based on pattern structures and needs adaptations of standard algorithms to computing with interval algebra the two methods are described in details and discussed the second method is shown to be more computationally efficient and providing more readable results experiments with gene expression data are discussed
due to an increasing volume of xml data it is considered prudent to store xml data on an industry strength database system instead of relying on domain specific application or file system for shredded xml data stored in the relational tables however it may not be straightforward to apply existing algorithms for twig query processing because most of the algorithms require xml data to be accessed in form of streams of elements grouped by their tags and sorted in particular order in order to support xml query processing within the common framework of relational database systems we first propose several bitmap indexes for supporting holistic twig joins on xml data stored in the relational tables since bitmap indexes are well supported in most of the commercial and open source database systems the proposed bitmap indexes and twig query processing algorithms can be incorporated into the relational query processing framework with more ease the proposed query processing algorithms are efficient in terms of both time and space since the compressed bitmap indexes stay compressed during query processing in addition we propose hybrid index which computes twig query solutions with only bit vectors without accessing labeled xml elements stored in the relational tables
incompleteness due to missing attribute values aka null values is very common in autonomous web databases on which user accesses are usually supported through mediators traditional query processing techniques that focus on the strict soundness of answer tuples often ignore tuples with critical missing attributes even if they wind up being relevant to user query ideally we would like the mediator to retrieve such possibleanswers and gauge their relevance by accessing their likelihood of being pertinent answers to the query the autonomous nature of web databases poses several challenges in realizing this objective such challenges include the restricted access privileges imposed on the data the limited support for query patterns and the bounded pool of database and network resources in the web environment we introduce novel query rewriting and optimization framework qpiad that tackles these challenges our technique involves reformulating the user query based on mined correlations among the database attributes the reformulated queries are aimed at retrieving the relevant possibleanswers in addition to the certain answers qpiad is able to gauge the relevance of such queries allowing tradeoffs in reducing the costs of database query processing and answer transmission to support this framework we develop methods for mining attribute correlations in terms of approximate functional dependencies value distributions in the form of na√Øve bayes classifiers and selectivity estimates we present empirical studies to demonstrate that our approach is able to effectively retrieve relevant possibleanswers with high precision high recall and manageable cost
it is envisaged that the application of the multilevel security mls scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on per user basis however the critical problem with the current model is that the belief higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported critics also argue that it is imperative for mls database users to theorize about the belief of others perhaps at different security levels an apparatus that is currently missing and the absence of which in seriously feltthe impetus for our current research is the need to provide an adequate framework for belief reasoning in mls databases in this paper we show that these concepts can be captured in logic style declarative query language called multilog for mls deductive databases for which proof theoretic model theoretic and fixpoint semantics exist this development is significant from database perspective as it now enables us to compute the semantics of multilog databases in bottom up fashion we also define bottom up procedure to compute unique models of stratified multilog databases finally we establish the equivalence of multilog's three logical characterizations model theory fixpoint theory and proof theory
microprocessor trends are moving towards wider architectures and more aggressive speculation with the increasing transistor budgets energy consumption has become critical design constraint to address this problem several researchers have proposed and evaluated energy efficient variants of speculation mechanisms however such hardware is typically evaluated in isolation and its impact on the energy consumption of the rest of the processor for example due to wrong path executions is ignored moreover the available metrics that would provide thorough evaluation of an architectural optimization employ somewhat complicated formulas with hard to measure parametersin this paper we introduce simple method to accurately compare the energy efficiency of speculative architectures our metric is based on runtime analysis of the entire processor chip and thus captures the energy consumption due to the positive as well as the negative activities that arise from the speculation activities we demonstrate the usefulness of our metric on the example of value speculation where we found some proposed value predictors including low power designs not to be energy efficient
middle domain mobility management provides an efficient routing low registration cost and handoff latency for layer ip layer based mobile network environment in the middle domain the base station bs acts as an agent or proxy to manage mobile networks to achieve this goal the bs could only address external traffic but without internal case management in order to complement this defect an enhanced version for the middle domain mobility management is designed in this paper moreover we research and design the multicast extension for the middle domain by applying the idea of the enhancement which is called hmp hierarchical multicast protocol associated handoff scheme is also proposed in this paper since it is complicated case for designing the multicast service in network environment we need characteristic method to address this case in order to fulfill this achievement of designing hmp scheme we introduce reduction process rp in this paper by using the rp complicated based network environment can be actually reduced to simpler network environment the mathematical analysis and simulation study are presented for performance evaluation simulation results have demonstrated that the enhanced middle domain mobility management has the better network performance in terms of registration cost handoff latency and routing cost in comparing with conventional mobility management schemes moreover the proposed multicast extension for hmp scheme is simple and has scalability and network performance advantages over other approaches in mobile multicasting
there are many design challenges in the hardware software co design approach for performance improvement of data intensive streaming applications with general purpose microprocessor and hardware accelerator these design challenges are mainly to prevent hardware area fragmentation to increase resource utilization to reduce hardware reconfiguration cost and to partition and schedule the tasks between the microprocessor and the hardware accelerator efficiently for performance improvement and power savings of the applications in this paper modular and block based hardware configuration architecture named memory aware run time reconfigurable embedded system martres is proposed for efficient resource management and performance improvement of streaming applications subsequently we design task placement algorithm named hierarchical best fit ascending hbfa algorithm to prove that martres configuration architecture is very efficient in increased resource utilization and flexible in task mapping and power savings the time complexity of hbfa algorithm is reduced to compared to traditional best fit bf algorithm's time complexity of when the quality of the placement solution by hbfa is better than that of bf algorithm finally we design an efficient task partitioning and scheduling algorithm named balanced partitioned and placement aware partitioning and scheduling algorithm bpasa in bpasa we exploit the temporal parallelism in streaming applications to reduce reconfiguration cost of the hardware while keeping in mind the required throughput of the output data we balance the exploitation of spatial parallelism and temporal parallelism in streaming applications by considering the reconfiguration cost vs the data transfer cost the scheduler refers to the hbfa placement algorithm to check whether contiguous area on fpga is available before scheduling the task for hw or for sw
search on pcs has become less efficient than searching the web due to the increasing amount of stored data in this paper we present an innovative desktop search solution which relies on extracted metadata context information as well as additional background information for improving desktop search results we also present practical application of this approach the extensible beagle toolbox to prove the validity of our approach we conducted series of experiments by comparing our results against the ones of regular desktop search solution beagle we show an improved quality in search and overall performance
the precision of many type based analyses can be significantly increased given additional information about the programs execution for this reason it is not uncommon for such analyses to integrate supporting analyses computing for instance nil pointer or alias information such integration is problematic for number of reasons it obscures the original intention of the type system especially if multiple additional analyses are added it makes use of already available analyses difficult since they have to be rephrased as type systems and it is non modular changing the supporting analyses implies changing the entire type systemusing ideas from abstract interpretation we present method for parameterizing type systems over the results of abstract analyses in such way that one modular correctness proof can be obtained this is achieved by defining general format for information transferal and use of the information provided by the abstract analyses the key gain from this method is clear separation between the correctness of the analyses and the type system both in the implementation and correctness proof which leads to comparatively easy way of changing the parameterized analysis and making use of precise and hence complicated analysesin addition we exemplify the use of the framework by presenting parameterized type system that uses additional information to improve the precision of exception types in small imperative language with arrays
the recent trend in web application development is moving towards exposing the service functionality through application programming interface api many web based services start to offer apis to support application to application integration with their consumers on top of the web the trend has raised the demand for average web developers to know how to design apis for web based services in this paper we summarise list of inherent issues in the web that developers should pay attention to describe how web architecture may help to resolve these issues and suggest design considerations for web api design in addition we demonstrate an experimental design process through case study to design web api for social bookmarking service
co located collaborators can see the artifacts that others are working on which in turn enables casual interactions to help distributed collaborators maintain mutual awareness of people's electronic work artifacts we designed and implemented an awareness tool that leverages screen sharing methods people see portions of others screens in miniature can selectively raise larger views of screen to get more detail and can engage in remote pointing people balance awareness with privacy by using several privacy protection strategies built into the system preliminary evaluation with two groups using this system shows that people use it to maintain awareness of what others are doing project certain image of themselves monitor progress coordinate joint tasks determine others availability and engage in serendipitous conversation and collaboration while privacy was not large concern for these groups theoretical analysis suggests that privacy risks may differ for other user communities
combining fine grained opinion information to produce opinion summaries is important for sentiment analysis applications toward that end we tackle the problem of source coreference resolution linking together source mentions that refer to the same entity the partially supervised nature of the problem leads us to define and approach it as the novel problem of partially supervised clustering we propose and evaluate new algorithm for the task of source coreference resolution that outperforms competitive baselines
this paper introduces method for converting an image or volume sampled on regular grid into space efficient irregular point hierarchy the conversion process retains the original frequency characteristics of the dataset by matching the spatial distribution of sample points with the required frequency to achieve good blending the spherical points commonly used in volume rendering are generalized to ellipsoidal point primitives family of multiresolution oriented gabor wavelets provide the frequency space analysis of the dataset the outcome of this frequency analysis is the reduced set of points in which the sampling rate is decreased in originally oversampled areas during rendering the traversal of the hierarchy can be controlled by any suitable error metric or quality criteria the local level of refinement is also sensitive to the transfer function areas with density ranges mapped to high transfer function variability are rendered at higher point resolution than others our decomposition is flexible and can be used for iso surface rendering alpha compositing and ray rendering of volumes we demonstrate our hierarchy with an interactive splatting volume renderer in which the traversal of the point hierarchy for rendering is modulated by user specified frame rate
in this paper we describe design orientated field study in which we deploy novel digital display device to explore the potential integration of teenage and family photo displays at home as well as the value of situated photo display technologies for intergenerational expression this exploration is deemed timely given the contemporary take up of digital capture devices by teenagers and the unprecedented volume of photographic content that teens generate findings support integration and the display of photos on standalone device as well as demonstrating the interventional efficacy of the design as resource for provoking reflection on the research subject we also draw upon the theoretical concept of dialogism to understand how our design mediates intergenerational relationships and interaction aesthetics relating to the notion of constructive conflict
key dependent message kdm security was introduced by black rogaway and shrimpton to address the case where key cycles occur among encryptions eg key is encrypted with itself it was mainly motivated by key cycles in dolev yao models ie symbolic abstractions of cryptography by term algebras and corresponding computational soundness result was later shown by ad atilde et al however both the kdm definition and this soundness result do not allow the general active attacks typical for dolev yao models or for security protocols in general we extend these definitions to obtain soundness result under active attacks we first present definition akdm adaptive kdm as kdm equivalent of authenticated symmetric encryption ie it provides chosen ciphertext security and integrity of ciphertexts for key cycles however this is not yet sufficient for the desired computational soundness result and thus we define dkdm dynamic kdm that additionally allows limited dynamic revelation of keys we show that dkdm is sufficient for computational soundness even in the strong sense of blackbox reactive simulatability brsim uc and in cases with joint terms with other operators we also build on current kdm secure schemes to construct schemes secure under the new definitions moreover we prove implications or construct separating examples respectively for new definitions and existing ones for symmetric encryption
spatial joins find all pairs of objects that satisfy given spatial relationship in spatial joins using indexes original space indexes such as the tree are widely used an original space index is the one that indexes objects as represented in the original space since original space indexes deal with extents of objects it is relatively complex to optimize join algorithms using these indexes on the other hand transform space indexes which transform objects in the original space into points in the transform space and index them deal only with points but no extents thus optimization of join algorithms using these indexes can be relatively simple however the disadvantage of these join algorithms is that they cannot be applied to original space indexes such as the tree in this paper we present novel mechanism for achieving the best of these two types of algorithms specifically we propose the new notion of the transform space view and present the transform space view join algorithm the transform space view is virtual transform space index based on an original space index it allows us to interpret or view an existing original space index as transform space index with no space and negligible time overhead and without actually modifying the structure of the original space index or changing object representation the transform space view join algorithm joins two original space indexes in the transform space through the notion of the transform space view through analysis and experiments we verify the excellence of the transform space view join algorithm the transform space view join algorithm always outperforms existing ones for all the data sets tested in terms of all three measures used the one pass buffer size the minimum buffer size required for guaranteeing one disk access per page the number of disk accesses for given buffer size and the wall clock time thus it constitutes lower bound algorithm we believe that the proposed transform space view can be applied to developing various new spatial query processing algorithms in the transform space
form comparison is fundamental part of many anthropometric biological anthropological archaeological and botanical researches etc in traditional anthropometric form comparison methods geometry characteristics and internal structure of surface points are not adequately considered form comparison of anthropometric data can make up the deficiency of traditional methods in this paper methods for analyzing other than objects are highlighted we summarize the advance of form comparison techniques in the last decades according to whether they are based upon anatomical landmarks we partition them into two main categories landmark based methods and landmark free methods the former methods are further sub divided into deformation methods superimposition methods and methods based on linear distances while the latter methods are sub divided into shape statistics based methods methods based on function analysis view based methods topology based methods and hybrid methods examples for each method are presented the discussion about their advantages and disadvantages are also introduced
simple implementation of an sml like module system is presented as module parameterized by base language and its type checker this implementation is useful both as detailed tutorial on the harper ndash lillibridge ndash leroy module system and its implementation and as constructive demonstration of the applicability of that module system to wide range of programming languages
as the issue width of superscalar processors is increased instruction fetch bandwidth requirements will also increase it will become necessary to fetch multiple basic blocks per cycle conventional instruction caches hinder this effort because long instruction sequences are not always in contiguous cache locations we propose supplementing the conventional instruction cache with trace cache this structure caches traces of the dynamic instruction stream so instructions that are otherwise noncontiguous appear contiguous for the instruction benchmark suite ibs and spec integer benchmarks kilobyte trace cache improves performance on average by over conventional sequential fetching further it is shown that the trace cache's efficient low latency approach enables it to outperform more complex mechanisms that work solely out of the instruction cache
interactive tabletop and wall surfaces support collaboration and interactivity in novel ways apart from keyboards and mice such systems can also incorporate other input devices namely laser pointers marker pens with screen location sensors or touch sensitive surfaces similarly instead of vertically positioned desktop monitor collaborative setups typically use much larger displays which are oriented either vertically wall or horizontally tabletop or combine both kinds of surfaces in this paper we describe an empirical study that investigates how technical system constraints can affect group performance in high pace collaborative tasks for this we compare various input and output modalities in system that consists of several interactive tabletop and wall surface we observed that the performance of group of people scaled almost linearly with the number of participants on an almost perfectly parallel task we also found that mice were significantly faster than laser pointers but only by also interaction on walls was significantly faster than on the tabletop by
the widespread deployment of recommender systems has lead to user feedback of varying quality while some users faithfully express their true opinion many provide noisy ratings which can be detrimental to the quality of the generated recommendations the presence of noise can violate modeling assumptions and may thus lead to instabilities in estimation and prediction even worse malicious users can deliberately insert attack profiles in an attempt to bias the recommender system to their benefit while previous research has attempted to study the robustness of various existing collaborative filtering cf approaches this remains an unsolved problem approaches such as neighbor selection algorithms association rules and robust matrix factorization have produced unsatisfactory results this work describes new collaborative algorithm based on svd which is accurate as well as highly stable to shilling this algorithm exploits previously established svd based shilling detection algorithms and combines it with svd based cf experimental results show much diminished effect of all kinds of shilling attacks this work also offers significant improvement over previous robust collaborative filtering frameworks
distributed multimedia applications require support from the underlying operating system to achieve and maintain their desired quality of service qos this has led to the creation of novel task and message schedulers and to the development of qos mechanisms that allow applications to explicitly interact with relevant operating system services however the task scheduling techniques developed to date are not well equipped to take advantage of such interactions as result important events such as position update messages in virtual environments may be ignored if cpu scheduler ignores these events players will experience lack of responsiveness or even inconsistencies in the virtual world this paper argues that real time and multimedia applications can benefit from coordinatedel event delivery mechanism termed ecalls that supports such coordination we then show ecalls's ability to reduce variations in inter frame times for media streams
we have conducted an extensive experimental study on algorithms for fully dynamic transitive closure we have implemented the recent fully dynamic algorithms by king lsqb rsqb roditty lsqb rsqb roditty and zwick lsqb rsqb and demetrescu and italiano lsqb rsqb along with several variants and compared them to pseudo fully dynamic and simple minded algorithms developed in previous study lsqb frigioni et al rsqb we tested and compared these implementations on random inputs synthetic worst case inputs and on inputs motivated by real world graphs our experiments reveal that some of the dynamic algorithms can really be of practical value in many situations
this paper presents probabilistic mixture modeling framework for the hierarchic organisation of document collections it is demonstrated that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of document collection can be further exploited to create kernel which boosts the performance of state of the art support vector machine document classifiers it is shown that the performance of such classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning document corpus rather than the kernel associated with flat non hierarchic mixture model this has important implications for document classification when hierarchic ordering of topics exists this can be considered as the effective combination of documents with no topic or class labels unlabeled data labeled documents and prior domain knowledge in the form of the known hierarchic structure in providing enhanced document classification performance
we describe the design implementation and performance of new parallel sparse cholesky factorization code the code uses multifrontal factorization strategy operations on small dense submatrices are performed using new dense matrix subroutines that are part of the code although the code can also use the blas and lapack the new code is recursive at both the sparse and the dense levels it uses novel recursive data layout for dense submatrices and it is parallelized using cilk an extension of specifically designed to parallelize recursive codes we demonstrate that the new code performs well and scales well on smps in particular on up to processors the code outperforms two state of the art message passing codes the scalability and high performance that the code achieves imply that recursive schedules blocked data layouts and dynamic scheduling are effective in the implementation of sparse factorization codes
several researchers have analysed the performance of ary cubes taking into account channel bandwidth constraints imposed by implementation technology namely the constant wiring density and pin out constraints for vlsi and multiple chip technology respectively for instance dally ieee trans comput abraham issues in the architecture of direct interconnection networks schemes for multiprocessors phd thesis university of illinois at urbana champaign and agrawal ieee trans parallel distributed syst have shown that low dimensional ary cubes known as tori outperform their high dimensional counterparts known as hypercubes under the constant wiring density constraint however abraham and agrawal have arrived at an opposite conclusion when they considered the constant pin out constraint most of these analyses have assumed deterministic routing where message always uses the same network path between given pair of nodes more recent multicomputers have incorporated adaptive routing to improve performance this paper re examines the relative performance merits of the torus and hypercube in the context of adaptive routing our analysis reveals that the torus manages to exploit its wider channels under light traffic as traffic increases however the hypercube can provide better performance than the torus our conclusion under the constant wiring density constraint is different from that of the works mentioned above because adaptive routing enables the hypercube to exploit its richer connectivity to reduce message blocking
with the increasing use of multi core microprocessors and hardware accelerators in embedded media processing systems there is an increasing need to discover coarse grained parallelism in media applications written in and common versions of these codes use pointer heavy sequential programming model to implement algorithms with high levels of inherent parallelism the lack of automated tools capable of discovering this parallelism has hampered the productivity of parallel programmers and application specific hardware designers as well as inhibited the development of automatic parallelizing compilers automatic discovery is challenging due to shifts in the prevalent programming languages scalability problems of analysis techniques and the lack of experimental research in combining the numerous analyses necessary to achieve clear view of the relations among memory accesses in complex programs this paper is based on coherent prototype system designed to automatically find multiple levels of coarse grained parallelism it visits several of the key analyses that are necessary to discover parallelism in contemporary media applications distinguishing those that perform satisfactorily at this time from those that do not yet have practical scalable solutions we show that contrary to common belief compiler with strong synergistic portfolio of modern analysis capabilities can automatically discover very substantial amount of coarse grained parallelism in complex media applications such as an mpeg encoder these results suggest that an automatic coarse grained parallelism discovery tool can be built to greatly enhance the software and hardware development processes of future embedded media processing systems
wireless sensor networks offer the potential to span and monitor large geographical areas inexpensively sensors however have significant power constraint battery life making communication very expensive another important issue in the context of sensor based information systems is that individual sensor readings are inherently unreliable in order to address these two aspects sensor database systems like tinydb and cougar enable in network data aggregation to reduce the communication cost and improve reliability the existing data aggregation techniques however are limited to relatively simple types of queries such as sum count avg and min max in this paper we propose data aggregation scheme that significantly extends the class of queries that can be answered using sensor networks these queries include approximate quantiles such as the median the most frequent data values such as the consensus value histogram of the data distribution as well as range queries in our scheme each sensor aggregates the data it has received from other sensors into fixed user specified size message we provide strict theoretical guarantees on the approximation quality of the queries in terms of the message size we evaluate the performance of our aggregation scheme by simulation and demonstrate its accuracy scalability and low resource utilization for highly variable input data sets
finding correlated sequential patterns in large sequence databases is one of the essential tasks in data mining since huge number of sequential patterns are usually mined but it is hard to find sequential patterns with the correlation according to the requirement of real applications the needed data analysis should be different in previous mining approaches after mining the sequential patterns sequential patterns with the weak affinity are found even with high minimum support in this paper new framework is suggested for mining weighted support affinity patterns in which an objective measure sequential ws confidence is developed to detect correlated sequential patterns with weighted support affinity patterns to efficiently prune the weak affinity patterns it is proved that ws confidence measure satisfies the anti monotone and cross weighted support properties which can be applied to eliminate sequential patterns with dissimilar weighted support levels based on the framework weighted support affinity pattern mining algorithm wsminer is suggested the performance study shows that wsminer is efficient and scalable for mining weighted support affinity patterns
the implementation of rfid leads to improved visibility in supply chains however as consequence of the increased data collection and enhanced data granularity supply chain participants have to deal with new data management challenges in this paper we give an overview of the current challenges and solution proposals in the area of data collection and transformation data organisation and data security we also identify further research requirements
recent studies of internet traffic have shown that flow size distributions often exhibit high variability property in the sense that most of the flows are short and more than half of the total load is constituted by small percentage of the largest flows in the light of this observation it is interesting to revisit scheduling policies that are known to favor small jobs in order to quantify the benefit for small and the penalty for large jobs among all scheduling policies that do not require knowledge of job size the least attained service las scheduling policy is known to favor small jobs the most we investigate the las queue for both load and our analysis shows that for job size distributions with high variability property las favors short jobs with negligible penalty to the few largest jobs and that las achieves mean response time over all jobs that is close to the mean response time achieved by srptfinally we implement las in the ns network simulator to study its performance benefits for tcp flows when las is used to schedule packets over the bottleneck link more than of the shortest flows experience smaller mean response times under las than under fifo and only the largest jobs observe negligible increase in response time the benefit of using las as compared to fifo is most pronounced at high load
virtual immersive environments or telepresence setups often consist of multiple cameras that have to be calibrated we present convenient method for doing this the minimum is three cameras but there is no upper limit the method is fully automatic and freely moving bpdght spot is the only calibration object set of virtual points is made by waving the bright spot through the working volume its projections are found with subpixel precision and verified by robust ransac analysis the cameras do not have to see all points only reasonable overlap between camera subgroups is necessary projective structures are computed via rank factorization and the euclidean stratification is done by imposing geometric constraints this linear estimate initializes postprocessing computation of nonlinear distortion which is also fully automatic we suggest trick on how to use very ordinary laser pointer as the calibration object we show that it is possible to calibrate an immersive virtual environment with cameras in less than minutes reaching about pixel reprojection error the method has been successfully tested on numerous multicamera environments using varying numbers of cameras of varying quality
this letter analyzes the fisher kernel from statistical point of view the fisher kernel is particularly interesting method for constructing model of the posterior probability that makes intelligent use of unlabeled data ie of the underlying data density it is important to analyze and ultimately understand the statistical properties of the fisher kernel to this end we first establish sufficient conditions that the constructed posterior model is realizable ie it contains the true distribution realizability immediately leads to consistency results subsequently we focus on an asymptotic analysis of the generalization error which elucidates the learning curves of the fisher kernel and how unlabeled data contribute to learning we also point out that the squared or log loss is theoretically preferable because both yield consistent estimators to other losses such as the exponential loss when linear classifier is used together with the fisher kernel therefore this letter underlines that the fisher kernel should be viewed not as heuristics but as powerful statistical tool with well controlled statistical properties
this paper presents an approach to full body human pose recognition inputs to the proposed approach are pairs of silhouette images obtained from wide baseline binocular cameras through multilinear analysis low dimensional view invariant pose coefficient vectors can be extracted from these stereo silhouette pairs taking these pose coefficient vectors as features the universum method is trained and used for pose recognition experiment results obtained using real image data showed the efficacy of the proposed approach
information integration for distributed and heterogeneous data sources is still an open challenging and schema matching is critical in this process this paper presents an approach to automatic elements matching between xml application schemas using similarity measure and relaxation labeling the semantic modeling of xml application schema has also been presented the similarity measure method considers element categories and their properties in an effort to achieve an optimal matching contextual constraints are used in the relaxation labeling method based on the semantic modeling of xml application schemas the compatible constraint coefficients are devised in terms of the structures and semantic relationships as defined in the semantic model to examine the effectiveness of the proposed methods an algorithm for xml schema matching has been developed and corresponding computational experiments show that the proposed approach has high degree of accuracy
quality of service qos support in local and cluster area environments has become an issue of great interest in recent years most current high performance interconnection solutions for these environments have been designed to enhance conventional best effort traffic performance but are not well suited to the special requirements of the new multimedia applications the multimedia router mmr aims at offering hardware based qos support within compact interconnection component one of the key elements in the mmr architecture are the algorithms used in traffic scheduling these algorithms are responsible for the order in which information is forwarded through the internal switch thus they are closely related to the qos provisioning mechanisms in this paper several traffic scheduling algorithms developed for the mmr architecture are described their general organization is motivated by chances for parallelization and pipelining while providing the necessary support both to multimedia flows and to best effort traffic performance evaluation results show that the qos requirements of different connections are met in spite of the presence of best effort traffic while achieving high link utilizations
aggregating statistical representations of classes is an important task for current trends in scaling up learning and recognition or for addressing them in distributed infrastructures in this perspective we address the problem of merging probabilistic gaussian mixture models in an efficient way through the search for suitable combination of components from mixtures to be merged we propose new bayesian modelling of this combination problem in association to variational estimation technique that handles efficiently the model complexity issue main feature of the present scheme is that it merely resorts to the parameters of the original mixture ensuring low computational cost and possibly communication should we operate on distributed system experimental results are reported on real data
conceptually the common approach to manipulating probabilistic data is to evaluate relational queries and then calculate the probability of each tuple in the result this approach ignores the possibility that the probabilities of complete answers are too low and hence partial answers with sufficiently high probabilities become important therefore we consider the semantics in which answers are maximal ie have the smallest degree of incompleteness subject tothe constraint that the probability is still above given threshold we investigate the complexity of joining relations under the above semantics in contrast to the deterministic case this approach gives rise to two different enumeration problems the first is finding all maximal sets of tuples that are join consistent connected and have joint probability above the threshold the second is computing all maximal tuples that are answers of partial joins and have probability above the threshold both problems are tractable under data complexity we also consider query and data complexity which rules out as efficient the following naive algorithm compute all partial answers and then choose the maximal ones among those with probabilities above the threshold we give efficient algorithms for several important special cases we also show that in general the first problem is np hard whereas the secondis hard
as microprocessor designs become increasingly powerand complexity conscious future microarchitectures must decrease their reliance on expensive dynamic scheduling structures while compilers have generally proven adept at planning useful static instruction level parallelism relying solely on the compiler instruction execution arrangement performs poorly when cache misses occur because variable latency is not well tolerated this paper proposes new microarchitectural model multipass pipelining that exploits meticulous compile time scheduling on simple in order hardware while achieving excellent cache miss tolerance through persistent advance preexecution beyond otherwise stalled instructions the pipeline systematically makes multiple passes through instructions that follow stalled instruction each pass increases the speed and energy efficiency of the subsequent ones by preserving computed results the concept of multiple passes and successive improvement of efficiency across passes in single pipeline distinguishes multipass pipelining from other runahead schemes simulation results show that the multipass technique achieves of the cycle reduction of aggressive out of order execution relative to in order execution in addition microarchitectural level power simulation indicates that benefits of multipass are achieved at fraction of the power overhead of full dynamic scheduling
an order dependent query is one whose result interpreted as multiset changes if the order of the input records is changed in stock quotes database for instance retrieving all quotes concerning given stock for given day does not depend on order because the collection of quotes does not depend on order by contrast finding stock's five price moving average in trades table gives result that depends on the order of the table query languages based on the relational data model can handle order dependent queries only through add ons sql for instance has new window mechanism which can sort data in limited parts of query add ons make order dependent queries dicult to write and to optimize in this paper we show that order can be natural property of the underlying data model and algebra we introduce new query language and algebra called aquery that supports order from the ground up new order related query transformations arise in this setting we show by experiment that this framework language plus optimization techniques brings orders of magnitude improvement over sql systems on many natural order dependent queries
datalog database query language based on the logic programming paradigm is described the syntax and semantics of datalog and its use for querying relational database are presented optimization methods for achieving efficient evaluations of datalog queries are classified and the most relevant methods are presented various improvements of datalog currently under study are discussed and what is still needed in order to extend datalog's applicability to the solution of real life problems is indicated
cardinality estimation is the problem of estimating the number of tuples returned by query it is fundamentally important task in data management used in query optimization progress estimation and resource provisioning we study cardinality estimation in principled framework given set of statistical assertions about the number of tuples returned by fixed set of queries predict the number of tuples returned by new query we model this problem using the probability space over possible worlds that satisfies all provided statistical assertions and maximizes entropy we call this the entropy maximization model for statistics maxent in this paper we develop the mathematical techniques needed to use the maxent model for predicting the cardinality of conjunctive queries
we present our current research on the implementation of gaze as an efficient and usable pointing modality supplementary to speech for interacting with augmented objects in our daily environment or large displays especially immersive virtual reality environments such as reality centres and caves we are also addressing issues relating to the use of gaze as the main interaction input modality we have designed and developed two operational user interfaces one for providing motor disabled users with easy gaze based access to map applications and graphical software the other for iteratively testing and improving the usability of gaze contingent displays
attackers exploit software vulnerabilities to control or crash programs bouncer uses existing software instrumentation techniques to detect attacks and it generates filters automatically to block exploits of the target vulnerabilities the filters are deployed automatically by instrumenting system calls to drop exploit messages these filters introduce low overhead and they allow programs to keep running correctly under attack previous work computes filters using symbolic execution along the path taken by sample exploit but attackers can bypass these filters by generating exploits that follow different execution path bouncer introduces three techniques to generalize filters so that they are harder to bypass new form of program slicing that uses combination of static and dynamic analysis to remove unnecessary conditions from the filter symbolic summaries for common library functions that characterize their behavior succinctly as set of conditions on the input and generation of alternative exploits guided by symbolic execution bouncer filters have low overhead they do not have false positives by design and our results show that bouncer can generate filters that block all exploits of some real world vulnerabilities
true multi user multimodal interaction over digital table lets co located people simultaneously gesture and speak commands to control an application we explore this design space through case study where we implemented an application that supports the kj creativity method as used by industrial designers four key design issues emerged that have significant impact on how people would use such multi user multimodal system first parallel work is affected by the design of multimodal commands second individual mode switches can be confusing to collaborators especially if speech commands are used third establishing personal and group territories can hinder particular tasks that require artefact neutrality finally timing needs to be considered when designing joint multimodal commands we also describe our model view controller architecture for true multi user multimodal interaction
as mobility permeates into todays computing and communication arena we envision application infrastructures that will increasingly rely on mobile technologies traditional database applications and information service applications will need to integrate mobile entities people and computers in this paper we develop distributed database framework for mobile environments key requirement in such an environment is to support frequent connection and disconnection of database sites we present algorithms that implement this framework in an asynchronous system
all large scale projects contain degree of risk and uncertainty software projects are particularly vulnerable to overruns due to the this uncertainty and the inherent difficulty of software project cost estimation in this paper we introduce search based approach to software project robustness the approach is to formulate this problem as multi objective search based software engineering problem in which robustness and completion time are treated as two competing objectives the paper presents the results of the application of this new approach to four large real world software projects using two different models of uncertainty
for given text which has been encoded by static huffman code the possibility of locating given pattern directly in the compressed text is investigated the main problem is one of synchronization as an occurrence of the encoded pattern in the encoded text does not necessarily correspond to an occurrence of the pattern in the text simple algorithm is suggested which reduces the number of erroneously declared matches the probability of such false matches is analyzed and empirically tested
this paper studies text mining problem comparative sentence mining csm comparative sentence expresses an ordering relation between two sets of entities with respect to some common features for example the comparative sentence canon's optics are better than those of sony and nikon expresses the comparative relation better optics canon sony nikon given set of evaluative texts on the web eg reviews forum postings and news articles the task of comparative sentence mining is to identify comparative sentences from the texts and to extract comparative relations from the identified comparative sentences this problem has many applications for example product manufacturer wants to know customer opinions of its products in comparison with those of its competitors in this paper we propose two novel techniques based on two new types of sequential rules to perform the tasks experimental evaluation has been conducted using different types of evaluative texts from the web results show that our techniques are very promising
one significant problem in tile based texture synthesis is the presence of conspicuous seams in the tiles the reason is that the sample patches employed as primary patterns of the tile set may not be well stitched if carelessly picked in this paper we introduce an optimized approach that can stably generate an omega tile set of high pattern diversity and high quality firstly an extendable rule is introduced to increase the number of sample patches to vary the patterns in an omega tile set secondly in contrast to the other concurrent techniques that randomly choose sample patches for tile construction our technique uses genetic algorithm to select the feasible patches from the input example this operation insures the quality of the whole tile set experimental results verify the high quality and efficiency of the proposed algorithm
in this paper we focus on the minimal deterministic finite automaton that recognizes the set of suffixes of word up to errors as first result we give characterization of the nerode's right invariant congruence that is associated with this result generalizes the classical characterization described in blumer blumer haussler ehrenfeucht chen seiferas the smallest automaton recognizing the subwords of text theoretical computer science as second result we present an algorithm that makes use of to accept in an efficient way the language of all suffixes of up to errors in every window of size of text where is the repetition index of moreover we give some experimental results on some well known words like prefixes of fibonacci and thue morse words finally we state conjecture and an open problem on the size and the construction of the suffix automaton with mismatches
routing overlays have become viable approach to working around slow bgp convergence and sub optimal path selection as well as to deploy novel forwarding architectures common sub component of routing overlay is routing mesh the route selection algorithm considers only those virtual links inter node links in an overlay in the routing mesh rather than all virtual links connecting an node overlay doing so reduces routing overhead thereby improving the scalability of the overlay as long as the process of constructing the mesh doesn't itself introduce overheadthis paper proposes and evaluates low cost approach to building topology aware routing mesh that eliminates virtual links that contain duplicate physical segments in the underlying network an evaluation of our method on planetlab shows that conservative link pruning algorithm reduces routing overhead by factor of two without negatively impacting route selection additional analysis quanti es the impact on route selection of defining an even sparser mesh on top of the topology aware routing mesh documenting the cost benefit tradeoff that is intrinsic to routing it also shows that constructing sparser routing mesh on the topology aware routing mesh rather than directly on the internet itself benefit from having the reduced number of duplicate physical segments in the underlying network which improves the resilience of the resulting routing mesh
independent parties that produce perfectly complementary components may form alliances or coalitions or groups to better coordinate their pricing decisions when they sell their products to downstream buyers this paper studies how market demand conditions ie the form of the demand function demand uncertainty and price sensitive demand drive coalition formation among complementary suppliers in deterministic demand model we show that for an exponential or isoelastic demand function suppliers always prefer selling in groups for linear power demand function suppliers may all choose to sell independently in equilibrium these results are interpreted through the pass through rate associated with the demand function in an uncertain demand model we show that in general the introduction of multiplicative stochastic element in demand has an insignificant impact on stable coalitions and that an endogenous retail price ie demand is price sensitive increases suppliers incentives to form alliances relative to the case with fixed retail price we also consider the impact of various other factors on stable outcomes in equilibrium eg sequential decision making by coalitions of different sizes the cost effect due to alliance formation either cost savings or additional costs and system without an assembler
user contributed wireless mesh networks are disruptive technology that may fundamentally change the economics of edge network access and bring the benefits of computer network infrastructure to local communities at low cost anywhere in the world to achieve high throughput despite highly unpredictable and lossy wireless channels it is essential that such networks take advantage of transmission opportunities wherever they emerge however as opportunistic routing departs from the traditional but less effective deterministic shortest path based routing user nodes in such networks may have less incentive to follow protocols and contribute in this paper we present the first routing protocols in which it is incentive compatible for each user node to honestly participate in the routing despite opportunistic transmissions we not only rigorously prove the properties of our protocols but also thoroughly evaluate complete implementation of our protocols experiments show that there is gain in throughput when compared with an opportunistic routing protocol that does not provide incentives and users can act selfishly
the proactive desk is new digital desk with haptic feedback the concept of digital desk was proposed by wellner in for the first time typical digital desk enables user to seamlessly handie both digital and physical objects on the desk with common gui standard the user however handles them as virtual gui objects our proactive desk allows the user to handle both digital and physical objects on digital desk with realistic feeling in the proactive desk two linear induction motors are equipped to generate an omnidirectional translational force on the user's hand or on physical object on the desk without any mechanical links or wires thereby preserving the advantages of the digital desk in this article we first discuss applications of digital desk with hatptic feedback then we mention the design and structure of the first trial proactive desk and its performance
the design development and deployment of interactive systems can substantively impact individuals society and the natural environment now and potentially well into the future yet scarcity of methods exists to support long term emergent systemic thinking in interactive design practice toward addressing this gap we propose four envisioning criteria stakeholders time values and pervasiveness distilled from prior work in urban planning design noir and value sensitive design we characterize how the criteria can support systemic thinking illustrate the integration of the envisioning criteria into established design practice scenariobased design and provide strategic activities to serve as generative envisioning tools we conclude with suggestions for use and future work key contributions include four envisioning criteria to support systemic thinking value scenarios extending scenario based design and strategic activities for engaging the envisioning criteria in interactive system design practice
number of vertical mining algorithms have been proposed recently for association mining which have shown to be very effective and usually outperform horizontal approaches the main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids tids and automatic pruning of irrelevant data the main problem with these approaches is when intermediate results of vertical tid lists become too large for memory thus affecting the algorithm scalabilityin this paper we present novel vertical data representation called diffset that only keeps track of differences in the tids of candidate pattern from its generating frequent patterns we show that diffsets drastically cut down the size of memory required to store intermediate results we show how diffsets when incorporated into previous vertical mining methods increase the performance significantly
we introduce comprehensive biomechanical model of the human upper body our model confronts the combined challenge of modeling and controlling more or less all of the relevant articular bones and muscles as well as simulating the physics based deformations of the soft tissues its dynamic skeleton comprises bones with jointed degrees of freedom including those of each vertebra and most of the ribs to be properly actuated and controlled the skeletal submodel requires comparable attention to detail with respect to muscle modeling we incorporate muscles each of which is modeled as piecewise uniaxial hill type force actuator to simulate biomechanically realistic flesh deformations we also develop coupled finite element model with the appropriate constitutive behavior in which are embedded the detailed anatomical geometries of the hard and soft tissues finally we develop an associated physics based animation controller that computes the muscle activation signals necessary to drive the elaborate musculoskeletal system in accordance with sequence of target poses specified by an animator
real time production systems and other dynamic environments often generate tremendous potentially infinite amount of stream data the volume of data is too huge to be stored on disks or scanned multiple times can we perform on line multi dimensional analysis and data mining of such data to alert people about dramatic changes of situations and to initiate timely high quality responses this is challenging task in this paper we investigate methods for on line multi dimensional regression analysis of time series stream data with the following contributions our analysis shows that only small number of compressed regression measures instead of the complete stream of data need to be registered for multi dimensional linear regression analysis to facilitate on line stream data analysis partially materialized data cube model with regression as measure and tilt time frame as its time dimension is proposed to minimize the amount of data to be retained in memory or stored on disks and an exception guided drilling approach is developed for on line multi dimensional exception based regression analysis based on this design algorithms are proposed for efficient analysis of time series data streams our performance study compares the proposed algorithms and identifies the most memory and time efficient one for multi dimensional stream data analysis
dynamic load balancing is key problem for the efficient use of parallel systems when solving applications with unpredictable load estimates however depending on the underlying programming paradigm single program multiple data spmd or multiple program multiple data mpmd the balancing requirements vary in spmd scenarios perfect load balance is desired whereas in mpmd scenarios it might be better to quickly obtain large reduction in load imbalance in short period of time we propose extending the local domain of given processor in the load balancing algorithms to find better scope for each paradigm for that purpose we present generalised version of the diffusion algorithm searching unbalanced domains called ds dasud which extends the local domain of each processor beyond its immediate neighbour ds dasud belongs to the iterative distributed load balancing idlb class and in its original formulation operates in diffusion scheme where processor balances its load with all its immediate neighbours ds we evaluate this algorithm for the two programming paradigms varying the domain size the evaluation was carried out using two simulators load balancing and network simulators for large set of load distributions that exhibit different degrees of initial workload unbalancing these distributions were applied to torus and hypercube topologies and the number of processors ranged from to from these experiments we conclude that the dasud fits well for spmd scenarios whereas for mpmd dasud and dasud for hypercube and torus topologies respectively obtain the best trade off between the imbalance reduction up to and the cost incurred in reaching it
we propose general methodology for analysing the behaviour of open systems modelled as coordinators ie open terms of suitable process calculi coordinator is understood as process with holes or placeholders where other coordinators and components ie closed terms can be plugged in thus influencing its behaviour the operational semantics of coordinators is given by means of symbolic transition system where states are coordinators and transitions are labeled by spatial modal formulae expressing the potential interaction that plugged components may enable behavioural equivalences for coordinators like strong and weak bisimilarities can be straightforwardly defined over such transition system different from other approaches based on universal closures ie where two coordinators are considered equivalent when all their closed instances are equivalent our semantics preserves the openness of the system during its evolution thus allowing dynamic instantiation to be accounted for in the semantics to further support the adequacy of the construction we show that our symbolic equivalences provide correct approximations of their universally closed counterparts coinciding with them over closed components for process calculi in suitable formats we show how tractable symbolic semantics can be defined constructively using unification
in decision support systems having knowledge on the top values is more informative and crucial than the maximum value unfortunately the naive method involves high computational cost and the existing methods for range max query are inefficient if applied directly in this paper we propose pre computed partition top method ppt to partition the data cube and pre store number of top values for improving query performance the main focus of this study is to find the optimum values for two parameters ie the partition factor and the number of pre stored values through analytical approach cost function based on poisson distribution is used for the analysis the analytical results obtained are verified against simulation results it is shown that the ppt method outperforms other alternative methods significantly when proper and are used
in this paper we concentrate on aspects related to modeling and formal verification of embedded systems first we define formal model of computation for embedded systems based on petri nets that can capture important features of such systems and allows their representation at different levels of granularity our modeling formalism has well defined semantics so that it supports precise representation of the system the use of formal methods to verify its correctness and the automation of different tasks along the design process second we propose an approach to the problem of formal verification of embedded systems represented in our modeling formalism we make use of model checking to prove whether certain properties expressed as temporal logic formulas hold with respect to the system model we introduce systematic procedure to translate our model into timed automata so that it is possible to use available model checking tools we propose two strategies for improving the verification efficiency the first by applying correctness preserving transformations and the second by exploring the degree of parallelism characteristic to the system some examples including realistic industrial case demonstrate the efficiency of our approach on practical applications
under frequent node arrival and departure churn in an overlay network structure the problem of preserving accessibility is addressed by maintaining valid entries in the routing tables towards nodes that are alive however if the system fails to replace the entries of dead nodes with entries of live nodes in the routing tables soon enough requests may fail in such cases mechanisms to route around failures are required to increase the tolerance to node failures existing distributed hash tables dhts overlays include extensions to provide fault tolerance when looking up keys however these are often insufficient we analyze the case of greedy routing often preferred for its simplicity but with limited dependability even when extensions are applied the main idea is that fault tolerance aspects need to be dealt with already at design time of the overlay we thus propose simple overlay that offers support for alternative paths and we create routing strategy which takes advantage of all these paths to route the requests while keeping maintenance cost low experimental evaluation demonstrates that our approach provides an excellent resilience to failures
in this paper we discuss the efforts underway at the pacific northwest national laboratory in understanding the dynamics of multi party discourse across number of communication modalities such as email instant messaging traffic and meeting data two prototype systems are discussed the conversation analysis tool chat is an experimental test bed for the development of computational linguistic components and enables users to easily identify topics or persons of interest within multi party conversations including who talked to whom when the entities that were discussed etc the retrospective analysis of communication events race prototype leveraging many of the chat components is an application built specifically for knowledge workers and focuses on merging different types of communication data so that the underlying message can be discovered in an efficient timely fashion
we present the rd redundancy detector rd identifies redundant code fragments in large software systems written in lisp for each pair of code fragments rd uses combination of techniques ranging from syntax based analysis to semantics based analysis that detects positive and negative evidences regarding the redundancy of the analyzed code fragments these evidences are combined according to well defined model and sufficiently redundant fragments are reported to the user rd explores several techniques and heuristics to operate within reasonable time and space bounds and is designed to be extensible
in the origin detection problem an algorithm is given set of documents ordered by creation time and query document it needs to output for every consecutive sequence of alphanumeric terms in the earliest document in in which the sequence appeared if such document exists algorithms for the origin detection problem can for example be used to detect the origin of text segments in and thus to detect novel content in they can also find the document from which the author of has copied the most or show that is mostly original we concentrate on solutions that use only fixed amount of memory we propose novel algorithms for this problem and evaluate them together with large number of previously published algorithms our results show that detecting the origin of text segments efficiently can be done with very high accuracy even when the space used is less than of the size of the documents in the precision degrades smoothly with the amount of available space various estimation techniques can be used to increase the performance of the algorithms
although deforming surfaces are frequently used in numerous domains only few works have been proposed until now for simplifying such data in this paper we propose new method for generating progressive deforming meshes based on shape feature analysis and deformation area preservation by computing the curvature and torsion of each vertex in the original model we add the shape feature factor to its quadric error metric when calculating each qem edge collapse cost in order to preserve the areas with large deformation we add deformation degree weight to the aggregated quadric errors when computing the unified edge contraction sequence finally the edge contraction order is slightly adjusted to further reduce the geometric distortion for each frame our approach is fast easy to implement and as result good quality dynamic approximations with well preserved fine details can be generated at any given frame
this paper presents reachout chat based tool for peer support collaboration and community building we describe the philosophy behind the tool and explain how posting questions in the open directly benefits the creation distribution and use of organizational knowledge in addition to enhancing the cohesion of the community involved reachout proposes new methods of handling problems that include locating selecting and approaching the right set of potential advisers we discuss the advantages of public discussions over private one on one sessions and how this is enhanced by our unique combination of synchronous and asynchronous communication we present and analyze results from pilot of reachout and conclude with plans for future research and development
software architecture has been key area of concern in software industry due to its profound impact on the productivity and quality of software products this is even more crucial in case of software product line because it deals with the development of line of products sharing common architecture and having controlled variability the main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting comprehensive empirical investigation covering broad range of organizations currently involved in the business of software product lines this is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge the results of this investigation provide empirical evidence that software product line architecture process activities play significant role in successfully developing and managing software product line
software system interacts with third party libraries through various apis using these library apis often needs tofollow certain usage patterns furthermore ordering rules specifications exist between apis and these rules govern the secure and robust operation of the system using these apis but these patterns and rules may not be well documented by the api developers previous approaches mine frequent association rules itemsets or subsequences that capture api call patterns shared by api client code however these frequent api patterns cannot completely capture some useful orderings shared by apis especially when multiple apis are involved across different procedures in this paper we present framework to automatically extract usage scenarios among user specified apis as partial orders directly from the source code api client code we adapt model checker to generate interprocedural control flow sensitive static traces related to the apis of interest different api usage scenarios are extracted from the static traces by our scenario extraction algorithm and fed to miner the miner summarizes different usage scenarios as compact partial orders specifications are extracted from the frequent partial orders using our specification extraction algorithm our experience of applying the framework on clients with loc in total has shown that theextracted api partial orders are useful in assisting effective api reuse and checking
reverse nearest neighbor rnn search is very crucial in many real applications in particular given database and query object an rnn query retrieves all the data objects in the database that have the query object as their nearest neighbors often due to limitation of measurement devices environmental disturbance or characteristics of applications for example monitoring moving objects data obtained from the real world are uncertain imprecise therefore previous approaches proposed for answering an rnn query over exact precise database cannot be directly applied to the uncertain scenario in this paper we re define the rnn query in the context of uncertain databases namely probabilistic reverse nearest neighbor prnn query which obtains data objects with probabilities of being rnns greater than or equal to user specified threshold since the retrieval of prnn query requires accessing all the objects in the database which is quite costly we also propose an effective pruning method called geometric pruning gp that significantly reduces the prnn search space yet without introducing any false dismissals furthermore we present an efficient prnn query procedure that seamlessly integrates our pruning method extensive experiments have demonstrated the efficiency and effectiveness of our proposed gp based prnn query processing approach under various experimental settings
in this paper we consider how discrete time quantum walks can be applied to graph matching problems the matching problem is abstracted using an auxiliary graph that connects pairs of vertices from the graphs to be matched by way of auxiliary vertices discrete time quantum walk is simulated on this auxiliary graph and the quantum interference on the auxiliary vertices indicates possible matches when dealing with graphs for which there is no exact match the interference amplitudes together with edge consistencies are used to define consistency measure we also explore the use of the method for inexact graph matching problems we have tested the algorithm on graphs derived from the nci molecule database and found it to significantly reduce the space of possible permutation matchings typically by factor of thereby allowing the graphs to be matched directly an analysis of the quantum walk in the presence of structural errors between graphs is used as the basis of the consistency measure we test the performance of this measure on graphs derived from images in the coil database
this paper presents novel software pipelining approach which is called swing modulo scheduling sms it generates schedules that are near optimal in terms of initiation interval register requirements and stage count swing modulo scheduling is heuristic approach that has low computational cost this paper first describes the technique and evaluates it for the perfect club benchmark suite on generic vliw architecture sms is compared with other heuristic methods showing that it outperforms them in terms of the quality of the obtained schedules and compilation time to further explore the effectiveness of sms the experience of incorporating it into production quality compiler for the equator map processor is described implementation issues are discussed as well as modifications and improvements to the original algorithm finally experimental results from using set of industrial multimedia applications are presented
content based image similarity search plays key role in multimedia retrieval each image is usually represented as point in high dimensional feature space the key challenge of searching similar images from large database is the high computational overhead due to the curse of dimensionality reducing the dimensionality is an important means to tackle the problem in this paper we study dimensionality reduction for top image retrieval intuitively an effective dimensionality reduction method should not only preserve the close locations of similar images or points but also separate those dissimilar ones far apart in the reduced subspace existing dimensionality reduction methods mainly focused on the former we propose novel idea called locality condensation lc to not only preserve localities determined by neighborhood information and their global similarity relationship but also ensure that different localities will not invade each other in the low dimensional subspace to generate non overlapping localities in the subspace lc first performs an elliptical condensation which condenses each locality with an elliptical shape into more compact hypersphere to enlarge the margins among different localities and estimate the projection in the subspace for overlap analysis through convex optimization lc further performs scaling condensation on the obtained hyperspheres based on their projections in the subspace with minimal condensation degrees by condensing the localities effectively the potential overlaps among different localities in the low dimensional subspace are prevented consequently for similarity search in the subspace the number of false hits ie distant points that are falsely retrieved will be reduced extensive experimental comparisons with existing methods demonstrate the superiority of our proposal
we evaluate the energy efficiency and performance of number of synchronization mechanisms adapted for embedded devices we focus on simple hardware accelerators for common software synchronization patterns we compare the energy efficiency of range of shared memory benchmarks using both spin locks and simple hardware transactional memory in most cases transactional memory provides both significantly reduced energy consumption and increased throughput we also consider applications that employ concurrency patterns based on semaphores such as pipelines and barriers we propose and evaluate novel energy efficient hardware semaphore construction in which cores spin on local scratchpad memory reducing the load on the shared bus
this paper explores the use of multi instance enrollment as means to improve the performance of face recognition experiments are performed using the nd face data set which contains scans of subjects this is the largest face data set currently available and contains substantial amount of varied facial expression results indicate that the multi instance enrollment approach outperforms state of the art component based recognition approach in which the face to be recognized is considered as an independent set of regions
media consumption is an inherently social activity serving to communicate ideas and emotions across both small and large scale communities the migration of the media experience to personal computers retains social viewing but typically only via non social strictly personal interface this paper presents an architecture and implementation for media content selection content re organization and content sharing within user community that is heterogeneous in terms of both participants and devices in addition our application allows the user to enrich the content as differentiated personalization activity targeted to his her peer group we describe the goals architecture and implementation of our system in this paper in order to validate our results we also present results from two user studies involving disjoint sets of test participants
in spatial database outsourcing data owner delegates its data management tasks to location based service lbs which indexes the data with an authenticated data structure ads the lbs receives queries ranges nearest neighbors originating from several clients subscribers each query initiates the computation of verification object vo based on the ads the vo is returned to the client that can verify the result correctness using the public key of the owner our first contribution is the mr tree space efficient ads that supports fast query processing and verification our second contribution is the mr tree modified version of the mr tree which significantly reduces the vo size through novel embedding technique finally whereas most adss must be constructed and maintained by the owner we outsource the mr and mr tree construction and maintenance to the lbs thus relieving the owner from this computationally intensive task
in question answering qa system the fundamental problem is how to measure the distance between question and an answer hence ranking different answers we demonstrate that such distance can be precisely and mathematically defined not only such definition is possible it is actually provably better than any other feasible definitions not only such an ultimate definition is possible but also it can be conveniently and fruitfully applied to construct qa system we have built such system quanta extensive experiments are conducted to justify the new theory
recent advances in information technology demand handling complex data types such as images video audio time series and genetic sequences distinctly from traditional data such as numbers short strings and dates complex data do not possess the total ordering property yielding relational comparison operators useless even equality comparisons are of little help as it is very unlikely to have two complex elements exactly equal therefore the similarity among elements has emerged as the most important property for comparisons in such domains leading to the growing relevance of metric spaces to data search regardless of the data domain properties the systems need to track evolution of data over time when handling multidimensional data temporal information is commonly treated as just one or more dimensions however metric data do not have the concept of dimensions thus adding plain temporal dimension does not make sense in this paper we propose novel metric temporal data representation and exploit its properties to compare elements by similarity taking into account time related evolution we also present experimental evaluation which confirms that our technique effectively takes into account the contributions of both the metric and temporal data components moreover the experiments showed that the temporal information always improves the precision of the answer
to conquer the weakness of existing integrity measurement and verification mechanisms based on trusted computing technology an integrity assurance mechanism for run time programs is proposed in this paper based on dynamic integrity measuring module the proposed integrity assurance mechanism solves the difficulties that may be encountered when attesting to the integrity of running programs the paper also describes the design and implementation details of the proposed module an example of applying the proposed mechanism to protect the vtpm instances in xen hypervisor is presented at last
abduction is fundamental form of nonmonotonic reasoning that aims at finding explanations for observed manifestations this process underlies many applications from car configuration to medical diagnosis we study here the computational complexity of deciding whether an explanation exists in the case when the application domain is described by propositional knowledge base building on previous results we classify the complexity for local restrictions on the knowledge base and under various restrictions on hypotheses and manifestations in comparison to the many previous studies on the complexity of abduction we are able to give much more detailed picture for the complexity of the basic problem of deciding the existence of an explanation it turns out that depending on the restrictions the problem in this framework is always polynomial time solvable np complete conp complete or complete based on these results we give an posteriori justification of what makes propositional abduction hard even for some classes of knowledge bases which allow for efficient satisfiability testing and deduction this justification is very simple and intuitive but it reveals that no nontrivial class of abduction problems is tractable indeed tractability essentially requires that the language for knowledge bases is unable to express both causal links and conflicts between hypotheses this generalizes similar observation by bylander et al for set covering abduction
it is time for us to focus on sound analyses for our critical systems software that is we must focus on analyses that ensure the absence of defects of particular known types rather than best effort bug finding tools this paper presents three sample analyses for linux that are aimed at eliminating bugs relating to type safety deallocation and blocking these analyses rely on lightweight programmer annotations and run time checks in order to make them practical and scalable sound analyses of this sort can check wide variety of properties and will ultimately yield more reliable code than bug finding alone
this paper presents new technique compiler directed page coloring that eliminates conflict misses in multiprocessor applications it enables applications to make better use of the increased aggregate cache size available in multiprocessor this technique uses the compiler's knowledge of the access patterns of the parallelized applications to direct the operating system's virtual memory page mapping strategy we demonstrate that this technique can lead to significant performance improvements over two commonly used page mapping strategies for machines with either direct mapped or two way set associative caches we also show that it is complementary to latency hiding techniques such as prefetchingwe implemented compiler directed page coloring in the suif parallelizing compiler and on two commercial operating systems we applied the technique to the specfp benchmark suite representative set of numeric programs we used the simos machine simulator to analyze the applications and isolate their performance bottlenecks we also validated these results on real machine an eight processor mhz digital alphaserver compiler directed page coloring leads to significant performance improvements for several applications overall our technique improves the specfp rating for eight processors by over digital unix's page mapping policy and by over page coloring standard page mapping policy the suif compiler achieves specfp ratio of the highest ratio to date
requirements specification defines the requirements for the future system at conceptual level ie class or type level in contrast scenario represents concrete example of current or future system usage in early re phases scenarios are used to support the definition of high level requirements goals to be achieved by the new system in many cases those goals can to large degree be elicited by observing documenting and analyzing scenarios about current system usage ie the new system must often fulfill many of the functional and nonfunctional goals of the existing system to support the elicitation and validation of the goals achieved by the existing system and to illustrate problems of the old system we propose to capture current system usage using rich media eg video speech pictures etc and to interrelate those observations with the goal definitions thus we particularly aim at making the abstraction process which leads to the definition of the conceptual models more transparent and traceablemore precisely we relate the parts of the observations which have caused the definition of goal or against which goal was validated with the corresponding goal these interrelations provide the basis for explaining and illustrating goal model to eg untrained stakeholders and or new team members and thereby improving common understanding of the goal model detecting analyzing and resolving different interpretation of the observations comparing different observations using computed goal annotations and refining or detailing goal model during later process phases using the prime implementation framework we have implemented the prime crews environment which supports the interrelation of conceptual models and captured system usage observations we report on our experiences with prime crews gained in first experimental case study
given set of keywords conventional keyword search ks returns set of tuples each of which is obtained from single relation or by joining multiple relations and ii contains all the keywords in this paper proposes relevant problem called frequent co occurring term fct retrieval specifically given keyword set and an integer fct query reports the terms that are not in but appear most frequently in the result of ks query with the same fct search is able to discover the concepts that are closely related to furthermore it is also an effective tool for refining the keyword set of traditional keyword search while fct query can be trivially supported by solving the corresponding ks query we provide faster algorithm that extracts the correct results without evaluating any ks query at all the effectiveness and efficiency of our techniques are verified with extensive experiments on real data
this paper presents an intermediate language level optimization framework for dynamic binary translation performance is important to dynamic binary translation system so there has been growing interest in exploring new optimization algorithms the framework proposed in this paper includes efficient profiling hot code recognition and smart code cache management policies profiling is responsible for collecting runtime information which will be used by hot code recognition and code cache management algorithms we only focus on recognizing the hottest code and assign priorities to basic blocks according to their hotness to facilitate code cache management
modern software applications require internationalization to be distributed to different regions of the world in various situations many software applications are not internationalized at early stages of development to internationalize such an existing application developers need to externalize some hard coded constant strings to resource files so that translators can easily translate the application into local language without modifying its source code since not all the constant strings require externalization locating those need to translate constant strings is necessary task that developers must complete for internationalization in this paper we present an approach to automatically locating need to translate constant strings our approach first collects list of api methods related to the graphical user interface gui and then searches for need to translate strings from the invocations of these api methods based on string taint analysis we evaluated our approach on four real world open source applications rtext risk artofillusion and megamek the results show that our approach effectively locates most of the need to translate constant strings in all the four applications
we investigate design principles for placing striped delay sensitive data on number of disks in distributed environment the cost formulas of our performance model allow us to calculate the maximum number of users that can be supported by disks as well as to study the impact of other performance tuning options we show that for fixed probabilities of accessing the delay sensitive objects partitioning the set of disks is always better than striping in all of the disks then given number of disks and distinct delay sensitive objects with probabilities of access pr that must be striped across different disk partitions ie nonoverlapping subsets of the disks we use the theory of schur functions in order to find what is the optimal number of disks that must be allocated to each partition for objects with different consumption rates we provide an analytic solution to the problem of disk partitioning we analyze the problem of grouping the more and less popular delay sensitive objects together in partitions when the partitions are less than the objects so that the number of supported users is maximized finally we analyze the trade off of striping on all the disks versus partitioning the set of the disks when the access probabilities of the delay sensitive objects change with time
the cost of electricity for datacenters is substantial operational cost that can and should be managed not only for saving energy but also due to the ecologic commitment inherent to power consumption often pursuing this goal results in chronic underutilization of resources luxury most resource providers do not have in light of their corporate commitments this work proposes formalizes and numerically evaluates deep sam for clearing provisioning markets based on the maximization of welfare subject to utility level dependant energy costs and customer satisfaction levels we focus specifically on linear power models and the implications of the inherent fixed costs related to energy consumption of modern datacenters and cloud environments we rigorously test the model by running multiple simulation scenarios and evaluate the results critically we conclude with positive results and implications for long term sustainable management of modern datacenters
virtual try on vto applications are still under development even if few simplified applications start to be available true vto should let the user specify its measurements so that realistic avatar can be generated also the avatar should be animatable so that the worn cloth can be seen in motion this later statement requires two technologies motion adaptation and real time cloth simulation both have been extensively studied during the past decade and state of the art techniques may now enable the creation of high quality vto allowing user to virtually try on garments while shopping online this paper reviews the pieces that should be put together to build such an application
we consider the natural extension of the well known single disk caching problem to the parallel disk model pdm the main challenge is to achieve as much parallelism as possible and avoid bottlenecks we are given fast memory cache of size memory blocks along with request sequence bn where each block bi resides on one of disks in each parallel step at most one block from each disk can be fetched the task is to serve in the minimum number of parallel os thus each is analogous to page fault the difference here is that during each page fault up to blocks can be brought into memory as long as all of the new blocks entering the memory reside on different disks the problem has long history note that this problem is non trivial even if all requests in are unique this restricted version is called read once despite the progress in the offline version and read once version the general online problem still remained open here we provide comprehensive results with full general solution for the problem with asymptotically tight competitive ratios to exploit parallelism any parallel disk algorithm needs certain amount of lookahead into future requests to provide effective caching an online algorithm must achieve competitive ratio we show lower bound that states for lookahead any online algorithm must be competitive for lookahead greater than where is constant the tight upper bound of md on competitive ratio is achieved by our algorithm skew the previous algorithm tlru was md competitive and this was also shown to be tight for an lru based strategy we achieve the tight ratio using fairly different strategy than lru we also show tight results for randomized algorithms against oblivious adversary and give an algorithm achieving better bounds in the resource augmentation model
it is approximately years since the first computational experiments were conducted in what has become known today as the field of genetic programming gp twenty years since john koza named and popularised the method and ten years since the first issue appeared of the genetic programming evolvable machines journal in particular during the past two decades there has been significant range and volume of development in the theory and application of gp and in recent years the field has become increasingly applied there remain number of significant open issues despite the successful application of gp to number of challenging real world problem domains and progress in the development of theory explaining the behavior and dynamics of gp these issues must be addressed for gp to realise its full potential and to become trusted mainstream member of the computational problem solving toolkit in this paper we outline some of the challenges and open issues that face researchers and practitioners of gp we hope this overview will stimulate debate focus the direction of future research to deepen our understanding of gp and further the development of more powerful problem solving algorithms
configuration problems are thriving application area for declarative knowledge representation that currently experiences constant increase in size and complexity of knowledge bases automated support of the debugging process of such knowledge bases is necessary prerequisite for effective development of configurators we show that this task can be achieved by consistency based diagnosis techniques based on the formal definition of consistency based configuration we develop framework suitable for diagnosing configuration knowledge bases during the test phase of configurators valid and invalid examples are used to test the correctness of the system in case such examples lead to unintended results debugging of the knowledge base is initiated starting from clear definition of diagnosis in the configuration domain we develop an algorithm based on conflicts our framework is general enough for its adaptation to diagnosing customer requirements to identify unachievable conditions during configuration sessionsa prototype implementation using commercial constraint based configurator libraries shows the feasibility of diagnosis within the tight time bounds of interactive debugging sessions finally we discuss the usefulness of the outcomes of the diagnostic process in different scenarios
we present method for visualizing short video clips in single static image using the visual language of storyboards these schematic storyboards are composed from multiple input frames and annotated using outlines arrows and text describing the motion in the scene the principal advantage of this storyboard representation over standard representations of video generally either static thumbnail image or playback of the video clip in its entirety is that it requires only moment to observe and comprehend but at the same time retains much of the detail of the source video our system renders schematic storyboard layout based on small amount of user interaction we also demonstrate an interaction technique to scrub through time using the natural spatial dimensions of the storyboard potential applications include video editing surveillance summarization assembly instructions composition of graphic novels and illustration of camera technique for film studies
web cache replacement algorithms have received lot of attention during the past years though none of the proposed algorithms deals efficiently with all the particularities of the web environment namely relatively weak temporal locality due to filtering effects of caching hierarchies heterogeneity in size and origin of request streams in this paper we present the crf replacement policy whose development is mainly motivated by two factors the first is the filtering effects of web caching hierarchies and the second is the intention of achieving balance between hit and byte hit rates crf's decisions for replacement are based on combination of the recency and frequency criteria in way that requires no tunable parameters
data replication is key technique for ensuring data availability traditionally researchers have focused on the availability of individual objects even though user level tasks called operations typically request multiple objects our recent experimental study has shown that the assignment of object replicas to machines results in subtle yet dramatic effects on the availability of these operations even though the availability of individual objects remains the same this paper is the first to approach the assignment problem from theoretical perspective and obtains series of results regarding assignments that provide the best and the worst availability for user level operations we use range of techniques to obtain our results from standard combinatorial techniques and hill climbing methods to janson's inequality strong probabilistic tool some of the results demonstrate that even quite simple versions of the assignment problem can have surprising answers
in this paper we focus on classifying documents according to opinion and value judgment they contain the main originality of our approach is to combine linguistic pre processing classification and voting system using several classification methods in this context the relevant representation of the documents allows to determine the features for storing textual data in data warehouses the conducted experiments on very large corpora from french challenge on text mining deft show the efficiency of our approach
in recent years design patterns gain more interest in software engineering communities for both software development and maintenance as template to solve certain recurring problem design pattern documents successful experiences of software experts and gradually becomes the design guidelines of software development applying design patterns correctly can improve the efficiency of software design in terms of reusability and enhance maintainability during reverse engineering software can be evolved when developers modify their initial designs as requirements change for instance developer may add delete set of design elements such as classes and methods modifications on software artifacts can introduce conflicts and inconsistencies in the previously applied design patterns which are difficult to find and time consuming to correct this paper presents graph transformation approach to pattern level design validation and evolution based on well founded formalism we validate given design by graph grammar parser and automatically evolve the design at pattern level using graph transformation system rules for potential pattern evolutions are predefined the graph transformation approach preserves the integrity and consistency of design patterns in the system when designs change prototype system is built and case study on the strategy pattern demonstrates the feasibility of pattern based design validation and evolution using graph transformation techniques
project design involves an initial selection of technologies which has strong consequences for later stages of design in this paper we describe an ethnographic based field work study of complex organization and how it addressed the issue of front end project and technology selection formal procedures were designed for the organization to perform repeatable definable and measurable actions yet formal procedures obscured much about the processes actually being applied in selecting technologies and projects in actuality the formal procedures were interwoven with sensemaking activities so that technologies could be understood compared and decision consensus could be reached we expect that the insights from this study can benefit design teams in complex organizations facing similar selection and requirements issues
trust management represents today promising approach for supporting access control in open environments while several approaches have been proposed for trust management and significant steps have been made in this direction major obstacle that still exists in the realization of the benefits of this paradigm is represented by the lack of adequate support in the dbmsin this paper we present design that can be used to implement trust management within current relational dbmss we propose trust model with sql syntax and illustrate the main issues arising in the implementation of the model in relational dbms specific attention is paid to the efficient verification of delegation path for certificates this effort permits relatively inexpensive realization of the services of an advanced trust management model within current relational dbmss
constraints are important not just for maintaining data integrity but also because they capture natural probabilistic dependencies among data items probabilistic xml database pxdb is the probability sub space comprising the instances of document that satisfy set of constraints in contrast to existing models that can express probabilistic dependencies it is shown that query evaluation is tractable in pxdbs the problems of sampling and determining well definedness ie whether the above subspace is nonempty are also tractable furthermore queries and constraints can include the aggregate functions count max min and ratio finally this approach can be easily extended to allow probabilistic interpretation of constraints
engineering of complex distributed real time applications is one of the hardest tasks faced by the software profession today all aspects of the process from design to implementation are made more difficult by the interaction of behavioral and platform constraints providing tools for this task is likewise not without major challenges in this paper we discuss tool suite which supports the development of complex distributed real time applications in suitable high level language crl the suite's component tools include compiler transformer optimizer an allocator migrator schedulability analyzer debugger monitor kernel and simulated network manager the overall engineering approach supported by the suite is to provide as simple and natural an integrated development paradigm as possible the suite tools address complexity due to distribution scheduling allocation and other sources in an integrated manner largely transparent to the developer to reflect the needs of propagation of functional and nonfunctional requirements throughout the development process number of robust code transformation and communication mechanisms have been incorporated into the suite to facilitate practical use of the suite the developed programs compile transform to safe subset of with appropriate libraries and runtime support in this safe subset the use of pointers is minimized aliases are not allowed unconstrained storage and time allocation is not allowed and constructions which lead to arbitrarily long executions or delays due to time or other resource allocation use are not permitted other safe features include strong typing including constrained variants only no side effects in expressions or functions etc
we address the problem of proving the total correctness of transformations of definite logic programs we consider general transformation rule called clause replacement which consists in transforming program into new program by replacing set of clauses occurring in by new set of clauses provided that and are equivalent in the least herbrand model of the program pwe propose general method for proving that transformations based on clause replacement are totally correct that is our method consists in showing that the transformation of into can be performed by adding extra arguments to predicates thereby deriving from the given program an annotated program overline ii applying variant of the clause replacement rule and transforming the annotated program overline into terminating annotated program overline and iii erasing the annotations from overline thereby getting qour method does not require that either or are terminating and it is parametric with respect to the annotations by providing different annotations we can easily prove the total correctness of program transformations based on various versions of the popular unfolding folding and goal replacement rules which can all be viewed as particular cases of our clause replacement rule
fundamental natural interaction concept is not yet fully exploited in most of the existing human computer interfaces recent technological advances have created the possibility to naturally and significantly enhance the interface perception by means of visual inputs the so called vision based interaction vbi in this paper we present gesture recognition algorithm where the user's movements are obtained through real time vision based motion capture system specifically we focus on recognizing users motions with particular mean that is gesture defining an appropriate representation of the user's motions based on temporal posture parameterization we apply non parametric techniques to learn and recognize the user's gestures in real time this scheme of recognition has been tested for controlling classical computer videogame the results obtained show an excellent performance in on line classification and it allows the possibility to achieve learning phase in real time due to its computational simplicity
in an attempt to cope with time varying workload traditional adaptive time warp protocols are designed to react in response to performance changes by altering control parameter configurations like the amount of available memory the size of the checkpointing interval the frequency of gvt computation fossil collection invocations etc we call those schemes reactive because all control decisions are undertaken based on historical performance information collected at runtime and come into effect in future system states what must be considered drawback of this class of approaches is that time warp logical processes lps have most likely reached state different from the one for which the control action was established thus inducing performance control activities which are always outdatedthis paper develops environment aware self adaptive time warp lps implementing pro active performance control scheme addressing the timeliness of control decisions opposed to reactive tw schemes our pro active control mechanism based on statistical analysis of the state history periodically collected in real time intervals of size forecasts future lp state performance control decision is established that is most appropriate for the expected future lp state ie the state when the corresponding control activity would become effective depending on the forecast quality pro active scheme will presumably exhibit performance superior to re active schemes at least for cases where state changes in the time frame delta are very likely in this paper we study the ability of pro active tw lps to adapt to sudden load changes especially to abruptly occurring background workloads injected by other applications executing concurrently with the tw simulation on network of workstations experimental results show that the protocol is able to capture abrupt changes in both computational and communication resource availability justifying the title shock resistant time warp
multiresolution meshes enable us to build representations of geometric objects at different levels of detail lods we introduce multiresolution scheme whose data structure allows us to separately restore the geometry and topology of mesh during the refinement process additionally we use topological criterion not geometric criterion as usual in the literature to quickly simplify mesh what seems to make the corresponding simplification algorithm adequate for real time applications such as for example on line computer games
recent progress in energy harvesting technologies made it possible to build sensor networks with rechargeable nodes which target an indefinitely long operation in these networks the goal of energy management is to allocate the available energy such that the important performance metrics such as the number of detected threats are maximized as the harvested energy is not sufficient for continuous operation the scheduling of the active and inactive time is one of the main components of energy management the active time scheduling protocols need to maintain the energy equilibrium of the nodes while considering the uncertainties of the energy income which is strongly influenced by the weather and the energy expenditures which are dependent on the behavior of the targets in this paper we describe and experimentally compare three active time scheduling protocols static active time dynamic active time based on multi parameter heuristic and utility based uniform sensing we show that protocols which take into consideration the probabilistic models of the energy income and expenditure and can dynamically adapt to changes in the environment can provide significant performance advantage
so far virtual machine vm migration has focused on transferring the run time memory state of the vms in local area networks lan however for wide area network wan migration it is crucial to not just transfer the vms image but also transfer its local persistent state its file system and its on going network connections in this paper we address both by combining block level solution with pre copying and write throttling we show that we can transfer an entire running web server including its local persistent state with minimal disruption three seconds in the lan and seconds in the wan by combining dyndns with tunneling existing connections can continue transparently while new ones are redirected to the new network location thus we show experimentally that by combining well known techniques in novel manner we can provide system support for migrating virtual execution environments in the wide area
communication set generation significantly influences the performance of parallel programs however studies seldom give attention to the problem of communication set generation for irregular applications in this paper we propose communication optimization techniques for the situation of irregular array references in nested loops in our methods the local array distribution schemes are determined so that the total number of communication messages is minimized then we explain how to support communication set generation at compile time by introducing some symbolic analysis techniques in our symbolic analysis symbolic solutions of set of symbolic expression are obtained by using certain restrictions we introduce symbolic analysis algorithms to obtain the solutions in terms of set of equalities and inequalities finally experimental results on parallel computer cm are presented to validate our approach
the successful deployment of security policy is closely related not only to the complexity of the security requirements but also to the capabilities functionalities of the security devices the complexity of the security requirements is additionally increased when contextual constraints are taken into account such situations appear when addressing the dynamism of some security requirements or when searching finer granularity for the security rules the context denotes those specific conditions in which the security requirements are to be met re deploying contextual security policy depends on the security device functionalities either the devices include all functionalities necessary to deal with context and the policy is consequently deployed for ensuring its automatic changes or the devices do not have the right functionalities to entirely interpret contextual requirement we present solution to cope with this issue the re deployment of access control policies in system that lacks the necessary functionalities to deal with contexts
memory management is critical issue in stream processing involving stateful operators such as join traditionally the memory requirement for stream join is query driven query has to explicitly define window for each potentially unbounded input the window essentially bounds the size of the buffer allocated for that stream however output produced this way may not be desirable if the window size is not part of the intended query semantic due to the volatile input characteristics we discover that when streams are ordered or partially ordered it is possible to use data driven memory management scheme to improve the performance in this work we present novel data driven memory management scheme called window oblivious join wo join which adaptively adjusts the state buffer size according to the input characteristics our performance study shows that compared to traditional window join join wo join is more robust with respect to the dynamic input and therefore produces higher quality results with lower memory costs
we investigate the differences in terms of bothquantitative performance and subjective preference between direct touch and mouse input for unimanual andbimanual tasks on tabletop displays the results of twoexperiments show that for bimanual tasks performed ontabletops users benefit from direct touch input however our results also indicate that mouse input may be moreappropriate for single user working on tabletop tasksrequiring only single point interaction
implementing first class continuations can pose challenge if the target machine makes no provisions for accessing and re installing the run time stack in this paper we present novel translation that overcomes this problem in the first half of the paper we introduce theoretical model that shows how to eliminate the capture and the use of first class continuations in the presence of generalized stack inspection mechanism the second half of the paper explains how to translate this model into practice in two different contexts first we reformulate the servlet interaction language in the plt web server which heavily relies on first class continuations using our technique servlet programs can be run directly under the control of non cooperative web servers such as apache second we show how to use our new technique to copy and reconstitute the stack on msilnet using exception handlers this establishes that scheme's first class continuations can exist on non cooperative virtual machines
automatic text classification is an important task for many natural language processing applications this paper presents neural approach to develop text classifier based on the learning vector quantization lvq algorithm the lvq model is classification method that uses competitive supervised learning algorithm the proposed method has been applied to two specific tasks text categorization and word sense disambiguation experiments were carried out using the reuters text collection for text categorization and the senseval corpus for word sense disambiguation the results obtained are very promising and show that our neural approach based on the lvq algorithm is an alternative to other classification systems
multicast is one of the most frequently used collective communication operations in multi core soc platforms bus as the traditional interconnect architecture for soc development has been highly efficient in delivering multicast messages since the bus is non scalable it can not address the bandwidth requirements of the large socs the networks on chip nocs emerged as scalable alternative to address the increasing communication demands of such systems however due to its hop to hop communication the nocs may not be able to deliver multicast operations as efficiently as buses do adopting multi port routers has been an approach to improve the performance of the multicast operations in interconnection networks this paper presents novel analytical model to compute communication latency of the multicast operation in wormhole routed interconnection networks employing asynchronous multi port routers scheme the model is applied to the quarc noc and its validity is verified by comparing the model predictions against the results obtained from discrete event simulator developed using omnet
queries that return list of frequently occurring items are important in the analysis of real time internet packet streams while several results exist for computing top queries using limited memory in the infinite stream model eg limited memory sliding windows to compute the statistics over sliding window synopsis data structure can be maintained for the stream to compute the statistics rapidly usually top query is always processed over an equal synopsis but it's very hard to implement over an unequal synopsis because of the resulting inaccurate approximate answers therefore in this paper we focus on periodically refreshed top queries over sliding windows on internet traffic streams we present deterministic dsw dynamic sub window algorithm to support the processing of top aggregate queries over an unequal synopsis and guarantee the accuracy of the approximation results
concurrent ml cml is statically typed higher order concurrent language that is embedded in standard ml its most notable feature is its support for first class synchronous operations this mechanism allows programmers to encapsulate complicated communication and synchronization protocols as first class abstractions which encourages modular style of programming where the underlying channels used to communicate with given thread are hidden behind data and type abstractionwhile cml has been in active use for well over decade little attention has been paid to optimizing cml programs in this paper we present new program analysis for statically typed higher order concurrent languages that enables the compile time specialization of communication operations this specialization is particularly important in multiprocessor or multicore setting where the synchronization overhead for general purpose operations are high preliminary results from prototype that we have built demonstrate that specialized channel operations are much faster than the general purpose operationsour analysis technique is modular ie it analyzes and optimizes single unit of abstraction at time which plays to the modular style of many cml programs the analysis consists of three steps the first is type sensitive control flow analysis that uses the program's type abstractions to compute more precise results the second is the construction of an extended control flow graph using the results of the cfa the last step is an iterative analysis over the graph that approximates the usage patterns of known channels our analysis is designed to detect special patterns of use such as one shot channels fan in channels and fan out channels we have proven the safety of our analysis and state those results
an spanner of graph is subgraph that approximates distances in within multiplicative factor and an additive error ensuring that for any two nodes dh dg this paper concerns algorithms for the distributed deterministic construction of sparse spanner for given graph and distortion parameters and it first presents generic distributed algorithm that in constant number of rounds constructs for every node graph and integer an spanner of Œ≤n edges where and are constants depending on for suitable parameters this algorithm provides spanner of at most kn edges in rounds matching the performances of the best known distributed algorithm by derbel et al podc for and constant it can also produce spanner of edges in constant time more interestingly for every integer it can construct in constant time spanner of edges such deterministic construction was not previously known the paper also presents second generic deterministic and distributed algorithm based on the construction of small dominating sets and maximal independent sets after computing such sets in sub polynomial time it constructs at its best spanner with Œ≤n edges where klog log for it provides spanner with edges the additive terms in the stretch of our constructions yield the best trade off currently known between and due to elkin and peleg stoc our distributed algorithms are rather short and can be viewed as unification and simplification of previous constructions
we introduce spidercast distributed protocol for constructing scalable churn resistant overlay topologies for supporting decentralized topic based pub sub communication spidercast is designed to effectively tread the balance between average overlay degree and communication cost of event dissemination it employs novel coverage optimizing heuristic in which the nodes utilize partial subscription views provided by decentralized membership service to reduce the average node degree while guaranteeing with high probability that the events posted on each topic can be routed solely through the nodes interested in this topic in other words the overlay is topic connected spidercast is unique in maintaining an overlay topology that scales well with the average number of topics node is subscribed to assuming the subscriptions are correlated insofar as found in most typical workloads furthermore the degree grows logarithmically in the total number of topics and slowly decreases as the number of nodes increases we show experimentally that for many practical work loads the spidercast overlays are both topic connected and have low per topic diameter while requiring each node to maintain low average number of connections these properties are satisfied even in very large settings involving up to nodes topics and subscriptions per node and under high churn rates in addition our results demonstrate that in large setting the average node degree in spidercast is at least smaller than in other overlays typically used to support decentralized pub sub communication such as eg similarity based rings based and random overlays
this chapter surveys the topic of active rules and active databases we analyze the state of the art of active databases and active rules their properties and applications in particular we describe the case of triggers following the sql standard committee point of view then we consider the case of dynamic constraints for which we use temporal logic formalism finally we discuss the applicability limitations and partial solutions found when attempting to ensure the satisfaction of dynamic constraints
the accuracy of methods for the assessment of mammographic risk analysis is heavily related to breast tissue characteristics previous work has demonstrated considerable success in developing an automatic breast tissue classification methodology which overcomes this difficulty this paper proposes unified approach for the application of number of rough and fuzzy rough set methods to the analysis of mammographic data indeed this is the first time that fuzzy rough approaches have been applied to this particular problem domain in the unified approach detailed here feature selection methods are employed for dimensionality reduction developed using rough sets and fuzzy rough sets number of classifiers are then used to examine the data reduced by the feature selection approaches and assess the positive impact of these methods on classification accuracy additionally this paper also employs new fuzzy rough classifier based on the nearest neighbour classification algorithm the novel use of such an approach demonstrates its efficiency in improving classification accuracy for mammographic data as well as considerably removing redundant irrelevant and noisy features this is supported with experimental application to two well known datasets the overall result of employing the proposed unified approach is that feature selection can identify only those features which require extraction this can have the positive effect of increasing the risk assessment accuracy rate whilst additionally reducing the time required for expert scrutiny which in turn means the risk analysis process is potentially quicker and involves less screening
many complex analysis problems can be most clearly and easily specified as logic rules and queries where rules specify how given facts can be combined to infer new facts and queries select facts of interest to the analysis problem at hand however it has been extremely challenging to obtain efficient implementations from logic rules and to understand their time and space complexities especially for on demand analysis driven by queriesthis paper describes powerful method for generating specialized rules and programs for demand driven analysis from datalog rules and queries and further for providing time and space complexity guarantees the method combines recursion conversion with specialization of rules and then uses method for program generation and complexity calculation from rules we compare carefully with the best prior methods by examining many variants of rules and queries for the same graph reachability problems and show the application of our method in implementing graph query languages in general
understanding the intent underlying user's queries may help personalize search results and therefore improve user satisfaction we develop methodology for using the content of search engine result pages serps along with the information obtained from query strings to study characteristics of query intent with particular focus on sponsored search this work represents an initial step towards the development and evaluation of an ontology for commercial search considering queries that reference specific products brands and retailers the characteristics of query categories are studied with respect to aggregated user's clickthrough behavior on advertising links we present model for clickthrough behavior that considers the influence of such factors as the location of ads and the rank of ads along with query category we evaluate our work using large corpus of clickthrough data obtained from major commercial search engine our findings suggest that query based features along with the content of serps are effective in detecting query intent the clickthrough behavior is found to be consistent with the classification for the general categories of query intent while for product brand and retailer categories all is true to lesser extent
advances in wireless and mobile computing environments allow mobile user to access wide range of applications for example mobile users may want to retrieve data about unfamiliar places or local life styles related to their location these queries are called location dependent queries furthermore mobile user may be interested in getting the query results repeatedly which is called location dependent continuous querying this continuous query emanating from mobile user may retrieve information from single zone single zq or from multiple neighbouring zones multiple zq we consider the problem of handling location dependent continuous queries with the main emphasis on reducing communication costs and making sure that the user gets correct current query result the key contributions of this paper include proposing hierarchical database framework tree architecture and supporting continuous query algorithm for handling location dependent continuous queries analysing the flexibility of this framework for handling queries related to single zq or multiple zq and propose intelligent selective placement of location dependent databases proposing an intelligent selective replication algorithm to facilitate time and space efficient processing of location dependent continuous queries retrieving single zq information demonstrating using simulation the significance of our intelligent selective placement and selective replication model in terms of communication cost and storage constraints considering various types of queries
studies have shown that for significant fraction of the time workstations are idle in this paper we present new scheduling policy called linger longer that exploits the fine grained availability of workstations to run sequential and parallel jobs we present two level workload characterization study and use it to simulate cluster of workstations running our new policy we compare two variations of our policy to two previous policies immediate eviction and pause and migrate our study shows that the linger longer policy can improve the throughput of foreign jobs on cluster by percent with only percent slowdown of local jobs for parallel computing we show that the linger longer policy outperforms reconfiguration strategies when the processor utilization by the local process is percent or less in both synthetic bulk synchronous and real data parallel applications
protection of one's intellectual property is topic with important technological and legal facets the significance of this issue is amplified nowadays due to the ease of data dissemination through the internet here we provide technological mechanisms for establishing the ownership of dataset consisting of multiple objects the objects that we consider in this work are shapes ie two dimensional contours which abound in disciplines such as medicine biology anthropology and natural sciences the protection of the dataset is achieved through means of embedding of an imperceptible ownership seal that imparts only minute visual distortions this seal needs to be embedded in the proper data space so that its removal or destruction is particularly difficult our technique is robust to many common transformations such as data rotation translation scaling noise addition and resampling in addition to that the proposed scheme also guarantees that important distances between the dataset shapes objects are not distorted we achieve this by preserving the geodesic distances between the dataset objects geodesic distances capture significant part of the dataset structure and their usefulness is recognized in many machine learning visualization and clustering algorithms therefore if practitioner uses the protected dataset as input to variety of mining machine learning or database operations the output will be the same as on the original dataset we illustrate and validate the applicability of our methods on image shapes extracted from anthropological and natural science data
we describe parallel real time garbage collector and present experimental results that demonstrate good scalability and good real time bounds the collector is designed for shared memory multiprocessors and is based on an earlier collector algorithm which provided fixed bounds on the time any thread must pause for collection however since our earlier algorithm was designed for simple analysis it had some impractical features this paper presents the extensions necessary for practical implementation reducing excessive interleaving handling stacks and global variables reducing double allocation and special treatment of large and small objects an implementation based on the modified algorithm is evaluated on set of sml benchmarks on sun enterprise way ultrasparc ii multiprocessor to the best of our knowledge this is the first implementation of parallel real time garbage collector the average collector speedup is at processors and at processors maximum pause times range from ms to ms in contrast non incremental collector whether generational or not has maximum pause times from ms to ms compared to non parallel stop copy collector parallelism has overhead while real time behavior adds an additional overhead since the collector takes about of total execution time these features have an overall time costs of and
distributed applications have become core component of the internet's infrastructure however many undergraduate curriculums especially at small colleges do not offer courses that focus on the design and implementation of distributed systems the courses that are offered address the theoretical aspects of system design but often fail to provide students with the opportunity to develop and evaluate distributed applications in real world environments as result undergraduate students are not as prepared as they should be for graduate study or careers in industry this paper describes an undergraduate course in distributed systems that not only studies the key design principles of distributed systems but also has unique emphasis on giving students hands on access to distributed systems through the use of shared computing testbeds such as planetlab and geni and open source technologies such as xen and hadoop using these platforms students can perform large scale distributed experimentation even at small colleges
network architectures for collaborative virtual reality have traditionally been dominated by client server and peer to peer approaches with peer to peer strategies typically being favored where minimizing latency is priority and client server where consistency is key with increasingly sophisticated behavior models and the demand for better support for haptics we argue that neither approach provides sufficient support for these scenarios and thus hybrid architecture is required we discuss the relative performance of different distribution strategies in the face of real network conditions and illustrate the problems they face finally we present an architecture that successfully meets many of these challenges and demonstrate its use in distributed virtual prototyping application which supports simultaneous collaboration for assembly maintenance and training applications utilizing haptics
we describe new application controlled file persistence model in which applications select the desired stability from range of persistence guarantees this new abstraction extends conventional abstractions by allowing applications to specify file's volatility and methods for automatic reconstruction in case of loss the model allows applications particularly ones with weak persistence requirements to leverage the memory space of other machines to improve their performance an automated filename matching interface permits legacy applications to take advantage of the variable persistence guarantees without being modified our prototype implementation shows significant speed ups in some cases more than an order of magnitude over conventional network file systems such as nfs version
data caches are key hardware means to bridge the gap between processor and memory speeds but only for programs that exhibit sufficient data locality in their memory accesses thus method for evaluating cache performance is required to both determine quantitatively cache misses and to guide data cache optimizations existing analytical models for data cache optimizations target mainly isolated perfect loop nests we present an analytical model that is capable of statically analyzing not only loop nest fragments but also complete numerical programs with regular and compile time predictable memory accesses central to the whole program approach are abstract call inlining memory access vectors and parametric reuse analysis which allow the reuse and interference both within and across loop nests to be quantified precisely in unified framework based on the framework the cache misses of program are specified using mathematical formulas and the miss ratio is predicted from these formulas based on statistical sampling techniques our experimental results using kernels and whole programs indicate accurate cache miss estimates in substantially shorter amount of time typically several orders of magnitude faster than simulation
indexes are commonly employed to retrieve portion of file or to retrieve its records in particular order an accurate performance model of indexes is essential to the design analysis and tuning of file management and database systems and particularly to database query optimization many previous studies have addressed the problem of estimating the number of disk page fetches when randomly accessing records out of given records stored on disk pages this paper generalizes these results relaxing two assumptions that usually do not hold in practice unlimited buffer and unique records for each key value experiments show that the performance of an index scan is very sensitive to buffer size limitations and multiple records per key value model for these more practical situations is presented and formula derived for estimating the performance of an index scan we also give closed form approximation that is easy to compute the theoretical results are validated using the distributed relational database system although we use database terminology throughout the paper the model is more generally applicable whenever random accesses are made using keys
visibility problems are central to many computer graphics applications the most common examples include hidden part removal for view computation shadow boundaries mutual visibility of objects for lighting simulation in this paper we present theoretical study of visibility properties for scenes of smooth convex objects we work in the space of light rays or more precisely of maximal free segments we group segments that see the same object this defines the visibility complex the boundaries of these groups of segments correspond to the visual events of the scene limits of shadows disappearance of an object when the viewpoint is moved etc we provide worst case analysis of the complexity of the visibility complex of scenes as well as probabilistic study under simple assumption for normal scenes we extend the visibility complex to handle temporal visibility we give an output sensitive construction algorithm and present applications of our approach
wireless sensor network wsn design is complicated by the interaction of three elements localization medium access control and routing the design of wireless sensor networks necessarily focuses on the network's intended purpose as well as an understanding of the synergies and tradeoffs between each of these three elements we propose five guidelines for the development of wireless sensor network and the performance and suitability of several protocols and algorithms are examined with respect to those guidelines we also suggest several performance metrics to provide fair comparisons between various localization medium access control and routing protocols
while the past research discussed several advantages of multiprocessor system on chip mpsoc architectures from both area utilization and design verification perspectives over complex single core based systems compilation issues for these architectures have relatively received less attention programming mpsocs can be challenging as several potentially conflicting issues such as data locality parallelism and load balance across processors should be considered simultaneously most of the compilation techniques discussed in the literature for parallel architectures not necessarily for mpsocs are loop based ie they consider each loop nest in isolation however one key problem associated with such loop based techniques is that they fail to capture the interactions between the different loop nests in the application this paper takes more global approach to the problem and proposes compiler driven data locality optimization strategy in the context of embedded mpsocs an important characteristic of the proposed approach is that in deciding the workloads of the processors ie in parallelizing the application it considers all the loop nests in the application simultaneously our experimental evaluation with eight embedded applications shows that the global scheme brings significant power performance benefits over the conventional loop based scheme
we propose new real time authentication scheme for memory as in previous proposals the scheme uses merkle tree to guarantee dynamic protection of memory we use the universal hash function family nh for speed and couple it with an aes encryption in order to achieve high level of security the proposed scheme is much faster compared to similar schemes achieved by cryptographic hash functions such as sha due to the finer grain incremental hashing ability provided by nh this advantage in speed becomes more vivid when the frequency of integrity checks becomes much lower than the frequency of memory updating this feature is mainly due to the incremental nature of nh moreover we show that with small variation in the universal hash function family used we can achieve fast and simple software implementation
query optimizer compares alternative plans in its search space to find the best plan for given query depending on the search space and the enumeration algorithm optimizers vary in their compilation time and the quality of the execution plan they can generate this paper describes compilation time estimator that provides quantified estimate of the optimizer compilation time for given query such an estimator is useful for automatically choosing the right level of optimization in commercial database systems in addition compilation time estimates can be quite helpful for mid query reoptimization for monitoring the progress of workload analysis tools where large number queries need to be compiled but not executed and for judicious design and tuning of an optimizerprevious attempts to estimate optimizer compilation complexity used the number of possible binary joins as the metric and overlooked the fact that each join often translates into different number of join plans because of the presence of physical properties we use the number of plans instead of joins to estimate query compilation time and employ two novel ideas reusing an optimizer's join enumerator to obtain actual number of joins but bypassing plan generation to save estimation overhead maintaining small number of interesting properties to facilitate plan counting we prototyped our approach in commercial database system and our experimental results show that we can achieve good compilation time estimates less than error on average for complex real queries using small fraction within of the actual compilation time
iterative compilation is an efficient approach to optimize programs on rapidly evolving hardware but it is still only scarcely used in practice due to necessity to gather large number of runs often with the same data set and on the same environment in order to test many different optimizations and to select the most appropriate ones naturally in many cases users cannot afford training phase will run each data set once develop new programs which are not yet known and may regularly change the environment the programs are run onin this article we propose to overcome that practical obstacle using collective optimization where the task of optimizing program leverages the experience of many other users rather than being performed in isolation and often redundantly by each user collective optimization is an unobtrusive approach where performance information obtained after each run is sent back to central database which is then queried for optimizations suggestions and the program is then recompiled accordingly we show that it is possible to learn across data sets programs and architectures in non dynamic environments using static function cloning and run time adaptation without even reference run to compute speedups over the baseline optimization we also show that it is possible to simultaneously learn and improve performance since there are no longer two separate training and test phases as in most studies we demonstrate that extensively relying on competition among pairs of optimizations program reaction to optimizations provides robust and efficient method for capturing the impact of optimizations and for reusing this knowledge across data sets programs and environments we implemented our approach in gcc and will publicly disseminate it in the near future
this paper describes memory discipline that combines region based memory management and copying garbage collection by extending cheney's copying garbage collection algorithm to work with regions the paper presents empirical evidence that region inference very significantly reduces the number of garbage collections and evidence that the fastest execution is obtained by using regions alone without garbage collection the memory discipline is implemented for standard ml in the ml kit compiler and measurements show that for variety of benchmark programs code generated by the compiler is as efficient both with respect to execution time and memory usage as programs compiled with standard ml of new jersey another state of the art standard ml compiler
this paper introduces unifi tool that attempts to automatically detect dimension errors in java programs unifi infers dimensional relationships across primitive type and string variables in program using an inter procedural context sensitive analysis it then monitors these dimensional relationships as the program evolves flagging inconsistencies that may be errors unifi requires no programmer annotations and supports arbitrary program specific dimensions thus providing fine grained dimensional consistency checking unifi exploits features of object oriented languages but can be used for other languages as well we have run unifi on real life java code and found that it is useful in exposing dimension errors we present case study of using unifi on nightly builds of line code base as it evolved over months
embedded digital signal processors for software defined radio have stringent design constraints including high computational bandwidth low power consumption and low interrupt latency furthermore due to rapidly evolving communication standards with increasing code complexity these processors must be compiler friendly so that code for them can quickly be developed in high level language in this paper we present the design of the sandblaster processor low power multithreaded digital signal processor for software defined radio the processor uses unique combination of token triggered threading powerful compound instructions and simd vector operations to provide real time baseband processing capabilities with very low power consumption we describe the processor's architecture and microarchitecture along with various techniques for achieving high performance and low power dissipation we also describe the processor's programming environment and the sb platform complete system on chip solution for software defined radio using super computer class vectorizing compiler the sb achieves real time performance in software on variety of communication protocols including gps am fm radio bluetooth gprs and wcdma in addition to providing programmable platform for sdr the processor also provides efficient support for wide variety of digital signal processing and multimedia applications
the logic programming paradigm provides the basis for new intensional view of higher order notions this view is realized primarily by employing the terms of typed lambda calculus as representational devices and by using richer form of unification for probing their structures these additions have important meta programming applications but they also pose non trivial implementation problems one issue concerns the machine representation of lambda terms suitable to their intended use an adequate encoding must facilitate comparison operations over terms in addition to supporting the usual reduction computation another aspect relates to the treatment of unification operation that has branching character and that sometimes calls for the delaying of the solution of unification problems final issue concerns the execution of goals whose structures become apparent only in the course of computation these various problems are exposed in this paper and solutions to them are described satisfactory representation for lambda terms is developed by exploiting the nameless notation of de bruijn as well as explicit encodings of substitutions special mechanisms are molded into the structure of traditional prolog implementations to support branching in unification and carrying of unification problems over other computation steps premium is placed in this context on exploiting determinism and on emulating usual first order behaviour an extended compilation model is presented that treats higher order unification and also handles dynamically emergent goals the ideas described here have been employed in the teyjus implementation of the lambda prolog language fact that is used to obtain preliminary assessment of their efficacy
new directions in the provision of end user computing experiences mean that we need to determine the best way to share data between small mobile computing devices partitioning large structures so that they can be shared efficiently provides basis for data intensive applications on such platforms in conjunction with such an approach dictionary based compression techniques provide additional benefits and help to prolong battery life
there has been wide interest recently in managing probabilistic data but in order to follow the rich literature on probabilistic databases one is often required to take detour into probability theory correlations conditionals monte carlo simulations error bounds topics that have been studied extensively in several areas of computer science and mathematics because of that it is often difficult to get to the algorithmic and systems level aspects of probabilistic data management in this tutorial we will distill these aspects from the often theory heavy literature on probabilistic databases we will start by describing real application at the university of washington using the rfid ecosystem we will show how probabilities arise naturally and why we need to cope with them we will then describe what an implementor needs to know to process sql queries on probabilistic databases in the second half of the tutorial we will discuss more advanced issues such as event processing over probabilistic streams and views over probabilistic data
specifications that are used in detailed design and in the documentation of existing code are primarily written and read by programmers however most formal specification languages either make heavy use of symbolic mathematical operators which discourages use by programmers or limit assertions to expressions of the underlying programming language which makes it difficult to write exact specifications moreover using assertions that are expressions in the underlying programming language can cause problems both in runtime assertion checking and in formal verification because such expressions can potentially contain side effects the java modeling language jml avoids these problems it uses side effect free subset of java's expressions to which are added few mathematical operators such as the quantifiers forall and exists jml also hides mathematical abstractions such as sets and sequences within library of java classes the goal is to allow jml to serve as common notation for both formal verification and runtime assertion checking this gives users the benefit of several tools without the cost of changing notations
data caching on mobile clients is widely seen as an effective solution to improve system performance in particular cooperative caching based on the idea of sharing and coordination of cache data among multiple users can be particularly effective for information access in mobile ad hoc networks where mobile clients are moving frequently and network topology is changing dynamically most existing cache strategies perform replacement independently and they seldom consider coordinated replacement and energy saving issues in the context of mobile ad hoc network in this paper we analyse the impact of energy on designing cache replacement policy and formulate the energy efficient coordinated cache replacement problem ecorp as knapsack problem dynamic programming algorithm called ecorp dp and heuristic algorithm called ecorp greedy are presented to solve the problem simulations using both synthetic workload traces and real workload traces in our experiments show that the proposed policies can significantly reduce energy consumption and access latency when compared to other replacement policies
human participation in business processes needs to be addressed in process modeling bpelpeople with ws humantask covers this concern in the context of bpel bound to specific workflow technology this leads to number of problems firstly maintaining and migrating processes to new or similar technologies is expensive secondly the low level technical standards make it hard to communicate the process models to human domain experts model driven approaches can help to easier cope with technology changes and present the process models at higher level of abstraction than offered by the technology standards in this paper we extend the model driven approach with view based framework for business process modeling in which models can be viewed at different abstraction levels and different concerns of model can be viewed separately our approach enables developers to work with meta models that represent technical view on the human participation whereas human domain experts can have an abstract view on human participation in business process in order to validate our work mapping to bpelpeople technology will be demonstrated
large datasets on the order of gb and tb are increasingly common as abundant computational resources allow practitioners to collect produce and store data at higher rates as dataset sizes grow it becomes more challenging to interactively manipulate and analyze these datasets due to the large amounts of data that need to be moved and processed application independent caches such as operating system page caches and database buffer caches are present throughout the memory hierarchy to reduce data access times and alleviate transfer overheads we claim that an application aware cache with relatively modest memory requirements can effectively exploit dataset structure and application information to speed access to large datasets we demonstrate this idea in the context of system named the tree cache to reduce query latency to large octree datasets by an order of magnitude
mobile devices have become indispensable in daily life and hence how to take advantage of these portable and powerful facilities to share resources and information begins to emerge as an interesting problem in this paper we investigate the problem of information retrieval in mobile peer to peer network the prevailing approach to information retrieval is to apply flooding methods because of its quick response and easy maintenance obviously this kind of approach wastes huge amount of communication bandwidth which greatly affects the availability of the network and the battery power which significantly shortens the serving time of mobile devices in the network to tackle this problem we propose novel approach by mimicking different human behaviors of social networks which takes advantages of intelligence accuracy ia mechanism that evaluates the distance from node to certain resources in the network extensive experimental results show the efficiency and effectiveness of our approach as well as its scalability in volatile environment
search computing is novel discipline whose goal is to answer complex multi domain queries such queries typically require combining in their results domain knowledge extracted from multiple web resources therefore conventional crawling and indexing techniques which look at individual web pages are not adequate for them in this paper we sketch the main characteristics of search computing and we highlight how various classical computer science disciplines including software engineering web engineering service oriented architectures data management and human computing interaction are challenged by the search computing approach
the formation of secure transportation corridors where cargoes and shipments from points of entry can be dispatched safely to highly sensitive and secure locations is high national priority one of the key tasks of the program is the detection of anomalous cargo based on sensor readings in truck weigh stations due to the high variability dimensionality and or noise content of sensor data in transportation corridors appropriate feature representation is crucial to the success of anomaly detection methods in this domain in this paper we empirically investigate the usefulness of manifold embedding methods for feature representation in anomaly detection problems in the domain of transportation corridors we focus on both linear methods such as multi dimensional scaling mds as well as nonlinear methods such as locally linear embedding lle and isometric feature mapping isomap our study indicates that such embedding methods provide natural mechanism for keeping anomalous points away from the dense normal regions in the embedding of the data we illustrate the efficacy of manifold embedding methods for anomaly detection through experiments on simulated data as well as real truck data from weigh stations
advances in service oriented architecture soa have brought us close to the once imaginary vision of establishing and running virtual business business in which most or all of its business functions are outsourced to online services cloud computing offers realization of soa in which it resources are offered as services that are more affordable flexible and attractive to businesses in this paper we briefly study advances in cloud computing and discuss the benefits of using cloud services for businesses and trade offs that they have to consider we then present layered architecture for the virtual business and conceptual architecture for virtual business operating environment we discuss the opportunities and research challenges that are ahead of us in realizing the technical components of this conceptual architecture we conclude by giving the outlook and impact of cloud services on both large and small businesses
with advances in process technology soft errors se are becoming an increasingly critical design concern due to their large area and high density caches are worst hit by soft errors although error correction code based mechanisms protect the data in caches they have high performance and power overheads since multimedia applications are increasingly being used in mission critical embedded systems where both reliability and energy are major concern there is de nite need to improve reliability in embedded systems without too much energy overhead we observe that while soft error in multimedia data may only result in minor loss in qos soft error in avariable that controls the execution ow of the program may be fatal consequently we propose to partition the data space into failure critical and failure non critical data and provide high degree of soft error protection only to the failure critical data in horizontally partitioned caches experimental results demonstrate that our selective data protection can achieve the failure rate close to that of soft error protected cache system while retaining the performance and energy consumption similar to those of traditional cache system with some degradation in qos for example for conventional con guration as in intelxscale our approach achieves the same failure rate while improving performance by and reducing energy consumption by in comparison with soft error protected cache
when ranking texts retrieved for query semantics of each term in the texts is fundamental basis the semantics often depends on locality context neighboring terms of in the texts in this paper we present technique ctfatr that improves text rankers by encoding the term locality contexts to the assessment of term frequency tf of each term in the texts results of the tf assessment may be directly used to improve various kinds of text rankers without calling for any revisions to algorithms and development processes of the rankers moreover ctfatr is efficient to conduct the tf assessment online and neither training process nor training data is required empirical evaluation shows that ctfatr significantly improves various kinds of text rankers the contributions are of practical significance since many text rankers were developed and if they consider tf in ranking ctfatr may be used to enhance their performance without incurring any cost to them
this paper presents capriccio scalable thread package for use with high concurrency servers while recent work has advocated event based systems we believe that thread based systems can provide simpler programming model that achieves equivalent or superior performanceby implementing capriccio as user level thread package we have decoupled the thread package implementation from the underlying operating system as result we can take advantage of cooperative threading new asynchronous mechanisms and compiler support using this approach we are able to provide three key features scalability to threads efficient stack management and resource aware schedulingwe introduce linked stack management which minimizes the amount of wasted stack space by providing safe small and non contiguous stacks that can grow or shrink at run time compiler analysis makes our stack implementation efficient and sound we also present resource aware scheduling which allows thread scheduling and admission control to adapt to the system's current resource usage this technique uses blocking graph that is automatically derived from the application to describe the flow of control between blocking points in cooperative thread package we have applied our techniques to the apache web server demonstrating that we can achieve high performance and scalability despite using simple threaded programming model
we present novel framework for the query performance prediction task that is estimating the effectiveness of search performed in response to query in lack of relevance judgments our approach is based on using statistical decision theory for estimating the utility that document ranking provides with respect to an information need expressed by the query to address the uncertainty in inferring the information need we estimate utility by the expected similarity between the given ranking and those induced by relevance models the impact of relevance model is based on its presumed representativeness of the information need specific query performance predictors instantiated from the framework substantially outperform state of the art predictors over five trec corpora
existing sensor network architectures are based on the assumption that data will be polled therefore they are not adequate for long term battery powered use in applications that must sense or react to events that occur at unpredictable times in response and motivated by structural autonomous crack monitoring acm application from civil engineering that requires bursts of high resolution sampling in response to aperiodic vibrations in buildings and bridges we have designed implemented and evaluated lucid dreaming hardware software technique to dramatically decrease sensor node power consumption in this and other event driven sensing applications this work makes the following main contributions we have identified the key mismatches between existing polling based sensor network architectures and event driven applications we have proposed hardware software technique to permit the power efficient use of sensor networks in event driven applications we have analytically characterized the situations in which the proposed technique is appropriate and we have designed implemented and tested hardware software solution for standard crossbow motes that embodies the proposed technique in the building and bridge structural integrity monitoring application the proposed technique achieves the power consumption of existing sensor network architectures thereby dramatically increasing battery lifespan or permitting operation based on energy scavenging we believe that the proposed technique will yield similar benefits in wide range of applications printed circuit board specification files permitting reproduction of the current implementation are available for free use in research and education
content addressable storage cas system is valuable tool for building storage solutions providing efficiency by automatically detecting and eliminating duplicate blocks it can also be capable of high throughput at least for streaming access however the absence of standardized api is barrier to the use of cas for existing applications additionally applications would have to deal with the unique characteristics of cas such as immutability of blocks and high latency of operations an attractive alternative is to build file system on top of cas since applications can use its interface without modification mapping file system onto cas system efficiently so as to obtain high duplicate elimination and high throughput requires very different design than for traditional disk subsystem in this paper we present the design implementation and evaluation of hydrafs file system built on top of hydrastor scalable distributed content addressable block storage system hydrafs provides high performance reads and writes for streaming access achieving of the hydrastor throughput while maintaining high duplicate elimination
we present griffin hybrid storage device that uses hard disk drive hdd as write cache for solid state device ssd griffin is motivated by two observations first hdds can match the sequential write bandwidth of mid range ssds second both server and desktop workloads contain significant fraction of block overwrites by maintaining log structured hdd cache and migrating cached data periodically griffin reduces writes to the ssd while retaining its excellent performance we evaluate griffin using variety of traces from windows systems and show that it extends ssd lifetime by factor of two and reduces average latency by
as most applications in wireless sensor networks wsn are location sensitive in this paper we explore the problem of location aided multicast for wsn we present four strategies to construct the geomulticast routing tree namely sarf sam cofam and msam especially we discuss cofam in detail and give the algorithm for setting up multicast tree in cone based forwarding area this algorithm is distributed and energy efficient extensive simulations have been conducted to evaluate the performance of the proposed routing schemas simulation results have shown that when constructing multicast tree fewer messages must be transmitted in our schemas
on the basis of case study we demonstrate the usefulness of topology invariants for model driven systems development considering graph grammar semantics for relevant fragment of uml where graph represents an object diagram allows us to apply topology analysis particular abstract interpretation of graph grammars the outcome of this analysis is finite and concise over approximation of all possible reachable object diagrams the so called topology invariant we discuss how topology invariants can be used to verify that constraints on given model are respected by the behaviour and how they can be viewed as synthesised constraints providing insight into the dynamic behaviour of the model
as the scale and complexity of parallel systems continue to grow failures become more and more an inevitable fact for solving large scale applications in this research we present an analytical study to estimate execution time in the presence of failures of directed acyclic graph dag based scientific applications and provide guideline for performance optimization the study is four fold we first introduce performance model to predict individual subtask computation time under failures next layered iterative approach is adopted to transform dag into layered dag which reflects full dependencies among all the subtasks then the expected execution time under failures of the dag is derived based on stochastic analysis unlike existing models this newly proposed performance model provides both the variance and distribution it is practical and can be put to real use finally based on the model performance optimization weak point identification and enhancement are proposed intensive simulations with real system traces are conducted to verify the analytical findings they show that the newly proposed model and weak point enhancement mechanism work well
bounded treewidth and monadic second order mso logic have proved to be key concepts in establishing fixed para meter tractability results indeed by courcelle's theorem we know any property of finite structures which is expressible by an mso sentence can be decided in linear time data complexity if the structures have bounded treewidth in principle courcelle's theorem can be applied directly to construct concrete algorithms by transforming the mso evaluation problem into tree language recognition problem the latter can then be solved via finite tree automaton fta however this approach has turned out to be problematical since even relatively simple mso formulae may lead to state explosion of the fta in this work we propose monadic datalog ie data log where all intentional predicate symbols are unary as an alternative method to tackle this class of fixed parameter tractable problems we show that if some property of finite structures is expressible in mso then this property can also be expressed by means of monadic datalog program over the structure plus the treedecomposition moreover we show that the resulting fragment of datalogcan be evaluated in linear time both wrt the program size and wrt the data size this new approach is put to work by devising new algorithm for the primality problem ie testing if some attribute in relational schema is part of key we also report on experimental results with prototype implementation
in this article we present the findings of two ethnographic studies embedded into two broader projects on interactive television in the home environment based on previous research on the home context and inspired by ongoing trends around interactive television we explored basic concepts such as the extended home and new interaction techniques in particular those related to future developments of the remote control for the two studies we also developed two variations of the cultural probes method creative probing and playful probing this methodological approach proved to be appropriate for gathering in depth data on participants opinions attitudes and ideas in way favorable to the participants overall our results support existing research data on user media behavior and expectations and show trends in and beyond the living room concerned with personalization privacy and security as well as communication
in our prior work we presented highly effective local search based heuristic algorithm called the largest expanding sweep search less to solve the minimum energy broadcast meb problem over wireless ad hoc or sensor networks in this paper the performance is further strengthened by using iterated local optimization ilo techniques at the cost of additional computational complexity to the best of our knowledge this implementation constitutes currently the best performing algorithm among the known heuristics for meb we support this claim through extensive simulation study comparing with globally optimal solutions obtained by an integer programming ip solver for small network size up to nodes which is imposed by practical limitation of the ip solver the ilo based algorithm produces globally optimal solutions with very high frequency and average performance is within of the optimal solution
classification of head models based on their shape attributes for subsequent indexing and retrieval are important in many applications as in hierarchical content based retrieval of these head models for virtual scene composition and the automatic annotation of these characters in such scenes while simple feature representations are preferred for more efficient classification operations these features may not be adequate for distinguishing between the subtly different head model classes in view of these we propose an optimization approach based on genetic algorithm ga where the original model representation is transformed in such way that the classification rate is significantly enhanced while retaining the efficiency and simplicity of the original representation specifically based on the extended gaussian image egi representation for models which summarizes the surface normal orientation statistics we consider these orientations as random variables and proceed to search for an optimal transformation for these variables based on genetic optimization the resulting transformed distributions for these random variables are then used as the modified classifier inputs experiments have shown that the optimized transformation results in significant improvement in classification results for large variety of class structures more importantly the transformation can be indirectly realized by bin removal and bin count merging in the original histogram thus retaining the advantage of the original egi representation
many distributed applications have to meet their performance or quality of service goals in environments where available resources change contantly important classes of distributed applications including distributed multimedia codes applications for mobile devices and computational grid codes use runtime adaptation in order to achieve their goals the adaptation behavior in these applications is usually programmed in ad hoc code that is directly incorporated into the base application resulting in systems that are complex to develop maintain modify and debug furthermore it is virtually impossible to extract high level information about adaptive behaviour using program analysis even if there were compiler and runtime sytems that could exploit such information the goal of our research is to develop compiler and programming language support to simplify the development and improve the performance of adaptive distribtued applications we describe simple set of language extensions for adaptive distributed applications and discuss potential compiler techniqes to support such appliations we also propose task graph based framework that can be used to formalize the description of wide range of adaptation operations
shore scalable heterogeneous object repository is persistent object system under development at the university of wisconsin shore represents merger of object oriented database and file system technologies in this paper we give the goals and motivation for shore and describe how shore provides features of both technologies we also describe some novel aspects of the shore architecture including symmetric peer to peer server architecture server customization through an extensible value added server facility and support for scalability on multiprocessor systems an initial version of shore is already operational and we expect release of version in mid
in teaching operating systems at an undergraduate level we belive that it is important to provide project that is realistic enought to show how real operating systems work yet is simple enough that the students can understand and modify it in significant ways number of these instructional saystems have been created over the last two decades but recent advances in hardware and software design along with the increasing power of available computational resources have changed the basis for many of the tradeoffs made by these systems we have implemented an instructional operating system called nachos and designed series of assignments to go with it our system includes cpu and device simulatiors and it runs as regulat unix process nachos illustrates and takes advantage of modern operating systems technology such as threads and remote procedure calls recent harware advances such as risc's and the prevalence of memory hierarchies and modern software design techniques such as protocol layering and object oriented programming nachos has been used to teach undergraduate operating systems classes at several universities with positive results
the huge number of images on the web gives rise to the content based image retrieval cbir as the text based search techniques cannot cater to the needs of precisely retrieving web images however cbir comes with fundamental flaw the semantic gap between high level semantic concepts and low level visual features consequently relevance feedback is introduced into cbir to learn the subjective needs of users however in practical applications the limited number of user feedbacks is usually overwhelmed by the large number of dimensionalities of the visual feature space to address this issue novel semi supervised learning method for dimensionality reduction namely kernel maximum margin projection kmmp is proposed in this paper based on our previous work of maximum margin projection mmp unlike traditional dimensionality reduction algorithms such as principal component analysis pca and linear discriminant analysis lda which only see the global euclidean structure kmmp is designed for discovering the local manifold structure after projecting the images into lower dimensional subspace kmmp significantly improves the performance of image retrieval the experimental results on corel image database demonstrate the effectiveness of our proposed nonlinear algorithm
mobile users of computation and communication services have been rapidly adopting battery powered mobile handhelds such as pocketpcs and smartphones for their work however the limited battery lifetime of these devices restricts their portability and applicability and this weakness can be exacerbated by mobile malware targeting depletion of battery energy such malware are usually difficult to detect and prevent and frequent outbreaks of new malware variants also reduce the effectiveness of commonly seen signature based detection to alleviate these problems we propose power aware malware detection framework that monitors detects and analyzes previously unknown energy depletion threats the framework is composed of power monitor which collects power samples and builds power consumption history from the collected samples and data analyzer which generates power signature from the constructed history to generate power signature simple and effective noise filtering and data compression are applied thus reducing the detection overhead similarities between power signatures are measured by the distance reducing both false positive and false negative detection rates according to our experimental results on an hp ipaq running windows mobile os the proposed framework achieves significant up to storage savings without losing the detection accuracy and true positive rate in classifying mobile malware
we present an algorithm for the layered segmentation of video data in multiple views the approach is based on computing the parameters of layered representation of the scene in which each layer is modelled by its motion appearance and occupancy where occupancy describes probabilistically the layer's spatial extent and not simply its segmentation in particular view the problem is formulated as the map estimation of all layer parameters conditioned on those at the previous time step ie sequential estimation problem that is equivalent to tracking multiple objects in given number views expectation maximisation is used to establish layer posterior probabilities for both occupancy and visibility which are represented distinctly evidence from areas in each view which are described poorly under the model is used to propose new layers automatically since these potential new layers often occur at the fringes of images the algorithm is able to segment and track these in single view until such time as suitable candidate match is discovered in the other views the algorithm is shown to be very effective at segmenting and tracking non rigid objects and can cope with extreme occlusion we demonstrate an application of this representation to dynamic novel view synthesis
one of the critiques on program slicing is that slices presented to the user are hard to understand this is mainly related to the problem that slicing lsquo dumps rsquo the results onto the user without any explanation this work will present an approach that can be used to lsquo filter rsquo slices this approach basically introduces lsquo barriers rsquo which are not allowed to be passed during slice computation an earlier filtering approach is chopping which is also extended to obey such barrier the barrier variants of slicing and chopping provide filtering possibilities for smaller slices and better comprehensibility the concept of barriers is then applied to path conditions which provide necessary conditions under which an influence between the source and target criterion exists barriers make those conditions more precise
language supported synchronization is source of serious performance problems in many java programs even single threaded applications may spend up to half their time performing useless synchronization due to the thread safe nature of the java libraries we solve this performance problem with new algorithm that allows lock and unlock operations to be performed with only few machine instructions in the most common cases our locks only require partial word per object and were implemented without increasing object size we present measurements from our implementation in the jdk for aix demonstrating speedups of up to factor of in micro benchmarks and up to factor of in real programs
the task of linking databases is an important step in an increasing number of data mining projects because linked data can contain information that is not available otherwise or that would require time consuming and expensive collection of specific data the aim of linking is to match and aggregate all records that refer to the same entity one of the major challenges when linking large databases is the efficient and accurate classification of record pairs into matches and non matches while traditionally classification was based on manually set thresholds or on statistical procedures many of the more recently developed classification methods are based on supervised learning techniques they therefore require training data which is often not available in real world situations or has to be prepared manually an expensive cumbersome and time consuming process the author has previously presented novel two step approach to automatic record pair classification in the first step of this approach training examples of high quality are automatically selected from the compared record pairs and used in the second step to train support vector machine svm classifier initial experiments showed the feasibility of the approach achieving results that outperformed means clustering in this paper two variations of this approach are presented the first is based on nearest neighbour classifier while the second improves svm classifier by iteratively adding more examples into the training sets experimental results show that this two step approach can achieve better classification results than other unsupervised approaches
the web of data has emerged as way of exposing structured linked data on the web it builds on the central building blocks of the web uris http and benefits from its simplicity and wide spread adoption it does however also inherit the unresolved issues such as the broken link problem broken links constitute major challenge for actors consuming linked data as they require them to deal with reduced accessibility of data we believe that the broken link problem is major threat to the whole web of data idea and that both linked data consumers and providers will require solutions that deal with this problem since no general solutions for fixing such links in the web of data have emerged we make three contributions into this direction first we provide concise definition of the broken link problem and comprehensive analysis of existing approaches second we present dsnotify generic framework able to assist human and machine actors in fixing broken links it uses heuristic feature comparison and employs time interval based blocking technique for the underlying instance matching problem third we derived benchmark datasets from knowledge bases such as dbpedia and evaluated the effectiveness of our approach with respect to the broken link problem our results show the feasibility of time interval based blocking approach for systems that aim at detecting and fixing broken links in the web of data
as xml has become an emerging standard for information exchange on the world wide web it has gained great attention among database communities with respect to extraction of information from xml which is considered as database model xml queries enable users to issue many kinds of complex queries using regular path expressions however they usually require large search space during query processing so the problem of xml query processing has received significant attention this paper surveys the state of the art on the problem of xml query evaluation we consider the problem in three dimensions xml instance storage xml query languages and xml views and xml query language processing we describe the problem definition algorithms proposed to solve it and the relevant research issues
effective load distribution is of great importance at grids which are complex heterogeneous distributed systems in this paper we study site allocation scheduling of nonclairvoyant jobs in level heterogeneous grid architecture three scheduling policies at grid level which utilize site load information are examined the aim is the reduction of site load information traffic while at the same time mean response time of jobs and fairness in utilization between the heterogeneous sites are of great interest simulation model is used to evaluate performance under various conditions simulation results show that considerable decrement in site load information traffic and utilization fairness can be achieved at the expense of slight increase in response time
traditional coherence protocols present set of difficult tradeoffs the reliance of snoopy protocols on broadcast and ordered interconnects limits their scalability while directory protocols incur performance penalty on sharing misses due to indirection this work introduces patch predictive adaptive token counting hybrid coherence protocol that provides the scalability of directory protocols while opportunistically sending direct requests to reduce sharing latency patch extends standard directory protocol to track tokens and use token counting rules for enforcing coherence permissions token counting allows patch to support direct requests on an unordered interconnect while mechanism called token tenure uses local processor timeouts and the directorys per block point of ordering at the home node to guarantee forward progress without relying on broadcast patch makes three main contributions first patch introduces token tenure which provides broadcast free forward progress for token counting protocols second patch deprioritizes best effort direct requests to match or exceed the performance of directory protocols without restricting scalability finally patch provides greater scalability than directory protocols when using inexact encodings of sharers because only processors holding tokens need to acknowledge requests overall patch is one size fits all coherence protocol that dynamically adapts to work well for small systems large systems and anywhere in between
this paper describes new software based registration and fusion of visible and thermal infrared ir image data for face recognition in challenging operating environments that involve illumination variations the combined use of visible and thermal ir imaging sensors offers viable means for improving the performance of face recognition techniques based on single imaging modality despite successes in indoor access control applications imaging in the visible spectrum demonstrates difficulties in recognizing the faces in varying illumination conditions thermal ir sensors measure energy radiations from the object which is less sensitive to illumination changes and are even operable in darkness however thermal images do not provide high resolution data data fusion of visible and thermal images can produce face images robust to illumination variations however thermal face images with eyeglasses may fail to provide useful information around the eyes since glass blocks large portion of thermal energy in this paper eyeglass regions are detected using an ellipse fitting method and replaced with eye template patterns to preserve the details useful for face recognition in the fused image software registration of images replaces special purpose imaging sensor assembly and produces co registered image pairs at reasonable cost for large scale deployment face recognition techniques using visible thermal ir and data fused visible thermal images are compared using commercial face recognition software faceit and two visible thermal face image databases the nist equinox and the utk iris databases the proposed multiscale data fusion technique improved the recognition accuracy under wide range of illumination changes experimental results showed that the eyeglass replacement increased the number of correct first match subjects by nist equinox and utk iris
we introduce feature based method to detect unusual patterns the property of normality allows us to devise framework to quickly prune the normal observations observations that can not be combined into any significant pattern are considered unusual rules that are learned from the dataset are used to construct the patterns for which we compute score function to measure the interestingness of the unusual patterns experiments using the kdd cup dataset show that our approach can discover most of the attack patterns those attacks are in the top set of unusual patterns and have higher score than the patterns of normal connections the experiments also show that the algorithm can run very fast
numerous qos routing strategies focus on end to end delays to provide time constrained routing protocols in wireless sensor networks wsns with the arrival of wireless multimedia sensor networks traffic can be composed of time sensitive packets and reliability demanding packets in such situations some works also take into account link reliability to provide probabilistic qos the trade off between the guarantee of the qos requirements and the network lifetime remains an open issue especially in large scale wsns this paper proposes promising multipath qos routing protocol based on separation of the nodes into two sub networks the first part includes specific nodes that are occasionally involved in routing decisions while the remaining nodes in the second sub network fully take part in them the qos routing is formulated as an optimization problem that aims to extend the network lifetime subject to qos constraints using the percolation theory routing algorithm is designed to solve the problem on the respective sub networks simulation results show the efficiency of this novel approach in terms of average end to end delays on time packet delivery ratio and network lifetime
to resolve some of lexical disagreement problems between queries and faqs we propose reliable faq retrieval system using query log clustering on indexing time the proposed system clusters the logs of users queries into predefined faq categories to increase the precision and the recall rate of clustering the proposed system adopts new similarity measure using machine readable dictionary on searching time the proposed system calculates the similarities between users queries and each cluster in order to smooth faqs by virtue of the cluster based retrieval technique the proposed system could partially bridge lexical chasms between queries and faqs in addition the proposed system outperforms the traditional information retrieval systems in faq retrieval
abstract we present new method for converting photo or image to synthesized painting following the painting style of an example painting treating painting styles of brush strokes as sample textures we reduce the problem of learning an example painting to texture synthesis problem the proposed method uses hierarchical patch based approach to the synthesis of directional textures the key features of our method are painting styles are represented as one or more blocks of sample textures selected by the user from the example painting image segmentation and brush stroke directions defined by the medial axis are used to better represent and communicate shapes and objects present in the synthesized painting image masks and hierarchy of texture patches are used to efficiently synthesize high quality directional textures the synthesis process is further accelerated through texture direction quantization and the use of gaussian pyramids our method has the following advantages first the synthesized stroke textures can follow direction field determined by the shapes of regions to be painted second the method is very efficient the generation time of synthesized painting ranges from few seconds to about one minute rather than hours as required by other existing methods on commodity pc furthermore the technique presented here provides new and efficient solution to the problem of synthesizing directional texture we use number of test examples to demonstrate the efficiency of the proposed method and the high quality of results produced by the method
for efficient image retrieval the image database should be processed to extract representing feature vector for each member image in the database reliable and robust statistical image indexing technique based on stochastic model of an image color content has been developed based on the developed stochastic model compact dimensional feature vector was defined to tag images in the database system the entries of the defined feature vector are the mean variance and skewness of the image color histogram distributions as well as correlation factors between color components of the rgb color space it was shown using statistical analysis that the feature vector provides sufficient knowledge about the histogram distribution the reliability and robustness of the proposed technique against common intensity artifacts and noise was validated through several experiments conducted for that purpose the proposed technique outperforms traditional and other histogram based techniques in terms of feature vector size and properties as well as performance
release consistency is widely accepted memory model for distributed shared memory systems eager release consistency represents the state of the art in release consistent protocols for hardware coherent multiprocessors while lazy release consistency has been shown to provide better performance for software distributed shared memory dsm several of the optimizations performed by lazy protocols have the potential to improve the performance of hardware coherent multiprocessors as well but their complexity has precluded hardware implementation with the advent of programmable protocol processors it may become possible to use them after all we present and evaluate lazy release consistent protocol suitable for machines with dedicated protocol processors this protocol admits multiple concurrent writers sends write notices concurrently with computation and delays invalidations until acquire operations we also consider lazier protocol that delays sending write notices until release operations our results indicate that the first protocol outperforms eager release consistency by as much as across variety of applications the lazier protocol on the other hand is unable to recoup its high synchronization overhead this represents qualitative shift from the dsm world where lazier protocols always yield performance improvements based on our results we conclude that machines with flexible hardware support for coherence should use protocols based on lazy release consistency but in less aggressively lazy form than is appropriate for dsm
in order to establish consolidated standards in novel data mining areas newly proposed algorithms need to be evaluated thoroughly many publications compare new proposition if at all with one or two competitors or even with so called na√Øve ad hoc solution for the prolific field of subspace clustering we propose software framework implementing many prominent algorithms and thus allowing for fair and thorough evaluation furthermore we describe how new algorithms for new applications can be incorporated in the framework easily
distributed coalition supports distributed mandatory access controls for resources whose security policies differ for each group of components over nodes and provides secure information operations and exchanges with nodes that handle information over which conflicts of interest may occur many projects have proposed distributed coalitions using virtual machine monitor but this approach for strong confinement tends to hinder successful deployments in real world scenarios that involve complicated operations and management for applications because such access control is coarse grained for the resources in this paper we propose chinese wall process confinement cwpc for practical application level distributed coalitions that provide fine grained access controls for resources and that emphasize minimizing the impact on the usability using program transparent reference monitor we implemented prototype system named aldc for standard office applications on microsoft windows that are used on daily basis for business purposes and that may involve conflicts of interests evaluated its performance and influence on usability and show that our approach is practical
dynamic software optimization methods are becoming increasingly popular for improving software performance and power the first step in dynamic optimization consists of detecting frequently executed code or critical regions most previous critical region detectors have been targeted to desktop processors we introduce critical region detector targeted to embedded processors with the unique features of being very size and power efficient and being completely nonintrusive to the software's execution features needed in timing sensitive embedded systems our detector not only finds the critical regions but also determines their relative frequencies potentially important feature for selecting among alternative dynamic optimization methods our detector uses tiny cache like structure coupled with small amount of logic we provide results of extensive explorations across embedded system benchmarks we show that highly accurate results can be achieved with only percent power overhead acceptable size overhead and zero runtime overhead our detector is currently being used as part of dynamic hardware software partitioning approach but is applicable to wide variety of situations
web applications routinely handle sensitive data and many people rely on them to support various daily activities so errors can have severe and broad reaching consequences unlike most desktop applications many web applications are written in scripting languages such as php the dynamic features commonly supported by these languages significantly inhibit static analysis and existing static analysis of these languages can fail to produce meaningful results on realworld web applications automated test input generation using the concolic testing framework has proven useful for finding bugs and improving test coverage on and java programs which generally emphasize numeric values and pointer based data structures however scripting languages such as php promote style of programming for developing web applications that emphasizes string values objects and arrays in this paper we propose an automated input test generation algorithm that uses runtime values to analyze dynamic code models the semantics of string operations and handles operations whose argument and return values may not share common type as in the standard concolic testing framework our algorithm gathers constraints during symbolic execution our algorithm resolves constraints over multiple types by considering each variable instance individually so that it only needs to invert each operation by recording constraints selectively our implementation successfully finds bugs in real world web applications which state of the art static analysis tools fail to analyze
understanding large grid platform configurations and generating representative synthetic configurations is critical for grid computing research this paper presents an analysis of existing resource configurations and proposes grid platform generator that synthesizes realistic configurations of both computing and communication resources our key contributions include the development of statistical models for currently deployed resources and using these estimates for modeling the characteristics of future systems through the analysis of the configurations of clusters and over processors we identify appropriate distributions for resource configuration parameters in many typical clusters using well established statistical tests we validate our models against second resource collection of clusters and over processors and show that our models effectively capture the resource characteristics found in real world resource infrastructures these models are realized in resource generator which can be easily recalibrated by running it on training sample set
mobile communication devices may be used for spreading multimedia data without support of an infrastructure such scheme where the data is carried by people walking around and relayed from device to device by means of short range radio could potentially form public content distribution system that spans vast urban areas the transport mechanism is the flow of people and it can be studied but not engineered the question addressed in this paper is how well pedestrian content distribution may work we answer this question by modeling the mobility of people moving around in city constrained by given topology our contributions are both the queuing analytic model that captures the flow of people and the results on the feasibility of pedestrian content distribution furthermore we discuss possible extensions to the mobility model to capture speed distance relations that emerge in dense crowds
the past few years have witnessed an significant interest in probabilistic logic learning ie in research lying at the intersection of probabilistic reasoning logical representations and machine learning rich variety of different formalisms and learning techniques have been developed this paper provides an introductory survey and overview of the state of the art in probabilistic logic learning through the identification of number of important probabilistic logical and learning concepts
one may wish to use computer graphic images to carry out road visibility studies unfortunately most display devices still have limited luminance dynamic range especially in driving simulators in this paper we propose tone mapping operator tmo to compress the luminance dynamic range while preserving the driver's performance for visual task relevant for driving situation we address three display issues of some consequences for road image display luminance dynamics image quantization and high minimum displayable luminance our tmo characterizes the effects of local adaptation with bandpass decomposition of the image using laplacian pyramid and processes the levels separately in order to mimic the human visual system the contrast perception model uses the visibility level usual index in road visibility engineering applications to assess our algorithm psychophysical experiment devoted to target detection task was designed using landolt ring the visual performances of observers were measured they stared first at high dynamic range image and then at the same image processed by tmo and displayed on low dynamic range monitor for comparison the evaluation was completed with visual appearance evaluation our operator gives good performances for three typical road situations one in daylight and two at night after comparison with four standard tmos from the literature the psychovisual assessment of our tmo is limited to these driving situations
many mobile phones integrate services such as personal calendars given the social nature of the stored data however users often need to access such information as part of phone conversation in typical non headset use this re quires users to interrupt their conversations to look at the screen we investigate counter intuitive solution to avoid the need for interruption we replace the visual interface with one based on auditory feedback surprisingly this can be done without interfering with the phone conversation we present blindsight prototype application that replaces the traditionally visual in call menu of mobile phone users interact using the phone keypad without looking at the screen blindsight responds with auditory feedback this feedback is heard only by the user not by the person on the other end of the line we present the results of two user studies of our prototype the first study verifies that useful keypress accuracy can be obtained for the phone at ear position the second study compares the blindsight system against visual baseline condition and finds preference for blindsight
whenever an array element is accessed java virtual machines execute compare instruction to ensure that the index value is within the valid bounds this reduces the execution speed of java programs array bounds check elimination identifies situations in which such checks are redundant and can be removed we present an array bounds check elimination algorithm for the java hotspot tm vm based on static analysis in the just in time compiler the algorithm works on an intermediate representation in static single assignment form and maintains conditions for index expressions it fully removes bounds checks if it can be proven that they never fail whenever possible it moves bounds checks out of loops the static number of checks remains the same but check inside loop is likely to be executed more often if such check fails the executing program falls back to interpreted mode avoiding the problem that an exception is thrown at the wrong place the evaluation shows speedup near to the theoretical maximum for the scientific scimark benchmark suite and also significant improvements for some java grande benchmarks the algorithm slightly increases the execution speed for the specjvm benchmark suite the evaluation of the dacapo benchmarks shows that array bounds checks do not have significant impact on the performance of object oriented applications
with the increasing use of research paper search engines such as citeseer for both literature search and hiring decisions the accuracy of such systems is of paramount importance this article employs conditional random fields crfs for the task of extracting various common fields from the headers and citation of research papers crfs provide principled way for incorporating various local features external lexicon features and globle layout features the basic theory of crfs is becoming well understood but best practices for applying them to real world data requires additional exploration we make an empirical exploration of several factors including variations on gaussian laplace and hyperbolic priors for improved regularization and several classes of features based on crfs we further present novel approach for constraint co reference information extraction ie improving extraction performance given that we know some citations refer to the same publication on standard benchmark dataset we achieve new state of the art performance reducing error in average by and word error rate by in comparison with the previous best svm results accuracy compares even more favorably against hmms on four co reference ie datasets our system significantly improves extraction performance with an error rate reduction of
software reuse is regarded is as key software development objective leading to reduction of costs associated with software development and maintenance however there is mounting evidence that software reuse is difficult to achieve in practice and that software development approaches such as component based development and more recently service oriented computing have failed to achieve anticipated levels of reuse in this paper we identify the determinants of service reusability and argue that the design of services plays an important role in achieving high levels of reuse we examine the relationship between service granularity and reuse and note that extensive use of coarse grained document centric services by soa practitioners makes achieving reuse particularly challenging
detection of malicious software malware using machine learning methods has been explored extensively to enable fast detection of new released malware the performance of these classifiers depends on the induction algorithms being used in order to benefit from multiple different classifiers and exploit their strengths we suggest using an ensemble method that will combine the results of the individual classifiers into one final result to achieve overall higher detection accuracy in this paper we evaluate several combining methods using five different base inducers decision tree naive bayes knn vfi and oner on five malware datasets the main goal is to find the best combining method for the task of detecting malicious files in terms of accuracy auc and execution time
this article addresses the problem of recognizing the behavior of person suffering from alzheimer's disease at early intermediate stages we present keyhole plan recognition model based on lattice theory and action description logic which transforms the recognition problem into classification issue this approach allows us to formalize the plausible incoherent intentions of the patient resulting from the symptoms of his cognitive impairment such as disorientation memory lapse etc an implementation of this model was tested in our smart home laboratory by simulating set of real case scenarios
in this paper we present novel approach to three dimensional human motion estimation from monocular video data we employ particle filter to perform the motion estimation the novelty of the method lies in the choice of state space for the particle filter using non linear inverse kinematics solver allows us to perform the filtering in end effector space this effectively reduces the dimensionality of the state space while still allowing for the estimation of large set of motions preliminary experiments with the strategy show good results compared to full pose tracker
xml is one of the primary encoding schemes for data and knowledge we investigate incremental physical data clustering in systems that store xml documents using native format we formulate the xml clustering problem as an augmented with sibling edges tree partitioning problem and propose the pixsar practical incremental xml sibling augmented reclustering algorithm for incrementally clustering xml documents we show the fundamental importance of workload driven dynamically rearranging storage pixsar incrementally executes reclustering operations on selected subgraphs of the global augmented document tree the subgraphs are implied by significant changes in the workload as the workload changes pixsar incrementally djusts the xml data layout so as to better fit the workload pixsar's main parameters are the radius in pages of the augmented portion to be reclustered and the way reclustering is triggered we briefly explore some of the effects of indexes full treatment of indexes is the subject of another paper we use an experimental data clustering system that includes fast disk simulator and file system simulator for storing native xml data we use novel method for exporting the saxon query processor into our setting experimental results indicate that using pixsar significantly reduces the number of page faults counting all page faults incurred while querying the document as well as maintenance operations thereby resulting in improved query performance
this article presents new method to model fast volume preservation of mass spring system to achieve realistic and efficient deformable object animation without using internal volumetric meshing with this method the simulated behavior is comparable to finite element method based model at fraction of the computational cost
in recent years the growth of the internet has facilitated the rapid emergence of online communities in this paper we survey key research issues on online communities from the perspectives of both social science and computing technologies we also sample several major online community applications and propose some directions for future research
we present novel approach to interdomain traffic engineering based on the concepts of nash bargaining and dual decomposition under this scheme isps use an iterative procedure to jointly optimize social cost function referred to as the nash product we show that the global optimization problem can be separated into subproblems by introducing appropriate shadow prices on the interdomain flows these subproblems can then be solved independently and in decentralized manner by the individual isps our approach does not require the isps to share any sensitive internal information such as network topology or link weights more importantly our approach is provably pareto efficient and fair therefore we believe that our approach is highly amenable to adoption by isps when compared to past approaches we also conduct simulation studies of our approach over several real isp topologies our evaluation shows that the approach converges quickly offers equitable performance improvements to isps is significantly better than unilateral approaches eg hot potato routing and offers the same performance as centralized solution with full knowledge
we propose deterministic fault tolerant and deadlock free routing protocol in two dimensional meshes based on dimension order routing and the odd even turn model the proposed protocol called extended routing does not use any virtual channels by prohibiting certain locations of faults and destinations faults are contained in set of disjointed rectangular regions called faulty blocks the number of faults to be tolerated is unbounded as long as nodes outside faulty blocks are connected in the mesh network the extended routing can also be used under special convex fault region called an orthogonal faulty block which can be derived from given faulty block by activating some nonfaulty nodes in the block extensions to partially adaptive routing traffic and adaptivity balanced using virtual networks and routing without constraints using virtual channels and virtual networks are also discussed
java application servers are gaining popularity as way for businesses to conduct day to day operations while strong emphasis has been placed on how to obtain peak performance only few research efforts have focused on these servers ability to sustain top performance in spite of the ever changing demands from users as preliminary study we conducted an experiment to observe the throughput degradation behavior of widely used java application server running standardized benchmark and found that throughput performance degrades ungracefully thus the goal of this work is three fold to identify the primary factors that cause poor throughput degradation ii to investigate how these factors affect throughput degradation and iii to observe how changes in algorithms and policies governing these factors affect throughput degradation
formal semantics for xquery with side effects have been proposed in we propose different semantics which is better suited for database compilation we substantiate this claim by formalizing the compilation of xquery extended with updates into database algebra we prove the correctness of the proposed compilation by mapping both the source language and the algebra to common core language with list comprehensions and extensible tuples
in this paper we describe an investigation into the requirements for and the use of in situ authoring in the creation of location based pervasive and ubicomp experiences we will focus on the co design process with users that resulted in novel visitor experience to historic country estate this has informed the design of new in situ authoring tools supplemented with tools for retrospective revisiting and reorganization of content an initial trial of these new tools will be discussed and conclusions drawn as to the appropriateness of such tools further enhancements as part of future trials will also be described
abstract we consider data clustering problems where partial grouping is known priori we formulate such biased grouping problems as constrained optimization problem where structural properties of the data define the goodness of grouping and partial grouping cues define the feasibility of grouping we enforce grouping smoothness and fairness on labeled data points so that sparse partial grouping information can be effectively propagated to the unlabeled data considering the normalized cuts criterion in particular our formulation leads to constrained eigenvalue problem by generalizing the rayleigh ritz theorem to projected matrices we find the global optimum in the relaxed continuous domain by eigendecomposition from which near global optimum to the discrete labeling problem can be obtained effectively we apply our method to real image segmentation problems where partial grouping priors can often be derived based on crude spatial attentional map that binds places with common salient features or focuses on expected object locations we demonstrate not only that it is possible to integrate both image structures and priors in single grouping process but also that objects can be segregated from the background without specific object knowledge
computing world distances of scene features from the captured images is common task in image analysis and scene understanding projective geometry based methods focus on measuring distance from one single image the scope of measurable scene is limited by the field of view fov of one single camera with full view panorama the scope of measurable scene is no longer limited by fov however the scope of measurable scene is limited by the fixed capture location of single panorama in this paper we propose one method of measuring distances of line segments in real world scene using panoramic video representation the scope of measurable scene is largely extended without the limitation of fov and fixed capture location prototype system called pv measure is developed to allow user to interactively measure the distances of line segments in panoramic video experiment results verify that the method offers good accuracy
cost based xml query optimization calls for accurate estimation of the selectivity of path expressions some other interactive and internet applications can also benefit from such estimations while there are number of estimation techniques proposed in the literature almost none of them has any guarantee on the estimation accuracy within given space limit in addition most of them assume that the xml data are more or less static ie with few updates in this paper we present framework for xml path selectivity estimation in dynamic context specifically we propose novel data structure bloom histogram to approximate xml path frequency distribution within small space budget and to estimate the path selectivity accurately with the bloom histogram we obtain the upper bound of its estimation error and discuss the trade offs between the accuracy and the space limit to support updates of bloom histograms efficiently when underlying xml data change dynamic summary layer is used to keep exact or more detailed xml path information we demonstrate through our extensive experiments that the new solution can achieve significantly higher accuracy with an even smaller space than the previous methods in both static and dynamic environments
we formulate new approach for evaluating prefetching algorithm we first carry out profiling run of program to identify all of the misses and corresponding locations in the program where prefetches for the misses can be initiated we then systematically control the number of misses that are prefetched the timeliness of these prefetches and the number of unused prefetches we validate the accuracy of our approach by comparing it to one based on markov prefetch algorithm this allows us to measure the potential benefit that any application can receive from prefetching and to analyze application behavior under conditions that cannot be explored with any known prefetching algorithm next we analyze system parameter that is vital to prefetching performance the line transfer interval which is the number of processor cycles required to transfer cache line this interval is determined by technology and bandwidth we show that under ideal conditions prefetching can remove nearly all of the stalls associated with cache misses unfortunately real processor implementations are less than ideal in particular the trend in processor frequency is outrunning on chip and off chip bandwidths today it is not uncommon for processor frequency to be three or four times bus frequency under these conditions we show that nearly all of the performance benefits derived from prefetching are eroded and in many cases prefetching actually degrades performance we carry out quantitative and qualitative analyses of these tradeoffs and show that there is linear relationship between overall performance and three metrics percentage of misses prefetched percentage of unused prefetches and bandwidth
we discuss general techniques centered around the layerwise separation property lsp of planar graph problem that allow to develop algorithms with running time given an instance of problem on planar graphs with parameter problems having lsp include planar vertex cover planar independent set and planar dominating set extensions of our speed up technique to basically all fixed parameter tractable planar graph problems are also exhibited moreover we relate eg the domination number or the vertex cover number with the treewidth of plane graph
we discuss alternative heap architectures for languages that rely on automatic memory management and implement concurrency through asynchronous message passing we describe how interprocess communication and garbage collection happens in each architecture and extensively discuss the tradeoffs that are involved in an implementation setting the erlang otp system where the rest of the runtime system is unchanged we present detailed experimental comparison between these architectures using both synthetic programs and large commercial products as benchmarks
user interface dui design and development requires practitioners designers and developers to represent their ideas in representations designed for machine execution rather than natural representations hampering development of effective duis as such concept oriented design cod was created as theory of software development for both natural and executable design and development instantiated in the toolkit chasm chasm is natural tiered executable user interface description language uidl for duis resulting in improved understandability as well as reduced complexity and reuse chasm's utility is shown through evaluations by domain experts case studies of long term use and an analysis of spaces
in the recent past the recognition and localization of objects based on local point features has become widely accepted and utilized method among the most popular features are currently the sift features the more recent surf features and region based features such as the mser for time critical application of object recognition and localization systems operating on such features the sift features are too slow ms for images of size on ghz cpu the faster surf achieve computation time of ms which is still too slow for active tracking of objects or visual servoing applications in this paper we present combination of the harris corner detector and the sift descriptor which computes features with high repeatability and very good matching properties within approx ms while just computing the sift descriptors for computed harris interest points would lead to an approach that is not scale invariant we will show how scale invariance can be achieved without time consuming scale space analysis furthermore we will present results of successful application of the proposed features within our system for recognition and localization of textured objects an extensive experimental evaluation proves the practical applicability of our approach
this paper presents novel algorithm for computing graph edit distance ged in image categorization this algorithm is purely structural ie it needs only connectivity structure of the graph and does not draw on node or edge attributes there are two major contributions introducing edge direction histogram edh to characterize shape features of images it is shown that ged can be employed as distance of edhs this algorithm is completely independent on cost function which is difficult to be defined exactly computing distance of edhs with earth mover distance emd which takes neighborhood bins into account so as to compute distance of edhs correctly set of experiments demonstrate that the newly presented algorithm is available for classifying and clustering images and is immune to the planar rotation of images compared with ged from spectral seriation our algorithm can capture the structure change of graphs better and consume time used by the former one the average classification rate is and average clustering rate is higher than the spectral seriation method
type system with linearity is useful for checking software protocols andresource management at compile time linearity provides powerful reasoning about state changes but at the price of restrictions on aliasing the hard division between linear and nonlinear types forces the programmer to make trade off between checking protocol on an object and aliasing the object most onerous is the restriction that any type with linear component must itself be linear because of this checking protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object we propose new type system that reduces these restrictions with the adoption and focus constructs adoption safely allows programmer to alias objects on which she is checking protocols and focus allows the reverse programmer can alias data structures that point to linear objects and use focus for safe access to those objects we discuss how we implemented these ideas in the vault programming language
in information retrieval cluster based retrieval is well known attempt in resolving the problem of term mismatch clustering requires similarity information between the documents which is difficult to calculate at feasible time the adaptive document clustering scheme has been investigated by researchers to resolve this problem however its theoretical viewpoint has not been fully discovered in this regard we provide conceptual viewpoint of the adaptive document clustering based on query based similarities by regarding the user's query as concept as result adaptive document clustering scheme can be viewed as an approximation of this similarity based on this idea we derive three new query based similarity measures in language modeling framework and evaluate them in the context of cluster based retrieval comparing with means clustering and full document expansion evaluation result shows that retrievals based on query based similarities significantly improve the baseline while being comparable to other methods this implies that the newly developed query based similarities become feasible criterions for adaptive document clustering
past studies have shown that objects are created and then die in phases thus one way to sustain good garbage collection efficiency is to have large enough heap to allow many allocation phases to complete and most of the objects to die before invoking garbage collection however such an operating environment is hard to maintain in large multithreaded applications because most typical time sharing schedulers are not allocation phase cognizant ie they often schedule threads in way that prevents them from completing their allocation phases quickly thus when garbage collection is invoked most allocation phases have yet to be completed resulting in poor collection efficiency we introduce two new scheduling strategies larf lower allocation rate first and mqrr memory quantum round robin designed to be allocation phase aware by assigning higher execution priority to threads in computation oriented phases the simulation results show thatallthe reductions of the garbage collection time in generational collector can range from when compare to round robin scheduler the reductions of the overall execution time and the average thread turnaround time range from and respectively
in this paper we propose an improved method of an efficient secure access control labeling under dynamic xml data streams environment the proposed method enables an efficient secure real time processing of query in mobile terminal and supports insertion of new nodes at arbitrary positions in the xml tree without re labeling and without conflicting although some researches have been done to maintain the document order in updating the main drawbacks in the most of these works are if the deletion and or insertion occur regularly then expensive re computing of affected labels is needed therefore we focus on how to design an efficient secure labeling scheme for xml trees which are frequently updated under dynamic xml data streams
fundamental to data cleaning is the need to account for multiple data representations we propose formal framework that can be used to reason about and manipulate data representations the framework is declarative and combines elements of generative grammar with database querying it also incorporates actions in the spirit of programming language compilers this framework has multiple applications such as parsing and data normalization data normalization is interesting in its own right in preparing data for analysis as well as in pre processing data for further cleansing we empirically study the utility of the framework over several real world data cleaning scenarios and find that with the right normalization often the need for further cleansing is minimized
customers purchase behavior may vary over time traditional collaborative filtering cf methods make recommendations to target customer based on the purchase behavior of customers whose preferences are similar to those of the target customer however the methods do not consider how the customers purchase behavior may vary over time in contrast the sequential rule based recommendation method analyzes customers purchase behavior over time to extract sequential rules in the form purchase behavior in previous periods purchase behavior in the current period if target customer's purchase behavior history is similar to the conditional part of the rule then his her purchase behavior in the current period is deemed to be the consequent part of the rule although the sequential rule method considers the sequence of customers purchase behavior over time it does not utilize the target customer's purchase data for the current period to resolve the above problems this work proposes novel hybrid recommendation method that combines the segmentation based sequential rule method with the segmentation based knn cf method the proposed method uses customers rfm recency frequency and monetary values to cluster customers into groups with similar rfm values for each group of customers sequential rules are extracted from the purchase sequences of that group to make recommendations meanwhile the segmentation based knn cf method provides recommendations based on the target customer's purchase data for the current period then the results of the two methods are combined to make final recommendations experiment results show that the hybrid method outperforms traditional cf methods
this paper defines the basic notions of local and non local tasks and determines the minimum information about failures that is necessary to solve any non local task in message passing systems it also introduces natural weakening of the well known set agreement task and show that in some precise sense it is the weakest non local task in message passing systems
in several applications such as databases planning and sensor networks parameters such as selectivity load or sensed values are known only with some associated uncertainty the performance of such system as captured by some objective function over the parameters is significantly improved if some of these parameters can be probed or observed in resource constrained situation deciding which parameters to observe in order to optimize system performance itself becomes an interesting and important optimization problem this problem is the focus of this paper unfortunately designing optimal observation schemes is np hard even for the simplest objective functions leading to the study of approximation algorithms in this paper we present general techniques for designing non adaptive probing algorithms which are at most constant factor worse than optimal adaptive probing schemes interestingly this shows that for several problems of interest while probing yields significant improvement in the objective function being adaptive about the probing is not beneficial beyond constant factors
reps formerly called dsls are multiscale medial means for modeling and rendering solid geometry they are particularly well suited to model anatomic objects and in particular to capture prior geometric information effectively in deformable models segmentation approaches the representation is based on figural models which define objects at coarse scale by hierarchy of figures mdash each figure generally slab representing solid region and its boundary simultaneously this paper focuses on the use of single figure models to segment objects of relatively simple structurea single figure is sheet of medial atoms which is interpolated from the model formed by net ie mesh or chain of medial atoms hence the name reps each atom modeling solid region via not only position and width but also local figural frame giving figural directions and an object angle between opposing corresponding positions on the boundary implied by the rep the special capability of an rep is to provide spatial and orientational correspondence between an object in two different states of deformation this ability is central to effective measurement of both geometric typicality and geometry to image match the two terms of the objective function optimized in segmentation by deformable models the other ability of reps central to effective segmentation is their ability to support segmentation at multiple levels of scale with successively finer precision objects modeled by single figures are segmented first by similarity transform augmented by object elongation then by adjustment of each medial atom and finally by displacing dense sampling of the rep implied boundary while these models and approaches also exist in we focus on objectsthe segmentation of the kidney from ct and the hippocampus from mri serve as the major examples in this paper the accuracy of segmentation as compared to manual slice by slice segmentation is reported
in this paper new strategy is proposed to defend against colluding malicious nodes in sensor network the strategy is based on new relaxation labelling algorithm to classify nodes into benign or malicious ones only reports from benign nodes can then be used to perform localisation and obtain accurate results experimental results based on simulations and field experiments illustrate the performance of the algorithm
with the explosive growth of web and the recent development in digital media technology the number of images on the web has grown tremendously consequently web image clustering has emerged as an important application some of the initial efforts along this direction revolved around clustering web images based on the visual features of images or textual features by making use of the text surrounding the images however not much work has been done in using multimodal information for clustering web images in this paper we propose graph theoretical framework for simultaneously integrating visual and textual features for efficient web image clustering specifically we model visual features images and words from surrounding text using tripartite graph partitioning this graph leads to clustering of the web images although graph partitioning approach has been adopted before the main contribution of this work lies in new algorithm that we propose consistent isoperimetric high order co clustering cihc for partitioning the tripartite graph computationally cihc is very quick as it requires simple solution to sparse system of linear equations our theoretical analysis and extensive experiments performed on real web images demonstrate the performance of cihc in terms of the quality efficiency and scalability in partitioning the visual feature image word tripartite graph
in this paper we propose prima pri vacy ma nager privacy protection mechanism which supports semi automated generation of access rules for users profile information prima access rules are tailored by the users privacy preferences for their profile data the sensitivity of the data itself and the objective risk of disclosing this data to other users the resulting rules are simple yet powerful specifications indicating the adequate level of protection for each user and are dynamically adapted to the ever changing setting of the users preferences and sn configuration
munisocket multiple network interface socket provides mechanisms to enhance the communication performance properties such as throughput transfer time and reliability by utilizing the existing multiple network interface cards on communicating hosts although the munisocket model has some communication performance advantages over the regular socket it also has number of usability and manageability drawbacks including the complexity of establishing multiple channels and configuring them for good communication performance this paper discusses some enhancements for munisocket using autonomic computing techniques these techniques include self discovery for discovering the existence of network interfaces and their performance properties self configuration for establishing channels over the interfaces and self optimization for selecting the best channels combinations for efficiently sending messages of varying sizes while these techniques enhance the communication performance among computers they also reduce the complexity of configuring munisocket and make its interface compatible with the regular tcp socket interface which in turn allows for transparent use of munisocket by the applications
bounded semantics of ltl with existential interpretation and that of ectl the existential fragment of ctl and the characterization of these existentially interpreted properties have been studied and used as the theoretical basis for sat based bounded model checking this has led to lot of successful work with respect to error detection in the checking of ltl and actl the universal fragment of ctl properties by satisfiability testing bounded semantics of ltl with the universal interpretation and that of actl and the characterization of such properties by propositional formulas have not been successfully established and this hinders practical verification of valid universal properties by satisfiability checking this paper studies this problem and the contribution is bounded semantics for actl and characterization of actl properties by propositional formulas firstly we provide simple bounded semantics for actl without considering the practical aspect of the semantics based on converting kripke model to model called model in which the transition relation is captured by set of paths each path with transitions this bounded semantics is not practically useful for the evaluation of formula since it involves too many paths in the model then the technique is to divide the model into submodels with limited number of paths which depends on and the actl property to be verified such that if an actl property is true in every such model then it is true in the model as well this characterization can then be used as the basis for practical verification of valid actl properties by satisfiability checking simple case study is provided to show the use of this approach for both verification and error detection of an abstract two process program written as first order transition system
energy efficiency is one of the main concerns in the wireless information dissemination system this paper presents wireless broadcast stream organization scheme which enables complex queries eg aggregation queries to be processed in an energy efficient way for efficient processing of complex queries we propose an approach of broadcasting their pre computed results with the data stream wherein the way of replication of index and pre computation results are investigated through analysis and experiments we show that the new approach can achieve significant performance enhancement for complex queries with respect to the access time and tuning time
weblogs have become prevalent source of information for people to express themselves in general there are two genres of contents in weblogs the first kind is about the webloggers personal feelings thoughts or emotions we call this kind of weblogs affective articles the second kind of weblogs is about technologies and different kinds of informative news in this paper we present machine learning method for classifying informative and affective articles among weblogs we consider this problem as binary classification problem by using machine learning approaches we achieve about on information retrieval performance measures including precision recall and we set up three studies on the applications of above classification approach in both research and industrial fields the above classification approach is used to improve the performance of classification of emotions from weblog articles we also develop an intent driven weblog search engine based on the classification techniques to improve the satisfaction of web users finally our approach is applied to search for weblogs with great deal of informative articles
main memory cache performance continues to play an important role in determining the overall performance of object oriented object relational and xml databases an effective method of improving main memory cache performance is to prefetch or pre load pages in advance to their usage in anticipation of main memory cache misses in this paper we describe framework for creating prefetching algorithms with the novel features of path and cache consciousness path consciousness refers to the use of short sequences of object references at key points in the reference trace to identify paths of navigation cache consciousness refers to the use of historical page access knowledge to guess which pages are likely to be main memory cache resident most of the time and then assumes these pages do not exist in the context of prefetching we have conducted number of experiments comparing our approach against four highly competitive prefetching algorithms the results shows our approach outperforms existing prefetching techniques in some situations while performing worse in others we provide guidelines as to when our algorithm should be used and when others maybe more desirable
in this paper we propose means to enhance an architecture description language with description of component behavior notation used for this purpose should be able to express the interplay on the component's interfaces and reflect step by step refinement of the component's specification during its design in addition the notation should be easy to comprehend and allow for formal reasoning about the correctness of the specification refinement and also about the correctness of an implementation in terms of whether it adheres to the specification targeting all these requirements together the paper proposes employing behavior protocols which are based on notation similar to regular expressions as proof of the concept the behavior protocols are used in the sofa architecture description language at three levels interface frame and architecture key achievements of this paper include the definitions of bounded component behavior and protocol conformance relation using these concepts the designer can verify the adherence of component's implementation to its specification at runtime while the correctness of refining the specification can be verified at design time
the problem of scheduling resources for tasks with variable requirements over time can be stated as follows we are given two sequences of vectors and sequence represents resource availability during time intervals where each vector has elements sequence represents resource requirements of task during intervals where each vector has elements we wish to find the earliest time interval termed latency such that for where and are the jth elements of vectors and respectively one application of this problem is scheduling for multimedia presentations the fastest known algorithm to compute the optimal solution of this problem has mathcal sqrt log computation time amir and farach in proceedings of the acm siam symposium on discrete algorithms soda san francisco ca pp inf comput we propose technique that approximates the optimal solution in linear time mathcal we evaluated the performance of our algorithm when used for multimedia scheduling our results show that of the time our solution is within of the optimal
aspects are now commonly used to add functionality that otherwise would cut across the structure of object systems in this survey both directions in the connection between aspects and formal methods are examined on the one hand the use of aspects to facilitate general software verification and especially model checking is demonstrated on the other hand the new challenges to formal specification and verification posed by aspects are defined and several existing solutions are described
to advance research and improve the scientific return on data collection and interpretation efforts in the geosciences we have developed methods of interactive visualization with special focus on immersive virtual reality vr environments earth sciences employ strongly visual approach to the measurement and analysis of geologic data due to the spatial and temporal scales over which such data ranges as observations and simulations increase in size and complexity the earth sciences are challenged to manage and interpret increasing amounts of data reaping the full intellectual benefits of immersive vr requires us to tailor exploratory approaches to scientific problems these applications build on the visualization method's strengths using both perception and interaction with data and models to take advantage of the skills and training of the geological scientists exploring their data in the vr environment this interactive approach has enabled us to develop suite of tools that are adaptable to range of problems in the geosciences and beyond
nested datatypes are families of datatypes that are indexed over all types such that the constructors may relate different family members moreover the argument types of the constructors refer to indices given by expressions where the family name may occur especially in this case of true nesting there is no direct support by theorem provers to guarantee termination of functions that traverse these data structuresa joint article with abel and uustalu tcs pp proposes iteration schemes that guarantee termination not by structural requirements but just by polymorphic typing they are generic in the sense that no specific syntactic form of the underlying datatype functor is required in subsequent work accepted for the journal of functional programming the author introduced an induction principle for the verification of programs obtained from mendler style iteration of rank which is one of those schemes and justified it in the calculus of inductive constructions through an implementation in the theorem prover coqthe new contribution is an extension of this work to generalized mendler iteration introduced in abel et al cited above leading to map fusion theorem for the obtained iterative functions the results and their implementation in coq are used for case study on representation of untyped lambda calculus with explicit flattening substitution is proven to fulfill two of the three monad laws the third only for hereditarily canonical terms but this is rectified by relativisation of the whole construction to those terms
we study the problem of tree pattern query rewriting using multiple views for the class of tree patterns in previous work has considered the rewriting problem using single view we consider two different ways of combining multiple views define rewritings of tree pattern using these combinations and study the relationship between them we show that when rewritings using single views do not exist we may use such combinations of multiple views to rewrite query and even if rewritings using single views do exist the rewritings using combinations of multiple views may provide more answers than those provided by the union of the rewritings using the individual views we also study properties of intersections of tree patterns and present algorithms for finding rewritings using intersections of views
the development of shape repositories and databases rises the need of online visualization of objects the main issue with the remote visualization of large meshes is the transfer latency of the geometric information the remote viewer requires the transfer of all polygons before allowing object's manipulation to avoid this latency problem an approach is to send several levels of details of the same object so that lighter versions can be displayed sooner and replaced with more detailed version later on this strategy requires more bandwidth implies abruptly changes in object aspect as the geometry refines as well as non negligible precomputing time since the appearance of model is more influenced by its normal field than its geometry we propose framework in which the object's lod is replaced with single simplified mesh with lod of appearance by using appearance preserving octree textures apo this appearance lod is encoded in unique texture and the details are progressively downloaded when they are needed our apo based framework achieves nearly immediate object rendering while details are transmitted and smoothly added to the texture scenes keep low geometry complexity while being displayed at interactive framerate with maximum of visual details leading to better visual quality over bandwith ratio than pure geometric lod schemes our implementation is platform independent as it uses jogl and runs on simple web browser furthermore the framework doesn't require processing on the server side during the client rendering
transactional memory tm is concurrent programming paradigm that aims to make concurrent programming easier than fine grain locking whilst providing similar performance and scalability several tm systems have been made available for research purposes however there is lack of wide range of non trivial benchmarks with which to thoroughly evaluate these tm systemsthis paper introduces lee tm non trivial and realistic tm benchmark suite based on lee's routing algorithm the benchmark suite provides sequential lock based and transactional implementations to enable direct performance comparison lee's routing algorithm has several of the desirable properties of non trivial tm benchmark such as large amounts of parallelism complex contention characteristics and wide range of transaction durations and lengths sample evaluation shows unfavourable transactional performance and scalability compared to lock based execution in contrast to much of the published tm evaluations and highlights the need for non trivial tm benchmarks
transactional memory offers significant advantages for concurrency control compared to locks this paper presents the design and implementation of transactional memory constructs in an unmanaged language unmanaged languages pose unique set of challenges to transactional memory constructs for example lack of type and memory safety use of function pointers aliasing of local variables and others this paper describes novel compiler and runtime mechanisms that address these challenges and optimize the performance of transactions in an unmanaged environment we have implemented these mechanisms in production quality compiler and high performance software transactional memory runtime we measure the effectiveness of these optimizations and compare the performance of lock based versus transaction based programming on set of concurrent data structures and the splash benchmark suite on processor smp system the transaction based version of the splash benchmarks scales much better than the coarse grain locking version and performs comparably to the fine grain locking version compiler optimizations significantly reduce the overheads of transactional memory so that on single thread the transaction based version incurs only about overhead compared to the lock based version for the splash benchmark suite thus our system is the first to demonstrate that transactions integrate well with an unmanaged language and can perform as well as fine grain locking while providing the programming ease of coarse grain locking even on an unmanaged environment
in single second modern processor can execute billions of instructions obtaining bird's eye view of the behavior of program at these speeds can be difficult task when all that is available is cycle by cycle examination in many programs behavior is anything but steady state and understanding the patterns of behavior at run time can unlock multitude of optimization opportunitiesin this paper we present unified profiling architecture that can efficiently capture classify and predict phase based program behavior on the largest of time scales by examining the proportion of instructions that were executed from different sections of code we can find generic phases that correspond to changes in behavior across many metrics by classifying phases generically we avoid the need to identify phases for each optimization and enable unified prediction scheme that can forecast future behavior our analysis shows that our design can capture phases that account for over of execution using less that bytes of on chip memory
every human being reading short report concerning road accident gets an idea of its causes the work reported here attempts to enable computer to do the same ie to determine the causes of an event from textual description of it it relies heavily on the notion of norm for two reasons the notion of cause has often been debated but remains poorly understood we postulate that what people tend to take as the cause of an abnormal event like an accident is the fact that specific norm has been violated natural language processing has given prominent place to deduction and for what concerns semantics to truth based inference however norm based inference is much more powerful technique to get the conclusions that human readers derive from text the paper describes complete chain of treatments from the text to the determination of the cause the focus is set on what is called linguistic and semantico pragmatic reasoning the former extracts so called semantic literals from the result of the parse and the latter reduces the description of the accident to small number of kernel literals which are sufficient to determine its cause both of them use non monotonic reasoning system viz lparse and smodels several issues concerning the representation of modalities and time are discussed and illustrated by examples taken from corpus of reports obtained from an insurance company
link analysis algorithms have been extensively used in web information retrieval however current link analysis algorithms generally work on flat link graph ignoring the hierarchal structure of the web graph they often suffer from two problems the sparsity of link graph and biased ranking of newly emerging pages in this paper we propose novel ranking algorithm called hierarchical rank as solution to these two problems which considers both the hierarchical structure and the link structure of the web in this algorithm web pages are first aggregated based on their hierarchical structure at directory host or domain level and link analysis is performed on the aggregated graph then the importance of each node on the aggregated graph is distributed to individual pages belong to the node based on the hierarchical structure this algorithm allows the importance of linked web pages to be distributed in the web page space even when the space is sparse and contains new pages experimental results on the gov collection of trec and show that hierarchical ranking algorithm consistently outperforms other well known ranking algorithms including the pagerank blockrank and layerrank in addition experimental results show that link aggregation at the host level is much better than link aggregation at either the domain or directory levels
dynamic tree data structures maintain forests that change over time through edge insertions and deletions besides maintaining connectivity information in logarithmic time they can support aggregation of information over paths trees or both we perform an experimental comparison of several versions of dynamic trees st trees et trees rc trees and two variants of top trees self adjusting and worst case we quantify their strengths and weaknesses through tests with various workloads most stemming from practical applications we observe that simple linear time implementation is remarkably fast for graphs of small diameter and that worst case and randomized data structures are best when queries are very frequent the best overall performance however is achieved by self adjusting st trees
this paper revisits the general hypermedia architecture based on perspective of peer to peer pp networking and pervasive computing and argues that pp has much to offer open hypermedia
virtual machine monitors are becoming popular tools for the deployment of database management systems and other enterprise software applications in this paper we consider common resource consolidation scenario in which several database management system instances each running in virtual machine are sharing common pool of physical computing resources we address the problem of optimizing the performance of these database management systems by controlling the configurations of the virtual machines in which they run these virtual machine configurations determine how the shared physical resources will be allocated to the different database instances we introduce virtualization design advisor that uses information about the anticipated workloads of each of the database systems to recommend workload specific configurations offine furthermore runtime information collected after the deployment of the recommended configurations can be used to refine the recommendation to estimate the effect of particular resource allocation on workload performance we use the query optimizer in new what if mode we have implemented our approach using both postgresql and db and we have experimentally evaluated its effectiveness using dss and oltp workloads
discrete transforms are of primary importance and fundamental kernels in many computationally intensive scientific applications in this paper we investigate the performance of two such algorithms fast fourier transform fft and discrete wavelet transform dwt on the sony toshiba ibm cell broadband engine cell be heterogeneous multicore chip architected for intensive gaming applications and high performance computing we design an efficient parallel implementation of fast fourier transform fft to fully exploit the architectural features of the cell be our fft algorithm uses an iterative out of place approach and for to complex input samples outperforms all other parallel implementations of fft on the cell be including fftw our fft implementation obtains single precision performance of gflop on the cell be outperforming intel duo core woodcrest for inputs of greater than samples we also optimize discrete wavelet transform dwt in the context of jpeg for the cell be dwt has an abundant parallelism however due to the low temporal locality of the algorithm memory bandwidth becomes significant bottleneck in achieving high performance we introduce novel data decomposition scheme to achieve highly efficient dma data transfer and vectorization with low programming complexity also we merge the multiple steps in the algorithm to reduce the bandwidth requirement this leads to significant enhancement in the scalability of the implementation our optimized implementation of dwt demonstrates and times speedup using one cell be chip to the baseline code for the lossless and lossy transforms respectively we also provide the performance comparison with the amd barcelona quad core opteron processor and the cell be excels the amd barcelona processor this highlights the advantage of the cell be over general purpose multicore processors in processing regular and bandwidth intensive scientific applications
software development is cooperative activity that heavily relies on the quality and effectiveness of the communication channels established within the development team and with the end user in the software engineering field several software engineering environments see have been developed to support and facilitate software development the most recent generation of these environments called process centered see psee supports the definition and the execution of various phases of the software process this is achieved by explicitly defining cooperation procedures and by supporting synchronization and data sharing among its usersactually cooperation support is theme of general interest and applies to all domains where computers can be exploited to support human intensive activities this has generated variety of research initiatives and support technology that is usually denoted by the acronym cscw computer supported cooperative work psee and cscw technologies have been developed rather independently from each other leading to large amount of research results tools and environments and practical experiences we argue that we have reached stage in technology development where it is necessary to assess and evaluate the effectiveness of the research efforts carried out so far moreover it is important to understand how to integrate and exploit the results of these different effortsthe goal of the paper is to understand which kind of basic functionalities psee can and should offer and how these environments can be integrated with other tools to effectively support cooperation in software development in particular the paper introduces process model we have built to support cooperative activity related to anomaly management in an industrial software factory the core of the paper is then constituted by the presentation and discussion of the experiences and results that we have derived from this modeling activity and how they related to the general problem of supporting cooperation in software development the project was carried out using the spade psee and the imaginedesk cscw toolkit both developed at politecnico di milano and cefriel during the past four years
this paper introduces gophers social game for mobile devices that utilises task oriented gameplay to create novel entertainment experience the study combines number of key research themes mobile social gaming acquiring useful data through gameplay and content sharing in mobile settings the experience of trialling the game in the real world is discussed and the findings from the study are presented
in this paper we compare and contrast two techniques to improve capacity conflict miss traffic in cc numa dsm clusters page migration replication optimizes read write accesses to page used by single processor by migrating the page to that processor and replicates all read shared pages in the sharers local memories numa optimizes read write accesses to any page by allowing processor to cache that page in its main memory page migration replication requires less hardware complexity as compared to numa but has limited applicability and incurs much higher overheads even with tuned hardware software support in this paper we compare and contrast page migration replication and numa on simulated clusters of symmetric multiprocessors executing shared memory applications our results show that both page migration replication and numa significantly improve the system performance over ldquo first touch rdquo migration in many applications page migration replication has limited opportunity and can not eliminate all the capacity conflict misses even with fast hardware support and unlimited amount of memory numa always performs best given page cache large enough to fit an application's primary working set and subsumes page migration replication numa benefits more from hardware support to accelerate page operations than page migration replication and integrating page migration replication into numa to help reduce the hardware cost requires sophisticated mechanisms and policies to select candidates for page migration replication
the problem of implementing shared object of one type from shared objects of other types has been extensively researched recent focus has mostly been on wait free implementations which permit every process to complete its operations on implemented objects regardless of the speeds of other processes it is known that shared objects of different types have differing abilities to support wait free implementations it is therefore natural to want to arrange types in hierarchy that reflects their relative abilities to support wait free implementations in this paper we formally define robustness and other desirable properties of hierarchies roughly speaking hierarchy is robust if each type is ldquo stronger rdquo than any combination of lower level types we study two specific hierarchies one that we call hrm in which the level of type is based on the ability of an unbounded number of objects of that type and another hierarchy that we call hr in which type's level is based on the ability of fixed number of objects of that type we prove that resource bounded hierarchies such as hr and its variants are not robust we also establish the unique importance of hrm every nontrivial robust hierarchy if one exists is necessarily ldquo coarsening rdquo of hrm
lazy dfa deterministic finite automata approach has been recently proposed to for efficient xml stream data processing this paper discusses the drawbacks of the approach suggests several optimizations as solutions and presents detailed analysis for the processing model the experiments show that our proposed approach is indeed effective and scalable
formal methods are helpful for many issues raised in the web services area in this article we advocate the use of process algebra as first step in the design and development of executable web services verification tools can be used to validate the correct execution of these formal descriptions we define some guidelines to encode abstract specifications of services to be written using these calculi into executable web services as back end language we consider bpel as the orchestration language we illustrate our approach through the development of simple business application
the limited built in configurability of linux can lead to expensive code size overhead when it is used in the embedded market to overcome this problem we propose the application of link time compaction and specialization techniques that exploit the priori known fixed runtime environment of many embedded systems in experimental setups based on the arm xscale and platforms the proposed techniques are able to reduce the kernel memory footprint with over percnt we also show how relatively simple additions to existing binary rewriters can implement the proposed techniques for complex very unconventional program such as the linux kernel we note that even after specialization lot of seemingly unnecessary code remains in the kernel and propose to reduce the footprint of this code by applying code compression techniques this technique combined with the previous ones reduces the memory footprint with over percnt for the platform and percnt for the arm platform finally we pinpoint an important code size growth problem when compaction and compression techniques are combined on the arm platform
modern intrusion detection systems are comprised of three basically different approaches host based network based and third relatively recent addition called procedural based detection the first two have been extremely popular in the commercial market for number of years now because they are relatively simple to use understand and maintain however they fall prey to number of shortcomings such as scaling with increased traffic requirements use of complex and false positive prone signature databases and their inability to detect novel intrusive attempts this intrusion detection system interacts with the access control system to deny further access when detection occurs and represent practical implementation addressing these and other concerns this paper presents an overview of our work in creating practical database intrusion detection system based on many years of database security research the proposed solution detects wide range of specific and general forms of misuse provides detailed reports and has low false alarm rate traditional commercial implementations of database security mechanisms are very limited in defending successful data attacks authorized but malicious transactions can make database useless by impairing its integrity and availability the proposed solution offers the ability to detect misuse and subversion through the direct monitoring of database operations inside the database host providing an important complement to host based and network based surveillance suites of the proposed solution may be deployed throughout network and their alarms man aged correlated and acted on by remote or local subscribing security services thus helping to address issues of decentralized management
rule bases are increasingly being used as repositories of knowledge content on the semantic web as the size and complexity of these rule bases increases developers and end users need methods of rule abstraction to facilitate rule management in this paper we describe rule abstraction method for semantic web rule language swrl rules that is based on lexical analysis and set of heuristics our method results in tree data structure that we exploit in creating techniques to visualize paraphrase and categorize swrl rules we evaluate our approach by applying it to several biomedical ontologies that contain swrl rules and show how the results reveal rule patterns within the rule base we have implemented our method as plug in tool for prot√©g√© owl the most widely used ontology modeling software for the semantic web our tool can allow users to rapidly explore content and patterns in swrl rule bases enabling their acquisition and management
this paper describes trap software tool that enables new adaptable behavior to be added to existing programs in transparent fashion in previous investigations we used an aspect oriented approach to manually define aspects for adaptation infrastructure which were woven into the original application code at compile time in follow on work we developed trap transparent shaping technique for automatically generating adaptation aspects where trap is specific instantiation of trap this paper presents our work into building trap which was intended to be port of trap into designing trap required us to overcome two major hurdles lack of reflection in and the incompatibility between the management of objects in and the aspect weaving technique used in trap we used generative programming methods to produce two tools trapgen and trapcc that work together to produce the desired trap functionality details of the trap architecture and operation are presented which we illustrate with description of case study that adds dynamic auditing capabilities to an existing distributed application
schema matching is the task of matching between concepts describing the meaning of data in various heterogeneous distributed data sources with many heuristics to choose from several tools have enabled the use of schema matcher ensembles combining principles by which different schema matchers judge the similarity between concepts in this work we investigate means of estimating the uncertainty involved in schema matching and harnessing it to improve an ensemble outcome we propose model for schema matching based on simple probabilistic principles we then propose the use of machine learning in determining the best mapping and discuss its pros and cons finally we provide thorough empirical analysis using both real world and synthetic data to test the proposed technique we conclude that the proposed heuristic performs well given an accurate modeling of uncertainty in matcher decision making
many database applications that need to disseminate dynamic information from server to various clients can suffer from heavy communication costs data caching at client can help mitigate these costs particularly when individual rm push hbox rm pull decisions are made for the different semantic regions in the data space the server is responsible for notifying the client about updates in the rm push regions the client needs to contact the server for queries that ask for data in the rm pull regions we call the idea of partitioning the data space into rm push hbox rm pull regions to minimize communication cost data gerrymandering in this paper we present solutions to technical challenges in adopting this simple but powerful idea we give provably optimal cost dynamic programming algorithm for gerrymandering on single query attribute we propose family of efficient heuristics for gerrymandering on multiple query attributes we handle the dynamic case in which the workloads of queries and updates evolve over time we validate our methods through extensive experiments on real and synthetic data sets
we introduce the bounded deformation tree or bd tree which can perform collision detection with reduced deformable models at costs comparable to collision detection with rigid objects reduced deformable models represent complex deformations as linear superpositions of arbitrary displacement fields and are used in variety of applications of interactive computer graphics the bd tree is bounding sphere hierarchy for output sensitive collision detection with such models its bounding spheres can be updated after deformation in any order and at cost independent of the geometric complexity of the model in fact the cost can be as low as one multiplication and addition per tested sphere and at most linear in the number of reduced deformation coordinates we show that the bd tree is also extremely simple to implement and performs well in practice for variety of real time and complex off line deformable simulation examples
first come first served fcfs mutual exclusion me is the problem of ensuring that processes attempting to concurrently access shared resource do so one by one in fair order in this paper we close the complexity gap between fcfs me and me in the asynchronous shared memory model where processes communicate using atomic reads and writes only and do not fail our main result is the first known fcfs me algorithm that makes logn remote memory references rmrs per passage and uses only atomic reads and writes our algorithm is also adaptive to point contention more precisely the number of rmrs process makes per passage in our algorithm is min logn where is the point contention our algorithm matches known rmr complexity lower bounds for the class of me algorithms that use reads and writes only and beats the rmr complexity of prior algorithms in this class that have the fcfs property
collaborative prediction refers to the task of predicting user preferences on the basis of ratings by other users collaborative prediction suffers from the cold start problem where predictions of ratings for new items or predictions of new users preferences are required various methods have been developed to overcome this limitation exploiting side information such as content information and demographic user data in this paper we present matrix factorization method for incorporating side information into collaborative prediction we develop weighted nonnegative matrix co tri factorization wnmctf where we jointly minimize weighted residuals each of which involves nonnegative factor decomposition of target or side information matrix numerical experiments on movielens data confirm the useful behavior of wnmctf when operating from cold start
results clustering in web searching is useful for providing users with overviews of the results and thus allowing them to restrict their focus to the desired parts however the task of deriving single word or multiple word names for the clusters usually referred as cluster labeling is difficult because they have to be syntactically correct and predictive moreover efficiency is an important requirement since results clustering is an online task suffix tree clustering stc is clustering technique where search results mainly snippets can be clustered fast in linear time incrementally and each cluster is labeled with phrase in this paper we introduce variation of the stc called stc with scoring formula that favors phrases that occur in document titles and differs in the way base clusters are merged and novel non merging algorithm called nm stc that results in hierarchically organized clusters the comparative user evaluation showed that both stc and nm stc are significantly more preferred than stc and that nm stc is about two times faster than stc and stc
slg resolution uses tabling to evaluate nonfloundering normal logic pr ograms according to the well founded semantics the slg wam which forms the engine of the xsb system can compute in memory recursive queries an order of magnitute faster than current deductive databases at the same time the slg wam tightly intergrates prolog code with tabled slg code and executes prolog code with minimal overhead compared to the wam as result the slg wam brings to logic programming important termination and complexity properties of deductive databases this article describes the architecture of the slg wam for powerful class of programs the class of fixed order dynamically stratified programs we offer detailed description of the algorithms data structures and instructions that the slg wam adds to the wam and performance analysis of engine overhead due to the extensions
we report on an automated runtime anomaly detection method at the application layer of multi node computer systems although several network management systems are available in the market none of them have sufficient capabilities to detect faults in multi tier web based systems with redundancy we model web based system as weighted graph where each node represents service and each edge represents dependency between services since the edge weights vary greatly over time the problem we address is that of anomaly detection from time sequence of graphsin our method we first extract feature vector from the adjacency matrix that represents the activities of all of the services the heart of our method is to use the principal eigenvector of the eigenclusters of the graph then we derive probability distribution for an anomaly measure defined for time series of directional data derived from the graph sequence given critical probability the threshold value is adaptively updated using novel online algorithmwe demonstrate that fault in web application can be automatically detected and the faulty services are identified without using detailed knowledge of the behavior of the system
experience sampling has been employed for decades to collect assessments of subjects intentions needs and affective states in recent years investigators have employed automated experience sampling to collect data to build predictive user models to date most procedures have relied on random sampling or simple heuristics we perform comparative analysis of several automated strategies for guiding experience sampling spanning spectrum of sophistication from random sampling procedure to increasingly sophisticated active learning the more sophisticated methods take decision theoretic approach centering on the computation of the expected value of information of probe weighing the cost of the short term disruptiveness of probes with their benefits in enhancing the long term performance of predictive models we test the different approaches in field study focused on the task of learning predictive models of the cost of interruption
most of the complexity of common data mining tasks is due to the unknown amount of information contained in the data being mined the more patterns and corelations are contained in such data the more resources are needed to extract them this is confirmed by the fact that in general there is not single best algorithm for given data mining task on any possible kind of input dataset rather in order to achieve good performances strategies and optimizations have to be adopted according to the dataset specific characteristics for example one typical distinction in transactional databases is between sparse and dense datasets in this paper we consider frequent set counting as case study for data mining algorithms we propose statistical analysis of the properties of transactional datasets that allows for characterization of the dataset complexity we show how such characterization can be used in many fields from performance prediction to optimization
this paper presents the http request global distribution using the fuzzy neural decision making mechanism two efficient algorithms gardib and gardib are proposed to support http request routing to the websites the algorithms use the fuzzy neural decision making method to assign each incoming request to the website with the least expected response time the response time includes the transmission time over the network as well as the time elapsed on the responding website server simulation experiments showed that gardib performed slightly better than gardib and both proposed algorithms outperformed other competitive distribution algorithms in all simulated workload scenarios
chip multiprocessors cmps are promising candidates for the next generation computing platforms to utilize large numbers of gates and reduce the effects of high interconnect delays one of the key challenges in cmp design is to balance out the often conflicting demands specifically for today's image video applications and systems power consumption memory space occupancy area cost and reliability are as important as performance therefore compilation framework for cmps should consider multiple factors during the optimization process motivated by this observation this paper addresses the energy aware reliability support for the cmp architectures targeting in particular at array intensive image video applications there are two main goals behind our compiler approach first we want to minimize the energy wasted in executing replicas when there is no error during execution which should be the most frequent case in practice second we want to minimize the time to recover through the replicas from an error when it occurs this approach has been implemented and tested using four parallel array based applications from the image video processing domain our experimental evaluation indicates that the proposed approach saves significant energy over the case when all the replicas are run under the highest voltage frequency level without sacrificing any reliability over the latter
the speed project addresses the problem of computing symbolic computational complexity bounds of procedures in terms of their inputs we discuss some of the challenges that arise and present various orthogonal complementary techniques recently developed in the speed project for addressing these challenges
the question of determining which sets of constraints give rise to np complete problems and which give rise to tractable problems is an important open problem in the theory of constraint satisfaction it has been shown in previous papers that certain sufficient conditions for tractability and np completeness can be identified using algebraic properties of relations and that these conditions can be tested by solving particular form of constraint satisfaction problem the so called indicator problem this paper describes program which can solve the relevant indicator problems for arbitrary sets of constraints over small domains and for some sets of constraints over larger domains the main innovation in the program is its ability to deal with the many symmetries present in the problem it also has the ability to preserve symmetries in cases where this speeds up the solutionusing this program we have systematically investigated the complexity of all individual binary relations over domain of size four or less and of all individual ternary relations over domain of size three or less this automated analysis includes the derivation of more than new np completeness results and precisely identifies the small set of individual relations which cannot be classified as either tractable or np complete using the algebraic conditions presented in previous papers
the application of the tolerance paradigm to security intrusion tolerance has been raising reasonable amount of attention in the dependability and security communities in this paper we present novel approach to intrusion tolerence the idea is to use privileged components generically designated by wormholes to support the execution of intrusion tolerant protocols often called byzantine resilient in the literaturethe paper introduces the design of wormhole aware intrusion tolerant protocols using classical distributed systems problem consensus the system where the consensus protocol runs is mostly asynchronous and can fail in an arbitrary way except for the wormhole which is secure and synchronous using the wormhole to execute few critical steps the protocol manages to have low time complexity in the best case it runs in two rounds even if some processes are malicious the protocol also shows how often theoretical partial synchrony assumptions can be substantiated in practical distributed systems the paper shows the significance of the ttcb as an engineering paradigm since the protocol manages to be simple when compared with other protocols in the literature
this paper describes the design implementation and evaluation of areplication scheme to handle byzantine faults in transaction processing database systems the scheme compares answers from queries and updates on multiple replicas which are unmodified off the shelf systems to provide single database that is byzantine fault tolerant the scheme works when the replicas are homogeneous but it also allows heterogeneous replication in which replicas come from different vendors heterogeneous replicas reduce the impact of bugs and security compromises because they are implemented independently and are thus less likely to suffer correlated failures the main challenge in designing replication scheme for transactionprocessing systems is ensuring that the different replicas execute transactions in equivalent serial orders while allowing high degreeof concurrency our scheme meets this goal using novel concurrency control protocol commit barrier scheduling cbs we have implemented cbs in the context of replicated sql database hrdb heterogeneous replicated db which has been tested with unmodified production versions of several commercial and open source databases as replicas our experiments show an hrdb configuration that can tolerate one faulty replica has only modest performance overhead about for the tpc benchmark hrdb successfully masks several byzantine faults observed in practice and we have used it to find new bug in mysql
in large software development projects when programmer is assigned bug to fix she typically spends lot of time searching in an ad hoc manner for instances from the past where similar bugs have been debugged analyzed and resolved systematic search tools that allow the programmer to express the context of the current bug and search through diverse data repositories associated with large projects can greatly improve the productivity of debugging this paper presents the design implementation and experience from such search tool called debugadvisor the context of bug includes all the information programmer has about the bug including natural language text textual rendering of core dumps debugger output etc our key insight is to allow the programmer to collate this entire context as query to search for related information thus debugadvisor allows the programmer to search using fat query which could be kilobytes of structured and unstructured data describing the contextual information for the current bug information retrieval in the presence of fat queries and variegated data repositories all of which contain mix of structured and unstructured data is challenging problem we present novel ideas to solve this problem we have deployed debugadvisor to over users inside microsoft in addition to standard metrics such as precision and recall we present extensive qualitative and quantitative feedback from our users
graph has become increasingly important in modelling complicated structures and schemaless data such as proteins chemical compounds and xml documents given graph query it is desirable to retrieve graphs quickly from large database via graph based indices in this paper we investigate the issues of indexing graphs and propose novel solution by applying graph mining technique different from the existing path based methods our approach called gindex makes use of frequent substructure as the basic indexing feature frequent substructures are ideal candidates since they explore the intrinsic characteristics of the data and are relatively stable to database updates to reduce the size of index structure two techniques size increasing support constraint and discriminative fragments are introduced our performance study shows that gindex has times smaller index size but achieves times better performance in comparison with typical path based method graphgrep the gindex approach not only provides and elegant solution to the graph indexing problem but also demonstrates how database indexing and query processing can benefit form data mining especially frequent pattern mining furthermore the concepts developed here can be applied to indexing sequences trees and other complicated structures as well
in this paper we introduce family of expressive extensions of datalog called datalog as new paradigm for query answering over ontologies the datalog family admits existentially quantified variables in rule heads and has suitable restrictions to ensure highly efficient ontology querying we show in particular that datalog generalizes the dl lite family of tractable description logics which are the most common tractable ontology languages in the context of the semantic web and databases we also show how stratified negation can be added to datalog while keeping ontology querying tractable furthermore the datalog family is of interest in its own right and can moreover be used in various contexts such as data integration and data exchange
the current paper presents work in progress in integrated management of artefacts inautomotive electronics with focus on requirements case study on industrial requirements management was performed in swedish automotive supplier of software intensive systems based on this case study frame concept for integrated model management is proposed one core element in implementing the frame concept is an ontology based domain repository concepts architecture and realization of domain repository are introduced the domain repository is based on rdf rdfs and triple and includes extensions supporting navigation in the artefact net and derivation of new knowledge modlets actlets and spot views
high performance computing applications with data driven communication and computation characteristics require synchronization routines in the form of eureka barrier or termination synchronization in this paper we consider termination synchronization for two different execution models the ap and the aps model in the ap model processors are either active or passive and passive processor can be made active by another active processor in the aps model processors can also be in server state passive processor entering the server state does not become active again in addition server processor cannot change the status of other processors we describe and analyze solutions for both models and present experimental work highlighting the differences between the models we show that in almost all situations the use of an ap algorithm to detect termination in an aps environment will result in loss of performance our experimental work on the cray te provides insight into where and why this performance loss occurs
wireless sensor networks are increasingly being used in applications where the communication between nodes needs to be protected from eavesdropping and tampering such protection is typically provided using techniques from symmetric key cryptography the protocols in this domain suffer from one or more of the following problems weak security guarantees if some nodes are compromised lack of scalability high energy overhead for key management and increased end to end data latency in this paper we propose protocol called secos that mitigates these problems in static sensor networks secos divides the sensor field into control groups each with control node data exchange between nodes within control group happens through the mediation of the control head which provides the common key the keys are refreshed periodically and the control nodes are changed periodically to enhance security secos enhances the survivability of the network by handling compromise and failures of control nodes it provides the guarantee that the communication between any two sensor nodes remains secure despite the compromise of any number of other nodes in the network the experiments based on simulation model show seven time reduction in energy overhead and reduction in latency compared to spins which is one of the state of the art protocols for key management in sensor networks
we describe computational cognitive architecture for robots which we call act act embodied act is based on act but uses different visual auditory and movement modules we describe model that uses act to integrate visual and auditory information to perform conversation tracking in dynamic environment we also performed an empirical evaluation study which shows that people see our conversational tracking system as extremely natural
major challenge in frequent pattern mining is the sheer size of its mining results in many cases high minsup threshold may discover only commonsense patterns but low one may generate an explosive number of output patterns which severely restricts its usagein this paper we study the problem of compressing frequent pattern sets typically frequent patterns can be clustered with tightness measure delta called delta cluster and representative pattern can be selected for each cluster unfortunately finding minimum set of representative patterns is np hard we develop two greedy methods rpglobal and rplocal the former has the guaranteed compression bound but higher computational complexity the latter sacrifices the theoretical bounds but is far more efficient our performance study shows that the compression quality using rplocal is very close to rpglobal and both can reduce the number of closed frequent patterns by almost two orders of magnitude furthermore rplocal mines even faster than fpclose very fast closed frequent pattern mining method we also show that rpglobal and rplocal can be combined together to balance the quality and efficiency
we investigate the complexity of finding nash equilibria in which the strategy of each player is uniform on its support set we show that even for restricted class of win lose bimatrix games deciding the existence of such uniform equilibria is an np complete problem our proof is graph theoretical motivated by this result we also give np completeness results for the problems of finding regular induced subgraphs of large size or regularity which can be of independent interest
objects with mirroring optical characteristics are left out of the scope of most scanning methods we present here new automatic acquisition approach shape from distortion that focuses on that category of objects requires only still camera and color monitor and produces range scans plus normal and reflectance map of the target our technique consists of two steps first an improved environment matte is captured for the mirroring object using the interference of patterns with different frequencies to obtain sub pixel accuracy then the matte is converted into normal and depth map by exploiting the self coherence of surface when integrating the normal map along different paths the results show very high accuracy capturing even smallest surface details the acquired depth maps can be further processed using standard techniques to produce complete mesh of the object
separable assignment problem sap is defined by set of bins and set of items to pack in each bin value fij for assigning item to bin and separate packing constraint for each bin ie for bin family li of subsets of items that fit in bin the goal is to pack items into bins to maximize the aggregate value this class of problems includes the maximum generalized assignment problem gap and distributed caching problem dcp described in this papergiven beta approximation algorithm for finding the highest value packing of single bin we give polynomial time lp rounding based minus beta approximation algorithm simple polynomial time local search beta beta epsilon approximation algorithm for any epsilon therefore for all examples of sap that admit an approximation scheme for the single bin problem we obtain an lp based algorithm with epsilon approximation and local search algorithm with epsilon approximation guarantee furthermore for cases in which the subproblem admits fully polynomial approximation scheme such as for gap the lp based algorithm analysis can be strengthened to give guarantee of the best previously known approximation algorithm for gap is approximation by shmoys and tardos and chekuri and khanna our lp algorithm is based on rounding new linear programming relaxation with provably better integrality gapto complement these results we show that sap and dcp cannot be approximated within factor better than unless np sube dtime no log log even if there exists polynomial time exact algorithm for the single bin problemwe extend the approximation algorithm to nonseparable assignment problem with applications in maximizing revenue for budget constrained combinatorial auctions and the adwords assignment problem we generalize the local search algorithm to yield epsilon approximation algorithm for the median problem with hard capacities finally we study naturally defined game theoretic versions of these problems and show that they have price of anarchy of we also prove the existence of cycles of best response moves and exponentially long best response paths to pure or sink equilibria
the ability of reconfiguring software architectures in order to adapt them to new requirements or changing environment has been of growing interest but there is still not much formal work in the area most existing approaches deal with run time changes in deficient way the language to express computations is often at very low level of specification and the integration of two different formalisms for the computations and reconfigurations require sometimes substantial changes to address these problems we propose uniform algebraic approach with the following characteristicscomponents are written in high level program design language with the usual notion of statethe approach combines two existing frameworks mdash one to specify architectures the other to rewrite labelled graphs mdash just through small additions to either of themit deals with certain typical problems such as guaranteeing that new components are introduced in the correct state possibly transferred from the old components they replace it shows the relationships between reconfigurations and computations while keeping them separate because the approach provides semantics to given architecture through the algebraic construction of an equivalent program whose computations can be mirrored at the architectural level
deadlocks are possibly the best known bug pattern in computer systems in general certainly they are the best known in concurrent programming numerous articles some dating back more than years have been dedicated to the questions of how to design deadlock free programs how to statically or dynamically detect possible deadlocks how to avoid deadlocks at runtime and how to resolve deadlocks once they happen we start the paper with an investigation on how to exhibit potential deadlocks exhibiting deadlocks is very useful in testing as verifying if potential deadlock can actually happen is time consuming debugging activity there was recently some very interesting research in this direction however we believe our approach is more practical has no scaling issues and in fact is already industry readythe second contribution of our paper is in the area of healing multi threaded programs so they do not get into deadlocks this is an entirely new approach which is very different from the approaches in the literature that were meant for multi process scenarios and are not suitable and indeed not used in multithreaded programming while the basic ideas are fairly simple the details here are very important as any mistake is liable to actually create new deadlocks the paper describes the basic healing idea and its limitations the pitfalls and how to overcome them and experimental results
bursts in data center workloads are real problem for storage subsystems data volumes can experience peak request rates that are over an order of magnitude higher than average load this requires significant overprovisioning and often still results in significant request latency during peaks in order to address this problem we propose everest which allows data written to an overloaded volume to be temporarily off loaded into short term virtual store everest creates the short term store by opportunistically pooling underutilized storage resources either on server or across servers within the data center writes are temporarily off loaded from overloaded volumes to lightly loaded volumes thereby reducing the load on the former everest is transparent to and usable by unmodified applications and does not change the persistence or consistency of the storage system we evaluate everest using traces from production exchange mail server as well as other benchmarks our results show times reduction in mean response times during peaks
mutation testing measures the adequacy of test suite by seeding artificial defects mutations into program if mutation is not detected by the test suite this usually means that the test suite is not adequate however it may also be that the mutant keeps the program's semantics unchanged and thus cannot be detected by any test such equivalent mutants have to be eliminated manually which is tedious we assess the impact of mutations by checking dynamic invariants in an evaluation of our javalanche framework on seven industrial size programs we found that mutations that violate invariants are significantly more likely to be detectable by test suite as consequence mutations with impact on invariants should be focused upon when improving test suites with less than of equivalent mutants our approach provides an efficient precise and fully automatic measure of the adequacy of test suite
in this paper we present improvements to recursive bisection basedplacement in contrast to prior work our horizontal cut lines arenot restricted to row boundaries this avoids narrow region problem to support these new cut line positions dynamic programmingbased legalization algorithm has been developed thecombination of these has improved the stability and lowered thewire lengths produced by our feng shui placement toolon benchmarks derived from industry partitioning examples our results are close to those of the annealing based tool dragon while taking only fraction of the run time on synthetic benchmarks our wire lengths are nearly better than those of dragonfor both benchmark suites our results are substantially better thanthose of the recursive bisection based tool capo and the analyticplacement tool kraftwerk
metamodels are increasingly being used in software engineering particularly in standards from both the omg and iso it is therefore critical that these metamodels be used correctly in this paper we investigate some of the pitfalls observed in the use of metamodelling ideas in software engineering and from these observations deduce some rules of thumb to help increase the quality of usage of metamodels in software engineering in the future
wireless mesh networks wmns consist of mesh routers and mesh clients where mesh routers have minimal mobility and form the backbone of wmns they provide network access for both mesh and conventional clients the integration of wmns with other networks such as the internet cellular ieee ieee ieee sensor networks etc can be accomplished through the gateway and bridging functions in the mesh routers mesh clients can be either stationary or mobile and can form client mesh network among themselves and with mesh routers wmns are anticipated to resolve the limitations and to significantly improve the performance of ad hoc networks wireless local area networks wlans wireless personal area networks wpans and wireless metropolitan area networks wmans they are undergoing rapid progress and inspiring numerous deployments wmns will deliver wireless services for large variety of applications in personal local campus and metropolitan areas despite recent advances in wireless mesh networking many research challenges remain in all protocol layers this paper presents detailed study on recent advances and open research issues in wmns system architectures and applications of wmns are described followed by discussing the critical factors influencing protocol design theoretical network capacity and the state of the art protocols for wmns are explored with an objective to point out number of open research issues finally testbeds industrial practice and current standard activities related to wmns are highlighted ed by discussing the critical factors influencing protocol design theoretical network capacity and the state of the art protocols for wmns are explored with an objective to outline number of open research issues finally testbeds industrial practice and current standard activities related to wmns are highlighted
regression test suite prioritization techniques reorder test cases so that on average more faults will be revealed earlier in the test suite's execution than would otherwise be possible this paper presents genetic algorithm based test prioritization method that employs wide variety of mutation crossover selection and transformation operators to reorder test suite leveraging statistical analysis techniques such as tree model construction through binary recursive partitioning and kernel density estimation the paper's empirical results highlight the unique role that the selection operators play in identifying an effective ordering of test suite the study also reveals that while truncation selection consistently outperformed the tournament and roulette operators in terms of test suite effectiveness increasing selection pressure consistently produces the best results within each class of operator after further explicating the relationship between selection intensity termination condition fitness landscape and the quality of the resulting test suite this paper demonstrates that the genetic algorithm based prioritizer is superior to random search and hill climbing and thus suitable for many regression testing environments
information search and retrieval interactions usually involve information content in the form of document collections information retrieval systems and interfaces and the user to fully understand information search and retrieval interactions between users cognitive space and the information space researchers need to turn to cognitive models and theories in this article the authors use one of these theories the basic level theory use of the basic level theory to understand human categorization is both appropriate and essential to user centered design of taxonomies ontologies browsing interfaces and other indexing tools and systems analyses of data from two studies involving free sorting by participants of images were conducted the types of categories formed and category labels were examined results of the analyses indicate that image category labels generally belong to superordinate to the basic level and are generic and interpretive implications for research on theories of cognition and categorization and design of image indexing retrieval and browsing systems are discussed copy wiley periodicals inc
garbage collectors are notoriously hard to verify due to their low level interaction with the underlying system and the general difficulty in reasoning about reachability in graphs several papers have presented verified collectors but either the proofs were hand written or the collectors were too simplistic to use on practical applications in this work we present two mechanically verified garbage collectors both practical enough to use for real world benchmarks the collectors and their associated allocators consist of assembly language instructions and macro instructions annotated with preconditions postconditions invariants and assertions we used the boogie verification generator and the automated theorem prover to verify this assembly language code mechanically we provide measurements comparing the performance of the verified collector with that of the standard bartok collectors on off the shelf benchmarks demonstrating their competitiveness
reasoning about the termination of equational programs in sophisticated equational languages such as elan maude obj cafeobj haskell and so on requires support for advanced features such as evaluation strategies rewriting modulo use of extra variables in conditions partiality and expressive type systems possibly including polymorphism and higher order however many of those features are at best only partially supported by current term rewriting termination tools for instance mu term me aprove ttt termptation etc while they may be essential to ensure termination we present sequence of theory transformations that can be used to bridge the gap between expressive membership equational programs and such termination tools and prove the correctness of such transformations we also discuss prototype tool performing the transformations on maude equational programs and sending the resulting transformed theories to some of the aforementioned standard termination tools
as practical opportunity for educating japanese young developers in the field of embedded software development software design contest involving the design of software to automatically control line trace robot and conduct running performance tests was held in this paper we give the results of the contest from the viewpoint of software quality evaluation we create framework for evaluating the software quality which integrated design model quality and the final system performance and conduct analysis using the framework as result of analysis it is found that the quantitative measurement of the structural complexity of the design models bears strong relationship to qualitative evaluation of the design conducted by judges it is also found that there is no strong correlation between design model quality evaluated by the judges and the final system performance for embedded software development it is particularly important to estimate and verify reliability and performance in the early stages using the model based on the analysis result we consider possible remedies with respect to the models submitted the evaluation methods used and the contest specifications in order to adequately measure several non functional quality characteristics including performance on the model it is necessary to improve the way of developing robot software such as applying model driven development and reexamine the evaluation methods
semantic web data exhibits very skewed frequency distributions among terms efficient large scale distributed reasoning methods should maintain load balance in the face of such highly skewed distribution of input data we show that term based partitioning used by most distributed reasoning approaches has limited scalability due to load balancing problems we address this problem with method for data distribution based on clustering in elastic regions instead of as signing data to fixed peers data flows semi randomly in the network data items speed date while being temporarily collocated in the same peer we introduce bias in the routing to allow semantically clustered neighborhoods to emerge our approach is self organising efficient and does not require any central coordination we have implemented this method on the marvin platform and have performed experiments on large real world datasets using cluster of up to nodes we compute the rdfs closure over different datasets and show that our clustering algorithm drastically reduces computation time calculating the rdfs closure of million triples in minutes
many end users wish to customize their applications automating common tasks and routines unfortunately this automation is difficult today users must choose between brittle macros and complex scripting languages programming by demonstration pbd offers middle ground allowing users to demonstrate procedure multiple times and generalizing the requisite behavior with machine learning unfortunately many pbd systems are almost as brittle as macro recorders offering few ways for user to control the learning process or correct the demonstrations used as training examples this paper presents chinle system which automatically constructs pbd systems for applications based on their interface specification the resulting pbd systems have novel interaction and visualization methods which allow the user to easily monitor and guide the learning process facilitating error recovery during training chinle constructed pbd systems learn procedures with conditionals and perform partial learning if the procedure is too complex to learn completely
review is provided of some database and representation issues involved in the implementation of geographic information systems gis the acm portal is published by the association for computing machinery copyright acm inc terms of usage privacy policy code of ethics contact us useful downloads adobe acrobat quicktime windows media player real player
the automated and semi automated analysis of source code has remained topic of intense research for more than thirty years during this period algorithms and techniques for source code analysis have changed sometimes dramatically the abilities of the tools that implement them have also expanded to meet new and diverse challenges this paper surveys current work on source code analysis it also provides road map for future work over the next five year period and speculates on the development of source code analysis applications techniques and challenges over the next and years
successful application of data mining to bioinformatics is protein classification number of techniques have been developed to classify proteins according to important features in their sequences secondary structures or three dimensional structures in this paper we introduce novel approach to protein classification based on significant patterns discovered on the surface of protein we define notion called alpha hbox rm surface we discuss the geometric properties of alpha hbox rm surface and present an algorithm that calculates the alpha hbox rm surface from finite set of points in we apply the algorithm to extracting the alpha hbox rm surface of protein and use pattern discovery algorithm to discover frequently occurring patterns on the surfaces the pattern discovery algorithm utilizes new index structure called the delta rm tree we use these patterns to classify the proteins while most existing techniques focus on the binary classification problem we apply our approach to classifying three families of proteins experimental results show the good performance of the proposed approach
web pages like people are often known by others in variety of contexts when those contexts are sufficiently distinct page's importance may be better represented by multiple domains of authority rather than by one that indiscriminately mixes reputations in this work we determine domains of authority by examining the contexts in which page is cited however we find that it is not enough to determine separate domains of authority our model additionally determines the local flow of authority based upon the relative similarity of the source and target authority domains in this way we differentiate both incoming and outgoing hyperlinks by topicality and importance rather than treating them indiscriminately we find that this approach compares favorably to other topical ranking methods on two real world datasets and produces an approximately improvement in precision and quality of the top ten results over pagerank
this paper presents an automatic deforestation system stream fusion based on equational transformations that fuses wider range of functions than existing short cut fusion systems in particular stream fusion is able to fuse zips left folds and functions over nested lists including list comprehensions distinguishing feature of the framework is its simplicity by transforming list functions to expose their structure intermediate values are eliminated by general purpose compiler optimisations we have reimplemented the haskell standard list library on top of our framework providing stream fusion for haskell lists by allowing wider range of functions to fuse we see an increase in the number of occurrences of fusion in typical haskell programs we present benchmarks documenting time and space improvements
in today√Ωs fast paced world it's increasingly difficult to understand and act promptly upon the content from the many available information streams temporal pooling addresses this problem by producing visual summary of recent stream content this summary is often in motion to incorporate newly arrived content this article reviews the perception of motion and change blindness offering several guidelines for the use of motion in visualization it then describes textpool tool for visualizing live text streams such as newswires blogs and closed captioning textpool√Ωs temporal pooling summarization is dynamic textual collage that clusters related terms textpool was tested with the content of several rss newswire feeds streaming stories per day textpool handled this bandwidth well producing useful summarizations of stream content
in semi supervised learning number of labeled examples are usually required for training an initial weakly useful predictor which is in turn used for exploiting the unlabeled examples however in many real world applications there may exist very few labeled training examples which makes the weakly useful predictor difficult to generate and therefore these semisupervised learning methods cannot be applied this paper proposes method working under two view setting by taking advantages of the correlations between the views using canonical component analysis the proposed method can perform semi supervised learning with only one labeled training example experiments and an application to content based image retrieval validate the effectiveness of the proposed method
touch is compelling input modality for interactive devices however touch input on the small screen of mobile device is problematic because user's fingers occlude the graphical elements he wishes to work with in this paper we present lucidtouch mobile device that addresses this limitation by allowing the user to control the application by touching the back of the device the key to making this usable is what we call pseudo transparency by overlaying an image of the user's hands onto the screen we create the illusion of the mobile device itself being semi transparent this pseudo transparency allows users to accurately acquire targets while not occluding the screen with their fingers and hand lucid touch also supports multi touch input allowing users to operate the device simultaneously with all fingers we present initial study results that indicate that many users found touching on the back to be preferable to touching on the front due to reduced occlusion higher precision and the ability to make multi finger input
this paper evaluates five supervised learning methods in the context of statistical spam filtering we study the impact of different feature pruning methods and feature set sizes on each learner's performance using cost sensitive measures it is observed that the significance of feature selection varies greatly from classifier to classifier in particular we found support vector machine adaboost and maximum entropy model are top performers in this evaluation sharing similar characteristics not sensitive to feature selection strategy easily scalable to very high feature dimension and good performances across different datasets in contrast naive bayes commonly used classifier in spam filtering is found to be sensitive to feature selection methods on small feature set and fails to function well in scenarios where false positives are penalized heavily the experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned cost much higher than spams such as lambda so as to maintain better than baseline performance an interesting finding is the effect of mail headers on spam filtering which is often ignored in previous studies experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only this implies that message headers can be reliable and powerfully discriminative feature sources for spam filtering
scientific data are available through an increasing number of heterogeneous independently evolving sources although the sources themselves are independently evolving the data stored in them are not there exist inherent and intricate relationships between the distributed data sets and scientists are routinely required to write distributed queries in this setting being non experts in computer science the scientists are faced with two major challenges how to express such distributed queries this is non trivial task even if we assume that scientists are familiar with query languages like sql such queries can get arbitrarily complex as more sources are considered ii how to efficiently evaluate such distributed queries an efficient evaluation must account for batches of hundreds or even thousands of submitted queries and must optimize all of them as whole in this demo we focus on the biological domain for illustration purposes our solutions are applicable to other scientific domains and we present system called bioscout that offers solutions in both of the above challenges in more detail we demonstrate the following functionality in bioscout scientists draw their queries graphically resulting in query graph the scientist is unaware of the query language used or of any optimization issues given the query graph the system is able to generate as first step an optimal query plan for the submitted query ii bioscout uses four different strategies to combine the optimal query plans of individual queries to generate global query plan for all the submitted queries in the demo we illustrate graphically how each of the four strategies works
reusing software artifacts for system development is showing increasing promise as an approach to reducing the time and effort involved in building new systems and to improving the software development process and the quality of its outcome however software reuse has an associated steep learning curve since practitioners must become familiar with third party rationale for representing and implementing reusable assets for this reason enabling systematic approach to the reuse process by making software reuse tasks explicit allowing software frameworks to be instantiated using pre defined primitive and complex reuse operations and supporting the reuse process in semi automated way become crucial goals in this paper we present systematic reuse approach and the reuse description language rdl language designed to specify object oriented framework instantiation processes and an rdl execution environment which is the tool support for definition and execution of reuse processes and framework instantiations that lead to domain specific applications we illustrate our approach using dtframe framework for creating drawing editors
offering online personalized recommendation services helps improve customer satisfaction conventionally recommendation system is considered as success if clients purchase the recommended products however the act of purchasing itself does not guarantee satisfaction and truly successful recommendation system should be one that maximizes the customer's after use gratification by employing an innovative associative classification method we are able to predict customer's ultimate pleasure based on customer's characteristics product will be recommended to the potential buyer if our model predicts his her satisfaction level will be high the feasibility of the proposed recommendation system is validated through laptop inspiron
this paper draws several observations from our experiences in building support for object groups these observations actually go beyond our experiences and may apply to many other developments of object based distributed systemsour first experience aimed at building support for smalltalk object replication using the isis process group toolkit it was quite easy to achieve group transparency but we were confronted with strong mismatch between the rigidity of the process group model and the flexible nature of object interactions consequently we decided to build our own object oriented protocol framework specifically dedicated to support object groups instead of using process group toolkit we built our framework in such way that basic distributed protocols such as failure detection and multicasts are considered as first class entities directly accessible to the programmers to achieve flexible and dynamic protocol composition we had to go beyond inheritance and objectify distributed algorithmsour second experience consisted in building corba service aimed at managing group of objects written on different languages and running on different platforms this experience revealed mismatch between the asynchrony of group protocols and the synchrony of standard corba interaction mechanisms which limited the portability of our corba object group service we restricted the impact of this mismatch by encapsulating asynchrony issues inside specific messaging sub servicewe dissect the cost of object group transparency in our various implementations and we point out the recurrent sources of overheads namely message indirection marshaling unmarshaling and strong consistency
several parallel architectures such as gpus and the cell processor have fast explicitly managed on chip memories in addition to slow off chip memory they also have very high computational power with multiple levels of parallelism significant challenge in programming these architectures is to effectively exploit the parallelism available in the architecture and manage the fast memories to maximize performance in this paper we develop an approach to effective automatic data management for on chip memories including creation of buffers in on chip local memories for holding portions of data accessed in computational block automatic determination of array access functions of local buffer references and generation of code that moves data between slow off chip memory and fast local memories we also address the problem of mapping computation in regular programs to multi level parallel architectures using multi level tiling approach and study the impact of on chip memory availability on the selection of tile sizes at various levels experimental results on gpu demonstrate the effectiveness of the proposed approach
adaptation online learning by autonomous virtual characters due to interaction with human user in virtual environment is difficult and important problem in computer animation in this article we present novel multi level technique for fast character adaptation we specifically target environments where there is cooperative or competitive relationship between the character and the human that interacts with that characterin our technique distinct learning method is applied to each layer of the character's behavioral or cognitive model this allows us to efficiently leverage the character's observations and experiences in each layer this also provides convenient temporal distinction between what observations and experiences provide pertinent lessons for each layer thus the character can quickly and robustly learn how to better interact with any given unique human user relying only on observations and natural performance feedback from the environment no explicit feedback from the human our technique is designed to be general and can be easily integrated into most existing behavioral animation systems it is also fast and memory efficient
we compress storage and accelerate performance of precomputed radiance transfer prt which captures the way an object shadows scatters and reflects light prt records over many surface points transfer matrix at run time this matrix transforms vector of spherical harmonic coefficients representing distant low frequency source lighting into exiting radiance per point transfer matrices form high dimensional surface signal that we compress using clustered principal component analysis cpca which partitions many samples into fewer clusters each approximating the signal as an affine subspace cpca thus reduces the high dimensional transfer signal to low dimensional set of per point weights on per cluster set of representative matrices rather than computing weighted sum of representatives and applying this result to the lighting we apply the representatives to the lighting per cluster on the cpu and weight these results per point on the gpu since the output of the matrix is lower dimensional than the matrix itself this reduces computation we also increase the accuracy of encoded radiance functions with new least squares optimal projection of spherical harmonics onto the hemisphere we describe an implementation on graphics hardware that performs real time rendering of glossy objects with dynamic self shadowing and interreflection without fixing the view or light as in previous work our approach also allows significantly increased lighting frequency when rendering diffuse objects and includes subsurface scattering
structured documents are commonly edited using free form editor even though every string is an acceptable input it makes sense to maintain structured representation of the edited document the structured representation has number of uses structural navigation and optional structural editing structure highlighting etc the construction of the structure must be done incrementally to be efficient the time to process an edit operation should be proportional to the size of the change and ideally independent of the total size of the document we show that combining lazy evaluation and caching of intermediate partial results enables incremental parsing we build complete incremental parsing library for interactive systems with support for error correction
we present novel semi supervised boosting algorithms that incrementally build linear combinations of weak classifiers through generic functional gradient descent using both labeled and unlabeled training data our approach is based on extending information regularization framework to boosting bearing loss functions that combine log loss on labeled data with the information theoretic measures to encode unlabeled data even though the information theoretic regularization terms make the optimization non convex we propose simple sequential gradient descent optimization algorithms and obtain impressively improved results on synthetic benchmark and real world tasks over supervised boosting algorithms which use the labeled data alone and state of the art semi supervised boosting algorithm
fast and efficient page ranking mechanism for web crawling and retrieval remains as challenging issue recently several link based ranking algorithms like pagerank hits and opic have been proposed in this paper we propose novel recursive method based on reinforcement learning which considers distance between pages as punishment called distancerank to compute ranks of web pages the distance is defined as the number of average clicks between two pages the objective is to minimize punishment or distance so that page with less distance to have higher rank experimental results indicate that distancerank outperforms other ranking algorithms in page ranking and crawling scheduling furthermore the complexity of distancerank is low we have used university of california at berkeley's web for our experiments
ecommerce personalization can help web sites build and retain relationships with customers but it also raises number of privacy concerns this paper outlines the privacy risks associated with personalization and describes number of approaches to personalization system design that can reduce these risks this paper also provides an overview of the fair information practice principles and discusses how they may be applied to the design of personalization systems and introduces privacy laws and self regulatory guidelines relevant to personalization privacy risks can be reduced when personalization system designs allow for pseudonymous interactions client side data stores and task based personalization in addition interfaces that allow users to control the collection and use of their profile information can further ease privacy concerns
radio frequency identification rfid technologies are used in many applications for data collection however raw rfid readings are usually of low quality and may contain many anomalies an ideal solution for rfid data cleansing should address the following issues first in many applications duplicate readings by multiple readers simultaneously or by single reader over period of time of the same object are very common the solution should take advantage of the resulting data redundancy for data cleaning second prior knowledge about the readers and the environment eg prior data distribution false negative rates of readers may help improve data quality and remove data anomalies and desired solution must be able to quantify the degree of uncertainty based on such knowledge third the solution should take advantage of given constraints in target applications eg the number of objects in same location cannot exceed given value to elevate the accuracy of data cleansing there are number of existing rfid data cleansing techniques however none of them support all the aforementioned features in this paper we propose bayesian inference based approach for cleaning rfid raw data our approach takes full advantage of data redundancy to capture the likelihood we design an state detection model and formally prove that the state model can maximize the system performance moreover in order to sample from the posterior we devise metropolis hastings sampler with constraints mh which incorporates constraint management to clean rfid raw data with high efficiency and accuracy we validate our solution with common rfid application and demonstrate the advantages of our approach through extensive simulations
searchable symmetric encryption sse allows party to outsource the storage of its data to another party server in private manner while maintaining the ability to selectively search over it this problem has been the focus of active research in recent years in this paper we show two solutions to sse that simultaneously enjoy the following properties both solutions are more efficient than all previous constant round schemes in particular the work performed by the server per returned document is constant as opposed to linear in the size of the data both solutions enjoy stronger security guarantees than previous constant round schemes in fact we point out subtle but serious problems with previous notions of security for sse and show how to design constructions which avoid these pitfalls further our second solution also achieves what we call adaptive sse security where queries to the server can be chosen adaptively by the adversary during the execution of the search this notion is both important in practice and has not been previously consideredsurprisingly despite being more secure and more efficient our sse schemes are remarkably simple we consider the simplicity of both solutions as an important step towards the deployment of sse technologiesas an additional contribution we also consider multi user sse all prior work on sse studied the setting where only the owner of the data is capable of submitting search queries we consider the natural extension where an arbitrary group of parties other than the owner can submit search queries we formally define sse in the multi user setting and present an efficient construction that achieves better performance than simply using access control mechanisms
in this work we analyze the behavior on company internal social network site to determine which interaction patterns signal closeness between colleagues regression analysis suggests that employee behavior on social network sites snss reveals information about both professional and personal closeness while some factors are predictive of general closeness eg content recommendations other factors signal that employees feel personal closeness towards their colleagues but not professional closeness eg mutual profile commenting this analysis contributes to our understanding of how sns behavior reflects relationship multiplexity the multiple facets of our relationships with sns connections
in this paper we present new approach that incorporates semantic structure of sentences in form of verb argument structure to measure semantic similarity between sentences the variability of natural language expression makes it difficult for existing text similarity measures to accurately identify semantically similar sentences since sentences conveying the same fact or concept may be composed lexically and syntactically different inversely sentences which are lexically common may not necessarily convey the same meaning this poses significant impact on many text mining applications performance where sentence level judgment is involved the evaluation has shown that by processing sentence at its semantic level the performance of similarity measures is significantly improved
the aim in this paper is to develop method for clustering together image views of the same object class local invariant feature methods such as sift have been proven effective for image clustering however they have made either relatively little use or too complex use of geometric constraints and are confounded when the detected features are superabundant here we make two contributions aimed at overcoming these problems first we rank the sift points sift using visual saliency second we use the reduced set of sift features to construct specific hyper graph cshg model of holistic structure based on the cshg model two stage clustering method is proposed in which images are clustered according to the pairwise similarity of the graphs which is combination of the traditional similarity of local invariant feature vectors and the geometric similarity between two graphs this method comprehensively utilizes both sift and geometric constraints and hence combines both global and local information experiments reveal that the method gives excellent clustering performance
the development of wireless communication technologies offers the possibility to provide new services to the users other than the web surfing futur is wireless telecommunication company interested in designing implementing and managing wmans wireless metropolitan area networks that has launched project called luna large unwired network applications with the objective of covering main locations in the province of trento trentino alto adige italy with wireless mesh network the futur business model is based on very low cost access to the luna network offering users services with tailor made advertising in this paper we present the luna ads client side application able to provide contents together with advertising considering the usability and accessibility requirements
the file system api of contemporary systems makes programs vulnerable to tocttou time of check to time of use race conditions existing solutions either help users to detect these problems by pinpointing their locations in the code or prevent the problem altogether by modifying the kernel or its api the latter alternative is not prevalent and the former is just the first step programmers must still address tocttou flaws within the limits of the existing api with which several important tasks can not be accomplished in portable straightforward manner recently dean and hu addressed this problem and suggested probabilistic hardness amplification approach that alleviated the matter alas shortly after borisov et al responded with an attack termed filesystem maze that defeated the new approach we begin by noting that mazes constitute generic way to deterministically win many tocttou races gone are the days when the probability was small in the face of this threat we develop new user level defense that can withstand mazes and show that our method is undefeated even by much stronger hypothetical attacks that provide the adversary program with ideal conditions to win the race enjoying complete and instantaneo us knowledge about the defending program's actions and being able to perfectly synchronize accordingly the fact that our approach is immune to these unrealistic attacks suggests it can be used as simple and portable solution to large class of tocttou vulnerabilities without requiring modifications to the underlying operating system
distributed model management aims to support the wide spread sharing and usage of decision support models web services is promising technology for supporting distributed model management activities such as model creation and delivery model composition model execution and model maintenance to fulfill dynamic decision support and problem solving requests we propose web services based framework for model management called mm ws to support various activities of the model management life cycle the framework is based on the recently proposed integrated service planning and execution isp approach for web services integration we discuss encoding of domain knowledge as individual models and utilize the mm ws framework to interleave synthesis of composite models with their execution prototypical implementation with an example is used to illustrate the utility of the framework to enable distributed model management and knowledge integration benefits and issues of using the framework to support model based decision making in organizational contexts are outlined
coordinations in noun phrases often pose the problem that elliptified parts have to be reconstructed for proper semantic interpretation unfortunately the detection of coordinated heads and identification of elliptified elements notoriously lead to ambiguous reconstruction alternatives while linguistic intuition suggests that semantic criteria might play an important if not superior role in disambiguating resolution alternatives our experiments on the reannotated wsj part of the penn treebank indicate that solely morpho syntactic criteria are more predictive than solely lexico semantic ones we also found that the combination of both criteria does not yield any substantial improvement
this paper describes an approach to modeling the evolution of non secure applications into secure applications in terms of the software requirements model and software architecture model the requirements for security services are captured separately from application requirements and the security services are encapsulated in connectors in the software architecture separately from the components providing functional services the enterprise architecture is described in terms of use case models static models and dynamic models the software architecture is described in terms of components and connectors which can be deployed to distributed configurations by separating application concerns from security concerns the evolution from non secure application to secure application can be achieved with less impact on the application an electronic commerce system is described to illustrate the approach
this article reports on the development of utility based mechanism for managing sensing and communication in cooperative multisensor networks the specific application on which we illustrate our mechanism is that of glacsweb this is deployed system that uses battery powered sensors to collect environmental data related to glaciers which it transmits back to base station so that it can be made available world wide to researchers in this context we first develop sensing protocol in which each sensor locally adjusts its sensing rate based on the value of the data it believes it will observe the sensors employ bayesian linear model to decide their sampling rate and exploit the properties of the kullback leibler divergence to place an appropriate value on the data then we detail communication protocol that finds optimal routes for relaying this data back to the base station based on the cost of communicating it derived from the opportunity cost of using the battery power for relaying data finally we empirically evaluate our protocol by examining the impact on efficiency of static network topology dynamic network topology the size of the network the degree of dynamism of the environment and the mobility of the nodes in so doing we demonstrate that the efficiency gains of our new protocol over the currently implemented method over month period are percnt percnt percnt and percnt respectively furthermore we show that our system performs at percnt percnt percnt and percnt of the theoretical optimal respectively despite being distributed protocol that operates with incomplete knowledge of the environment
information systems support data privacy by constraining user's access to public views and thereby hiding the non public underlying data the privacy problem is to prove that none of the private data can be inferred from the information which is made public we present formal definition of the privacy problem which is based on the notion of certain answer then we investigate the privacy problem in the contexts of relational databases and ontology based information systems
object petri nets opns provide natural and modular method for modelling many real world systems we give structure pre serving translation of opns to prolog by encoding the opn semantics avoiding the need for an unfolding to flat petri net the translation provides support for reference and value semantics and even allows different objects to be treated as copyable or non copyable the method is developed for opns with arbitrary nesting we then apply logic programming tools to animate compile and model check opns in particular we use the partial evaluation system logen to produce an opn compiler and we use the model checker xtl to verify ctl formulae we also use logen to produce special purpose model checkers we present two case studies along with experimental results comparison of opn translations to maude specifications and model checking is given showing that our approach is roughly twice as fast for larger systems we also tackle infinite state model checking using the ecce system
part decomposition and conversely the construction of composite objects out of individual parts have long been recognized as ubiquitous and essential mechanisms involving abstraction this applies in particular in areas such as cad manufacturing software development and computer graphics although the part of relationship is distinguished in object oriented modeling techniques it ranks far behind the concept of generalization specialization and rigorous definition of its semantics is still missing in this paper we first show in which ways shift in emphasis on the part of relationship leads to analysis and design models that are easier to understand and to maintain we then investigate the properties of part of relationships in order to define their semantics this is achieved by means of categorization of part of relationships and by associating semantic constraints with individual categories we further suggest precise and compared with existing techniques less redundant specification of constraints accompanying part of categories based on the degree of exclusiveness and dependence of parts on composite objects although the approach appears generally applicable the object oriented unified modeling language uml is used to present our findings several examples demonstrate the applicability of the categories introduced
distributed denial of service ddos is major threat to the availability of internet services the anonymity allowed by ip networking together with the distributed large scale nature of the internet makes ddos attacks stealthy and difficult to counter to make the problem worse attack traffic is often indistinguishable from normal traffic as various attack tools become widely available and require minimum knowledge to operate automated anti ddos systems become increasingly important many current solutions are either excessively expensive or require universal deployment across many administrative domains this paper proposes two perimeter based defense mechanisms for internet service providers isps to provide the anti ddos service to their customers these mechanisms rely completely on the edge routers to cooperatively identify the flooding sources and establish rate limit filters to block the attack traffic the system does not require any support from routers outside or inside of the isp which not only makes it locally deployable but also avoids the stress on the isp core routers we also study new problem of perimeter based ip traceback and provide three solutions we demonstrate analytically and by simulations that the proposed defense mechanisms react quickly in blocking attack traffic while achieving high survival ratio for legitimate traffic even when percent of all customer networks attack the survival ratio for traffic from the other customer networks is still close to percent
query suggestion aims to suggest relevant queries for given query which help users better specify their information needs previously the suggested terms are mostly in the same language of the input query in this paper we extend it to cross lingual query suggestion clqs for query in one language we suggest similar or relevant queries in other languages this is very important to scenarios of cross language information retrieval clir and cross lingual keyword bidding for search engine advertisement instead of relying on existing query translation technologies for clqs we present an effective means to map the input query of one language to queries of the other language in the query log important monolingual and cross lingual information such as word translation relations and word co occurrence statistics etc are used to estimate the cross lingual query similarity with discriminative model benchmarks show that the resulting clqs system significantly out performs baseline system based on dictionary based query translation besides the resulting clqs is tested with french to english clir tasks on trec collections the results demonstrate higher effectiveness than the traditional query translation methods
data mining research typically assumes that the data to be analyzed has been identified gathered cleaned and processed into convenient form while data mining tools greatly enhance the ability of the analyst to make data driven discoveries most of the time spent in performing an analysis is spent in data identification gathering cleaning and processing the data similarly schema mapping tools have been developed to help automate the task of using legacy or federated data sources for new purpose but assume that the structure of the data sources is well understood however the data sets to be federated may come from dozens of databases containing thousands of tables and tens of thousands of fields with little reliable documentation about primary keys or foreign keyswe are developing system bellman which performs data mining on the structure of the database in this paper we present techniques for quickly identifying which fields have similar values identifying join paths estimating join directions and sizes and identifying structures in the database the results of the database structure mining allow the analyst to make sense of the database content this information can be used to eg prepare data for data mining find foreign key joins for schema mapping or identify steps to be taken to prevent the database from collapsing under the weight of its complexity
bansal and sviridenko bansal sviridenko new approximability and inapproximability results for dimensional bin packing in proceedings of the th annual acm siam symposium on discrete algorithms soda pp proved that there is no asymptotic ptas for dimensional orthogonal bin packing without rotations unless np we show that similar approximation hardness results hold for several and dimensional rectangle packing and covering problems even if rotations by ninety degrees are allowed moreover for some of these problems we provide explicit lower bounds on asymptotic approximation ratio of any polynomial time approximation algorithm our hardness results apply to the most studied case of dimensional problems with unit square bins and for dimensional strip packing and covering problems with strip of unit square base
an understanding of the topological structure of the internet is needed for quite number of networking tasks making decisions about peering relationships choice of upstream providers inter domain traffic engineering one essential component of these tasks is the ability to predict routes in the internet however the internet is composed of large number of independent autonomous systems ases resulting in complex interactions and until now no model of the internet has succeeded in producing predictions of acceptable accuracywe demonstrate that there are two limitations of prior models they have all assumed that an autonomous system as is an atomic structure it is not and ii models have tended to oversimplify the relationships between ases our approach uses multiple quasi routers to capture route diversity within the ases and is deliberately agnostic regarding the types of relationships between ases the resulting model ensures that its routing is consistent with the observed routes exploiting large number of observation points we show that our model provides accurate predictions for unobserved routes first step towards developing structural mod els of the internet that enable real applications
in standard text retrieval systems the documents are gathered and indexed on single server in distributed information retrieval dir the documents are held in multiple collections answers to queries are produced by selecting the collections to query and then merging results from these collections however in most prior research in the area collections are assumed to be disjoint in this paper we investigate the effectiveness of different combinations of server selection and result merging algorithms in the presence of duplicates we also test our hash based method for efficiently detecting duplicates and near duplicates in the lists of documents returned by collections our results based on two different designs of test data indicate that some dir methods are more likely to return duplicate documents and show that removing such redundant documents can have significant impact on the final search effectiveness
distributed hash tables dhts excel at exact match lookups but they do not directly support complex queries such as semantic search that is based on content in this paper we propose novel approach to efficient semantic search on dht overlays the basic idea is to place indexes of semantically close files into same peer nodes with high probability by exploiting information retrieval algorithms and locality sensitive hashing query for retrieving semantically close files is answered with high recall by consulting only small number eg of nodes that stores the indexes of the files semantically close to the query our approach adds only index information to peer nodes imposing only small storage overhead via detailed simulations we show that our approach achieves high recall for queries at very low cost ie the number of nodes visited for query is about independent of the overlay size
we propose notion of deterministic association rules for ordered data we prove that our proposed rules can be formally justified by purely logical characterization namely natural notion of empirical horn approximation for ordered data which involves background horn conditions these ensure the consistency of the propositional theory obtained with the ordered context the whole framework resorts to concept lattice models from formal concept analysis but adapted to ordered contexts we also discuss general method to mine these rules that can be easily incorporated into any algorithm for mining closed sequences of which there are already some in the literature
this paper presents novel architectural technique to hide fetch latency overhead of hardware encrypted and authenticated memory number of recent secure processor designs have used memory block encryption and authentication to protect un trusted external memory however the latency overhead of certain encryption modes or authentication schemes can be intolerably high this paper proposes novel techniques called frequent value ciphertext speculation and frequent value mac speculation that synergistically combine value prediction and the inherently pipelined cryptography hardware to address the issue of latency for memory decryption and authentication without sacrificing security frequent value ciphertext speculation can speed up memory decryption or mac integrity verification by speculatively encrypting predictable memory values and comparing the result ciphertext with the fetched ciphertext in mac speculation secure processor pre computes mac for speculated frequent values and compares the mac result with the fetched mac from memory using spec benchmark suite and detailed architecture simulator our results show that ciphertext speculation and mac speculation can significantly improve performance for direct memory encryption modes based on only most frequent bit values for eight benchmark programs the speedup is over and some benchmark programs achieve more than speedup for counter mode encrypted memory mac speculation can also significantly reduce the authentication overhead
an abstract type groups variables that are used for related purposes in program we describe dynamic unification based analysis for inferring abstract types initially each run time value gets unique abstract type run time interaction among values indicates that they have the same abstract type so their abstract types are unified also at run time abstract types for variables are accumulated from abstract types for values the notion of interaction may be customized permitting the analysis to compute finer or coarser abstract types these different notions of abstract type are useful for different tasks we have implemented the analysis for compiled binaries and for java bytecodes our experiments indicate that the inferred abstract types are useful for program comprehension improve both the results and the run time of follow on program analysis and are more precise than the output of comparable static analysis without suffering from overfitting
context the workshop was held to explore the potential for adapting the ideas of evidence based practices as used in medicine and other disciplines for use in software engineeringobjectives to devise ways of developing suitable evidence based practices and procedures especially the use of structured literature reviews and introducing these into software engineering research and practicemethod three sessions were dedicated to mix of presentations based on position papers and interactive discussion while the fourth focused upon the key issues as decided by the participantsresults an initial scoping of the major issues identification of useful parallels and some plans for future development of an evidence based software engineering communityconclusions while there are substantial challenges to introducing evidence based practices there are useful experiences to be drawn from variety of other domains
in multi hop wireless networks the mobile nodes usually act as routers to relay packets generated from other nodes however selfish nodes do not cooperate but make use of the honest ones to relay their packets which has negative effect on fairness security and performance of the network in this paper we propose novel incentive mechanism to stimulate cooperation in multi hop wireless networks fairness can be achieved by using credits to reward the cooperative nodes the overhead can be significantly reduced by using cheating detection system cds to secure the payment extensive security analysis demonstrates that the cds can identify the cheating nodes effectively under different cheating strategies simulation results show that the overhead of the proposed incentive mechanism is incomparable with the existing ones
finite automaton simply referred to as robot has to explore graph whose nodes are unlabeled and whose edge ports are locally labeled at each node the robot has no priori knowledge of the topology of the graph or of its size its task is to traverse all the edges of the graph we first show that for any state robot and any there exists planar graph of maximum degree with at most nodes that the robot cannot explore this bound improves all previous bounds in the literature more interestingly we show that in order to explore all graphs of diameter and maximum degree robot needs log memory bits even if we restrict the exploration to planar graphs this latter bound is tight indeed simple dfs up to depth enables robot to explore any graph of diameter and maximum degree using memory of size log bits we thus prove that the worst case space complexity of graph exploration is log bits
access control models are usually static ie permissions are granted based on policy that only changes seldom especially for scenarios in health care and disaster management more flexible support of access control ie the underlying policy is needed break glass is one approach for such flexible support of policies which helps to prevent system stagnation that could harm lives or otherwise result in losses today break glass techniques are usually added on top of standard access control solutions in an ad hoc manner and therefore lack an integration into the underlying access control paradigm and the systems access control enforcement architecture we present an approach for integrating in fine grained manner break glass strategies into standard access control models and their accompanying enforcement architecture this integration provides means for specifying break glass policies precisely and supporting model driven development techniques based on such policies
we present new linear time algorithm to compute good order for the point set of delaunay triangulation in the plane such good order makes reconstruction in linear time with simple algorithm possible similarly to the algorithm of snoeyink and van kreveld proceedings of th european symposium on algorithms esa pp our algorithm constructs such orders in logn phases by repeatedly removing constant fraction of vertices from the current triangulation compared to proceedings of th european symposium on algorithms esa pp we improve the guarantee on the number of removed vertices in each such phase if we restrict the degree of the points at the time they are removed to our algorithm removes at least of the points while the algorithm from proceedings of th european symposium on algorithms esa pp gives guarantee of we achieve this improvement by removing the points sequentially using breadth first search bfs based procedure that in contrast to proceedings of th european symposium on algorithms esa pp does not necessarily remove an independent set besides speeding up the algorithm removing more points in single phase has the advantage that two consecutive points in the computed order are usually closer to each other for this reason we believe that our approach is better suited for vertex coordinate compression we implemented prototypes of both algorithms and compared their running time on point sets uniformly distributed in the unit cube our algorithm is slightly faster to compare the vertex coordinate compression capabilities of both algorithms we round the resulting sequences of vertex coordinates to bit integers and compress them with simple variable length code our algorithm achieves about better vertex data compression than the algorithm from proceedings of th european symposium on algorithms esa pp
we investigate proof rules for information hiding using the recent formalism of separation logic in essence we use the separating conjunction to partition the internal resources of module from those accessed by the module's clients the use of logical connective gives rise to form of dynamic partitioning where we track the transfer of ownership of portions of heap storage between program components it also enables us to enforce separation in the presence of mutable data structures with embedded addresses that may be aliased
in order to utilize the potential advantages of replicated collaborative cad system to support natural free fast and less constrained multi user human to human interaction local locking mechanism which can provide fast modeling response is adopted as concurrency control mechanism for replicated on line collaborative cad system human human interactive modeling is achieved by immediate local execution of modeling operations and exchange of modeling operations across all collaborative sites real time in particular an approach to achieve topological entity correspondence across collaborative sites during modeling procedure which is critical to guarantee the correctness and consistency of collaborative modeling result is proposed prototype system based on the acis geometric modeling kernel is implemented to verify availability of the proposed solution
in any collaborative system there are both symmetries and asymmetries present in the design of the technology and in the ways that technology is appropriated yet media space research tends to focus more on supporting and fostering the symmetries than the asymmetries throughout more than years of media space research the pursuit of increased symmetry whether achieved through technical or social means has been recurrent theme the research literature on the use of contemporary awareness systems in contrast displays little if any of this emphasis on symmetrical use indeed this body of research occasionally highlights the perceived value of asymmetry in this paper we unpack the different forms of asymmetry present in both media spaces and contemporary awareness systems we argue that just as asymmetry has been demonstrated to have value in contemporary awareness systems so might asymmetry have value in media spaces and in other cscw systems more generally to illustrate we present media space that emphasizes and embodies multiple forms of asymmetry and does so in response to the needs of particular work context
compilers for polymorphic languages can use runtime type inspection to support advanced implementation techniques such as tagless garbage collection polymorphic marshalling and flattened data structures intensional type analysis is type theoretic framework for expressing and certifying such type analyzing computations unfortunately existing approaches to intensional analysis do not work well on types with universal existential or fixpoint quantifiers this makes it impossible to code applications such as garbage collection persistence or marshalling which must be able to examine the type of any runtime valuewe present typed intermediate language that supports fully reflexive intensional type analysis by fully reflexive we mean that type analyzing operations are applicable to the type of any runtime value in the language in particular we provide both type level and term level constructs for analyzing quantified types our system supports structural induction on quantified types yet type checking remains decidable we show how to use reflexive type analysis to support type safe marshalling and how to generate certified type analyzing object code
efficient subsumption checking deciding whether subscription or publication is covered by set of previously defined subscriptions is of paramount importance for publish subscribe systems it provides the core system functionality matching of publications to subscriber needs expressed as subscriptions and additionally reduces the overall system load and generated traffic since the covered subscriptions are not propagated in distributed environments as the subsumption problem was shown previously to be co np complete and existing solutions typically apply pairwise comparisons to detect the subsumption relationship we propose monte carlo type probabilistic algorithm for the general subsumption problem it determines whether publication subscription is covered by disjunction of subscriptions in where is the number of subscriptions is the number of distinct attributes in subscriptions and is the number of tests performed to answer subsumption question the probability of error is problem specific and typically very small and sets an upper bound on our experimental results show significant gains in term of subscription set reduction which has favorable impact on the overall system performance as it reduces the total computational costs and networking traffic furthermore the expected theoretical bounds underestimate algorithm performance because it performs much better in practice due to introduced optimizations and is adequate for fast forwarding of subscriptions in case of high subscription rate
previous research has shown that staged execution se ie dividing program into segments and executing each segment at the core that has the data and or functionality to best run that segment can improve performance and save power however se's benefit is limited because most segments access inter segment data ie data generated by the previous segment when consecutive segments run on different cores accesses to inter segment data incur cache misses thereby reducing performance this paper proposes data marshaling dm new technique to eliminate cache misses to inter segment data dm uses profiling to identify instructions that generate inter segment data and adds only bytes core of storage overhead we show that dm significantly improves the performance of two promising staged execution models accelerated critical sections and producer consumer pipeline parallelism on both homogeneous and heterogeneous multi core systems in both models dm can achieve almost all of the potential of ideally eliminating cache misses to inter segment data dm's performance benefit increases with the number of cores
power dissipation limits have emerged as major constraint in the design of microprocessors this is true not only at the low end where cost and battery life are the primary drivers but also now at the midrange and high end system server level thus the ability to estimate power consumption at the high level during the early stage definition and trade off studies is key new methodology enhancement sought by design and performance architects we first review the fundamentals in terms of power estimation and power performance trade offs at the microarchitecture level we then discuss the opportunities of saving power that can be exposed via microarchitecture level modeling in particular the potential savings that can be effected through straightforward clock gating techniques is cited as an example we also describe some future ideas and trends in power efficient processor design examples of how microarchitectural observations can be used toward power saving circuit design optimizations are described the design and modeling challenges are in the context of work in progress within ibm research this research is in support of future high end processor development within ibm
federated system is popular paradigm for multiple organizations to collaborate and share resources for common benefits in practice however these organizations are often within different administrative domains and thus demand autonomous resource management as opposed to blindly exporting their resources for efficient search to address these challenges we present new resource discovery middleware called roads that can facilitate voluntary sharing in roads the participants can associate with each other at their own will and dictate the extent of sharing by properly exporting summaries condensed representation of their resource records to enable efficient search these summaries are aggregated and replicated along an overlay assisted server hierarchy and the queries are routed to those servers that are likely to hold the desired resources our experimental results show that roads provides not only flexible resource sharing for federated systems but also efficient resource discovery with performance comparable to centrally managed system
in this paper we give brief history on conceptual modeling in computer science and we discuss state of the art approaches it is claimed that number of problems remain to be solved schema first is no longer viable approach to meet the data needs in the dynamic world of the internet and web services this is also true for the schema later approach simply because data and data sources constantly change in depth and breadth and can neither wait for the schema to be completed nor for the data to be collected as solution we advocate for new schema during approach in which the process of conceptual modeling is in continuum with the operations in the database
this paper is proposing new platform for implementing services in future service oriented architectures the basic premise of our proposal is that by combining the large volume of uncontracted resources with small clusters of dedicated resources we can dramatically reduce the amount of dedicated resources while the goodput provided by the overall system remains at high level this paper presents particular strategies for implementing this idea for particular class of applications we performed very detailed simulations on synthetic and real traces to evaluate the performance of the proposed strategies our findings on compute intensive applications show that preemptive reallocation of resources is necessary for assured services the proposed preemption based scheduling heuristic can significantly improve utilization of the dedicated resources by opportunistically offloading the peak loads on uncontracted resources while keeping the service quality virtually unaffected
we describe work in progress on tools and infrastructure to support adaptive component based software for mobile devices in our case apple iphones our high level aim is design for appropriation ie system design for uses and contexts that designers may not be able to fully predict or model in advance logs of users system operation are streamed back in real time to evaluators data visualisation tools so that they can assess design problems and opportunities evaluators and developers can then create new software components that are sent to the mobile devices these components are either integrated automatically on the fly or offered as recommendations for users to accept or reject by connecting developers users and evaluators we aim to quicken the pace of iterative design so as to improve the process of creating and sustaining contextually fitting software
feature subset selection has become an important challenge in areas of pattern recognition machine learning and data mining as different semantics are hidden in numerical and categorical features there are two strategies for selecting hybrid attributes discretizing numerical variables or numericalize categorical features in this paper we introduce simple and efficient hybrid attribute reduction algorithm based on generalized fuzzy rough model theoretic framework of fuzzy rough model based on fuzzy relations is presented which underlies foundation for algorithm construction we derive several attribute significance measures based on the proposed fuzzy rough model and construct forward greedy algorithm for hybrid attribute reduction the experiments show that the technique of variable precision fuzzy inclusion in computing decision positive region can get the optimal classification performance number of the selected features is the least but accuracy is the best
recent work has focused on creating models for generating traveler behavior for micro simulations with the increase in hand held computers and gps devices there is likely to be an increasing demand for extending this idea to predicting an individual's future travel plans for devices such as smart traveler's assistant in this work we introduce technique based on sequential data mining for predicting multiple aspects of an individual's next activity using combination of user history and their similarity to other travelers the proposed technique is empirically shown to perform better than more traditional approaches to this problem
we propose precomputation based approach for the real time rendering of scenes that include number of complex illumination phenomena such as radiosity and subsurface scattering and allows interactive modification of camera and lighting parameters at the heart of our approach lies novel parameterization of the rendering equation that is inherently supported by the modern gpu during the pre computation phase we build set of offset transfer maps based on the proposed parameterization which approximate the complete radiance transfer function for the scene the rendering phase is then reduced to set of texture blending and mapping operations that execute in real time on the gpu in contrast to the current state of the art which employs environment maps to produce global illumination our approach uses arbitrary first order lighting to compute final lighting solution and fully supports point and spot lights to discretize the transfer maps we develop an efficient method for generating and sampling continuous probability density functions from unordered data pointswe believe that the contributions of this paper offer significantly different approach to precomputed radiance transfer from those previously proposed
highly competitive and open environments should encompass mechanisms that will assist service providers sps in accounting for their interests ie offering at given period of time the adequate quality services in cost efficient manner assuming that user wishes to access specific service composed of distinct set of service tasks which can be served by various candidate service nodes csns problem that should be addressed is the assignment of service tasks to the most appropriate service nodes the pertinent problem is concisely defined optimally formulated and evaluated through simulation experiments on real network test bed
the postures character adopts over time are key expressive aspect of its movement while ik tools help character achieve positioning constraints there are few tools that help an animator with the expressive aspects of character's poses three aspects are required in good pose design achieving set of world space constraints finding body shape that reflects the character's inner state and personality and making adjustments to balance that act to strengthen the pose and also maintain realism this is routinely done in the performing arts but is uncommon in computer graphics our system combines all three components within single body shape solver the system combines feedback based balance control with hybrid ik system that utilizes optimization based and analytic ik components the ik system has been carefully designed to allow direct control over various aesthetically important aspects of body shape such as the type of curve in the spine and the relationship between the collar bones the system allows for both low level control and for higher level shape sets to be defined and used shape sets allow an animator to use single scalar to vary character's pose within specified shape class providing an intuitive parameterization of posture changing shape set allows an animator to quickly experiment with different posture options for movement sequence supporting rapid exploration of the aesthetic space
we introduce new sublinear space data structure the count min sketch for summarizing data streams our sketch allows fundamental queries in data stream summarization such as point range and inner product queries to be approximately answered very quickly in addition it can be applied to solve several important problems in data streams such as finding quantiles frequent items etc the time and space bounds we show for using the cm sketch to solve these problems significantly improve those previously known typically from to in factor
previous object code compression schemes have employed static and semiadaptive compression algorithms to reduce the size of instruction memory in embedded systems the suggestion by number of researchers that adaptive compression techniques are unlikely to yield satisfactory results for code compression has resulted in virtually no investigation of their application to that domain this paper presents new adaptive approach to code compression which operates at the granularity of program's cache lines where the context for compression is determined by an analysis of control flow in the code being compressed we introduce novel data structure the compulsory miss tree that is used to identify partial order in which compulsory misses will have occurred in an instruction cache whenever cache miss occurs this tree is used as basis for dynamically building and maintaining an lzw dictionary for compression decompression of individual instruction cache lines we applied our technique to eight benchmarks taken from the mibench and mediabench suites which were compiled with size optimization and subsequently compacted using link time optimizer prior to compressionresults from our experiments demonstrate object code size elimination averaging between and of the original linked code size depending on the cache line length under inspection
it is generally accepted that the management of imprecision and vagueness will yield more intelligent and realistic knowledge based applications in this paper we present fuzzy description logics framework based on certainty lattices our main feature is that an assertion is not just true or false like in classical description logics but certain to some degree where the certainty value is taken from certainty lattice we extend the well known fuzzy description logic based on fuzzy set theory shin to the fuzzy description logic based on certainty lattices theory shin the syntax semantics and logical properties of the shin are given and sound complete and terminating tableaux algorithm for deciding fuzzy abox consistency wrt rbox for the shin is presented in this paper various extensions of fuzzy description logics over lattices are also discussed
we extend fagin's result on the equivalence between functional dependencies in relational databases and propositional horn clauses it is shown that this equivalence still holds for functional dependencies in databases that support complex values via nesting of records lists sets and multisetsthe equivalence has several implications firstly it extends well known result from relational databases to databases which are not in first normal form secondly it characterises the implication of functional dependencies in complex value databases in purely logical terms the database designer can take advantage of this equivalence to reduce database design problems to simpler problems in propositional logic an algorithm is presented for such an application furthermore relational database design tools can be reused to solve problems for complex value databases
web service selection enables user to find the most desirable service based on his her preferences however user preferences in real world can be either incomplete or inconsistent such that service selection cannot be conducted properly this paper presents system to facilitate web service selection in face of incomplete or inconsistent user preferences the system utilizes the information of historical users to amend the active user's preference so as to improve the results of service selection we present detailed design of the system and verify its efficiency through extensive experiments
this paper presents scalable framework for real time raycasting of large unstructured volumes that employs hybrid bricking approach it adaptively combines original unstructured bricks in important focus regions with structured bricks that are resampled on demand in less important context regions the basis of this focus context approach is interactive specification of scalar degree of interest doi function thus rendering always considers two volumes simultaneously scalar data volume and the current doi volume the crucial problem of visibility sorting is solved by raycasting individual bricks and compositing in visibility order from front to back in order to minimize visual errors at the grid boundary it is always rendered accurately even for resampled bricks variety of different rendering modes can be combined including contour enhancement very important property of our approach is that it supports variety of cell types natively ie it is not constrained to tetrahedral grids even when interpolation within cells is used moreover our framework can handle multi variate data eg multiple scalar channels such as temperature or pressure as well as time dependent data the combination of unstructured and structured bricks with different quality characteristics such as the type of interpolation or resampling resolution in conjunction with custom texture memory management yields very scalable system
recently there is growing interest in the design development and deployment of sensor systems for applications of high level inference which leads to an increasing demand on connecting internet protocol ip network users to wireless sensor networks and accessing the available services and applications in this paper we first identify key requirements of designing an efficient and flexible architecture for integrating ip and sensor networks based on the requirements we outline an ip and sensor network integration architecture ipsa which provides ip mobility support and universal interface for ip mobile users to access sensor networks while considering application specific requirements with on demand processing code assigned to the middleware layer of an ip mobile user ipsa provides the flexibility for enabling diverse applications and manages network resources efficiently without additional requirements on sensor nodes except for the limited additional hardware requirement at ip mobile nodes
mining data streams of changing class distributions is important for real time business decision support the stream classifier must evolve to reflect the current class distribution this poses serious challenge on the one hand relying on historical data may increase the chances of learning obsolete models on the other hand learning only from the latest data may lead to biased classifiers as the latest data is often an unrepresentative sample of the current class distribution the problem is particularly acute in classifying rare events when for example instances of the rare class do not even show up in the most recent training data in this paper we use stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one from the historical and the current training data that we have observed find the most likely current distribution and learn classifier based on the most likely distribution we derive an analytic solution and approximate this solution with an efficient algorithm which calibrates the influence of historical data carefully to create an accurate classifier we evaluate our algorithm with both synthetic and real world datasets our results show that our algorithm produces accurate and efficient classification
recent work is beginning to reveal the potential of numerical optimization as an approach to generating interfaces and displays optimization based approaches can often allow mix of independent goals and constraints to be blended in ways that would be difficult to describe algorithmically while optimization based techniques appear to offer several potential advantages further research in this area is hampered by the lack of appropriate tools this paper presents gadget an experimental toolkit to support optimization for interface and display generation gadget provides convenient abstractions of many optimization concepts gadget also provides mechanisms to help programmers quickly create optimizations including an efficient lazy evaluation framework powerful and configurable optimization structure and library of reusable components together these facilities provide an appropriate tool to enable exploration of new class of interface and display generation techniques
real time garbage collection rtgc has recently advanced to the point where it is being used in production for financial trading military command and control and telecommunications however among potential users of rtgc there is enormous diversity in both application requirements and deployment environments previously described rtgcs tend to work well in narrow band of possible environments leading to fragile systems and limiting adoption of real time garbage collection technology this paper introduces collector scheduling methodology called tax and spend and the collector design revisions needed to support it tax and spend provides general mechanism which works well across variety of application machine and operating system configurations tax and spend subsumes the predominant pre existing rtgc scheduling techniques it allows different policies to be applied in different contexts depending on the needs of the application virtual machines can co exist compositionally on single machine we describe the implementation of our system metronome ts as an extension of the metronome collector in ibm's real time virtual machine product and we evaluate it running on an way smp blade with real time linux kernel compared to the state of the art metronome system on which it is based implemented in the identical infrastructure it achieves almost shorter latencies comparable utilization at shorter time window and mean throughput improvements of
object oriented and object relational dbms support set valued attributes which are natural and concise way to model complex information however there has been limited research to date on the evaluation of query operators that apply on sets in this paper we study the join of two relations on their set valued attributes various join types are considered namely the set containment set equality and set overlap joins we show that the inverted file powerful index for selection queries can also facilitate the efficient evaluation of most join predicates we propose join algorithms that utilize inverted files and compare them with signature based methods for several set comparison predicates
power management is an important problem in battery powered sensor networks as the sensors are required to operate for long time usually several weeks to several months one of the challenges in developing power management protocols for sensor networks is prototyping specifically existing programming platforms for sensor networks eg nesc tinyos use an event driven programming model and hence require the designers to be responsible for stack management buffer management flow control etc therefore the designers simplify prototyping their solutions either by implementing their own discrete event simulators or by modeling them in specialized simulators to enable the designers to prototype power management protocols in target platform eg nesc tinyos in this paper we use prose programming tool for sensor networks prose enables the designers to specify their programs in simple abstract models while hiding low level challenges of sensor networks and programming level challenges as case study in this paper we specify power management protocol with prose automatically generate the corresponding nesc tinyos code and evaluate its performance based on the performance results we expect that prose enables the designers to rapidly prototype quickly deploy and easily evaluate their protocols
this paper describes simple and effective quadratic placement algorithm called rql we show that good quadratic placement followed by local wirelength driven spreading can produce excellent results on large scale industrial asic designs as opposed to the current top performing academic placers rql does not embed linearization technique within the solver instead it only requires simpler pure quadratic objective function in the spirit of experimental results show that rql outperforms all available academic placers on the ispd placement contest benchmarks in particular rql obtains an average wire length improvement of and versus mpl ntuplace kraftwerk aplace and capo respectively in addition rql is three seven and ten times faster than mpl capo and aplace respectively on the ispd placement contest benchmarks on average rql obtains the best scaled wirelength among all available academic placers
real time content based access to live video data requires content analysis applications that are able to process the video data at least as fast as the video data is made available to the application and with an acceptable error rate statements as this express quality of service qos requirements to the application in order to provide some level of control of the qos provided the video content analysis application must be scalable and resource aware so that requirements of timeliness and accuracy can be met by allocating additional processing resourcesin this paper we present general architecture of video content analysis applications including model for specifying requirements of timeliness and accuracy the salient features of the architecture include its combination of probabilistic knowledge based media content analysis with qos and distributed resource management to handle qos requirements and its independent scalability at multiple logical levels of distribution we also present experimental results with an algorithm for qos aware selection of configurations of feature extractor and classification algorithms that can be used to balance requirements of timeliness and accuracy against available processing resources experiments with an implementation of real time motion vector based object tracking application demonstrate the scalability of the architecture
we present an efficient algorithm to approximate the swept volume sv of complex polyhedron along given trajectory given the boundary description of the polyhedron and path specified as parametric curve our algorithm enumerates superset of the boundary surfaces of sv it consists of ruled and developable surface primitives and the sv corresponds to the outer boundary of their arrangement we approximate this boundary by using five stage pipeline this includes computing bounded error approximation of each surface primitive computing unsigned distance fields on uniform grid classifying all grid points using fast marching front propagation iso surface reconstruction and topological refinement we also present novel and fast algorithm for computing the signed distance of surface primitives as well as number of techniques based on surface culling fast marching level set methods and rasterization hardware to improve the performance of the overall algorithm we analyze different sources of error in our approximation algorithm and highlight its performance on complex models composed of thousands of polygons in practice it is able to compute bounded error approximation in tens of seconds for models composed of thousands of polygons sweeping along complex trajectory
accurate feature detection and localization is fundamentally important to computer vision and feature locations act as input to many algorithms including camera calibration structure recovery and motion estimation unfortunately feature localizers in common use are typically not projectively invariant even in the idealized case of continuous image this results in feature location estimates that contain bias which can influence the higher level algorithms that make use of them while this behavior has been studied in the case of ellipse centroids and then used in practical calibration algorithm those results do not trivially generalize to the center of mass of radially symmetric intensity distribution this paper introduces the generalized result of feature location bias with respect to perspective distortion and applies it to several specific radially symmetric intensity distributions the impact on calibration is then evaluated finally an initial study is conducted comparing calibration results obtained using center of mass to those obtained with an ellipse detector results demonstrate that feature localization error over range of increasingly large projective distortions can be stabilized at less than tenth of pixel versus errors that can grow to larger than pixel in the uncorrected case
in order to maintain performance per watt in microprocessors there is shift towards the chip level multiprocessing paradigm microprocessor manufacturers are experimenting with tens of cores forecasting the arrival of hundreds of cores per single processor die in the near future with such large scale integration and increasing power densities thermal management continues to be significant design effort to maintain performance and reliability in modern process technologies in this paper we present two mechanisms to perform frequency scaling as part of dynamic frequency and voltage scaling dvfs to assist dynamic thermal management dtm our frequency selection algorithms incorporate the physical interaction of the cores on large scale system onto the emergency intervention mechanisms for temperature reduction of the hotspot while aiming to minimize the performance impact of frequency scaling on the core that is in thermal emergency our results show that our algorithm consistently succeeds in maximizing the operating frequency of the most critical core while successfully relieving the thermal emergency of the core comparison of our two alternative techniques reveals that our physical aware criticality based algorithm results in faster clock frequencies compared to our aggressive scaling algorithm we also show that our technique is extremely fast and is suited for real time thermal management
as means of transmitting not only data but also code encapsulated within functions higher order channels provide an advanced form of task parallelism in parallel computations in the presence of mutable references however they pose safety problem because references may be transmitted to remote threads where they are no longer valid this paper presents an ml like parallel language with type safe higher order channels by type safety we mean that no value written to channel contains references or equivalently that no reference escapes via channel from the thread where it is created the type system uses typing judgment that is capable of deciding whether the value to which term evaluates contains references or not the use of such typing judgment also makes it easy to achieve another desirable feature of channels channel locality that associates every channel with unique thread for serving all values addressed to it our type system permits mutable references in sequential computations and also ensures that mutable references never interfere with parallel computations thus it provides both flexibility in sequential programming and ease of implementing parallel computations
microarchitectures increasingly rely on dynamic optimization to improve performance in ways that are dif ficult or impossible for ahead of time compilers dynamic optimizers in turn require continuous portable low cost and accurate control flow profiles to inform their decisions but prior approaches have struggled to meet these goals simultaneously this paper presents pep hybrid instrumentation and sampling approach for continuous path and edge profiling that is efficient accurate and portable pep uses subset of ball larus path profiling to identify paths with low overhead and uses sampling to mitigate the expense of storing paths pep further reduces overhead by using profiling to guide instrumentation placement pep improves profile accuracy with modified version of arnold grove sampling the resulting system has average and maximum overhead path profile accuracy and edge profile accuracy on set of java benchmarks
query rewriting using views is technique that allows query to be answered efficiently by using pre computed materialized views it has many applications such as data caching query optimization schema integration etc this issue has been studied extensively for relational databases and as result the technology is maturing for xml data however the work is inadequate recently several frameworks have been proposed for query rewriting using views for xpath queries with the requirement that rewriting must be complete in this paper we study the problem of query rewriting using views for xpath queries without requiring that the rewriting be complete this will increase its applicability since in many cases complete rewritings using views do not exist we give formal definitions for various concepts to formulate the problem and then propose solutions our solutions are built under the framework for query containment we look into the problem from both theoretic perspectives and algorithmic approaches two methods to generate rewritings using views are proposed with different characteristics in terms of generalities and efficiencies the maximality properties of the rewritings generated by these methods are discussed
the combination of the cornea of an eye and camera viewing the eye form catadioptric mirror lens imaging system with very wide field of view we present detailed analysis of the characteristics of this corneal imaging system anatomical studies have shown that the shape of normal cornea without major defects can be approximated with an ellipsoid of fixed eccentricity and size using this shape model we can determine the geometric parameters of the corneal imaging system from the image then an environment map of the scene with large field of view can be computed from the image the environment map represents the illumination of the scene with respect to the eye this use of an eye as natural light probe is advantageous in many relighting scenarios for instance it enables us to insert virtual objects into an image such that they appear consistent with the illumination of the scene the eye is particularly useful probe when relighting faces it allows us to reconstruct the geometry of face by simply waving light source in front of the face finally in the case of an already captured image eyes could be the only direct means for obtaining illumination information we show how illumination computed from eyes can be used to replace face in an image with another one we believe that the eye not only serves as useful tool for relighting but also makes relighting possible in situations where current approaches are hard to use
new tool for shape decomposition is presented it is function defined on the shape domain and computed using linear system of equations it is demonstrated that the level curves of the new function provide hierarchical partitioning of the shape domain into visual parts without requiring any features to be estimated the new tool is an unconventional distance transform where the minimum distance to the union of the shape boundary and an unknown critical curve is computed this curve divides the shape domain into two parts one corresponding to the coarse scale structure and the other one corresponding to the fine scale structurethe connection of the new function to variety of morphological concepts skeleton by influence zone aslan skeleton and weighted distance transforms is discussed
knowing which method parameters may be mutated during method's execution is useful for many software engineering tasks parameter reference is immutable if it cannot be used to modify the state of its referent object during the method's execution we formally define this notion in core object oriented language having the formal definition enables determining correctness and accuracy of tools approximating this definition and unbiased comparison of analyses and tools that approximate similar definitionswe present pidasa tool for classifying parameter reference immutability pidasa combines several lightweight scalable analyses in stages with each stage refining the overall result the resulting analysis is scalable and combines the strengths of its component analyses as one of the component analyses we present novel dynamic mutability analysis and show how its results can be improved by random input generation experimental results on programs of up to kloc show that compared to previous approaches pidasa increases both run time performance and overall accuracy of immutability inference
in this paper we compare the simulated performance of family of multiprocessor architectures based on global shared memory the processors are connected to the memory through caches that snoop one or more shared buses in crossbar arrangement we have simulated number of configurations in order to assess the relative performance of multiple versus wide bus machines with varying amounts of prefetch four programs with widely differing characteristics were run on each configuration the configurations that gave the best all round results were multiple narrow buses with words of prefetch
the design of workflows is complicated task in those cases where the control flow between activities cannot be modeled in advance but simply occurs during enactment time run time we speak of ad hoc processes ad hoc processes allow for the flexibility needed in real life business processes since ad hoc processes are highly dynamic they represent one of the most difficult challenges both technically and conceptually caramba is one of the few process aware collaboration systems allowing for ad hoc processes unlike in classical workflow systems the users are no longer restricted by the system therefore it is interesting to study the actual way people and organizations work in this paper we propose process mining techniques and tools to analyze ad hoc processes we introduce process mining discuss the concept of mining in the context of ad hoc processes and demonstrate concrete application of the concept using caramba process mining tools such as emit and minson and newly developed extraction tool named teamlog
the proliferation of the world wide web has brought information retrieval ir techniques to the forefront of search technology to the average computer user ldquo searching rdquo now means using ir based systems for finding information on the www or in other document collections ir query evaluation methods and workloads differ significantly from those found in database systems in this paper we focus on three such differences first due to the inherent fuzziness of the natural language used in ir queries and documents an additional degree of flexibility is permitted in evaluating queries second ir query evaluation algorithms tend to have access patterns that cause problems for traditional buffer replacement policies third ir search is often an iterative process in which query is repeatedly refined and resubmitted by the user based on these differences we develop two complementary techniques to improve the efficiency of ir queries buffer aware query evaluation which alters the query evaluation process based on the current contents of buffers and ranking aware buffer replacement which incorporates knowledge of the query processing strategy into replacement decisions in detailed performance study we show that using either of these techniques yields significant performance benefits and that in many cases combining them produces even further improvements
novel junction detector is presented that fits the neighborhood around point to junction model the junction model segments the neighborhood into wedges by determining set of radial edges the radial edges are invariant to affine transforms creating an affine invariant junction detector the radial edges are evaluated based upon the pixels along the edge the angle between the pixel gradient and the vector to the potential junction point forms the initial basis for the measurement an initial set of radial edges is selected based upon identifying local maximums within given arc distance an energy function is applied to the resulting radial segmentation and greedy optimization routine is used to construct the minimal set of radial edges to identify the final junctions second energy function is used that combines the components of the first energy function with the resulting change in standard deviation by separation into radial segments the junctions with the most energy in their local neighborhoods are selected as potential junctions the neighborhoods about the potential junctions are analyzed to determine if they represent single line or multiple non parallel lines if the neighborhood represents multiple non parallel lines the point is classified as junction point the junction detector is tested on several images including both synthetic and real images highlights of radially segmented junction points are displayed for the real images
there are many human activities for which information about the geographical location where they take place is of paramount importance in the last years there has been increasing interest in the combination of computer supported collaborative work cscw and geographical information in this paper we analyze the concepts and elements of cscw that are most relevant to geocollaboration we define model facilitating the design of shared artifacts capable to build shared awareness of the geographical context the paper also describes two case studies using the model to design geocollaborative applications
many studies have shown the limits of support confidence framework used in apriori like algorithms to mine association rules there are lot of efficient implementations based on the antimonotony property of the support but candidate set generation is still costly in addition many rules are uninteresting or redundant and one can miss interesting rules like nuggets one solution is to get rid of frequent itemset mining and to focus as soon as possible on interesting rules for that purpose algorithmic properties were first studied especially for the confidence they allow all confidence rules to be found without preliminary support pruning more recently in the case of class association rules the concept of optimal rules gave pruning strategy compatible with more measures however all these properties have been demonstrated for limited number of interestingness measures we present new formal framework which allows us to make the link between analytic and algorithmic properties of the measures we apply this framework to optimal rules and we demonstrate necessary and sufficient condition of existence for this pruning strategy which can be applied to any measure
we present method for end to end out of core simplification and view dependent visualization of large surfaces the method consists of three phases memory insensitive simplification memory insensitive construction of multiresolution hierarchy and run time output sensitive view dependent rendering and navigation of the mesh the first two off line phases are performed entirely on disk and use only small constant amount of memory whereas the run time system pages in only the rendered parts of the mesh in cache coherent manner as result we are able to process and visualize arbitrarily large meshes given sufficient amount of disk space constant multiple of the size of the input meshsimilar to recent work on out of core simplification our memory insensitive method uses vertex clustering on rectilinear octree grid to coarsen and create hierarchy for the mesh and quadric error metric to choose vertex positions at all levels of resolution we show how the quadric information can be used to concisely represent vertex position surface normal error and curvature information for anisotropic view dependent coarsening and silhouette preservationthe run time component of our system uses asynchronous rendering and view dependent refinement driven by screen space error and visibility the system exploits frame to frame coherence and has been designed to allow preemptive refinement at the granularity of individual vertices to support refinement on time budgetour results indicate significant improvement in processing speed over previous methods for out of core multiresolution surface construction meanwhile all phases of the method are disk and memory efficient and are fairly straightforward to implement
as internet usage continues to expand rapidly careful attention needs to be paid to the design of internet servers for achieving high performance and end user satisfaction currently the memory system continues to remain significant performance bottleneck for internet servers employing multi ghz processors in this paper our aim is two fold to characterize the cache memory performance of web server workloads and to propose and evaluate cache design alternatives for future web servers we chose specweb as the representative web server workload and our entire characterization and evaluation methodology is based on our casper simulation framework we begin by exploring the processor cache design space for single and dual processor servers based on our observations we then evaluate other cache hierarchy alternatives such as chipset caches coherence filters and decompressed page stores we show the sensitivity of these components to basic organization parameters such as cache size line size and degree of associativity we also present the performance implications of routing memory requests initiated by devices through these caches based on detailed simulation data and its implications on system level performance this paper shows that chipset caches have significant potential for improving future web server performance
the preservation of digital artifacts represents an unanswered challenge for the modern information society xml and its query languages provide an effective environment to address this challenge because of their ability to support temporal information and queries and make it easy to publish database history to the web in this paper we focus on the problem of preserving publishing and querying efficiently the history of relational database past research on temporal databases revealed the difficulty of achieving satisfactory solutions using flat relational tables and sql here we show that the problem can be solved using xml to support temporally grouped representations of the database history and xquery to express powerful temporal queries on such representations furthermore the approach is quite general and it can be used to preserve and query the history of multi version xml documents then we turn to the problem of efficient implementation and we investigate alternative approaches including xml dbms ii shredding xml into relational tables and using sql xml on these tables iii sql nested tables and iv or dbms extended with xml support these experiments suggest that combination of temporal xml views and physical relational tables provides the best approach for managing temporal database information
the nature of distributed systems is constantly and steadily changing as the hardware and software landscape evolves porting applications and adapting existing middleware systems to ever changing computational platforms has become increasingly complex and expensive therefore the design of applications as well as the design of next generation middleware systems must follow set of guiding principles in order to insure long term survivability without costly re engineering from our practical experience the key determinants to success in this endeavor are adherence to the following principles design for change provide for storage subsystem coordination employ workload partitioning and load balancing techniques employ caching schedule the workload and understand the workload in order to support these principles we have collected extensive experimental results comparing three middleware systems targeted at data and compute intensive applications implemented by our research group during the course of the last decade on single data and compute intensive application the main contribution of this work is the analysis of level playing field where we discuss and quantify how adherence to these guiding principles impacts overall system throughput and response time
optimization of complex xqueries combining many xpath steps and joins is currently hindered by the absence of good cardinality estimation and cost models for xquery additionally the state of the art of even relational query optimization still struggles to cope with cost model estimation errors that increase with plan size as well as with the effect of correlated joins and selections in this research we propose to radically depart from the traditional path of separating the query compilation and query execution phases by having the optimizer execute materialize partial results and use sampling based estimation techniques to observe the characteristics of intermediates the proposed technique takes as input join graph where the edges are either equi joins or xpath steps and the execution environment provides value and structural join algorithms as well as structural and value based indices while run time optimization with sampling removes many of the vulnerabilities of classical optimizers it brings its own challenges with respect to keeping resource usage under control both with respect to the materialization of intermediates as well as the cost of plan exploration using sampling our approach deals with these issues by limiting the run time search space to so called zero investment algorithms for which sampling can be guaranteed to be strictly linear in sample size all operators and xml value indices used by rox for sampling have the zero investment property we perform extensive experimental evaluation on large xml datasets that shows that our run time query optimizer finds good query plans in robust fashion and has limited run time overhead
with the delta between processor clock frequency and memory latency ever increasing and with the standard locality improving transformations maturing compilers increasingly seek to modify an application's data layout to improve spatial and temporal locality and to reduce cache miss and page fault penalties in this paper we describe practical implementation of the data layout optimizations structure splitting structure peeling structure field reordering and dead field removal both for profile and non profile based compilations we demonstrate significant performance gains but find that automatic transformations fail for relatively high number of record types because of legality violations or profitability constraints additionally we find class of desirable transformations for which the framework cannot provide satisfying results to address this issue we complement the automatic transformations with an advisory tool we reuse the compiler analysis done for automatic transformation and correlate its results with peformance data collected during runtime for structure fields such as data cache misses and latencies we then use the compiler as pefomtance analysis and reporting tool and provide insight into how to layout structure types more eficiently
this paper presents novel approach to image segmentation based on hypergraph cut techniques natural images contain more components edge homogeneous region noise so to facilitate the natural image analysis we introduce an image neighborhood hypergraph representation inh this representation extracts all features and their consistencies in the image data and its mode of use is close to the perceptual grouping then we formulate an image segmentation problem as hypergraph partitioning problem and we use the recent way hypergraph techniques to find the partitions of the image into regions of coherent brightness color experimental results of image segmentation on wide range of images from berkeley database show that the proposed method provides significant performance improvement compared with the stat of the art graph partitioning strategy based on normalized cut ncut criteria
we explore computational approach to proving intractability of certain counting problems more specifically we study the complexity of holant of regular graphs these problems include concrete problems such as counting the number of vertex covers or independent sets for regular graphs the high level principle of our approach is algebraic which provides sufficient conditions for interpolation to succeed another algebraic component is holographic reductions we then analyze in detail polynomial maps on induced by some combinatorial constructions these maps define sufficiently complicated dynamics of that we can only analyze them computationally we use both numerical computation as intuitive guidance and symbolic computation as proof theoretic verification to derive that certain collection of combinatorial constructions in myriad combinations fulfills the algebraic requirements of proving hardness the final result is dichotomy theorem for class of counting problems
pointer aliasing analysis is used to determine if two object names containing dereferences and or field selectors eg may refer to the same location during execution such information is necessary for applications such as data flow based testers program understanding tools and debuggers but is expensive to calculate with acceptable precision incremental algorithms update data flow information after program change rather than recomputing it from scratch under the assumption that the change impact will be limited two versions of practical incremental pointer aliasing algorithm have been developed based on landi ryder flow and context sensitive alias analysis empirical results attest to the time savings over exhaustive analysis six fold speedup on average and the precision of the approximate solution obtained on average same solution as exhaustive algorithm for of the tests
peer to peer storage systems assume that their users consume resources in proportion to their contribution unfortunately users are unlikely to do this without some enforcement mechanism prior solutions to this problem require centralized infrastructure constraints on data placement or ongoing administrative costs all of these run counter to the design philosophy of peer to peer systemssamsara enforces fairness in peer to peer storage systems without requiring trusted third parties symmetric storage relationships monetary payment or certified identities each peer that requests storage of another must agree to hold claim in return placeholder that accounts for available space after an exchange each partner checks the other to ensure faithfulness samsara punishes unresponsive nodes probabilistically because objects are replicated nodes with transient failures are unlikely to suffer data loss unlike those that are dishonest or chronically unavailable claim storage overhead can be reduced when necessary by forwarding among chains of nodes and eliminated when cycles are created forwarding chains increase the risk of exposure to failure but such risk is modest under reasonable assumptions of utilization and simultaneous persistent failure
we present mathematical framework for enforcing energy conservation in bidirectional reflectance distribution function brdf by specifying halfway vector distributions in simple two dimensional domains energy conserving brdfs can produce plausible rendered images with accurate reflectance behavior especially near grazing angles using our framework we create an empirical brdf that allows easy specification of diffuse specular and retroreflective materials we also present second brdf model that is useful for data fitting although it does not preserve energy it uses the same halfway vector domain as the first model we show that this data fitting brdf can be used to match measured data extremely well using only small set of parameters we believe that this is an improvement over table based lookups and factored versions of brdf data
rossnet brings together the four major areas of networking research network modeling simulation measurement and protocol design rossnet is tool for computing large scale design of experiments through components such as discrete event simulation engine default and extensible model designs and state of the art xml interface rossnet reads in predefined descriptions of network topologies and traffic scenarios which allows for in depth analysis and insight into emerging feature interactions cascading failures and protocol stability in variety of situations developers will be able to design and implement their own protocol designs network topologies and modeling scenarios as well as implement existing platforms within the rossnet platform also using rossnet designers are able to create experiments with varying levels of granularity allowing for the highest degree of scalability
the widespread interest in program slicing within the source code analysis and manipulation community has led to the introduction of large number of different forms of slicing each preserves some aspect of program's behaviour and simplifies the program to focus exclusively upon this behaviour in order to understand the similarities and differences between forms of slicing formal mechanism is required this paper further develops formal framework for comparing forms of slicing using theory of program projection this framework is used to reveal the ordering relationship between various static dynamic simultaneous and conditioned forms of slicing
property testing algorithms are ultra efficient algorithms that decide whether given object eg graph has certain property eg bipartiteness or is significantly different from any object that has the property to this end property testing algorithms are given the ability to perform local queries to the input though the decision they need to make usually concerns properties with global nature in the last two decades property testing algorithms have been designed for many types of objects and properties amongst them graph properties algebraic properties geometric properties and more in this monograph we survey results in property testing where our emphasis is on common analysis and algorithmic techniques among the techniques surveyed are the following the self correcting approach which was mainly applied in the study of property testing of algebraic properties the enforce and test approach which was applied quite extensively in the analysis of algorithms for testing graph properties in the dense graphs model as well as in other contexts szemer√©di's regularity lemma which plays very important role in the analysis of algorithms for testing graph properties in the dense graphs model the approach of testing by implicit learning which implies efficient testability of membership in many functions classes and algorithmic techniques for testing properties of sparse graphs which include local search and random walks
this paper presents swing closed loop network responsive traffic generator that accurately captures the packet interactions of range of applications using simple structural model starting from observed traffic at single point in the network swing automatically extracts distributions for user application and network behavior it then generates live traffic corresponding to the underlying models in network emulation environment running commodity network protocol stacks we find that the generated traces are statistically similar to the original traces further to the best of our knowledge we are the first to reproduce burstiness in traffic across range of timescales using model applicable to variety of network settings an initial sensitivity analysis reveals the importance of capturing and recreating user application and network characteristics to accurately reproduce such burstiness finally we explore swing's ability to vary user characteristics application properties and wide area network conditions to project traffic characteristics into alternate scenarios
in this paper we investigate the extent to which knowledge compilation can be used to circumvent the complexity of skeptical inference from stratified belief base sbb we first analyze the compilability of skeptical inference from an sbb under various requirements concerning both the selection policy under consideration the possibility to make the stratification vary at the on line query answering stage and the expected complexity of inference from the compiled form not surprisingly the results are mainly negative however since they concern the worst case situation only they do not prevent compilation based approach from being practically useful for some families of instances while many approaches to compile an sbb can be designed we are primarily interested in those which take advantage of existing knowledge compilation techniques for classical inference specifically we present general framework for compiling sbbs into so called normal sbbs where is any tractable class for clausal entailment which is the target class of compilation function another major advantage of the proposed approach lies in the flexibility of the normal belief bases obtained which means that changing the stratification does not require to re compile the sbb for several families of compiled sbbs and several selection policies the complexity of skeptical inference is identified some tractable restrictions are exhibited for each policy finally some empirical results are presented
crucial role in the microsoft net framework common language runtime clr security model is played by type safety of the common intermediate language cil in this paper we formally prove type safety of large subset of cil to do so we begin by specifying the static and dynamic semantics of cil by providing an abstract interpreter for cil programs we then formalize the bytecode verification algorithm whose job it is to compute well typing for given method we then prove type safety of well typed methods ie the execution according to the semantics model of legal and well typed methods does not lead to any run time type violations finally to prove cil's type safety we show that the verification algorithm is sound ie the typings it produces are well typings and complete ie if well typing exists then the algorithm computes one
exhaustive model checking search techniques are ineffective for error discovery in large and complex multi threaded software systems distance estimate heuristics guide the concrete execution of the program toward possible error location the estimate is lower bound computed on statically generated abstract model of the program that ignores all data values and only considers control flow in this paper we describe new distance estimate heuristic that efficiently computes tighter lower bound in programs with polymorphism when compared to the state of the art distance heuristic we statically generate conservative distance estimates and refine the estimates when the targets of dynamic method invocations are resolved in our empirical analysis the state of the art approach is computationally infeasible for large programs with polymorphism while our new distance heuristic can quickly detect the errors
we present new multiphase method for efficiently simplifying polygonal surface models of arbitrary size it operates by combining an initial out of core uniform clustering phase with subsequent in core iterative edge contraction phase these two phases are both driven by quadric error metrics and quadrics are used to pass information about the original surface between phases the result is method that produces approximations of quality comparable to quadric based iterative edge contraction but at fraction of the cost in terms of running time and memory consumption
we introduce new visual search interface for search engines the interface is user friendly and informative graphical front end for organizing and presenting search results in the form of topic groups such semantics oriented search result presentation is in contrast with conventional search interfaces which present search results according to the physical structures of the information given user query our interface first retrieves relevant online materials via third party search engine and then we analyze the semantics of search results to detect latent topics in the result set once the topics are detected we map the search result pages into topic clusters according to the topic clustering result we divide the available screen space for our visual interface into multiple topic displaying regions one for each topic for each topic's displaying region we summarize the information contained in the search results under the corresponding topic so that only key messages will be displayed with this new visual search interface users are conveyed the key information in the search results expediently with the key information users can navigate to the final desired results with less effort and time than conventional searching supplementary materials for this paper are available at http wwwcshkuhk songhua visualsearch
this paper explores interposed request routing in slice new storage system architecture for high speed networks incorporating network attached block storage slice interposes request switching filter called Œºproxy along each client's network path to the storage service eg in network adapter or switch the Œºproxy intercepts request traffic and distributes it across server ensemble we propose request routing schemes for and file service traffic and explore their effect on service structure the slice prototype uses packet filter Œºproxy to virtualize the standard network file system nfs protocol presenting to nfs clients unified shared file volume with scalable bandwidth and capacity experimental results from the industry standard specsfs workload demonstrate that the architecture enables construction of powerful network attached storage services by aggregating cost effective components on switched gigabit ethernet lan
performance and power are the first order design metrics for network on chips nocs that have become the de facto standard in providing scalable communication backbones for multicores cmps however nocs can be plagued by higher power consumption and degraded throughput if the network and router are not designed properly towards this end this paper proposes novel router architecture where we tune the frequency of router in response to network load to manage both performance and power we propose three dynamic frequency tuning techniques freqboost freqthrtl and freqtune targeted at congestion and power management in nocs as enablers for these techniques we exploit dynamic voltage and frequency scaling dvfs and the imbalance in generic router pipeline through time stealing experiments using synthetic workloads on wormhole switched mesh interconnect show that freqboost is better choice for reducing average latency maximum while freqthrtl provides the maximum benefits in terms of power saving and energy delay product edp the freqtune scheme is better candidate for optimizing both performance and power achieving on an average reduction in latency savings in power up to at high load and savings up to at high load in edp with application benchmarks we observe ipc improvement up to using our design the performance and power benefits also scale for larger nocs
the signature quadratic form distance is an adaptive similarity measure for flexible content based feature representations of multimedia data in this paper we present deep survey of the mathematical foundation of this similarity measure which encompasses the classic quadratic form distance defined only for the comparison between two feature histograms of the same length and structure moreover we give the benefits of the signature quadratic form distance and experimental evaluation on numerous real world databases
frequent pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets sequences and graphs however the bottleneck of frequent pattern mining is not at the efficiency but at the interpretability due to the huge number of patterns generated by the mining processin this paper we examine how to summarize collection of itemset patterns using only representatives small number of patterns that user can handle easily the representatives should not only cover most of the frequent patterns but also approximate their supports generative model is built to extract and profile these representatives under which the supports of the patterns can be easily recovered without consulting the original dataset based on the restoration error we propose quality measure function to determine the optimal value of parameter polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement empirical studies indicate that we can obtain compact summarization in real datasets
this paper presents vision framework which enables feature oriented appearance based navigation in large outdoor environments containing other moving objects the framework is based on hybrid topological geometrical environment representation constructed from learning sequence acquired during robot motion under human control at the higher topological layer the representation contains graph of key images such that incident nodes share many natural landmarks the lower geometrical layer enables to predict the projections of the mapped landmarks onto the current image in order to be able to start or resume their tracking on the fly the desired navigation functionality is achieved without requiring global geometrical consistency of the underlying environment representation the framework has been experimentally validated in demanding and cluttered outdoor environments under different imaging conditions the experiments have been performed on many long sequences acquired from moving cars as well as in large scale real time navigation experiments relying exclusively on single perspective vision sensor the obtained results confirm the viability of the proposed hybrid approach and indicate interesting directions for future work
power dissipation and thermal issues are increasingly significant in modern processors as result it is crucial that power performance tradeoffs be made more visible to chip architects and even compiler writers in addition to circuit designers most existing power analysis tools achieve high accuracy by calculating power estimates for designs only after layout or floorplanning are complete in addition to being available only late in the design process such tools are often quite slow which compounds the difficulty of running them for large space of design possibilities this paper presents wattch framework for analyzing and optimizing microprocessor power dissipation at the architecture level wattch is or more faster than existing layout level power tools and yet maintains accuracy within of their estimates as verified using industry tools on leading edge designs this paper presents several validations of wattch's accuracy in addition we present three examples that demonstrate how architects or compiler writers might use wattch to evaluate power consumption in their design process we see wattch as complement to existing lower level tools it allows architects to explore and cull the design space early on using faster higher level tools it also opens up the field of power efficient computing to wider range of researchers by providing power evaluation methodology within the portable and familiar simplescalar framework
in this paper we present compass an xml based agent model for supporting user in his web activities compass is the result of our attempt of synthesizing in unique context important guidelines currently characterizing the research in various computer science sectors indeed it constructs and handles rather rich even if light user profile this latter is exploited for supporting the user in an efficient search of information of his interest in this way it behaves as content based recommender system moreover it is particularly suited for constructing multi agent systems and therefore for implementing collaborative filtering recommendation techniques in addition since it widely uses xml technology it is particularly light and capable of operating on various hardware and software platforms the adoption of xml also facilitates the information exchange among compass agents and consequently makes the management and the exploitation of compass based multi agent systems easier
this paper brings up new concern regarding efficient re keying of large groups with dynamic membership minimizing the overall time it takes for the key server and the group members to process the re keying message specifically we concentrate on re keying algorithms based on the logical key hierarchy lkh and minimize the longest sequence of encryptions and decryptions that need to be done in re keying operation we first prove lower bound on the time required to perform re keying operation in this model then we provide an optimal schedule of re keying messages matching the above lower bound in particular we show that the optimal schedule can be found only when the ariety of the lkh key graph is chosen according to the available communication bandwidth and the users processing power our results show that key trees of ariety commonly assumed to be optimal are not optimal when used in high bandwidth networks or networks of devices with low computational power like sensor networks
there are major trends to advance the functionality of search engines to more expressive semantic level this is enabled by the advent of knowledge sharing communities such as wikipedia and the progress in automatically extracting entities and relationships from semistructured as well as natural language web sources recent endeavors of this kind include dbpedia entitycube knowitall readtheweb and our own yago naga project and others the goal is to automatically construct and maintain comprehensive knowledge base of facts about named entities their semantic classes and their mutual relations as well as temporal contexts with high precision and high recall this tutorial discusses state of the art methods research opportunities and open challenges along this avenue of knowledge harvesting
we address performance maximization of independent task sets under energy constraint on chip multi processor cmp architectures that support multiple voltage frequency operating states for each core we prove that the problem is strongly np hard we propose polynomial time approximation algorithms for homogeneous and heterogeneous cmps to the best of our knowledge our techniques offer the tightest bounds for energy constrained design on cmp architectures experimental results demonstrate that our techniques are effective and efficient under various workloads on several cmp architectures
space constrained optimization problems arise in variety of applications ranging from databases to ubiquitous computing typically these problems involve selecting set of items of interest subject to space constraint we show that in many important applications one faces variants of this basic problem in which the individual items are sets themselves and each set is associated with benefit value since there are no known approximation algorithms for these problems we explore the use of greedy and randomized techniques we present detailed performance and theoretical evaluation of the algorithms highlighting the efficiency of the proposed solutions
type inclusion test determines whether one type is subtype of another efficient type testing techniques exist for single subtyping but not for languages with multiple subtyping to date the fast constant time technique relies on binary matrix encoding of the subtype relation with quadratic space requirements in this paper we present three new encodings of the subtype relation the packed encoding the bit packed encoding and the compact encoding these encodings have different characteristics the bit packed encoding delivers the best compression rates on average for real life programs the packed encoding performs type inclusion tests in only machine instructions we present fast algorithm for computing these encoding which runs in less than milliseconds for pe and bpe and milliseconds for ce on an alpha processor finally we compare our results with other constant time type inclusion tests on suite of large benchmark hierarchies
the success of pp file sharing network highly depends on the scalability and versatility of its search mechanism two particularly desirable search features are scope ability to find infrequent items and support for partial match queries queries that contain typos or include subset of keywords while centralized index architectures such as napster can support both these features existing decentralized architectures seem to support at most one prevailing unstructured pp protocols such as gnutella and fasttrack deploy blind search mechanism where the set of peers probed is unrelated to the query thus they support partial match queries but have limited scope on the other extreme the recently proposed distributed hash tables dhts such as can and chord couple index location with the item's hash value and thus have good scope but can not effectively support partial match queries another hurdle to dhts deployment is their tight control of the overlay structure and the information part of the index each peer maintains which makes them more sensitive to failures and frequent joins and disconnects we develop new class of decentralized pp architectures our design is based on unstructured architectures such as gnutella and fasttrack and retains many of their appealing properties including support for partial match queries and relative resilience to peer failures yet we obtain orders of magnitude improvement in the efficiency of locating rare items our approach exploits associations inherent in human selections to steer the search process to peers that are more likely to have an answer to the query we demonstrate the potential of associative search using models analysis and simulations
we present clustering technique addressing redundancy for bounded distance clusters which means being able to determine the minimum number of cluster heads per node and the maximum distance from nodes to their cluster heads this problem is similar to computing dominating set ds of the network ds is defined as the problem of selecting minimum cardinality vertex set of the network such that every vertex not in is at distance smaller than or equal to from at least vertices in in mobile ad hoc networks manets clusters should be computed distributively because the topology may change frequently we present the first centralized and distributed solutions to the ds problem for arbitrary topologies the centralized algorithm computes kln approximation where is the largest cardinality among all hop neighborhoods in the network the distributed approach is extended for clustering applications while the centralized is used as lower bound for comparison purposes extensive simulations are used to compare the distributed solution with the centralized one as case study we propose novel multi core multicast protocol that applies the distributed solution for the election of cores the new protocol is compared against puma one of the best performing multicast protocols for manets simulation results show that the new protocol outperforms puma on the context of static networks
in order to provide certified security services we must provide indicators that can measure the level of assurance that complex business process can offer unfortunately the formulation of security indicators is not amenable to efficient algorithms able to evaluate the level of assurance of complex process from its componentsin this paper we show an algorithm based on fd graphs variant of directed hypergraphs that can be used to compute in polynomial time the overall assurance indicator of complex business process from its components for arbitrary monotone composition functions ii the subpart of the business process that is responsible for such assurance indicator ie the best security alternative
write buffering is one of many successful mechanisms that improves the performance and scalability of multiprocessors however it leads to more complex memory system behavior which cannot be described using intuitive consistency models such as sequential consistency it is crucial to provide programmers with specification of the exact behavior of such complex memories this article presents uniform framework for describing systems at different levels of abstraction and proving their equivalence the framework is used to derive and prove correct simple specifications in terms of program level instructions of the sparc total store order and partial store order memoriesthe framework is also used to examine the sparc relaxed memory order we show that it is not memory consistency model that corresponds to any implementation on multiprocessor that uses write buffers even though we suspect that the sparc version specification of relaxed memory order was intended to capture general write buffer architecture the same technique is used to show that coherence does not correspond to write buffer architecture corollary which follows from the relationship between coherence and alpha is that any implementation of alpha consistency using write buffers cannot produce all possible alpha computations that is there are some computations that satisfy the alpha specification but cannot occur in the given write buffer implementation
restricted by wireless communication technology and mobile computing environment it is difficult to improve the efficiency of the programs that resided in mobile end to solve this problem the cache mechanism is major and effective method in this paper we propose an application oriented semantic cache model it establishes semantic associated rules base according to the knowledge of application domains makes use of the semantic locality for data pre fetching and adopts two level lru algorithm for cache replacement several experiments demonstrate that the semantic driven cache model can achieve higher hit ratio than traditional cache models
materialized views rdbms silver bullet demonstrate its efficacy in many applications especially as data warehousing decison support system tool the pivot of playing materialized views efficiently is view selection though studied for over thirty years in rdbms the selection is hard to make in the context of xml databases where both the semi structured data and the expressiveness of xml query languages add challenges to the view selection problem we start our discussion on producing minimal xml views in terms of size as candidates for given workload query set to facilitate intuitionistic view selection we present view graph called vcube to structurally maintain all generated views by basing our selection on vcube for materialization we propose two view selection strategies targeting at space optimized and space time tradeoff respectively we built our implementation on top of berkeley db xml demonstrating that significant performance improvement could be obtained using our proposed approaches
peer to peer overlay networks have proven to be good support for storing and retrieving data in fully decentralized way sound approach is to structure them in such way that they reflect the structure of the application peers represent objects of the application so that neighbours in the peer to peer network are objects having similar characteristics from the application's point of view such structured peer to peer overlay networks provide natural support for range queries while some complex structures such as vorono√Ø tessellation where each peer is associated to cell in the space are clearly relevant to structure the objects the associated cost to compute and maintain these structures is usually extremely high for dimensions larger than we argue that an approximation of complex structure is enough to provide native support of range queries this stems from the fact that neighbours are important while the exact space partitioning associated to given peer is not as crucial in this paper we present the design analysis and evaluation of raynet loosely structured vorono√Ø based overlay network raynet organizes peers in an approximation of vorono√Ø tessellation in fully decentralized way it relies on monte carlo algorithm to estimate the size of cell and on an epidemic protocol to discover neighbours in order to ensure efficient polylogarithmic routing raynet is inspired from the kleinberg's small world model where each peer gets connected to close neighbours its approximate vorono√Ø neighbours in raynet and shortcuts long range neighbours implemented using an existing kleinberg like peer sampling
we design an adaptive admission control mechanism network early warning system news to protect servers and networks from flash crowds and maintain high performance for end users news detects flash crowds from performance degradation in responses and mitigates flash crowds by admitting incoming requests adaptively we evaluate news performance with both simulations and testbed experiments we first investigate network limited scenarion in simulations we find that news detects flash crowds within seconds by discarding percent of incoming requests news protects the target server and networks from overloading reducing the response packet drop rate from percent to percent for admitted requests news increases their response rate by two times this performance is similar to the best static rate limiter deployed in the same scenario we also investigate the impact of detection intervals on news performance showing it affects both detection delay and false alarm rate we further consider server memory limited scenario in testbed experiments confirming that news is also effective in this case we also examine the runtime cost of news traffic monitoring in practice and find that it consumes little cpu time and relatively small memory finally we show news effectively protects bystander traffic from flash crowds
users enter queries that are short as well as long the aim of this work is to evaluate techniques that can enable information retrieval ir systems to automatically adapt to perform better on such queries by adaptation we refer to modifications to the queries via user interaction and detecting that the original query is not good candidate for modification we show that the former has the potential to improve mean average precision map of long and short queries by and respectively and that simple user interaction can help towards this goal we observed that after inspecting the options presented to them users frequently did not select any we present techniques in this paper to determine beforehand the utility of user interaction to avoid this waste of time and effort we show that our techniques can provide ir systems with the ability to detect and avoid interaction for unpromising queries without significant drop in overall performance
the proliferation of multimedia applications over mobile resource constrained wireless networks has raised the need for techniques that adapt these applications both to clients quality of service qos requirements and to network resource constraints this article investigates the upper layer adaptation mechanisms to achieve end to end delay control for multimedia applications the proposed adaptation approach spans application layer middleware layer and network layer in application layer the requirement adaptor dynamically changes the requirement levels according to end to end delay measurement and acceptable qos requirements for the end users in middleware layer the priority adaptor is used to dynamically adjust the service classes for applications using feedback control theory in network layer the service differentiation scheduler assigns different network resources eg bandwidth to different service classes with the coordination of these three layers our approach can adaptively assign resources to multimedia applications to evaluate the impact of our adaptation scheme we built real ieee ad hoc network testbed the test bed experiments show that the proposed upper layer adaptation for end to end delay control successfully adjusts multimedia applications to meet delay requirements in many scenarios
in this paper we introduce clean slate architecture for improving the delivery of data packets in ieee wireless mesh networks opposed to the rigid tcp ip layer architecture which exhibits serious deficiencies in such networks we propose unitary layer approach that combines both routing and transport functionalities in single layer the new mesh transmission layer mtl incorporates cross interacting routing and transport modules for reliable data delivery based on the loss probabilities of wireless links due to the significant drawbacks of standard tcp over ieee we particularly focus on the transport module proposing pure rate based approach for transmitting data packets according to the current contention in the network by considering the ieee spatial reuse constraint and employing novel acknowledgment scheme the new transport module improves both goodput and fairness in wireless mesh networks in comparative performance study we show that mtl achieves up to more goodput and up to less packet drops than tcp ip while maintaining excellent fairness results
several attempts have been made to analyze customer behavior on online commerce sites some studies particularly emphasize the social networks of customers users reviews and ratings of product exert effects on other consumers purchasing behavior whether user refers to other users ratings depends on the trust accorded by user to the reviewer on the other hand the trust that is felt by user for another user correlates with the similarity of two users ratings this bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online communities because it suggests clustering of similar users and the evolution of strong communities this paper presents theoretical model along with analyses of an actual online commerce site we analyzed large community site in japan cosme the noteworthy characteristics of cosme are that users can bookmark their trusted users in addition they can post their own ratings of products which facilitates our analyses of the ratings bidirectional effects on trust and ratings we describe an overview of the data in cosme analyses of effects from trust to rating and vice versa and our proposition of measure of community gravity which measures how strongly user might be attracted to community our study is based on the cosme dataset in addition to the epinions dataset it elucidates important insights and proposes potentially important measure for mining online social networks
wireless sensor nodes are increasingly being tasked with computation and communication intensive functions while still subject to constraints related to energy availability on these embedded platforms once all low power design techniques have been explored duty cycling the various subsystems remains the primary option to meet the energy and power constraints this requires the ability to provide spurts of high mips and high bandwidth connections however due to the large overheads associated with duty cycling the computation and communication subsystems existing high performance sensor platforms are not efficient in supporting such an option in this article we present the design and optimizations taken in wireless gateway node wgn that bridges data from wireless sensor networks to wi fi networks in an on demand basis we discuss our strategies to reduce duty cycling related costs by partitioning the system and by reducing the amount of time required to activate or deactivate the high powered components we compare the design choices and performance parameters with those made in the intel stargate platform to show the effectiveness of duty cycling on our platform we have built working prototype and the experimental results with two different power management schemes show significant reductions in latency and average power consumption compared to the stargate the wgn running our power gating scheme performs about six times better in terms of average system power consumption than the stargate running the suspend system scheme for large working periods where the active power dominates for short working periods where the transition enable disable power becomes dominant we perform up to seven times better the comparative performance of our system is even greater when the sleep power dominates
separation logic is popular approach for specifying properties of recursive mutable data structures several existing systems verify subclass of separation logic specifications using static analysis techniques checking data structure specifications during program execution is an alternative to static verification it can enforce the sophisticated specifications for which static verification fails and it can help debug incorrect specifications and code by detecting concrete counterexamples to their validity this paper presents separation logic invariant checker slick runtime checker for separation logic specifications we show that although the recursive style of separation logic predicates is well suited for runtime execution the implicit footprint and existential quantification make efficient runtime checking challenging to address these challenges we introduce coloring technique for efficiently checking method footprints and describe techniques for inferring values of existentially quantified variables we have implemented our runtime checker in the context of tool for enforcing specifications of java programs our experience suggests that our runtime checker is useful companion to static verifier for separation logic specifications
this paper discusses the applications of rough ensemble classifier in two emerging problems of web mining the categorization of web services and the topic specific web crawling both applications discussed here consist of two major steps split of feature space based on internal tag structure of web services and hypertext to represent in tensor space model and combining classifications obtained on different tensor components using rough ensemble classifier in the first application we have discussed the classification of web services two step improvement on the existing classification results of web services has been shown here in the first step we achieve better classification results over existing by using tensor space model in the second step further improvement of the results has been obtained by using rough set based ensemble classifier in the second application we have discussed the focused crawling using rough ensemble prediction our experiment regarding this application has provided better harvest rate and better target recall for focused crawling
in this paper we explore the relationship between preference elicitation learning style problem that arises in combinatorial auctions and the problem of learning via queries studied in computational learning theory preference elicitation is the process of asking questions about the preferences of bidders so as to best divide some set of goods as learning problem it can be thought of as setting in which there are multiple target concepts that can each be queried separately but where the goal is not so much to learn each concept as it is to produce an optimal example in this work we prove number of similarities and differences between two bidder preference elicitation and query learning giving both separation results and proving some connections between these problems
caching and content delivery are important for content intensive publish subscribe applications this paper proposes several content distribution approaches that combine match based pushing and access based caching based on users subscription information and access patterns to study the performance of the proposed approaches we built simulator and developed workload to mimic the content and access dynamics of busy news site using purely access based caching approach as the baseline our best approaches yield over and relative gains for two request traces in terms of the hit ratio in local caches while keeping the traffic overhead comparable even when the subscription information is assumed not to reflect users accesses perfectly our best approaches still have about and relative improvement for the two traces to our knowledge this work is the first effort to investigate content distribution under the publish subscribe paradigm
the contribution of this work is the design and evaluation of programming language model that unifies aspects and classes as they appear in aspectj like languages we show that our model preserves the capabilities of aspectj like languages while improving the conceptual integrity of the language model and the compositionality of modules the improvement in conceptual integrity is manifested by the reduction of specialized constructs in favor of uniform orthogonal constructs the enhancement in compositionality is demonstrated by better modularization of integration and higher order crosscutting concerns
despite constant improvements in fabrication technology hardware components are consuming more power than ever with the ever increasing demand for higher performance in highly integrated systems and as battery technology falls further behind managing energy is becoming critically important to various embedded and mobile systems in this paper we propose and implement power aware virtual memory to reduce the energy consumed by the memory in response to workloads becoming increasingly data centric we can use the power management features in current memory technology to put individual memory devices into low power modes dynamically under software control to reduce the power dissipation however it is imperative that any techniques employed weigh memory energy savings against any potential energy increases in other system components due to performance degradation of the memory using novel power aware virtual memory implementation we estimate significant reduction in memory power dissipation from to based on rambus memory specifications while running various real world applications in working linux system unfortunately due to hardware bug in the chipset direct power measurement is currently not possible applying more advanced techniques we can reduce power dissipation further to depending on the actual workload with negligible effects on performance we also show this work is applicable to other memory architectures and is orthogonal to previously proposed hardware controlled power management techniques so it can be applied simultaneously to further enhance energy conservation in variety of platforms
in large scale networked computing systems component failures become norms instead of exceptions failure aware resource management is crucial for enhancing system availability and achieving high performance in this paper we study how to efficiently utilize system resources for high availability computing with the support of virtual machine vm technology we design reconfigurable distributed virtual machine rdvm infrastructure for networked computing systems we propose failure aware node selection strategies for the construction and reconfiguration of rdvms we leverage the proactive failure management techniques in calculating nodes reliability states we consider both the performance and reliability status of compute nodes in making selection decisions we define capacity reliability metric to combine the effects of both factors in node selection and propose best fit algorithms with optimistic and pessimistic selection strategies to find the best qualified nodes on which to instantiate vms to run user jobs we have conducted experiments using failure traces from production systems and the nas parallel benchmark programs on real world cluster system the results show the enhancement of system productivity by using the proposed strategies with practically achievable accuracy of failure prediction with the best fit strategies the job completion rate is increased by compared with that achieved in the current lanl hpc cluster the task completion rate reaches with utilization of relatively unreliable nodes
from design point of view coordination is radically undertheorized and under explored arguably playground games are the universal cross cultural venue in which people learn about and explore coordination between one another and between the worlds of articulated rules and the worlds of experience and action they can therefore teach us about the processes inherent in human coordination provide model of desirable coordinative possibilities and act as design framework from which to explore the relationship between game and game play or to put it in terms of an inherent tension in human computer interaction between plans and situated actions when brought together with computer language for coordination that helps us pare down coordinative complexity to essential components we can create systems that have highly distributed control structures in this paper we present the design of four such student created collaborative distributed interactive systems for face to face use these take their inspiration from playground games with respect to who can play plurality how appropriability and to what ends acompetitiveness as it happens our sample systems are themselves games however taking playground games as our model helps us create systems that support game play featuring not enforcement of plans but emergence of rules roles and turn taking
if conversion transforms control dependencies to data dependencies by using predication mechanism it is useful to eliminate hard to predict branches and to reduce the severe performance impact of branch mispredictions however the use of predicated execution in out of order processors has to deal with two problems there can be multiple definitions for single destination register at rename time and instructions with false predicated consume unnecessary resources predicting predicates is an effective approach to address both problems however predicting predicates that come from hard to predict branches is not beneficial in general because this approach reverses the if conversion transformation loosing its potential benefits in this paper we propose new scheme that dynamically selects which predicates are worthy to be predicted and which one are more effective in its if converted form we show that our approach significantly outperforms previous proposed schemes moreover it performs within of an ideal scheme with perfect predicate prediction
the paper describes an application of artificial intelligence technology to the implementation of rapid prototyping method in object oriented performance design oopd for real time systems oopd consists of two prototyping phases for real time systems each of these phases consists of three steps prototype construction prototype execution and prototype evaluation we present artificial intelligence based methods and tools to be applied to the individual steps in the prototype construction step rapid construction mechanism using reusable software components is implemented based on planning in the prototype execution step hybrid inference mechanism is used to execute the constructed prototype described in declarative knowledge representation mendel which is prolog based concurrent object oriented language can be used as prototype construction tool and prototype execution tool in the prototype evaluation step an expert system which is based on qualitative reasoning is implemented to detect and diagnose bottlenecks and generate an improvement plan for them
rdf schema rdfs as lightweight ontology language is gaining popularity and consequently tools for scalable rdfs inference and querying are needed sparql has become recently wc standard for querying rdf data but it mostly provides means for querying simple rdf graphs only whereas querying with respect to rdfs or other entailment regimes is left outside the current specification in this paper we show that sparql faces certain unwanted ramifications when querying ontologies in conjunction with rdf datasets that comprise multiple named graphs and we provide an extension for sparql that remedies these effects moreover since rdfs inference has close relationship with logic rules we generalize our approach to select custom ruleset for specifying inferences to be taken into account in sparql query we show that our extensions are technically feasible by providing benchmark results for rdfs querying in our prototype system giabata which uses datalog coupled with persistent relational database as back end for implementing sparql with dynamic rule based inference by employing different optimization techniques like magic set rewriting our system remains competitive with state of the art rdfs querying systems
the promise of rule based computing was to allow end users to create modify and maintain applications without the need to engage programmers but experience has shown that rule sets often interact in subtle ways making them difficult to understand and reason about this has impeded the widespread adoption of rule based computing this paper describes the design and implementation of xcellog user centered deductive spreadsheet system to empower non programmers to specify and manipulate rule based systems the driving idea underlying the system is to treat sets as the fundamental data type and rules as specifying relationships among sets and use the spreadsheet metaphor to create and view the materialized sets the fundamental feature that makes xcellog suitable for non programmers is that the user mainly sees the effect of the rules when rules or basic facts change the user sees the impact of the change immediately this enables the user to gain confidence in the rules and their modification and also experiment with what if scenarios without any programming preliminary experience with using xcellog indicates that it is indeed feasible to put the power of deductive spreadsheets for doing rule based computing into the hands of end users and do so without the requirement of programming or the constraints of canned application packages
many reputation management systems have been developed under the assumption that each entity in the system will use variant of the same scoring function much of the previous work in reputation management has focused on providing robustness and improving performance for given reputation scheme in this paper we present reputation based trust management framework that supports the synthesis of trust related feedback from many different entities while also providing each entity with the flexibility to apply different scoring functions over the same feedback data for customized trust evaluations we also propose novel scheme to cache trust values based on recent client activity to evaluate our approach we implemented our trust management service and tested it on realistic application scenario in both lan and wan distributed environments our results indicate that our trust management service can effectively support multiple scoring functions with low overhead and high availability
we present novel translation model based on tree to string alignment template tat which describes the alignment between source parse tree and target string tat is capable of generating both terminals and non terminals and performing reordering at both low and high levels the model is linguistically syntax based because tats are extracted automatically from word aligned source side parsed parallel texts to translate source sentence we first employ parser to produce source parse tree and then apply tats to transform the tree into target string our experiments show that the tat based model significantly outperforms pharaoh state of the art decoder for phrase based models
the event based paradigm has gained interest as solution for integrating large scale distributed and loosely coupled systems it also has been propelled by the growth of event based applications like ambient intelligence utility monitoring event driven supply chain management escm multi player games etc the pub sub paradigm is particularly relevant for implementing these types of applications the core of pub sub system is the notification service and there are several commercial products open source and research projects that implement it the nature of notification services distribution scale large amount of concurrent messages processed as well as the diversity of their implementations make their run time analysis difficult the few existing solutions that address this problem are mainly proprietary and focus on single aspects of the behavior general solution would significantly support research development and tuning of these services this paper introduces notification service independent analysis framework that enables online analysis of their behavior based on streamed observations flexibly defined metrics and visual representations thereof
in this paper we describe an off line unconstrained handwritten arabic word recognition system based on segmentation free approach and semi continuous hidden markov models schmms with explicit state duration character durations play significant part in the recognition of cursive handwriting the duration information is still mostly disregarded in hmm based automatic cursive handwriting recognizers due to the fact that hmms are deficient in modeling character durations properly we will show experimentally that explicit state duration modeling in the schmm framework can significantly improve the discriminating capacity of the schmms to deal with very difficult pattern recognition tasks such as unconstrained handwritten arabic recognition in order to carry out the letter and word model training and recognition more efficiently we propose new version of the viterbi algorithm taking into account explicit state duration modeling three distributions gamma gauss and poisson for the explicit state duration modeling have been used and comparison between them has been reported to perform word recognition the described system uses an original sliding window approach based on vertical projection histogram analysis of the word and extracts new pertinent set of statistical and structural features from the word image several experiments have been performed using the ifn enit benchmark database and the best recognition performances achieved by our system outperform those reported recently on the same database
growing system sizes together with increasing performance variability are making globally synchronous operation hard to realize mesochronous clocking constitutes possible solution to the problems faced the most fundamental of problems faced when communicating between mesochronously clocked regions concerns the possibility of data corruption caused by metastability this paper presents an integrated communication and mesochronous clocking strategy which avoids timing related errors while maintaining globally synchronous system perspective the architecture is scalable as timing integrity is based purely on local observations it is demonstrated with nm cmos standard cell network on chip design which implements completely timing safe global communication in modular system
getting the right software requirements under the right environment assumptions is critical precondition for developing the right software this task is intrinsically difficult we need to produce complete adequate consistent and well structured set of measurable requirements and assumptions from incomplete imprecise and sparse material originating from multiple often conflicting sources the system we need to consider comprises software and environment components including people and devices rich system model may significantly help us in this task such model must integrate the intentional structural functional and behavioral facets of the system being conceived rigorous techniques are needed for model construction analysis exploitation and evolution such techniques should support early and incremental reasoning about partial models for variety of purposes including satisfaction arguments property checks animations the evaluation of alternative options the analysis of risks threats and conflicts and traceability management the tension between technical precision and practical applicability calls for suitable mix of heuristic deductive and inductive forms of reasoning on suitable mix of declarative and operational specifications formal techniques should be deployed only when and where needed and kept hidden wherever possible the paper provides retrospective account of our research efforts and practical experience along this route problem oriented abstractions analyzable models and constructive techniques were permanent concerns
recent scientific and technological advances have witnessed an abundance of structural patterns modeled as graphs as result it is of special interest to process graph containment queries effectively on large graph databases given graph database and query raph the graph containment query is to retrieve all graphs in which contain as subgraph due to the vast number of graphs in and the nature of complexity for subgraph isomorphism testing it is desirable to make use of high quality graph indexing mechanisms to reduce the overall query processing cost in this paper we propose new cost effective graph indexing method based on frequent tree features of the graph database we analyze the effectiveness and efficiency of tree as indexing feature from three critical aspects feature size feature selection cost and pruning power in order to achieve better pruning ability than existing graph based indexing methods we select in addition to frequent tree features tree small number of discriminative graphs delta on demand without costly graph mining process beforehand our study verifies that tree delta is better choice than graph for indexing purpose denoted tree delta ge graph to address the graph containment query problem it has two implications the index construction by tree delta is efficient and the graph containment query processing by tree delta is efficient our experimental studies demonstrate that tree delta has compact index structure achieves an order of magnitude better performance in index construction and most importantly outperforms up to date graph based indexing methods gindex and tree in graph containment query processing
in this paper we describe restartable atomic sequences an optimistic mechanism for implementing simple atomic operations such as test and set on uniprocessor thread that is suspended within restartable atomic sequence is resumed by the operating system at the beginning of the sequence rather than at the point of suspension this guarantees that the thread eventually executes the sequence atomically restartable atomic sequence has significantly less overhead than other software based synchronization mechanisms such as kernel emulation or software reservation consequently it is an attractive alternative for use on uniprocessors that do no support atomic operations even on processors that do support atomic operations in hardware restartable atomic sequences can have lower overhead we describe different implementations of restartable atomic sequences for the mach and taos operating systems these systems thread management packages rely on atomic operations to implement higher level mutual exclusion facilities we show that improving the performance of low level atomic operations and therefore mutual exclusion mechanisms improves application performance
java is becoming viable platform for hard real time computing there are production and research real time java vms as well as applications in both military and civil sector technological advances and increased adoption of real time java contrast significantly with the lack of real time benchmarks the few benchmarks that exist are either low level synthetic micro benchmarks or benchmarks used internally by companies making it difficult to independently verify and repeat reported results this paper presents the collision detector benchmark suite an open source application benchmark suite that targets different hard and soft real time virtual machines is at its core real time benchmark with single periodic task which implements aircraft collision detection based on simulated radar frames the benchmark can be configured to use different sets of real time features and comes with number of workloads we describe the architecture of the benchmark and characterize the workload based on input parameters
this paper defines an extended polymorphic type system for an ml style programming language and develops sound and complete type inference algorithm different frdm the conventional ml type discipline the proposed type system allows full rank polymorphism where polymorphic types can appear in other types such as product types disjoint union types and range types of function types because of this feature the proposed type system significantly reduces the value only restriction of polymorphism which is currently adopted in most of ml style impure languages it also serves as basis for efficient implementation of type directed compilation of polymorphism the extended type system achieves more efficient type inference algorithm and it also contributes to develop more efficient type passing implementation of polymorphism we show that the conventional ml polymorphism sometimes introduces exponential overhead both at compile time elaboration and run time type passing execution and that these problems can be eliminated by our type inference system compared with more powerful rank type inference systems based on semi unification the proposed type inference algorithm infers most general type for any typable expression by using the conventional first order unification and it is therefore easily adopted in existing implementation of ml family of languages
in this paper we propose method for job migration policies by considering effective usage of global memory in addition to cpu load sharing in distributed systems the objective of this paper is to reduce the number of page faults caused by unbalanced memory allocations for jobs among distributed nodes which improves the overall performance of distributed system the proposed method which uses the high performance and high throughput approach with remote execution strategy performs the best for both cpu bound and memory bound jobs in homogeneous as well as in the heterogeneous networks in distributed system
this paper presents deterministic and efficient algorithm for online facility location the algorithm is based on simple hierarchical partitioning and is extremely simple to implement it also applies to variety of models ie models where the facilities can be placed anywhere in the region or only at customer sites or only at fixed locations the paper shows that the algorithm is log competitive under these various models where is the total number of customers it also shows that the algorithm is competitive with high probability and for any arrival order when customers are uniformly distributed or when they follow distribution satisfying smoothness property experimental results for variety of scenarios indicate that the algorithm behaves extremely well in practice
major trend in modern system on chip design is growing system complexity which results in sharp increase of communication traffic on the on chip communication bus architectures in real time embedded system task arrival rate inter task arrival time and data size to be transferred are not uniform over time this is due to the partial re configuration of an embedded system to cope with dynamic workload in this context the traditional application specific bus architectures may fail to meet the real time constraints thus to incorporate the random behavior of on chip communication this work proposes an approach to synthesize an on chip bus architecture which is robust for given distributions of random tasks the randomness of communication tasks is characterized by three main parameters which are the average task arrival rate the average inter task arrival time and the data size for synthesis an on chip bus requirement is guided by the worst case performance need while the dynamic voltage scaling technique is used to save energy when the workload is low or timing slack is high this in turn results in an effective utilization of communication resources under variable workload
web designers usually ignore how to model real user expectations and goals mainly due to the large and heterogeneous audience of the web this fact leads to websites which are difficult to comprehend by visitors and complex to maintain by designers in order to ameliorate this scenario an approach for using the modeling framework in web engineering has been developed in this paper furthermore we also present traceability approach for obtaining different kind of design artifacts tailored to specific web modeling method finally we include sample of our approach in order to show its applicability and we describe prototype tool as proof of concept of our research
interoperability and cross fertilization of multiple hypermedia domains are relatively new concerns to the open hypermedia and structural computing community recent work in this area has explored integrated data models and component based structural service architectures this paper focuses on the user interface aspects relating to client applications of the structural services it presents an integrative design of graphical hypermedia user interface for hypermedia based process centric enterprise models an enterprise model covers various important perspectives of an enterprise such as processes information organization and systems in this paper examples are given to show that these perspectives are presented better using mixture of hypermedia structures found in several hypermedia domains the visualization and interaction design for such hypermedia based enterprise models integrates many features found in navigational spatial taxonomic workflow and cooperative hypertext domains use case is provided to show that client applications based on this design allow users to see and interact with an enterprise model from multiple perspectives in addition some initial user experience is also reported
this paper describes fast html web page detection approach that saves computation time by limiting the similarity computations between two versions of web page to nodes having the same html tag type and by hashing the web page in order to provide direct access to node information this efficient approach is suitable as client application and for implementing server applications that could serve the needs of users in monitoring modifications to html web pages made over time and that allow for reporting and visualizing changes and trends in order to gain insight about the significance and types of such changes the detection of changes across two versions of page is accomplished by performing similarity computations after transforming the web page into an xml like structure in which node corresponds to an open close html tag performance and detection reliability results were obtained and showed speed improvements when compared to the results of previous approach
we describe new algorithm for fast global register allocation called linear scan this algorithm is not based on graph coloring but allocates registers to variables in single linear time scan of the variables live ranges the linear scan algorithm is considerably faster than algorithms based on graph coloring is simple to implement and results in code that is almost as efficient as that obtained using more complex and time consuming register allocators based on graph coloring the algorithm is of interest in applications where compile time is concern such as dynamic compilation systems ldquo just in time rdquo compilers and interactive development environments
number of linear classification methods such as the linear least squares fit llsf logistic regression and support vector machines svm's have been applied to text categorization problems these methods share the similarity by finding hyperplanes that approximately separate class of document vectors from its complement however support vector machines are so far considered special in that they have been demonstrated to achieve the state of the art performance it is therefore worthwhile to understand whether such good performance is unique to the svm design or if it can also be achieved by other linear classification methods in this paper we compare number of known linear classification methods as well as some variants in the framework of regularized linear systems we will discuss the statistical and numerical properties of these algorithms with focus on text categorization we will also provide some numerical experiments to illustrate these algorithms on number of datasets
computer systems security management can be largely improved by the installation of mechanisms for automatic discovery of complex attacks due to system vulnerabilities and middleware misconfigurations the aim of this paper is to propose new formal technique for modeling computer system vulnerabilities and automatic generation of attack scenarios exploiting these vulnerabilities
risk relief services rrss as complementary to online trust promoting services are becoming versatile options for risk reduction in online consumer to consumer auctions in this paper we identify factors that affect the behavior of buyers in an online auction market who had to either adopt or not adopt online escrow services oes an experimental cc auction system with embedded decision support features was used to collect data results show that market factors such as fraud rate product price and seller's reputation are important in determining buyers oes adoption this study also finds that sellers reputation has significant effect on buyer's risk perception which influences his oes adoption decision furthermore the buyers oes adoption decisions were found to be congruent with the implied recommendations that were based on expected utility calculations
functional validation of processor design through execution of suite of test programs is common industrial practice in this paper we develop high level architectural specification driven methodology for systematic test suite generation our primary contribution is an automated test suite generation methodology that covers all possible processor pipeline interactions to accomplish this automation we develop fully formal processor model based on communicating extended finite state machines and traverse the processor model for on the fly generation of short test programs covering all reachable states and transitions our test generation method achieves several orders of magnitude reduction in test suite size compared to the previously proposed formal approaches for test generation leading to drastic reduction in validation effort
searching information through the internet often requires users to separately contact several digital libraries use each library interface to author the query analyze retrieval results and merge them with results returned by other libraries such solution could be simplified by using centralized server that acts as gateway between the user and several distributed repositories the centralized server receives the user query forwards the user query to federated repositories mdash possibly translating the query in the specific format required by each repository mdash and fuses retrieved documents for presentation to the user to accomplish these tasks efficiently the centralized server should perform some major operations such as resource selection query transformation and data fusionin this paper we report on some aspects of mind system for managing distributed heterogeneous multimedia libraries mind http wwwmind projectorg in particular this paper focusses on the issue of fusing results returned by different image repositories the proposed approach is based on normalization of matching scores assigned to retrieved images by individual libraries experimental results on prototype system show the potential of the proposed approach with respect to traditional solutions
this paper explores speculative precomputation technique that uses idle thread context in multithreaded architecture to improve performance of single threaded applications it attacks program stalls from data cache misses by pre computing future memory accesses in available thread contexts and prefetching these data this technique is evaluated by simulating the performance of research processor based on the itanium trade isa supporting simultaneous multithreading two primary forms of speculative precomputation are evaluated if only the non speculative thread spawns speculative threads performance gains of up to are achieved when assuming ideal hardware however this speedup drops considerably with more realistic hardware assumptions permitting speculative threads to directly spawn additional speculative threads reduces the overhead associated with spawning threads and enables significantly more aggressive speculation overcoming this limitation even with realistic costs for spawning threads speedups as high as are achieved with an average speedup of
this article is proposal for database index structure the xpath accelerator that has been specifically designed to support the evaluation of xpath path expressions as such the index is capable to support all xpath axes including ancestor following preceding sibling descendant or self etc this feature lets the index stand out among related work on xml indexing structures which had focus on the child and descendant axes only the index has been designed with close eye on the xpath semantics as well as the desire to engineer its internals so that it can be supported well by existing relational database query processing technology the index permits set oriented or rather sequence oriented path evaluation and can be implemented and queried using well established relational index structures notably trees and treeswe discuss the implementation of the xpath accelerator on top of different database backends and show that the index performs well on all levels of the memory hierarchy including disk based and main memory based database systems
the design of survivable mesh based communication networks has received considerable attention in recent years one task is to route backup paths and allocate spare capacity in the network to guarantee seamless communications services survivable to set of failure scenarios this is complex multi constraint optimization problem called the spare capacity allocation sca problem this paper unravels the sca problem structure using matrix based model and develops fast and efficient approximation algorithm termed successive survivable routing ssr first per flow spare capacity sharing is captured by spare provision matrix spm method the spm matrix has dimension the number of failure scenarios by the number of links it is used by each demand to route the backup path and share spare capacity with other backup paths next based on special link metric calculated from spm ssr iteratively routes updates backup paths in order to minimize the cost of total spare capacity backup path can be further updated as long as it is not carrying any traffic furthermore the spm method and ssr algorithm are generalized from protecting all single link failures to any arbitrary link failures such as those generated by shared risk link groups or all single node failures numerical results comparing several sca algorithms show that ssr has the best trade off between solution optimality and computation speed
the most important features of string matching algorithm are its efficiency and its flexibility efficiency has traditionally received more attention while flexibility in the search pattern is becoming more and more important issue most classical string matching algorithms are aimed at quickly finding an exact pattern in text being knuth morris pratt kmp and the boyer moore bm family the most famous ones recent development uses deterministic suffix automata to design new optimal string matching algorithms eg bdm and turbobdm flexibility has been addressed quite separately by the use of bit parallelism which simulates automata in their nondeterministic form by using bits and exploiting the intrinsic parallelism inside the computer word eg the shift or algorithm those algorithms are extended to handle classes of characters and errors in the pattern and or in the text their drawback being their inability to skip text characters in this paper we merge bit parallelism and suffix automata so that nondeterministic suffix automaton is simulated using bit parallelism the resulting algorithm called bndm obtains the best from both worlds it is much simpler to implement than bdm and nearly as simple as shift or it inherits from shift or the ability to handle flexible patterns and from bdm the ability to skip characters bndm is faster than bdm and up to times faster than shift or when compared to the fastest existing algorithms on exact patterns which belong to the bm family bndm is from slower to times faster depending on the alphabet size with respect to flexible pattern searching bndm is by far the fastest technique to deal with classes of characters and is competitive to search allowing errors in particular bndm seems very adequate for computational biology applications since it is the fastest algorithm to search on dna sequences and flexible searching is an important problem in that area as theoretical development related to flexible pattern matching we introduce new automaton to recognize suffixes of patterns with classes of characters to the best of our knowledge this automaton has not been studied before
balancing the competing goals of collaboration and security is difficult multidimensional problem collaborative systems often focus on building useful connections among people tools and information while security seeks to ensure the availability confidentiality and integrity of these same elements in this article we focus on one important dimension of this problem access control the article examines existing access control models as applied to collaboration highlighting not only the benefits but also the weaknesses of these models
computer software must dynamically adapt to changing conditions in order to fully realize the benefit of dynamic adaptation it must be performed correctly the correctness of adaptation cannot be properly addressed without precisely specifying the requirements for adaptation this paper introduces an approach to formally specifying adaptation requirements in temporal logic we introduce ltl an adaptation based extension to linear temporal logic and use this logic to specify three commonly used adaptation semantics neighborhood composition and sequential composition techniques are developed and applied to ltl to construct the specification of an adaptive system we introduce adaptation semantics graphs to visually present the adaptation semantics specifications for adaptive systems can be automatically generated from adaptation semantics graphs
real time database systems support data processing needs of real time systems where transactions have time constraints here we consider repetitively executed transactions and assume that execution histories are logged well known priority assignment technique called earliest deadline first is biased towards short transactions in which short transactions have better chances of completing their executions within their deadlines we introduce the notion of fair scheduling in which the goal is to have similar completion ratios for all transaction classes short to long in sizes we propose priority assignment techniques that overcome the biased scheduling and show that they work via extensive simulation experiments
online analytical processing olap systems allow one to quickly provide answers to analytical queries on multidimensional data these systems are typically based on visualisations characterised by limited interaction and exploration capabilities in this paper we propose innovative visual and interaction techniques for the analysis of multidimensional data the proposed solution is based on three dimensional hypercube representations that can be explored using dynamic queries and that combines colour coding detail on demand cutting planes and viewpoint control techniques we demonstrate the effectiveness of our visualisation tool by providing some practical examples showing how it has been used for extracting information from real multidimensional data
in this paper we introduce method for web page ranking based on computational geometry to evaluate and test by examples order relationships among web pages belonging to different knowledge domains the goal is through an organising procedure to learn from these examples real valued ranking function that induces ranking via convexity feature we consider the problem of self organising learning from numerical data to be represented by well fitted convex polygon procedure in which the vertices correspond to descriptors representing domains of web pages results and statistical evaluation of procedure show that the proposed method may be characterised as accurate
navigation has added interactivity in nowadays multimedia applications which support effective accessing to objects of various formats and presentation requirements storage issues need to be reconsidered for the new type of navigational multimedia applications in order to improve system's performance this paper addresses the problem of multimedia data storage towards improving data accessibility and request servicing under navigational applications navigational graph based model for the multimedia data representation is proposed to guide the data placement under hierarchical storage topology the multimedia data dependencies access frequencies and timing constraints are used to characterize the graph nodes which correspond to multimedia objects allocated at the tertiary storage level based on certain defined popularity criteria data are elevated and placed on secondary level towards improving both the request servicing and data accessibility the proposed multimedia data elevation is prefetching approach since it is performed apriori not on demand based on previously extracted user access patterns appropriate data placement policies are also employed at the secondary level and simulation model has been developed based on current commercial tertiary and secondary storage devices this model is used to evaluate the proposed popularity based data elevation approach as employed under hierarchical storage subsystem experimentation is performed under artificial data workloads and it is shown that the proposed hierarchical data placement approach considerably improves data accessing and request servicing in navigational multimedia applications the iterative improvement placement is proven to outperform earlier related multimedia data placement policies with respect to commonly used performance metrics
while hypertext is often claimed to be tool that especially aids associative thinking intellectual work involves more than association so questions arise about the usefulness of hypertext tools in the more disciplined aspects of scholarly and argumentative writing examining the phases of scholarly writing reveals that different hypertext tools can aid different phases of intellectual work in ways other than associative thinking spatial hypertext is relevant at all phases while page and link hypertext is more appropriate to some phases than others
one of the biggest challenges in future application development is device heterogeneity in the future we expect to see rich variety of computing devices that can run applications these devices have different capabilities in processors memory networking screen sizes input methods and software libraries we also expect that future users are likely to own many types of devices depending on users changing situations and environments they may choose to switch from one type of device to another that brings the best combination of application functionality and device mobility size weight etc based on this scenario we have designed and implemented seamless application framework called the roam system that can both assist developers to build multiplatform applications that can run on heterogeneous devices and allow user to move migrate running application among heterogeneous devices in an effortless manner the roam system is based on partitioning of an application into components and it automatically selects the most appropriate adaptation strategy at the component level for target platform to evaluate our system we have created several multi platform roam applications including chess game connect game and shopping aid application we also provide measurements on application performance and describe our experience with application development in the roam system our experience shows that it is relatively easy to port existing applications to the roam system and runtime application migration latency is within few seconds and acceptable to most non real time applications
in this paper we focus on the design of bivariate edas for discrete optimization problems and propose new approach named hsmiec while the current edas require much time in the statistical learning process as the relationships among the variables are too complicated we employ the selfish gene theory sg in this approach as well as mutual information and entropy based cluster miec model is also set to optimize the probability distribution of the virtual population this model uses hybrid sampling method by considering both the clustering accuracy and clustering diversity and an incremental learning and resample scheme is also set to optimize the parameters of the correlations of the variables compared with several benchmark problems our experimental results demonstrate that hsmiec often performs better than some other edas such as bmda comit mimic and ecga
rare objects are often of great interest and great value until recently however rarity has not received much attention in the context of data mining now as increasingly complex real world problems are addressed rarity and the related problem of imbalanced data are taking center stage this article discusses the role that rare classes and rare cases play in data mining the problems that can result from these two forms of rarity are described in detail as are methods for addressing these problems these descriptions utilize examples from existing research so that this article provides good survey of the literature on rarity in data mining this article also demonstrates that rare classes and rare cases are very similar phenomena both forms of rarity are shown to cause similar problems during data mining and benefit from the same remediation methods
we present an algorithm for translating xslt programs into sql our context is that of virtual xml publishing in which single xml view is defined from relational database and subsequently queried with xslt programs each xslt program is translated into single sql query and run entirely in the database engine our translation works for large fragment of xslt which we define that includes descendant ancestor axis recursive templates modes parameters and aggregates we put considerable effort in generating correct and efficient sql queries and describe several optimization techniques to achieve this efficiency we have tested our system on all sql queries of the tpc database benchmark which we represented in xslt and then translated back to sql using our translator
we present new phrase based conditional exponential family translation model for statistical machine translation the model operates on feature representation in which sentence level translations are represented by enumerating all the known phrase level translations that occur inside them this makes the model good match with the commonly used phrase extraction heuristics the model's predictions are properly normalized probabilities in addition the model automatically takes into account information provided by phrase overlaps and does not suffer from reference translation reachability problems we have implemented an open source translation system sinuhe based on the proposed translation model our experiments on europarl and gigafren corpora demonstrate that finding the unique map parameters for the model on large scale data is feasible with simple stochastic gradient methods sinuhe is fast and memory efficient and the bleu scores obtained by it are only slightly inferior to those of moses
the objective of this article is to investigate the problem of generating both positive and negative exact association rules when formal context of positive attributes is provided straightforward solution to this problem consists of conducting an apposition of the initial context with its complementary context construct the concept lattice of apposed contexts and then extract rules more challenging problem consists of exploiting rules generated from each one of the contexts and to get the whole set of rules for the context in this paper we analyze set of identified situations based on distinct types of input and come out with set of properties obviously the global set of positive and negative rules is superset of purely positive rules ie rules with positive attributes only and purely negative ones since it generally contains mixed rules ie rules in which at least positive attribute and negative attribute coexist the paper presents also set of inference rules to generate subset of all mixed rules from positive negative and mixed ones finally two key conclusions can be drawn from our analysis the generic basis containing negative rules œÉk cannot be completely and directly inferred from the set œÉk of positive rules or from the concept lattice and ii the whole set of mixed rules may not be completely generated from œÉk alone œÉk œÉk alone or alone
the frequent pattern tree fp tree is an efficient data structure for association rule mining without generation of candidate itemsets it was used to compress database into tree structure which stored only large items it however needed to process all transactions in batch way in real world applications new transactions are usually incrementally inserted into databases in the past we proposed fast updated fp tree fufp tree structure to efficiently handle new transactions and to make the tree update process become easier in this paper we attempt to modify the fufp tree construction based on the concept of pre large itemsets pre large itemsets are defined by lower support threshold and an upper support threshold it does not need to rescan the original database until number of new transactions have been inserted the proposed approach can thus achieve good execution time for tree construction especially when each time small number of transactions are inserted experimental results also show that the proposed pre fufp maintenance algorithm has good performance for incrementally handling new transactions
abstract in this paper we argue the need for effective resource management mechanisms for sharing resources in commodity clusters to address this issue we present the design of sharc system that enables resource sharing among applications in such clusters sharc depends on single node resource management mechanisms such as reservations or shares and extends the benefits of such mechanisms to clustered environments we present techniques for managing two important resources cpu and network interface bandwidth on cluster wide basis our techniques allow sharc to support reservation of cpu and network interface bandwidth for distributed applications dynamically allocate resources based on past usage and provide performance isolation to applications our experimental evaluation has shown that sharc can scale to node clusters running applications these results demonstrate that sharc can be an effective approach for sharing resources among competing applications in moderate size clusters
an automatic guided vehicle agv transportation system is fully automated system that provides logistic services in an industrial environment such as warehouse or factory traditionally the agvs that execute the transportation tasks are controlled by central server via wireless communication in joint effort between egemin an industrial manufacturer of agv transportation systems and distrinet labs research at the katholieke universiteit leuven we developed an innovative decentralized architecture for controlling agvs the driving motivations behind decentralizing the control of agvs were new and future quality requirements such as flexibility and openness at the software architectural level the agv control system is structured as multi agent system the detailed design and implementation is object oriented in this paper we report our experiences with developing the agent based control system for agvs starting from system requirements we give an overview of the software architecture and we zoom in on number of concrete functionalities we reflect on our experiences and report lessons learned from applying multi agent systems for real world agv control
there has been growing interest in mining frequent itemsets in relational data with multiple attributes key step in this approach is to select set of attributes that group data into transactions and separate set of attributes that labels data into items unsupervised and unrestricted mining however is stymied by the combinatorial complexity and the quantity of patterns as the number of attributes grows in this paper we focus on leveraging the semantics of the underlying data for mining frequent itemsets for instance there are usually taxonomies in the data schema and functional dependencies among the attributes domain knowledge and user preferences often have the potential to significantly reduce the exponentially growing mining space these observations motivate the design of user directed data mining framework that allows such domain knowledge to guide the mining process and control the mining strategy we show examples of tremendous reduction in computation by using domain knowledge in mining relational data with multiple attributes
this paper presents multiple model real time tracking technique for video sequences based on the mean shift algorithm the proposed approach incorporates spatial information from several connected regions into the histogram based representation model of the target and enables multiple models to be used to represent the same object the use of several regions to capture the color spatial information into single combined model allow us to increase the object tracking efficiency by using multiple models we can make the tracking scheme more robust in order to work with sequences with illumination and pose changes we define model selection function that takes into account both the similarity of the model with the information present in the image and the target dynamics in the tracking experiments presented our method successfully coped with lighting changes occlusion and clutter
multicore hardware is making concurrent programs pervasive unfortunately concurrent programs are prone to bugs among different types of concurrency bugs atomicity violation bugs are common and important existing techniques to detect atomicity violation bugs suffer from one limitation requiring bugs to manifest during monitored runs which is an open problem in concurrent program testing this paper makes two contributions first it studies the interleaving characteristics of the common practice in concurrent program testing ie running program over and over to understand why atomicity violation bugs are hard to expose second it proposes ctrigger to effectively and efficiently expose atomicity violation bugs in large programs ctrigger focuses on special type of interleavings ie unserializable interleavings that are inherently correlated to atomicity violation bugs and uses trace analysis to systematically identify likely feasible unserializable interleavings with low occurrence probability ctrigger then uses minimum execution perturbation to exercise low probability interleavings and expose difficult to catch atomicity violation we evaluate ctrigger with real world atomicity violation bugs from four sever desktop applications apache mysql mozilla and pbzip and three splash applications on core machines ctrigger efficiently exposes the tested bugs within seconds two to four orders of magnitude faster than stress testing without ctrigger some of these bugs do not manifest even after full days of stress testing in addition without deterministic replay support once bug is exposed ctrigger can help programmers reliably reproduce it for diagnosis our tested bugs are reproduced by ctrigger mostly within seconds to over times faster than stress testing
we study oblivious routing in which the packet paths are constructed independently of each other we give simple oblivious routing algorithm for geometric networks in which the nodes are embedded in the euclidean plane in our algorithm packet path is constructed by first choosing random intermediate node in the space between the source and destination and then the packet is sent to its destination through the intermediate node we analyze the performance of the algorithm in terms of the stretch and congestion of the resulting paths we show that the stretch is constant and the congestion is near optimal when the network paths can be chosen to be close to the geodesic lines that connect the end points of the paths we give applications of our general result to the mesh topology and uniformly distributed disc graphs previous oblivious routing algorithms with near optimal congestion use many intermediate nodes and do not control the stretch
hardly predictable data addresses in many irregular applications have rendered prefetching ineffective in many cases the only accurate way to predict these addresses is to directly execute the code that generates them as multithreaded architectures become increasingly popular one attractive approach is to use idle threads on these machines to perform pre execution mdash essentially combined act of speculative address generation and prefetching mdash to accelerate the main thread in this paper we propose such pre execution technique for simultaneous multithreading smt processors by using software to control pre execution we are able to handle some of the most important access patterns that are typically difficult to prefetch compared with existing work on pre execution our technique is significantly simpler to implement eg no integration of pre execution results no need of shortening programs for pre execution and no need of special hardware to copy register values upon thread spawns consequently only minimal extensions to smt machines are required to support our technique despite its simplicity our technique offers an average speedup of in set of irregular applications which is speedup over state of the art software controlled prefetching
we propose texture function for realistic modeling and efficient rendering of materials that exhibit surface mesostructures translucency and volumetric texture variations the appearance of such complex materials for dynamic lighting and viewing directions is expensive to calculate and requires an impractical amount of storage to precompute to handle this problem our method models an object as shell layer formed by texture synthesis of volumetric material sample and homogeneous inner core to facilitate computation of surface radiance from the shell layer we introduce the shell texture function stf which describes voxel irradiance fields based on precomputed fine level light interactions such as shadowing by surface mesostructures and scattering of photons inside the object together with diffusion approximation of homogeneous inner core radiance the stf leads to fast and detailed raytraced renderings of complex materials
in this paper we present new method for alignment of models this approach is based on two types of symmetries of the models the reflective symmetry and the local translational symmetry along direction inspired by the work on the principal component analysis pca we select the best optimal alignment axes within the pca axes the plane reflection symmetry being used as selection criterion this pre processing transforms the alignment problem into an indexing scheme based on the number of the retained pca axes in order to capture the local translational symmetry of shape along direction we introduce new measure we call the local translational invariance cost ltic the mirror planes of model are also used to reduce the number of candidate coordinate frames when looking for the one which corresponds to the user's perception experimental results show that the proposed method finds the rotation that best aligns mesh
we present new approach for efficient collision handling of meshless objects undergoing geometric deformation the presented technique is based on bounding sphere hierarchies we show that information of the geometric deformation model can be used to significantly accelerate the hierarchy update the cost of the presented hierarchy update depends on the number of primitives in close proximity but not on the total number of primitives further the hierarchical collision detection is combined with level of detail response scheme since the collision response can be performed on any level of the hierarchy it allows for balancing accuracy and efficiency thus the collision handling scheme is particularly useful for time critical applications
consider set of servers and set of users where each server has coverage region ie an area of service and capacity ie maximum number of users it can serve our task is to assign every user to one server subject to the coverage and capacity constraints to offer the highest quality of service we wish to minimize the average distance between users and their assigned server this is an instance of well studied problem in operations research termed optimal assignment even though there exist several solutions for the static case where user locations are fixed there is currently no method for dynamic settings in this paper we consider the continuous assignment problem cap where an optimal assignment must be constantly maintained between mobile users and set of servers the fact that the users are mobile necessitates real time reassignment so that the quality of service remains high ie their distance from their assigned servers is minimized the large scale and the time critical nature of targeted applications require fast cap solutions we propose an algorithm that utilizes the geometric characteristics of the problem and significantly accelerates the initial assignment computation and its subsequent maintenance our method applies to different cost functions eg average squared distance and to any minkowski distance metric eg euclidean norm etc
demand driven spectrum allocation can drastically improve performance for wifi access points struggling under increasing user demands while their frequency agility makes cognitive radios ideal for this challenge performing adaptive spectrum allocation is complex and difficult process in this work we propose flex an efficient spectrum allocation architecture that efficiently adapts to dynamic traffic demands flex tunes network wide spectrum allocation by access points coordinating with peers minimizing network resets through local adaptations through detailed analysis and experimental evaluation we show that flex converges quickly provides users with proportional fair spectrum usage and significantly outperforms existing spectrum allocation proposals
this paper describes the techniques used to optimize relational queries in the sdd distributed database system queries are submitted to sdd in high level procedural language called datalanguage optimization begins by translating each datalanguage query into relational calculus form called an envelope which is essentially an aggregate free quel query this paper is primarily concerned with the optimization of envelopes envelopes are processed in two phases the first phase executes relational operations at various sites of the distributed database in order to delimit subset of the database that contains all data relevant to the envelope this subset is called reduction of the database the second phase transmits the reduction to one designated site and the query is executed locally at that site the critical optimization problem is to perform the reduction phase efficiently success depends on designing good repertoire of operators to use during this phase and an effective algorithm for deciding which of these operators to use in processing given envelope against given database the principal reduction operator that we employ is called semijoin in this paper we define the semijoin operator explain why semijoin is an effective reduction operator and present an algorithm that constructs cost effective program of semijoins given an envelope and database
the effectiveness of information retrieval ir systems is influenced by the degree of term overlap between user queries and relevant documents query document term mismatch whether partial or total is fact that must be dealt with by ir systems query expansion qe is one method for dealing with term mismatch ir systems implementing query expansion are typically evaluated by executing each query twice with and without query expansion and then comparing the two result sets while this measures an overall change in performance it does not directly measure the effectiveness of ir systems in overcoming the inherent issue of term mismatch between the query and relevant documents nor does it provide any insight into how such systems would behave in the presence of query document term mismatch in this paper we propose new approach for evaluating query expansion techniques the proposed approach is attractive because it provides an estimate of system performance under varying degrees of query document term mismatch it makes use of readily available test collections and it does not require any additional relevance judgments or any form of manual processing
this paper considers the problem of self diagnosis of wireless and mobile ad hoc networks manets using the comparison approach in this approach network manet consists of collection of independent heterogeneous mobile or stationary hosts interconnected via wireless links and it is assumed that at most of these hosts are faulty in order to diagnose the state of the manet tasks are assigned to pairs of hosts and the outcomes of these tasks are compared the agreements and disagreements between the hosts are the basis for identifying the faulty ones the comparison approach is believed to be one of the most practical fault identification approaches for diagnosing hard and soft faults we develop new distributed self diagnosis protocol called dynamic dsdp for manets that identifies both hard and soft faults in finite amount of time the protocol is constructed on top of reliable multi hop architecture correctness and complexity proofs are provided and they show that our dynamic dsdp performs better from communication complexity viewpoint than the existing protocols we have also developed simulator that is scalable to large number of nodes using the simulator we carried out simulation study to analyze the effectiveness of the self diagnosis protocol and its performance with regards to the number of faulty hosts the simulation results show that the proposed approach is an attractive and viable alternative or addition to present fault diagnosis techniques in manet environments
number of recent technological trends have made data intensive applications such as continuous media audio and video servers reality these servers are expected to play an important role in applications such as video on demand digital library news on demand distance learning etc continuous media applications are data intensive and might require storage subsystems that consist of hundreds of multi zone disk drives with the current technological trends homogeneous disk subsystem might evolve to consist of heterogeneous collection of disk drives given such storage subsystem the system must continue to support hiccup free display of audio and video clips this study describes extensions of four continuous display techniques for multi zone disk drives to heterogeneous platform these techniques include ibm's logical track hp's track pairing and usc's fixb and deadline driven techniques we quantify the performance tradeoff associated with these techniques using analytical models and simulation studies the obtained results demonstrate tradeoffs between the cost per simultaneous stream supported by technique the wasted disk space and the incurred startup latency
packet classification categorizes incoming packets into multiple forwarding classes in router based on predefined filters it is important in fulfilling the requirements of differentiated services to achieve fast packet classification new approach namely filter rephrasing is proposed to encode the original filters by exploiting the hierarchical property of the filters filter rephrasing could dramatically reduce the search and storage complexity incurred in packet classification we incorporate well known scheme rectangle search with filter rephrasing to improve the lookup speed by at least factor of and decreases of the storage expenses as compared with other existing schemes the proposed scheme exhibits better balance between speed storage and computation complexity consequently the scalable effect of filter rephrasing is suitable for backbone routers with great number of filters
recent work on incremental crawling has enabled the indexed document collection of search engine to be more synchronized with the changing world wide web however this synchronized collection is not immediately searchable because the keyword index is rebuilt from scratch less frequently than the collection can be refreshed an inverted index is usually used to index documents crawled from the web complete index rebuild at high frequency is expensive previous work on incremental inverted index updates have been restricted to adding and removing documents updating the inverted index for previously indexed documents that have changed has not been addressedin this paper we propose an efficient method to update the inverted index for previously indexed documents whose contents have changed our method uses the idea of landmarks together with the diff algorithm to significantly reduce the number of postings in the inverted index that need to be updated our experiments verify that our landmark diff method results in significant savings in the number of update operations on the inverted index
despite improvements in network interfaces and software messaging layers software communication overhead still dominates the hardware routing cost in most systems in this study we identify the sources of this overhead by analyzing software costs of typical communication protocols built atop the active messages layer on the cm we show that up to ndash of the software messaging costs are direct consequence of the gap between specific network features such as arbitrary delivery order finite buffering and limited fault handling and the user communication requirements of in order delivery end to end flow control and reliable transmission however virtually all of these costs can be eliminated if routing networks provide higher level services such as in order delivery end to end flow control and packet level fault tolerance we conclude that significant cost reductions require changing the constraints on messaging layers we propose designing networks and network interfaces which simplify or replace software for implementing user communication requirements
the aim of this study was to empirically evaluate an embodied conversational agent called greta in an effort to answer two main questions what are the benefits and costs of presenting information via an animated agent with certain characteristics in persuasion task compared to other forms of display how important is it that emotional expressions are added in way that is consistent with the content of the message in animated agents to address these questions positively framed healthy eating message was created which was variously presented via greta matched human actor greta's voice only no face or as text only furthermore versions of greta were created which displayed additional emotional facial expressions in way that was either consistent or inconsistent with the content of the message overall it was found that although greta received significantly higher ratings for helpfulness and likability presenting the message via greta led to the poorest memory performance among users importantly however when greta's additional emotional expressions were consistent with the content of the verbal message the negative effect on memory performance disappeared overall the findings point to the importance of achieving consistency in animated agents
scalable busy wait synchronization algorithms are essential for achieving good parallel program performance on large scale multiprocessors such algorithms include mutual exclusion locks reader writer locks and barrier synchronization unfortunately scalable synchronization algorithms are particularly sensitive to the effects of multiprogramming their performance degrades sharply when processors are shared among different applications or even among processes of the same application in this paper we describe the design and evaluation of scalable scheduler conscious mutual exclusion locks reader writer locks and barriers and show that by sharing information across the kernel application interface we can improve the performance of scheduler oblivious implementations by more than an order of magnitude
in shared memory multiprocessor system it may be more efficient to schedule taskon one processor than on another if relevant data already reside in particularprocessor's cache the effects of this type of processor affinity are examined it isobserved that tasks continuously alternate between executing at processor andreleasing this processor due to synchronization quantum expiration or preemptionqueuing network models of different abstract scheduling policies are formulated spanning the range from ignoring affinity to fixing tasks on processors these models are solved via mean value analysis where possible and by simulation otherwise an analytic cache model is developed and used in these scheduling models to include the effects of an initial burst of cache misses experienced by tasks when they return to processor forexecution mean value technique is also developed and used in the scheduling modelsto include the effects of increased bus traffic due to these bursts of cache misses onlya small amount of affinity information needs to be maintained for each task theimportance of having policy that adapts its behavior to changes in system load isdemonstrated
many modern applications have significant operating system os component the os execution affects various architectural states including the dynamic branch predictions which are widely used in today's high performance microprocessor designs to improve performance this impact tends to become more significant as the designs become more deeply pipelined and more speculative in this paper we focus on the issues of understanding the os effects on the branch predictions and designing architectural support to alleviate the bottlenecks that are created by misprediction in this work we characterize the control flow transfer of several emerging applications on commercial os it was observed that the exception driven intermittent invocation of os code and user os branch history interference increased misprediction in both user and kernel code we propose two simple os aware control flow prediction techniques to alleviate the destructive impact of user os branch interference the first consists of capturing separate branch correlation information for user and kernel code the second involves using separate branch prediction tables for user and kernel code we demonstrate in this paper that os aware branch predictions require minimal hardware modifications and additions moreover the os aware branch predictions can be integrated with many existing schemes to further improve their performance we studied the improvement contributed by os aware techniques to various branch prediction schemes ranging from the simple gshare to the more advanced agree multi hybrid and bi mode predictors on the entry predictors incorporating the os aware techniques yields up to percent percent percent and percent prediction accuracy improvement on the gshare multi hybrid agree and bi mode predictors respectively
many recent studies have convincingly demonstrated that network traffic exhibits noticeable self similar nature which has considerable impact on queuing performance however the networks used in current multicomputers have been primarily designed and analyzed under the assumption of the traditional poisson arrival process which is inherently unable to capture traffic self similarity consequently it is crucial to reexamine the performance properties of multicomputer networks in the context of more realistic traffic models before practical implementations show their potential faults in an effort toward this end this paper proposes the first analytical model for wormhole switched ary cubes in the presence of self similar traffic simulation experiments demonstrate that the proposed model exhibits good degree of accuracy for various system sizes and under different operating conditions the analytical model is then used to investigate the implications of traffic self similarity on network performance this study reveals that the network suffers considerable performance degradation when subjected to self similar traffic stressing the great need for improving network performance to ensure efficient support for this type of traffic
actions usually taken to prevent processors from overheating such as decreasing the frequency or stopping the execution flow also degrade performance multiprocessor systems however offer the possibility of moving the task that caused cpu to overheat away to some other cooler cpu so throttling becomes only last resort taken if all of system's processors are hot additionally the scheduler can take advantage of the energy characteristics of individual tasks and distribute hot tasks as well as cool tasks evenly among all cpusthis work presents mechanism for determining the energy characteristics of tasks by means of event monitoring counters and an energy aware scheduling policy that strives to assign tasks to cpus in way that avoids overheating individual cpus our evaluations show that the benefit of avoiding throttling outweighs the overhead of additional task migrations and that energy aware scheduling in many cases increases the system's throughput
in recent years computer and internet technologies have broadened the ways that people can stay in touch through interviews with parents and grandparents we examined how people use existing technologies to communicate and share with their extended family while most of our participants expressed desire for more communication and sharing with their extended family many felt that an increase would realistically be difficult to achieve due to challenges such as busy schedules or extended family members lack of technology use our results also highlight the complexity of factors that researchers and designers must understand when attempting to design technology to support and enhance relationships including trade offs between facilitating interaction while minimizing new obligations reducing effort without trivializing communication and balancing awareness with privacy
we use realistic interdomain routing experiment platform to conduct real time attack and defense exercises for training purposes our interdomain routing experiment platform integrates open source router software real time network simulation and light weight machine virtualization technologies and is capable of supporting realistic large scale routing experiments the network model used consists of major autonomous systems connecting swedish internet users with realistic routing configurations derived from the routing registry we conduct series of real time security exercises on this routing system to study the consequence of intentionally propagating false routing information on interdomain routing and the effectiveness of corresponding defensive measures we describe three kinds of simplistic bgp attacks in the context of security exercises designed specifically for training purposes while an attacker can launch attacks from compromised router by changing its routing policies administrators will be able to observe the adverse effect of these attacks and subsequently apply appropriate defensive measures to mitigate their impact such as installing filtering rules these exercises all carried out in real time demonstrate the feasibility of routing experiments using the real time routing experiment platform
reprogramming of sensor networks is an important and challenging problem as it is often necessary to reprogram the sensors in place in this article we propose mnp multihop reprogramming service designed for sensor networks one of the problems in reprogramming is the issue of message collision to reduce the problem of collision we propose sender selection algorithm that attempts to guarantee that in given neighborhood there is at most one source transmitting the program at time furthermore our sender selection is greedy in that it tries to select the sender that is expected to have the most impact we use pipelining to enable fast data propagation mnp is energy efficient because it reduces the active radio time of sensor node by putting the node into ldquo sleep rdquo state when its neighbors are transmitting segment that is not of interest we call this type of sleep contention sleep to further reduce the energy consumption we add noreq sleep where sensor node goes to sleep if none of its neighbors is interested in receiving the segment it is advertising we also introduce an optional init sleep to reduce the energy consumption in the initial phase of reprogramming finally we investigate the performance of mnp in different network settings
this paper presents scuba secure code update by attestation for detecting and recovering compromised nodes in sensor networks the scuba protocol enables the design of sensor network that can detect compromised nodes without false negatives and either repair them through code updates or revoke the compromised nodes the scuba protocol represents promising approach for designing secure sensor networks by proposing first approach for automatic recovery of compromised sensor nodes the scuba protocol is based on ice indisputable code execution primitive we introduce to dynamically establish trusted code base on remote untrusted sensor node
recently ubiquitous location based services lbs has been utilized in variety of practical and mission critical applications such as security services personalization services location based entertainment and location based commerce the essence of lbs is actually the concept of location awareness where location aware devices perform more intelligent services for users by utilizing their locations in order to realize achieve this concept mobile device should continuously monitor the real time contextual changes of user this is what we call location based monitoring in this paper we discuss the research and technical issues on designing location based real time monitoring systems in lbs along with three major subjects high performance spatial index monitoring query processing engine and distributed monitoring system with dynamic load balancing capability and energy efficient management
when program violates its specification model checker produces counterexample that shows an example of undesirable behavior it is up to the user to understand the error locate it and fix the problem previous work introduced technique for explaining and localizing errors based on finding the closest execution to counterexample with respect to distance metric that approach was applied only to concrete executions of programs this paper extends and generalizes the approach by combining it with predicate abstraction using an abstract state space increases scalability and makes explanations more informative differences between executions are presented in terms of predicates derived from the specification and program rather than specific changes to variable values reasoning to the cause of an error from the factthat in the failing run but in the successful execution is easier than reasoning from the information that in the failing run but in the successful execution an abstract explanation is automatically generalized predicate abstraction has previously been used in model checking purely as state space reduction technique however an abstraction good enough to enable model checking tool to find an error is also likely to be useful as an automatically generated high level description of state space suitable for use by programmers results demonstrating the effectiveness of abstract explanations support this claim
we address some crucial problem associated with text categorization local feature selection it seems that intuitionistic fuzzy sets can be an effective and efficient tool making it possible to assess each term from feature set for each category from point of view of both its indicative and non indicative ability it is important especially for high dimensional problems to improve text filtering via confident rejection of non relevant documents moreover we indicate that intuitionistic fuzzy sets are good tool for the classification of imbalanced and overlapping classes commonly encountered case in text categorization
steady progress in the development of optical disc technology over the past decade has brought it to the point where it is beginning to compete directly with magnetic disc technology worm optical discs in particular which permanently register information on the disc surface have significant advantages over magnetic technology for applications that are mainly archival in nature but require the ability to do frequent on line insertions in this paper we propose class of access methods that use rewritable storage for the temporary buffering of insertions to data sets stored on worm optical discs and we examine the relationship between the retrieval performance from worm optical discs and the utilization of disc storage space when one of these organizations is employed we describe the performance trade off as one of fast sequential retrieval of the contents of block versus wasted space owing to data replication model of specific instance of such an organization buffered hash file scheme is described that allows for the specification of retrieval performance objectives alternative strategies for managing data replication that allow trade offs between higher consumption rates and better average retrieval performance are also described we then provide an expected value analysis of the amount of disc space that must be consumed on worm disc to meet specified performance limits the analysis is general enough to allow easy extension to other types of buffered files systems for worm optical discs
there are many applications for which it is necessary to illustrate motion in static image using visual cues which do not represent physical entity in the scene yet are widely understood to convey motion for example consider the task of illustrating the desired movements for exercising dancing or given sport technique traditional artists have developed techniques to specify desired movements precisely technical illustrators and suggest motion cartoonists in an image in this paper we present an interactive system to synthesize image of an animated character by generating artist inspired motion cues derived from skeletal motion capture data the primary cues include directed arrows noise waves and stroboscopic motion first the user decomposes the animation into short sequences containing individual motions which can be represented by visual cues the system then allows the user to determine suitable viewpoint for illustrating the movement to select the proper level in the joint hierarchy as well as to fine tune various controls for the depiction of the cues themselves while the system does provide adapted default values for each control extracted from the motion capture data it allows fine tuning for greater expressiveness moreover these cues are drawn in real time and maintain coherent display with changing viewpoints we demonstrate the benefit of our interactive system on various motion capture sequences
in many data mining and machine learning problems the data items that need to be clustered or classified are not arbitrary points in high dimensional space but are distributions that is points on high dimensional simplex for distributions natural measures are not ell distances but information theoretic measures such as the kullback leibler and hellinger divergences similarly quantities such as the entropy of distribution are more natural than frequency moments efficient estimation of these quantities is key component in algorithms for manipulating distributions since the datasets involved are typically massive these algorithms need to have only sublinear complexity in order to be feasible in practice we present range of sublinear time algorithms in various oracle models in which the algorithm accesses the data via an oracle that supports various queries in particular we answer question posed by batu et al on testing whether two distributions are close in an information theoretic sense given independent samples we then present optimal algorithms for estimating various information divergences and entropy with more powerful oracle called the combined oracle that was also considered by batu et al finally we consider sublinear space algorithms for these quantities in the data stream model in the course of doing so we explore the relationship between the aforementioned oracle models and the data stream model this continues work initiated by feigenbaum et al an important additional component to the study is considering data streams that are ordered randomly rather than just those which are ordered adversarially
secure evaluation of private functions pf sfe allows two parties to compute private function which is known by one party only on private data of both it is known that pf sfe can be reduced to secure function evaluation sfe of universal circuit uc previous uc constructions only simulated circuits with gates of inputs while gates with inputs were decomposed into many gates with inputs which is inefficient for large as the size of uc heavily depends on the number of gateswe present generalized uc constructions to efficiently simulate any circuit with gates of inputs having efficient circuit representation our constructions are non trivial generalizations of previously known uc constructionsas application we show how to securely evaluate private functions such as neural networks nn which are increasingly used in commercial applications our provably secure pf sfe protocol needs only one round in the semi honest model or even no online communication at all using non interactive oblivious transfer and evaluates generalized uc that entirely hides the structure of the private nn this enables applications like privacy preserving data classification based on private nns without trusted third party while simultaneously protecting user's data and nn owner's intellectual property
the recognition of program constructs that are frequently used by software developers is powerful mechanism for optimizing and parallelizing compilers to improve the performance of the object code the development of techniques for automatic recognition of computational kernels such as inductions reductions and array recurrences has been an intensive research area in the scope of compiler technology during the this article presents new compiler framework that unlike previous techniques that focus on specific and isolated kernels recognizes comprehensive collection of computational kernels that appear frequently in full scale real applications the xark compiler operates on top of the gated single assignment gsa form of high level intermediate representation ir of the source code recognition is carried out through demand driven analysis of this high level ir at two different levels first the dependences between the statements that compose the strongly connected components sccs of the data dependence graph of the gsa form are analyzed as result of this intra scc analysis the computational kernels corresponding to the execution of the statements of the sccs are recognized second the dependences between statements of different sccs are examined in order to recognize more complex kernels that result from combining simpler kernels in the same code overall the xark compiler builds hierarchical representation of the source code as kernels and dependence relationships between those kernels this article describes in detail the collection of computational kernels recognized by the xark compiler besides the internals of the recognition algorithms are presented the design of the algorithms enables to extend the recognition capabilities of xark to cope with new kernels and provides an advanced symbolic analysis framework to run other compiler techniques on demand finally extensive experiments showing the effectiveness of xark for collection of benchmarks from different application domains are presented in particular the sparskit ii library for the manipulation of sparse matrices the perfect benchmarks the spec cpu collection and the pltmg package for solving elliptic partial differential equations are analyzed in detail
in this paper we initiate the study of the approximability of the facility location problem in distributed setting in particular we explore trade off between the amount of communication and the resulting approximation ratio we give distributed algorithm that for every constant achieves an mœÅ klog approximation in communication rounds where message size is bounded to log bits the number of facilities and clients are and respectively and is coefficient that depends on the cost values of the instance our technique is based on distributed primal dual approach for approximating linear program that does not form covering or packing program
risps reconfigurable instruction set processors are increasingly becoming popular as they can be customized to meet design constraints however existing instruction set customization methodologies do not lend well for mapping custom instructions on to commercial fpga architectures in this paper we propose design exploration framework that provides for rapid identification of reduced set of profitable custom instructions and their area costs on commercial architectures without the need for time consuming hardware synthesis process novel clustering strategy is used to estimate the utilization of the lut look up table based fpgas for the chosen custom instructions our investigations show that the area costs computations using the proposed hardware estimation technique on custom instructions are shown to be within of those obtained using hardware synthesis systematic approach has been adopted to select the most profitable custom instruction candidates our investigations show that this leads to notable reduction in the number of custom instructions with only marginal degradation in performance simulations based on domain specific application sets from the mibench and mediabench benchmark suites show that on average more than area utilization efficiency performance area can be achieved with the proposed technique
the web has been rapidly deepened by myriad searchable databases online where data are hidden behind query forms helping users query alternative deep web sources in the same domain eg books airfares is an important task with broad applications as core component of those applications dynamic query translation ie translating user's query across dynamically selected sources has not been extensively explored while existing works focus on isolated subproblems eg schema matching query rewriting to study we target at building complete query translator and thus face new challenges to complete the translator we need to solve the predicate mapping problem ie map source predicate to target predicates which is largely unexplored by existing works to satisfy our application requirements we need to design customizable system architecture to assemble various components addressing respective subproblems ie schema matching predicate mapping query rewriting tackling these challenges we develop light weight domain based form assistant which can generally handle alternative sources in the same domain and is easily customizable to new domains our experiment shows the effectiveness of our form assistant in translating queries for real web sources
we present two flexible internet protocol ip router hardware hw architectures that enable router to readily expand in capacity according to network traffic volume growth and reconfigure its functionalities according to the hierarchical network layer in which it is placed reconfigurability is effectuated by novel method called methodology for hardware unity and an associated functional unit special processing agent which can be built using state of the art technology more specifically reconfiguration between an edge and hub or backbone router can be done rapidly via simple open close connection approach such architectures may among other benefits significantly extend the intervals between router hw upgrades for internet services providers they can serve as the basis for development of the next generation ip routers and are directly applicable to the emerging concept of single layer ip network architecture
interactive cross language information retrieval clir process in which searcher and system collaborate to find documents that satisfy an information need regardless of the language in which those documents are written calls for designs in which synergies between searcher and system can be leveraged so that the strengths of one can cover weaknesses of the other this paper describes an approach that employs user assisted query translation to help searchers better understand the system's operation supporting interaction and interface designs are introduced and results from three user studies are presented the results indicate that experienced searchers presented with this new system evolve new search strategies that make effective use of the new capabilities that they achieve retrieval effectiveness comparable to results obtained using fully automatic techniques and that reported satisfaction with support for cross language searching increased the paper concludes with description of freely available interactive clir system that incorporates lessons learned from this research
using mixture of random variables to model data is tried and tested method common in data mining machine learning and statistics by using mixture modeling it is often possible to accurately model even complex multimodal data via very simple components however the classical mixture model assumes that data point is generated by single component in the model lot of datasets can be modeled closer to the underlying reality if we drop this restriction we propose probabilistic framework the mixture of subsets mos model by making two fundamental changes to the classical mixture model first we allow data point to be generated by set of components rather than just single component next we limit the number of data attributes that each component can influence we also propose an em framework to learn the mos model from dataset and experimentally evaluate it on real high dimensional datasets our results show that the mos model learned from the data represents the underlying nature of the data accurately
as the number and size of large timestamped collections eg sequences of digitized newspapers periodicals blogs increase the problem of efficiently indexing and searching such data becomes more important term burstiness has been extensively researched as mechanism to address event detection in the context of such collections in this paper we explore how burstiness information can be further utilized to enhance the search process we present novel approach to model the burstiness of term using discrepancy theory concepts this allows us to build parameter free linear time approach to identify the time intervals of maximum burstiness for given term finally we describe the first burstiness driven search framework and thoroughly evaluate our approach in the context of different scenarios
wireless local area networks wlans have become commonplace addition to the normal environments surrounding us based on ieee technology wlans can now be found in the working place at homes and in many cities central district area as open or commercial services these access points in the public areas are called hotspots they provide internet access in various types of public places such as shopping districts caf√©s airports and shops as the hotspots are being used by growing user base that is also quite heterogeneous their usability is becoming evermore important as hotspots can be accessed by number of devices differing in their capabilities size and user interfaces achieving good usability in accessing the services is not straightforward this paper reports user study and usability analysis on wlan access to discover user's needs and suggest enhancements to fight the usability problems in wlan access
many organizations rely on web applications that use back end databases to store important data testing such applications requires significant effort manual testing alone is often impractical so testers also rely on automated testing techniques however current automated testing techniques may produce false positives or false negatives even in perfectly working system because the outcome of test case depends on the state of the database which changes over time as data is inserted and deleted the automatic database tester autodbt generates functional test cases that account for database updates autodbt takes as input model of the application and set of testing criteria the model consists of state transition diagram that shows how users navigate pages data specification that captures how data flows and an update specification that shows how the database is updated autodbt generates guard queries to determine whether the database is in state conducive to performing and evaluating tests autodbt also generates partial oracles to help validate whether back end database is updated correctly during testing this paper describes the design of autodbt prototype implementation several experiments with the prototype and four case studies
hyperlinks are an essential feature of the world wide web highly responsible for its success xlink improves on html's linking capabilities in several ways in particular links after xlink can be out of line ie not defined at link source and collected in possibly several linkbases which considerably ease building complex link structuresregarding its architecture as distributed and open system the web differs significantly from traditional hypermedia systems modeling of link structures and processing of linkbases under the web's open world linking require rethinking the traditional approaches this unfortunately has been rather neglected in the design of xlinkadding notion of interface to xlink as suggested in this work can considerably improve modeling of link structures when link structure is traversed the relevant linkbase might become ambiguous we suggest three linkbase management modes governing the binding of linkbase to document to resolve this ambiguity
this paper presents discriminative alignment model for extracting abbreviations and their full forms appearing in actual text the task of abbreviation recognition is formalized as sequential alignment problem which finds the optimal alignment origins of abbreviation letters between two strings abbreviation and full form we design large amount of finegrained features that directly express the events where letters produce or do not produce abbreviations we obtain the optimal combination of features on an aligned abbreviation corpus by using the maximum entropy framework the experimental results show the usefulness of the alignment model and corpus for improving abbreviation recognition
in wormhole meshes reliable routing is supposed to be deadlock free and fault tolerant many routing algorithms are able to tolerate large number of faults enclosed by rectangular blocks or special convex none of them however is capable of handling two convex fault regions with distance two by using only two virtual networks in this paper fault tolerant wormhole routing algorithm is presented to tolerate the disjointed convex faulty regions with distance two or no less which do not contain any nonfaulty nodes and do not prohibit any routing as long as nodes outside faulty regions are connected in the mesh network the processors overlapping along the boundaries of different fault regions is allowed the proposed algorithm which routes the messages by routing algorithm in fault free region can tolerate convex fault connected regions with only two virtual channels per physical channel and is deadlock and livelock free the proposed algorithm can be easily extended to adaptive routing
natural time dependent similarity measure for two trajectories is their average distance at corresponding times we give algorithms for computing the most similar subtrajectories under this measure assuming the two trajectories are given as two polygonal possibly self intersecting lines when minimum duration is specified for the subtrajectories and they must start at exactly corresponding times in the input trajectories we give linear time algorithm for computing the starting time and duration of the most similar subtrajectories the algorithm is based on result of independent interest we present linear time algorithm to find for piece wise monotone function an interval of at least given length that has minimum average value when the two subtrajectories can start at different times in the two input trajectories it appears difficult to give an exact algorithm for the most similar subtrajectories problem even if the duration of the desired two subtrajectories is fixed to some length we show that the problem can be solved approximately and with performance guarantee more precisely we present epsilon approximation algorithms for computing the most similar subtrajectories of two input trajectories for the case where the duration is specified and also for the case where only minimum on the duration is specified
large information displays are common in public and semi public spaces but still require rapid and lightweight ways for users to interact with them we present bluetone framework for developing large display applications which will interpret and react to dual tone multi frequency sounds transmitted from mobile phones paired with the display using the bluetooth headset profile bluetone enables text entry cursor manipulation and menu selection without requiring the installation of any special software on user's mobile phone
as the computing industry enters the multicore era exponential growth in the number of transistors on chip continues to present challenges and opportunities for computer architects and system designers we examine one emerging issue in particular that of dynamic heterogeneity which can arise even among physically homogeneous cores from changing reliability power or thermal conditions different cache and tlb contents or changing resource configurations this heterogeneity results in constantly varying pool of hardware resources which greatly complicates software's traditional task of assigning computation to cores in part to address dynamic heterogeneity we argue that hardware should take more active role in the management of its computation resources we propose hardware techniques to virtualize the cores of multicore processor allowing hardware to flexibly reassign the virtual processors that are exposed even to single operating system to any subset of the physical cores we show that multicore virtualization operates with minimal overhead and that it enables several novel resource management applications for improving both performance and reliability
this work presents general mechanism for executing specifications that comply with given invariants which may be expressed in different formalisms and logics we exploit maude's reflective capabilities and its properties as general semantic framework to provide generic strategy that allows us to execute maude specifications taking into account user defined invariants the strategy is parameterized by the invariants and by the logic in which such invariants are expressed we experiment with different logics providing examples for propositional logic finite future time linear temporal logic and metric temporal logic
over the past five years large scale storage installations have required fault protection beyond raid leading to flurry of research on and development of erasure codes for multiple disk failures numerous open source implementations of various coding techniques are available to the general public in this paper we perform head to head comparison of these implementations in encoding and decoding scenarios our goals are to compare codes and implementations to discern whether theory matches practice and to demonstrate how parameter selection especially as it concerns memory has significant impact on code's performance additional benefits are to give storage system designers an idea of what to expect in terms of coding performance when designing their storage systems and to identify the places where further erasure coding research can have the most impact
in this paper we propose global visibility algorithm which computes from region visibility for all view cells simultaneously in progressive manner we cast rays to sample visibility interactions and use the information carried by ray for all view cells it intersects the main contribution of the paper is set of adaptive sampling strategies based on ray mutations that exploit the spatial coherence of visibility our method achieves more than an order of magnitude speedup compared to per view cell sampling this provides practical solution to visibility preprocessing and also enables new type of interactive visibility analysis application where it is possible to quickly inspect and modify coarse global visibility solution that is constantly refined
in this paper we describe study that explored the implications of the social translucence framework for designing systems that support communications at work two systems designed for communicating availability status were empirically evaluated to understand what constitutes successful way to achieve visibility of people's communicative state some aspects of the social translucence constructs visibility awareness and accountability were further operationalized into questionnaire and tested relationships between these constructs through path modeling techniques we found that to improve visibility systems should support people in presenting their status in contextualized yet abstract manner visibility was also found to have an impact on awareness and accountability but no significant relationship was seen between awareness and accountability we argue that to design socially translucent systems it is insufficient to visualize people's availability status it is also necessary to introduce mechanisms stimulating mutual awareness that allow for maintaining shared reciprocical knowledge about communicators availability state which then can encourage them to act in socially responsible way
pervasive games have become popular field of investigation in recent years in which natural human computer interaction hci plays key role in this paper vision based approach for human hand motion gesture recognition is proposed for natural hci in pervasive games led light pen is used to indicate the user's hand position while web camera is used to capture the hand motion rule based approach is used to design set of hand gestures which are classified into two categories linear gestures and arc shaped gestures determinate finite state automaton is developed to segment the captured hand motion trajectories the proposed interaction method has been applied to the traditional game tetris on pc with the hand held led light pen being used to drive the game instead of traditional key strokes experimental results show that the vision based interactions are natural and effective
we introduce green coordinates for closed polyhedral cages the coordinates are motivated by green's third integral identity and respect both the vertices position and faces orientation of the cage we show that green coordinates lead to space deformations with shape preserving property in particular in they induce conformal mappings and extend naturally to quasi conformal mappings in in both cases we derive closed form expressions for the coordinates yielding simple and fast algorithm for cage based space deformation we compare the performance of green coordinates with those of mean value coordinates and harmonic coordinates and show that the advantage of the shape preserving property is not achieved at the expense of speed or simplicity we also show that the new coordinates extend the mapping in natural analytic manner to the exterior of the cage allowing the employment of partial cages
data transfer in grid environment has become one critical activity in large number of applications that require access to huge volumes of data in these scenarios characterized by large latencies poor performance and complex dependencies the use of approaches such as multiagents or parallel can provide great benefit however all the attempts to improve the performance of data transfer in grids should achieve the interoperability with already established data transfer schemes gridftp is one of the most known and used data grid transfer protocols this paper describes mapfs dsi modification of the gridftp server based on multiagent parallel file system mapfs dsi increases the performance of data transfers but keeping the interoperability with existing gridftp servers
in this paper we propose conceptual architecture for personal semantic web information retrieval system it incorporates semantic web web service pp and multi agent technologies to enable not only precise location of web resources but also the automatic or semi automatic integration of web resources delivered through web contents and web services in this architecture the semantic issues concerning the whole lifecycle of information retrieval were considered consistently and the integration of web contents and web services is enabled seamlessly the architecture consists of three main components consumer provider and mediator all providers and consumers are constructed as semantic myportal which provides gateway to all the information relevant to user each provider describes its capabilities in what we call wscd web site capability description and each consumer will submit relevant queries based on user requirements when web search is necessary the mediator is composed of agents assigned to the consumer and providers using an agent community based pp information retrieval acpp method to fullfill the information sharing among semantic myportals some preliminary experimental results are presented to show the efficiency of the acpp method and the usefulness of two query response histories for looking up new information sources and for reducing communication loads
possibilistic defeasible logic programming delp is logic programming language which combines features from argumentation theory and logic programming incorporating the treatment of possibilistic uncertainty at the object language level in spite of its expressive power an important limitation in delp is that imprecise fuzzy information cannot be expressed in the object language one interesting alternative for solving this limitation is the use of pgl possibilistic logic over godel logic extended with fuzzy constants fuzzy constants in pgl allow expressing disjunctive information about the unknown value of variable in the sense of magnitude modelled as unary predicate the aim of this article is twofold firstly we formalize depgl possibilistic defeasible logic programming language that extends delp through the use of pgl in order to incorporate fuzzy constants and fuzzy unification mechanism for them secondly we propose way to handle conflicting arguments in the context of the extended framework
the system administrators of large organizations often receive large number of mails from its users and substantial amount of effort is devoted to reading and responding to these mails the content of these messages can range from trivial technical questions to complex problem reports often these queries can be classified into specific categories for example reports of file system that is full or requests to change the toner in particular printer in this project we have experimented with text mining techniques and developed tool for automatically classifying user mail queries in real time and pseudo automatically responding to these requests our experimental evaluations suggest that one cannot completely rely on totally automatic tool for sorting and responding to incoming mail however it can be resource saving compliment to an existing toolset that can increase the support efficiency and quality
fast instruction decoding is challenge for the design of cisc microprocessors well known solution to overcome this problem is using trace cache it stores and fetches already decoded instructions avoiding the need for decoding them again however implementing trace cache involves an important increase in the fetch architecture complexityin this paper we propose novel decoding architecture that reduces the fetch engine implementation cost instead of using special purpose buffer like the trace cache our proposal stores frequently decoded instructions in the memory hierarchy the address where the decoded instructions are stored is kept in the branch prediction mechanism enabling it to guide our decoding architecture this makes it possible for the processor front end to fetch already decoded instructions from memory instead of the original nondecoded instructions our results show that an wide superscalar processor achieves an average performance improvement by using our decoding architecture this improvement is comparable to the one achieved by using the more complex trace cache while requiring less chip area and less energy consumption in the fetch architecture
this article describes general purpose program analysis that computes global control flow and data flow information for higher order call by value languages the analysis employs novel form of polyvariance called polymorhic splitting that uses let expressions as syntactic clues to gain precision the information derived from the analysis is used both to eliminate run time checks and to inline procedure the analysis and optimizations have been applied to suite of scheme programs experimental results obtained from the prototype implementation indicate that the analysis is extremely precise and has reasonable cost compared to monovariant flow analyses such as cfa or analyses based on type inference such as soft typing the analysis eliminates significantly more run time checks run time check elimination and inlining together typically yield to performance improvement for the benchmark suite with some programs running four times as fast
organising large scale web information retrieval systems into hierarchies of topic specific search resources can improve both the quality of results and the efficient use of computing resources promising way to build such systems involves federations of topic specific search engines in decentralised search environments most of the previous research concentrated on various technical aspects of such environments eg routing of search queries or merging of results from multiple sources we focus on organisational dynamics what happens to topical specialisation of search engines in the absence of centralised control when each engine makes individual and self interested decisions on its service parameters we investigate this question in computational economics framework where search providers compete for user queries by choosing what topics to index we provide formalisation of the competition problem and then analyse theoretically and empirically the specialisation dynamics of such systems
adaptive body biasing abb is popularly used technique to mitigate the increasing impact of manufacturing process variations on leakage power dissipation the efficacy of the abb technique can be improved by partitioning design into number of body bias islands each with its individual body bias voltage in this paper we propose system level leakage variability mitigation framework to partition multiprocessor system into body bias islands at the processing element pe granularity at design time and to optimally assign body bias voltages to each island post fabrication as opposed to prior gate and circuit level partitioning techniques that constrain the global clock frequency of the system we allow each island to run at different speed and constrain only the relevant system performance metrics in our case the execution deadlines experimental results show the efficacy of the proposed framework in reducing the mean and standard deviation of leakage power dissipation compared to baseline system without abb at the same time the proposed techniques provide significant runtime improvements over previously proposed monte carlo based technique while providing similar reductions in leakage power dissipation
it is well known that web page classification can be enhanced by using hyperlinks that provide linkages between web pages however in the web space hyperlinks are usually sparse noisy and thus in many situations can only provide limited help in classification in this paper we extend the concept of linkages from explicit hyperlinks to implicit links built between web pages by observing that people who search the web with the same queries often click on different but related documents together we draw implicit links between web pages that are clicked after the same queries those pages are implicitly linked we provide an approach for automatically building the implicit links between web pages using web query logs together with thorough comparison between the uses of implicit and explicit links in web page classification our experimental results on large dataset confirm that the use of the implicit links is better than using explicit links in classification performance with an increase of more than in terms of the macro measurement
in march sigcse members contributed to mailing list discussion on the question of whether programming should be taught objects first or imperative first we analyse that discussion exploring how the cs community debates the issue and whether contributors positions are supported by the research literature on novice programmers we applied four distinct research methods to the discussion cognitive science rhetorical analysis in the critical tradition phenomenography and biography we identify the cognitive claims made in the email discussion and find there is not consensus in the research literature as to whether the objects first approach or the imperative approach is harder to learn from the rhetorical analysis we find that the discussion was not so much debate between oo first versus imperative first but instead was more for and against oo first our phenomenographic analysis identified and categorized the underlying complexity of the discussion we also applied biographical method to explore the extent to which the participants views are shaped by their own prior experience the paper concludes with some reflections upon paradigms and the manner in which the cs discipline community defines itself
the well definedness problem for database query language consists of checking given an expression and an input type that the expression never yields runtime error on any input adhering to the input type in this article we study the well definedness problem for query languages on trees that are built from finite set of partially defined base operations by adding variables constants conditionals let bindings and iteration we identify properties of base operations that can make the problem undecidable and give restrictions that are sufficient to ensure decidability as direct result we obtain large fragment of xquery for which well definedness is decidable
one promising approach for adding object oriented oo facilities to functional languages like ml is to generalize the existing datatype and function constructs to be hierarchical and extensible so that datatype variants simulate classes and function cases simulate methods this approach allows existing datatypes to be easily extended with both new operations and new variants resolving longstanding conflict between the functional and oo styles however previous designs based on this approach have been forced to give up modular typechecking requiring whole program checks to ensure type safety we describe extensible ml eml an ml like language that supports hierarchical extensible datatypes and functions while preserving purely modular typechecking to achieve this result eml's type system imposes few requirements on datatype and function extensibility but eml is still able to express both traditional functional and oo idioms we have formalized core version of eml and proven the associated type system sound and we have developed prototype interpreter for the language
in this paper we propose deferred workload based inter task dvs dynamic voltage scaling algorithm dwdvs which has two features for portable multimedia devices the first is that we reserve time interval for each task to execute and its workload can be completed in this time interval even in the worst case condition which means that the actual workload execution time of each task is equal to its worst case execution time in this way we can estimate the slack time from lower priority tasks more aggressively the second is that we defer these reserved time intervals which means that reserved time interval will be shifted to the deadline of its corresponding task as close as possible thus the operating frequency can be reduced even without slack time simulation results show that the proposed dwdvs reduces the energy consumption by and compared with the static voltage scaling static laedf and dra algorithms respectively and approaches theoretical low bound bound by margin of at most
during the iterative development of interactive software formative evaluation is often performed to find and fix usability problems early on the output of formative evaluation usually takes the form of prioritised list of usability findings each finding typically consisting of description of the problem how often it occurred and sometimes recommendation for possible solution unfortunately the valuable results of formative evaluations are usually collected into written document this makes it extremely difficult to automate the handling of usability findings more formalised electronic format for the handover of usability findings would make much more sense usabml is formalised structure for reporting usability findings expressed in xml it allows usability experts and software engineers to import usability findings into bug issue tracking systems to associate usability issues with parts of source code and to track progress in fixing them
almost all actions on computer are mediated by windows yet we know surprisingly little about how people coordinate their activities using these windows studies of window use are difficult for two reasons gathering longitudinal data is problematic and it is unclear how to extract meaningful characterisations from the data in this paper we present visualisation tool called window watcher that helps researchers understand and interpret low level event logs of window switching activities generated by our tool pylogger we describe its design objectives and demonstrate ways that it summarises and elucidates window use
many interactive systems require users to navigate through large sets of data and commands using constrained input devices mdash such as scroll rings rocker switches or specialized keypads mdash that provide less power and flexibility than traditional input devices like mice or touch screens while performance with more traditional devices has been extensively studied in human computer interaction there has been relatively little investigation of human performance with constrained input as result there is little understanding of what factors govern performance in these situations and how interfaces should be designed to optimize interface actions such as navigation and selection since constrained input is now common in wide variety of interactive systems such as mobile phones audio players in car navigation systems and kiosk displays it is important for designers to understand what factors affect performance to aid in this understanding we present the constrained input navigation cin model predictive model that allows accurate determination of human navigation and selection performance in constrained input scenarios cin identifies three factors that underlie user efficiency the performance of the interface type for single level item selection where interface type depends on the input and output devices the interactive behavior and the data organization the hierarchical structure of the information space and the user's experience with the items to be selected we show through experiments that after empirical calibration the model's predictions fit empirical data well and discuss why and how each of the factors affects performance models like cin can provide valuable theoretical and practical benefits to designers of constrained input systems allowing them to explore and compare much wider variety of alternate interface designs without the need for extensive user studies
this paper provides logical framework of negotiating agents who have capabilities of evaluating and building proposals given proposal an agent decides whether it is acceptable or not if the proposal is unacceptable as it is the agent seeks conditions to accept it this attitude is captured as process of making hypotheses by induction if an agent fails to find hypothesis it would concede by giving up some of its current belief this attitude is characterized using default reasoning we provide logical framework of such think act cycle of an agent and develop method for computing proposals using answer set programming
training statistical machine translation starts with tokenizing parallel corpus some languages such as chinese do not incorporate spacing in their writing system which creates challenge for tokenization moreover morphologically rich languages such as korean present an even bigger challenge since optimal token boundaries for machine translation in these languages are often unclear both rule based solutions and statistical solutions are currently used in this paper we present unsupervised methods to solve tokenization problem our methods incorporate information available from parallel corpus to determine good tokenization for machine translation
we present the first verification of full functional correctness for range of linked data structure implementations including mutable lists trees graphs and hash tables specifically we present the use of the jahob verification system to verify formal specifications written in classical higher order logic that completely capture the desired behavior of the java data structure implementations with the exception of properties involving execution time and or memory consumption given that the desired correctness properties include intractable constructs such as quantifiers transitive closure and lambda abstraction it is challenge to successfully prove the generated verification conditions our jahob verification system uses integrated reasoning to split each verification condition into conjunction of simpler subformulas then apply diverse collection of specialized decision procedures first order theorem provers and in the worst case interactive theorem provers to prove each subformula techniques such as replacing complex subformulas with stronger but simpler alternatives exploiting structure inherently present in the verification conditions and when necessary inserting verified lemmas and proof hints into the imperative source code make it possible to seamlessly integrate all of the specialized decision procedures and theorem provers into single powerful integrated reasoning system by appropriately applying multiple proof techniques to discharge different subformulas this reasoning system can effectively prove the complex and challenging verification conditions that arise in this context
today the world wide web is undergoing subtle but profound shift to web and semantic web technologies due to the increasing interest in the semantic web more and more semantic web applications are being developed one of the current main issues facing the development of semantic web applications is the simplicity and user friendliness for the end users especially for people with non it background this paper presents the feedrank system for the extraction aggregation semantic management and querying of web feeds the proposed system overcomes many of the limitations of conventional and passive web feed readers such as providing only simple presentations of what is received poor integration of correlated data from different sources and overwhelming the user with large traffic of feeds that are of no or low interest to them
in search engines ranking algorithms measure the importance and relevance of documents mainly based on the contents and relationships between documents user attributes are usually not considered in ranking this user neutral approach however may not meet the diverse interests of users who may demand different documents even with the same queries to satisfy this need for more personalized ranking we propose ranking framework social network document rank sndocrank that considers both document contents and the relationship between searched and document owners in social network this method combined the traditional tf idf ranking for document contents with out multi level actor similarity mas algorithm to measure to what extent document owners and the searcher are structurally similar in social network we implemented our ranking method in simulated video social network based on data extracted from youtube and tested its effectiveness on video search the results show that compared with the traditional ranking method like tf idfs the sndocrank algorithm returns more relevant documents more specifically searcher can get significantly better results be being in larger social network having more friends and being associated with larger local communities in social network
set sharing is an abstract domain in which each concrete object is represented by the set of local variables from which it might be reachable it is useful abstraction to detect parallelism opportunities since it contains definite information about which variables do not share in memory ie about when the memory regions reachable from those variables are disjoint set sharing is more precise alternative to pair sharing in which each domain element is set of all pairs of local variables from which common object may be reachable however the exponential complexity of some set sharing operations has limited its wider application this work introduces an efficient implementation of the set sharing domain using zero supressed binary decision diagrams zbdds because zbdds were designed to represent sets of combinations ie sets of sets they naturally represent elements of the set sharing domain we show how to synthesize the operations needed in the set sharing transfer functions from basic zbdd operations for some of the operations we devise custom zbdd algorithms that perform better in practice we also compare our implementation of the abstract domain with an efficient compact bitset based alternative and show that the zbdd version scales better in terms of both memory usage and running time
tiling is well known loop transformation to improve temporal locality of nested loops current compiler algorithms for tiling are limited to loops which are perfectly nested or can be transformed in trivial ways into perfect nest this paper presents number of program transformations to enable tiling for class of nontrivial imperfectly nested loops such that cache locality is improved we define program model for such loops and develop compiler algorithms for their tiling we propose to adopt odd even variable duplication to break anti and output dependences without unduly increasing the working set size and to adopt speculative execution to enable tiling of loops which may terminate prematurely due to eg convergence tests in iterative algorithms we have implemented these techniques in research compiler panorama initial experiments with several benchmark programs are performed on sgi workstations based on mips rk and rk processors overall the transformed programs run faster by to
recently progressive retrieval has been advocated as an alternate solution to multidimensional indexes or approximate techniques in order to accelerate similarity search of points in multidimensional spaces the principle of progressive search is to offer first subset of the answers to the user during retrieval if this subset satisfies the user's needs retrieval stops otherwise search resumes and after number of steps the exact answer set is returned to the user such process is justified by the fact that in large number of applications it is more interesting to rapidly bring first approximate answer sets rather than waiting for long time the exact answer set the contribution of this paper is first typology of existing techniques for progressive retrieval we survey variety of methods designed for image retrieval although some of them apply to general database browsing context which goes beyond cbir we also include techniques not designed for but that can easily be adapted to progressive retrieval
in large data warehousing environments it is often advantageous to provide fast approximate answers to complex aggregate queries based on statistical summaries of the full data in this paper we demonstrate the difficulty of providing good approximate answers for join queries using only statistics in particular samples from the base relations we propose join synopses as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins we present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known we also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations our extensive set of experiments on the tpc benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper
software or hardware data cache prefetching is an efficient way to hide cache miss latency however effectiveness of the issued prefetches have to be monitored in order to maximize their positive impact while minimizing their negative impact on performance in previous proposed dynamic frameworks the monitoring scheme is either achieved using processor performance counters or using specific hardware in this work we propose prefetching strategy which does not use any specific hardware component or processor performance counter our dynamic framework wants to be portable on any modern processor architecture providing at least prefetch instruction opportunity and effectiveness of prefetching loads is simply guided by the time spent to effectively obtain the data every load of program is monitored periodically and can be either associated to dynamically inserted prefetch instruction or not it can be associated to prefetch instruction at some disjoint periods of the whole program run as soon as it is efficient our framework has been implemented for itanium machines it involves several dynamic instrumentations of the binary code whose overhead is limited to only on average on large set of benchmarks our system is able to speed up some programs by
mitra is scalable storage manager that supports the display of continuous media data types eg audio and video clips it is software based system that employs off the shelf hardware components its present hardware platform is cluster of multi disk workstations connected using an atm switch mitra supports the display of mix of media types to reduce the cost of storage it supports hierarchical organization of storage devices and stages the frequently accessed objects on the magnetic disks for the number of displays to scale as function of additional disks mitra employs staggered striping it implements three strategies to maximize the number of simultaneous displays supported by each disk first the everest file system allows different files corresponding to objects of different media types to be retrieved at different block size granularities second the fixb algorithm recognizes the different zones of disk and guarantees continuous display while harnessing the average disk transfer rate third mitra implements the grouped sweeping scheme gss to minimize the impact of disk seeks on the available disk bandwidthin addition to reporting on implementation details of mitra we present performance results that demonstrate the scalability characteristics of the system we compare the obtained results with theoretical expectations based on the bandwidth of participating disks mitra attains between to of the theoretical expectations
this paper aims to match two sets of nonrigid feature points using random sampling methods by exploiting the principle eigenvector of corres pondence model linkage an adaptive sampling method is devised to efficiently deal with non rigid matching problems
despite the success of instruction level parallelism ilp optimizations in increasing the performance of microprocessors certain codes remain elusive in particular codes containing recursive data structure rds traversal loops have been largely immune to ilp optimizations due to the fundamental serialization and variable latency of the loop carried dependence through pointer chasing load to address these and other situations we introduce decoupled software pipelining dswp technique that statically splits single threaded sequential loop into multiple non speculative threads each of which performs useful computation essential for overall program correctness the resulting threads execute on thread parallel architectures such as simultaneous multithreaded smt cores or chip multiprocessors cmp expose additional instruction level parallelism and tolerate latency better than the original single threaded rds loop to reduce overhead these threads communicate using synchronization array dedicated hardware structure for pipelined inter thread communication dswp used in conjunction with the synchronization array achieves an to speedup in the optimized functions on both statically and dynamically scheduled processors
in many applications of wireless sensor networks sensor nodes are manually deployed in hostile environments where an attacker can disrupt the localization service and tamper with legitimate in network communication in this paper we introduce secure walking gps secure localization and key distribution solution for manual deployments of wsns using the location information provided by the gps and inertial guidance modules on special master node secure walking gps achieves accurate node localization and location based key distribution at the same time our analysis and simulation results indicate that the secure walking gps scheme makes deployed wsn resistant to the dolev yao the wormhole and the gps denial attacks has good localization and key distribution performance and is practical for large scale wsn deployments
we address the problem of energy efficient reliable wireless communication in the presence of unreliable or lossy wireless link layers in multi hop wireless networks prior work has provided an optimal energy efficient solution to this problem for the case where link layers implement perfect reliability however more common scenario link layer that is not perfectly reliable was left as an open problem in this paper we first present two centralized algorithms bamer and gamer that optimally solve the minimum energy reliable communication problem in presence of unreliable links subsequently we present distributed algorithm damer that approximates the performance of the centralized algorithm and leads to significant performance improvement over existing single path or multi path based techniques
shared variable is an abstraction of persistent interprocess communication processors execute operations often concurrently on shared variables to exchange information among themselves the behavior of operation executions is required to be ldquo consistent rdquo for effective interprocess communication consequently consistency specification of shared variable describes some guarantees on the behavior of the operation executions read write shared variable has two operations write stores specified value in the variable and read returns value from the variable for read write variables consistency specification describes what values reads may return using an intuitive notion of illegality of reads we propose framework that facilitates specifying large variety of read write variables
recently an elegant routing protocol the zone routing protocol zrp was proposed to provide hybrid routing framework that is locally proactive and globally reactive with the goal of minimizing the sum of the proactive and reactive control overhead the key idea of zrp is that each node proactively advertises its link state over fixed number of hops called the zone radius these local advertisements provide each node with an updated view of its routing zone the collection of all nodes and links that are reachable within the zone radius the nodes on the boundary of the routing zone are called peripheral nodes and play an important role in the reactive zone based route discovery the main contribution of this work is to propose novel hybrid routing protocol the two zone routing protocol tzrp as nontrivial extension of zrp in contrast with the original zrp where single zone serves dual purpose tzrp aims to decouple the protocol's ability to adapt to traffic characteristics from its ability to adapt to mobility in support of this goal in tzrp each node maintains two zones crisp zone and fuzzy zone by adjusting the sizes of these two zones independently lower total routing control overhead can be achieved extensive simulation results show that tzrp is general manet routing framework that can balance the trade offs between various routing control overheads more effectively than zrp in wide range of network conditions
we present model for delegation that is based on our decentralized administrative role graph model we use combination of user group assignment and user role assignment to support user to user permission to user and role to role delegation powerful source dependent revocation algorithm is described we separate our delegation model into static and dynamic models then discuss the static model and its operations we provide detailed partial revocation algorithms we also give details concerning changes to the role hierarchy user group structure and rbac operations that are affected by delegation
the visual exploration of large databases raises number of unresolved inference problems and calls for new interaction patterns between multiple disciplines both at the conceptual and technical level we present an approach that is based on the interaction of four disciplines database systems statistical analyses perceptual and cognitive psychology and scientific visualization at the conceptual level we offer perceptual and cognitive insights to guide the information visualization process we then choose cluster surfaces to exemplify the data mining process to discuss the tasks involved and to work out the interaction patterns
in this paper an improved differential cryptanalysis framework for finding collisions in hash functions is provided its principle is based on linearization of compression functions in order to find low weight differential characteristics as initiated by chabaud and joux this is formalized and refined however in several ways for the problem of finding conforming message pair whose differential trail follows linear trail condition function is introduced so that finding collision is equivalent to finding preimage of the zero vector under the condition function then the dependency table concept shows how much influence every input bit of the condition function has on each output bit careful analysis of the dependency table reveals degrees of freedom that can be exploited in accelerated preimage reconstruction under the condition function these concepts are applied to an in depth collision analysis of reduced round versions of the two sha candidates cubehash and md and are demonstrated to give by far the best currently known collision attacks on these sha candidates
this paper describes technique for improving separation of concerns at the level of domain modeling contribution of this new approach is the construction of support tools that facilitate the elevation of crosscutting modeling concerns to first class constructs in type system the key idea is the application of variant of the omg object constraint language to models that are stored persistently in xml with this approach weavers are generated from domain specific descriptions to assist modeler in exploring various alternative modeling scenarios the paper examines several facets of aspect oriented domain modeling aodm including domain specific model weavers language to support the concern separation an overview of code generation issues within meta weaver framework and comparison between aodm and aop an example of the approach is provided as well as description of several future concepts for extending the flexibility within aodm
the content user gap is the difference between the limited range of content relevant preferences that may be expressed using the mpeg user interaction tools and the much wider range of metadata that may be represented using the mpeg content tools one approach for closing this gap is to make the user and content metadata isomorphic by using the existing mpeg content tools to represent user as well as content metadata agius and angelides subsequently user preferences may be specified for all content without omission since there is wealth of user preference and history metadata within the mpeg user interaction tools that can usefully complement these specific content preferences in this paper we develop method by which all user and content metadata may be bridged
this paper presents case study of the organisational control principles present in credit application process at the branch level of bank the case study has been performed in the context of an earlier suggested formal framework for organisational control principles based on the alloy predicate logic and its facilities for automated formal analysis and exploration in particular we establish and validate the novel concepts of specific and general obligations the delegation of these two kinds of obligations must be controlled by means of review and supervision controls the example of credit application process is used to discuss these organisational controls
cooperation among computational nodes to solve common parallel application is one of the most outstanding features of grid environments however from the performance point of view it is very important that this cooperation will be made in an equilibrated way because otherwise some nodes can be overloaded whereas other nodes can be underused this paper presents camble cooperative awareness model for balancing the load in grid environments which applies some theoretical principles of awareness models to promote an efficient autonomous equilibrate and cooperative task delivery in grid environments this cooperative task management has been implemented and tested in real and heterogeneous grid infrastructure composed of several vo with very successful results this paper presents some of these outcomes while emphasizes on the overhead and efficacy of the system using this model
in this paper several enhanced sufficient conditions are given for minimal routing in dimensional meshes with faulty nodes contained in set of disjoint faulty blocks it is based on an early work of wu's minimal routing in meshes with faulty blocks unlike many traditional models that assume all the nodes know global fault distribution our approach is based on the notion of limited global fault information first fault model called faulty block is reviewed in which all faulty nodes in the system are contained in set of disjoint faulty blocks fault information is coded in tuple called extended safety level associated with each node of mesh to determine the feasibility of minimal routing specifically we study the existence of minimal route at given source node based on the associated extended safety level limited distribution of faulty block information and minimal routing an analytical model for the number of rows and columns that receive faulty block information is also given extensions to wang's minimal connected components mccs are also considered mccs are rectilinear monotone polygonal shaped fault blocks and are refinement of faulty blocks our simulation results show substantial improvement in terms of higher percentage of minimal routing in meshes under both fault models
this paper proposes and evaluates new mechanism rate windows for and network rate policing the goal of the proposed system is to provide simple yet effective way to enforce resource limits on target classes of jobs in system this work was motivated by our linger longer infrastructure which harvests idle cycles in networks of workstations network and throttling is crucial because linger longer can leave guest jobs on non idle nodes and machine owners should not be adversely affected our approach is quite simple we use sliding window of recent events to compute the average rate for target resource the assigned limit is enforced by the simple expedient of putting application processes to sleep when they issue requests that would bring their resource utilization out of the allowable profile our system call intercept model makes the rate windows mechanism light weight and highly portable our experimental results show that we are able to limit resource usage to within few percent of target usages
two oft cited file systems the fast file system ffs and the log structured file system lfs adopt two sharply different update strategies update in place and update out of place this paper introduces the design and implementation of hybrid file system called hfs which combines the strengths of ffs and lfs while avoiding their weaknesses this is accomplished by distributing file system data into two partitions based on their size and type in hfs data blocks of large regular files are stored in data partition arranged in ffs like fashion while metadata and small files are stored in separate log partition organized in the spirit of lfs but without incurring any cleaning overhead this segregation makes it possible to use more appropriate layouts for different data than would otherwise be possible in particular hfs has the ability to perform clustered on all kinds of data including small files metadata and large files we have implemented prototype of hfs on freebsd and have compared its performance against three file systems including ffs with soft updates port of netbsd's lfs and our lightweight journaling file system called yfs results on number of benchmarks show that hfs has excellent small file and metadata performance for example hfs beats ffs with soft updates in the range from to in the postmark benchmark
key scalability challenge for interprocedural dataflow analysis comes from large libraries our work addresses this challenge for the general category of interprocedural distributive environment ide dataflow problems using pre computed library summary information the proposed approach reduces significantly the cost of whole program ide analyses without any loss of precision we define an approach for library summary generation by using graph representation of dataflow summary functions and by abstracting away redundant dataflow facts that are internal to the library our approach also handles object oriented features by employing an ide type analysis as well as special handling of polymorphic library call sites whose target methods depend on the future unknown client code experimental results show that dramatic cost savings can be achieved with the help of these techniques
person name queries often bring up web pages that correspond to individuals sharing the same name the web people search weps task consists of organizing search results for ambiguous person name queries into meaningful clusters with each cluster referring to one individual this paper presents fuzzy ant based clustering approach for this multi document person name disambiguation problem the main advantage of fuzzy ant based clustering technique inspired by the behavior of ants clustering dead nestmates into piles is that no specification of the number of output clusters is required this makes the algorithm very well suited for the web person disambiguation task where we do not know in advance how many individuals each person name refers to we compare our results with state of the art partitional and hierarchical clustering approaches means and agnes and demonstrate favorable results this is particularly interesting as the latter involve manual setting of similarity threshold or estimating the number of clusters in advance while the fuzzy ant based clustering algorithm does not
in data stream applications data arrive continuously and can only be scanned once as the query processor has very limited memory relative to the size of the stream to work with hence queries on data streams do not have access to the entire data set and query answers are typically approximate while there have been many studies on the nearest neighbors knn problem in conventional multi dimensional databases the solutions cannot be directly applied to data streams for the above reasons in this paper we investigate the knn problem over data streams we first introduce the approximate knn eknn problem that finds the approximate knn answers of query point such that the absolute error of the th nearest neighbor distance is bounded by to support eknn queries over streams we propose technique called disc adaptive indexing on streams by space filling curves disc can adapt to different data distributions to either optimize memory utilization to answer eknn queries under certain accuracy requirements or achieve the best accuracy under given memory constraint at the same time disc provide efficient updates and query processing which are important requirements in data stream applications extensive experiments were conducted using both synthetic and real data sets and the results confirm the effectiveness and efficiency of disc
the main drawbacks of handheld devices small storage space small size of the display screen discontinuance of the connection to the wlan etc are often incompatible with the need of querying and browsing information extracted from enormous amounts of data which are accessible through the network in this application scenario data compression and summarization have leading role data in lossy compressed format can be transmitted more efficiently than the original ones and can be effectively stored in handheld devices setting the compression ratio accordingly in this paper we introduce very effective compression technique for multidimensional data cubes and the system hand olap which exploits this technique to allow handheld devices to extract and browse compressed two dimensional olap views coming from multidimensional data cubes stored on remote olap server localized on the wired network hand olap effectively and efficiently enables olap in mobile environments and also enlarges the potentialities of decision support systems by taking advantage from the naturally decentralized nature of such environments the idea which the system is based on is rather than querying the original multidimensional data cubes it may be more convenient to generate compressed olap view of them store such view into the handheld device and query it locally off line thus obtaining approximate answers that are suitable for olap applications
wikis have demonstrated how it is possible to convert community of strangers into community of collaborators semantic wikis have opened an interesting way to mix web advantages with the semantic web approach pp wikis have illustrated how wikis can be deployed on pp wikis and take advantages of its intrinsic qualities fault tolerance scalability and infrastructure cost sharing in this paper we present the first pp semantic wiki that combines advantages of semantic wikis and pp wikis building pp semantic wiki is challenging it requires building an optimistic replication algorithm that is compatible with pp constraints ensures an acceptable level of consistency and generic enough to handle semantic wiki pages the contribution of this paper is the definition of clear model for building pp semantic wikis we define the data model operations on this model intentions of these operations algorithms to ensure consistency and finally we implement the swooki prototype based on these algorithms
we discuss the parallelization of arithmetic operations on polynomials modulo triangular set we focus on parallel normal form computations since this is core subroutine in many high level algorithms such as triangular decompositions of polynomial systems when computing modulo triangular set multivariate polynomials are regarded recursively as univariate ones which leads to several implementation challenges when one targets highly efficient code we rely on an algorithm proposed in which addresses some of these issues we propose two level parallel scheme first we make use of parallel multidimensional fast fourier transform in order to perform multivariate polynomial multiplication secondly we extract parallelism from the structure of the sequential normal form algorithm of we have realized multithreaded implementation we report on different strategies for the management of tasks and threads
the invention of the hyperlink and the http transmission protocol caused an amazing new structure to appear on the internet the world wide web with the web there came spiders robots and web crawlers which go from one link to the next checking web health ferreting out information and resources and imposing organization on the huge collection of information and dross residing on the net this paper reports on the use of one such crawler to synthesize document collections on various topics in science mathematics engineering and technology such collections could be part of digital library
interacting with mobile applications and services remains difficult for users because of the impact of mobility on both device capabilities and the cognitive resources of users in this paper we explore the idea that interaction with mobile services can be made faster and more convenient if users are allowed to speculatively prepare in advance the services they wish to use and the data they will need essentially we propose to enable users to offset difficult interactions while mobile with larger number of interactions conducted in the desktop environment to illustrate the concept of offsetting we have developed prototype we call the context clipboard early feedback suggests that majority of the participants are in favour of the concept and that they may be prepared to use explicit preparation behaviour to simplify future interactions we close with our reflections on key implications for systems design and challenges for taking this work forward
this paper presents novel approach to automatic image annotation which combines global regional and contextual features by an extended cross media relevance model unlike typical image annotation methods which use either global or regional features exclusively as well as neglect the textual context information among the annotated words the proposed approach incorporates the three kinds of information which are helpful to describe image semantics to annotate images by estimating their joint probability specifically we describe the global features as distribution vector of visual topics and model the textual context as multinomial distribution the global features provide the global distribution of visual topics over an image while the textual context relaxes the assumption of mutual independence among annotated words which is commonly adopted in most existing methods both the global features and textual context are learned by probability latent semantic analysis approach from the training data the experiments over corel images have shown that combining these three kinds of information is beneficial in image annotation
in diverse applications ranging from stock trading to traffic monitoring popular data streams are typically monitored by multiple analysts for patterns of interest these analysts may submit similar pattern mining requests such as cluster detection queries yet customized with different parameter settings in this work we present an efficient shared execution strategy for processing large number of density based cluster detection queries with arbitrary parameter settings given the high algorithmic complexity of the clustering process and the real time responsiveness required by streaming applications serving multiple such queries in single system is extremely resource intensive the naive method of detecting and maintaining clusters for different queries independently is often in feasible in practice as its demands on system resources increase dramatically with the cardinality of the query workload to overcome this we analyze the interrelations between the cluster sets identified by queries with different parameters settings including both pattern specific and window specific parameters we introduce the notion of the growth property among the cluster sets identified by different queries and characterize the conditions under which it holds by exploiting this growth property we propose uniform solution called chandi which represents identified cluster sets as one single compact structure and performs integrated maintenance on them resulting in significant sharing of computational and memory resources our comprehensive experimental study using real data streams from domains of stock trades and moving object monitoring demonstrates that chandi is on average four times faster than the best alternative methods while using less memory space in our test cases it also shows that chandi scales in handling large numbers of queries on the order of hundreds or even thousands under high input data rates
when building enterprise applications that need to be accessed through variety of client devices developers usually strive to implement most of the business logic device independently while using web browser to display the user interface however when those web based front ends shall be rendered on different devices their differing capabilities may require device specific interaction patterns that still need to be specified and implemented efficiently we present an approach for specifying the dialog flows in multi channel web interfaces with very low redundancy and introduce framework that controls web interfaces device specific dialog flows according to those specifications while keeping the enterprise application logic completely device independent
besides the steady growing of size complexity and distribution of present day information systems business volatility with rapid changes in users wishes and technological upgrading are stressing an overwhelmingly need for more advanced conceptual modeling approaches such advanced conceptual models should coherently and soundly reflect these three crucial dimensions namely the size space and evolution over time dimensions in contribution towards such advanced conceptual approaches we presented in data know eng new form of integration of object orientation with emphasize on componentization into variety of algebraic petri nets we referred to as co netsthe purpose of the present paper is to soundly extend this proposal for coping with dynamic changing of structural and behavioral aspects of co nets components to this aim we are proposing an adequate petri net based meta level that may be sketched as follows first we construct two meta nets for each component one concerns the modification of behavioral aspects and the other is for dealing with structural aspects while the meta net for behavioral dynamic enables the dynamic of any transition in given component to be modified at runtime the meta net for structural aspects completes and enhances these capabilities by allowing involved messages and object signatures ie structure to be dynamically manipulated in addition of rigorous description of this meta level and its illustration using medium complexity banking system example we also discuss how this level brings satisfactory solution to the well known inheritance anomaly problem
aspect oriented programming aop is attracting attention from both research and industry as illustrated by the ever growing popularity of aspectj the de facto standard aop extension of java from compiler construction perspective aspectj is interesting as it is typical example of compositional language ie language composed of number of separate languages with different syntactical styles in addition to plain java aspectj includes language for defining pointcuts and one for defining advices language composition represents non trivial challenge for conventional parsing techniques first combining several languages with different lexical syntax leads to considerable complexity in the lexical states to processed second as new language features for aop are being explored many research proposals are concerned with further extending the aspectj language resulting in need for an extensible syntax definitionthis paper shows how scannerless parsing elegantly addresses the issues encountered by conventional techniques when parsing aspectj we present the design of modular extensible and formal definition of the lexical and context free aspects of the aspectj syntax in the syntax definition formalism sdf which is implemented by scannerless generalized lr parser sglr we introduce grammar mixins as novel application of sdf's modularity features which allows the declarative definition of different keyword policies and combination of extensions we illustrate the modular extensibility of our definition with syntax extensions taken from current research on aspect languages finally benchmarks show the reasonable performance of scannerless generalized lr parsing for this grammar
most contemporary database systems perform cost based join enumeration using some variant of system r's bottom up dynamic programming method the notable exceptions are systems based on the top down transformational search of volcano cascades as recent work has demonstrated bottom up dynamic programming can attain optimality with respect to the shape of the join graph no comparable results have been published for transformational search however transformational systems leverage benefits of top down search not available to bottom up methods in this paper we describe top down join enumeration algorithm that is optimal with respect to the join graph we present performance results demonstrating that combination of optimal enumeration with search strategies such as branch and bound yields an algorithm significantly faster than those previously described in the literature although our algorithm enumerates the search space top down it does not rely on transformations and thus retains much of the architecture of traditional dynamic programming as such this work provides migration path for existing bottom up optimizers to exploit top down search without drastically changing to the transformational paradigm
we present the empathic painting an interactive painterly rendering whose appearance adapts in real time to reflect the perceived emotional state of the viewer the empathic painting is an experiment into the feasibility of using high level control parameters namely emotional state to replace the plethora of low level constraints users must typically set to affect the output of artistic rendering algorithms we describe suite of computer vision algorithms capable of recognising users facial expressions through the detection of facial action units derived from the facs scheme action units are mapped to vectors within continuous space representing emotional state from which we in turn derive continuous mapping to the style parameters of simple but fast segmentation based painterly rendering algorithm the result is digital canvas capable of smoothly varying its painterly style at approximately frames per second providing novel user interactive experience using only commodity hardware
xml has become widely accepted standard for modelling storing and exchanging structured documents taking advantage of the document structure can result in improving the retrieval performance of xml documents notably growing number of these documents are stored in peer to peer networks which are promising self organizing infrastructures documents are distributed over the peer to peer network by either being stored locally on individual peers or by being assigned to collections such as digital libraries current search methods for xml documents in peer to peer networks lack the use of information retrieval techniques for vague queries and relevance detection our work aims for the development of search engine for xml documents where information retrieval methods are enhanced by using structural information documents and global index are distributed over peer to peer network building virtually unlimited storage space in this paper conceptual architecture for xml information retrieval in peer to peer networks is proposed based on this general architecture component structured architecture for concrete search engine is presented which uses an extension of the vector space model to compute relevance for dynamic xml documents
in this paper we propose new framework to perform motion compression for time dependent geometric data temporal coherence in dynamic geometric models can be used to achieve significant compression thereby leading to efficient storage and transmission of large volumes of data the displacement of the vertices in the geometric models is computed using the iterative closest point icp algorithm this forms the core of our motion prediction technique and is used to estimate the transformation between two successive data sets the motion between frames is coded in terms of few affine parameters with some added residues our motion segmentation approach separates the vertices into two groups within the first group motion can be encoded with few affine parameters without the need of residues in the second group the vertices need further encoding of residual errors also in this group for those vertices associated with large residual errors under affine mapping we encode their motion effectively using newtonian motion estimates this automatic segmentation enables our algorithm to he very effective in compressing time dependent geometric data dynamic range data captured from the real world as well as complex animations created using commercial tools can be compressed efficiently using this scheme
general technique combining model checking and abstraction is presented that allows property based analysis of systems consisting of an arbitrary number of featured components we show how parameterised systems can be specified in guarded command form with constraints placed on variables which occur in guards we prove that results that hold for small number of components can be shown to scale up we then show how featured systems can be specified in similar way by relaxing constraints on guards the main result is generalisation theorem for featured systems which we apply to two well known examples
sensor networks have been widely used to collect data about the environment when analyzing data from these systems people tend to ask exploratory questions they want to find subsets of data namely signal reflecting some characteristics of the environment in this paper we study the problem of searching for drops in sensor data specifically the search is to find periods in history when certain amount of drop over threshold occurs in data within time span we propose framework segdiff for extracting features compressing them and transforming the search into standard database queries approximate results are returned from the framework with the guarantee that no true events are missed and false positives are within user specified tolerance the framework efficiently utilizes space and provides fast response to users search experimental results with real world data demonstrate the efficiency of our framework with respect to feature size and search time
this paper describes new approach to finding performance bottlenecks in shared memory parallel programs and its embodiment in the paradyn parallel performance tools running with the blizzard fine grain distributed shared memory system this approach exploits the underlying system's cache coherence protocol to detect data sharing patterns that indicate potential performance bottlenecks and presents performance measurements in data centric manner as demonstration parodyn helped us improve the performance of new shared memory application program by factor of four
type checking and type inference are fundamentally similar problems however the algorithms for performing the two operations on the same type system often differ significantly the type checker is typically straightforward encoding of the original type rules for many systems type inference is performed using two phase constraint based algorithmwe present an approach that given the original type rules written as clauses in logic programming language automatically generates an efficient two phase constraint based type inference algorithm our approach works by partially evaluating the type checking rules with respect to the target program to yield set of constraints suitable for input to an external constraint solver this approach avoids the need to manually develop and verify separate type inference algorithm and is ideal for experimentation with and rapid prototyping of novel type systems
billions of devices on chip is around the corner and the trend of deep submicron dsm technology scaling will continue for at least another decade meanwhile designers also face severe on chip parameter variations soft hard errors and high leakage power how to use these billions of devices to deliver power efficient high performance and yet error resilient computation is challenging task in this paper we attempt to demonstrate some of our perspectives to address these critical issues we elaborate on variation aware synthesis holistic error modeling reliable multicore and synthesis for application specific multicore we also present some of our insights for future reliable computing
growing size and complexity of many websites have made navigation through these sites increasingly difficult attempting to automatically predict the next page for website user to visit has many potential benefits for example in site navigation automatic tour generation adaptive web applications recommendation systems web server optimisation web search and web pre fetching this paper describes an approach to link prediction using markov chain model based on an exponentially smoothed transition probability matrix which incorporates site usage statistics collected over multiple time periods the improved performance of this approach compared to earlier methods is also discussed
novel approach to real time string filtering of large databases is presented the proposed approach is based on combination of artificial neural networks and operates in two stages the first stage employs self organizing map for performing approximate string matching and retrieving those strings of the database which are similar to ie assigned to the same som node as the query string the second stage employs harmony theory network for comparing the previously retrieved strings in parallel with the query string and determining whether an exact match exists the experimental results demonstrate accurate fast and database size independent string filtering which is robust to database modifications the proposed approach is put forward for general purpose directory catalogue and glossary search and internet mail blocking intrusion detection systems url and username classification applications
many problems in distributed computing are impossible when no information about process failures is available it is common to ask what information about failures is necessary and sufficient to circumvent some specific impossibility eg consensus atomic commit mutual exclusion etc this paper asks what information about failures is needed to circumvent any impossibility and sufficient to circumvent some impossibility in other words what is the minimal yet non trivial failure informatio we present an abstraction denoted that provides very little failure information in every run of the distributed system eventually informs the processes that some set of processes in the system cannot be the set of correct processes in that run although seemingly weak for it might provide random information for an arbitrarily long period of time and it only excludes one possibility of correct set among many still captures non trivial failure information we show that is sufficient to circumvent the fundamental wait free set agreement impossibility while doing so we disprove previous conjectures about the weakest failure detector to solve set agreement and we prove that solving set agreement with registers is strictly weaker than solving process consensus using process consensus we prove that is in precise sense minimal to circumvent any wait free impossibility roughly we show that is the weakest eventually stable failure detect or to circumvent any wait free impossibility our results are generalized through an abstraction œÖf that we introduce and prove necessary to solve any problem that cannot be solved in an resilient manner and yet sufficient to solve resilient set agreement
data preparation is an important and critical step in neural network modeling for complex data analysis and it has huge impact on the success of wide variety of complex data analysis tasks such as data mining and knowledge discovery although data preparation in neural network data analysis is important some existing literature about the neural network data preparation are scattered and there is no systematic study about data preparation for neural network data analysis in this study we first propose an integrated data preparation scheme as systematic study for neural network data analysis in the integrated scheme survey of data preparation focusing on problems with the data and corresponding processing techniques is then provided meantime some intelligent data preparation solution to some important issues and dilemmas with the integrated scheme are discussed in detail subsequently cost benefit analysis framework for this integrated scheme is presented to analyze the effect of data preparation on complex data analysis finally typical example of complex data analysis from the financial domain is provided in order to show the application of data preparation techniques and to demonstrate the impact of data preparation on complex data analysis
the key to increasing performance without commensurate increase in power consumption in modern processors lies in increasing both parallelism and core specialization core specialization has been employed in the embedded space and is likely to play an important role in future heterogeneous multi core architectures as well in this paper the face recognition application domain is employed as case study to showcase an architectural design methodology which generates specialized core with high performance and very low powercharacteristics specifically we create asic like execution flows to sustain the high memory parallelism generated within the core the price of this benefit is significant increase in compilation complexity the crux of the problem is the need to co schedule the often conflicting constraints of data access data movement and computation modular compiler approach that employs integer linear programming ilp based interconnect aware instruction and data scheduling techniques to solve this problem is then described the resulting core running the compiled code delivers throughput improvement over high performance processor pentium while simultaneously achieving an energy delay improvement over an energy efficient processor xscale and performs real time face recognition at embedded power budgets
in the ubiquitous computing environment people will interact with everyday objects or computers embedded in them in ways different from the usual and familiar desktop user interface one such typical situation is interacting with applications through large displays such as televisions mirror displays and public kiosks with these applications the use of the usual keyboard and mouse input is not usually viable for practical reasons in this setting the mobile phone has emerged as an excellent device for novel interaction this article introduces user interaction techniques using camera equipped hand held device such as mobile phone or pda for large shared displays in particular we consider two specific but typical situations sharing the display from distance and interacting with touch screen display at close distance using two basic computer vision techniques motion flow and marker recognition we show how camera equipped hand held device can effectively be used to replace mouse and share select and manipulate and objects and navigate within the environment presented through the large display
similarity search has been proved suitable for searching in large collections of unstructured data objects number of practical index data structures for this purpose have been proposed all of them have been devised to process single queries sequentially however in large scale systems such as web search engines indexing multi media content it is critical to deal efficiently with streams of queries rather than with single queries in this paper we show how to achieve efficient and scalable performance in this context to this end we transform sequential index based on clustering into distributed one and devise algorithms and optimizations specially tailored to support high performance parallel query processing
data integration systems provide access to set of heterogeneous autonomous data sources through so called global schema there are basically two approaches for designing data integration system in the global as view approach one defines the elements of the global schema as views over the sources whereas in the local as view approach one characterizes the sources as views over the global schema it is well known that processing queries in the latter approach is similar to query answering with incomplete information and therefore is complex task on the other hand it is common opinion that query processing is much easier in the former approach in this paper we show the surprising result that when the global schema is expressed in the relational model with integrity constraints even of simple types the problem of incomplete information implicitly arises making query processing difficult in the global as view approach as well we then focus on global schemas with key and foreign key constraints which represents situation which is very common in practice and we illustrate techniques for effectively answering queries posed to the data integration system in this case
we consider the view selection problem for xml content based routing given network in which stream of xml documents is routed and the routing decisions are taken based on results of evaluating xpath predicates on these documents select set of views that maximize the throughput of the network while in view selection for relational queries the speedup comes from eliminating joins here the speedup is obtained from gaining direct access to data values in an xml packet without parsing that packet the views in our context can be seen as binary representation of the xml document tailored for the network's workloadin this paper we define formally the view selection problem in the context of xml content based routing and provide practical solution for it first we formalize the problem while the exact formulation is too complex to admit practical solutions we show that it can be simplified to manageable optimization problem without loss in precision second we show that the simplified problem can be reduced to the integer cover problem the integer cover problem is known to be np hard and to admit log greedy approximation algorithm third we show that the same greedy approximation algorithm performs much better on class of work loads called hierarchical workloads which are typical in xml stream processing namely it returns an optimal solution for hierarchical workloads and degrades gracefully to the log general bound as the workload becomes less hierarchical
this paper describes the denali isolation kernel an operating system architecture that safely multiplexes large number of untrusted internet services on shared hardware denali's goal is to allow new internet services to be pushed into third party infrastructure relieving internet service authors from the burden of acquiring and maintaining physical infrastructure our isolation kernel exposes virtual machine abstraction but unlike conventional virtual machine monitors denali does not attempt to emulate the underlying physical architecture precisely and instead modifies the virtual architecture to gain scale performance and simplicity of implementation in this paper we first discuss design principles of isolation kernels and then we describe the design and implementation of denali following this we present detailed evaluation of denali demonstrating that the overhead of virtualization is small that our architectural choices are warranted and that we can successfully scale to more than virtual machines on commodity hardware
security typed languages such as jif require the programmer to label variables with information flow security policies as part of application development the compiler then flags errors wherever information leaks may occur resolving these information leaks is critical task in security typed language application development unfortunately because information flows can be quite subtle simple error messages tend to be insufficient for finding and resolving the source of information leaks more sophisticated development tools are needed for this task to this end we provide set of principles to guide the development of such tools furthermore we implement subset of these principles in an integrated development environment ide for jif called jifclipse which is built on the eclipse extensible development platform our plug in provides jif programmer with additional tools to view hidden information generated by jif compilation to suggest fixes for errors and to get more specific information behind an error message better development tools are essential for making security typed application development practical jifclipse is first step in this process
requests for communication via mobile devices can be disruptive to the current task or social situation to reduce the frequency of disruptive requests one promising approach is to provide callers with cues of receiver's context through an awareness display allowing informed decisions of when to call existing displays typically provide cues based on what can be readily sensed which may not match what is needed during the call decision process in this paper we report results of four week diary study of mobile phone usage where users recorded what context information they considered when making call and what information they wished others had considered when receiving call our results were distilled into lessons that can be used to improve the design of awareness displays for mobile devices eg show frequency of receiver's recent communication and distance from receiver to her phone we discuss technologies that can enable cues indicated in these lessons to be realized within awareness displays as well as discuss limitations of such displays and issues of privacy
this paper considers the problem of synthesizing application specific network on chip noc architectures we propose two heuristic algorithms called cluster and decompose that can systematically examine different set partitions of communication flows and we propose rectilinear steiner tree rst based algorithms for generating an efficient network topology for each group in the partition different evaluation functions in fitting with the implementation backend and the corresponding implementation technology can be incorporated into our solution framework to evaluate the implementation cost of the set partitions and rst topologies generated in particular we experimented with an implementation cost model based on the power consumption parameters of nm process technology where leakage power is major source of energy consumption experimental results on variety of noc benchmarks showed that our synthesis results can on average achieve reduction in power consumption over the best standard mesh implementation to further gauge the effectiveness of our heuristic algorithms we also implemented an exact algorithm that enumerates all distinct set partitions for the benchmarks where exact results could be obtained our cluster and decompose algorithms on average can achieve results within and of exact results with execution times all under second whereas the exact algorithms took as much as hours
we show that large class of data flow analyses for imperative languages are describable as type systems in the following technical sense possible results of an analysis can be described in language of types so that program checks with type if and only if this type is supertype of the result of applying the analysis type checking is easy with the help of certificate that records the eureka bits of typing derivation certificate assisted type checking amounts to form of lightweight analysis la rose for secure information flow we obtain type system that is considerably more precise than that of volpano et al but not more sophisticated importantly our type systems are compositional
there has been much work in recent years on extending ml with recursive modules one of the most difficult problems in the development of such an extension is the double vision problem which concerns the interaction of recursion and data abstraction in previous work defined type system called rtg which solves the double vision problem at the level of system style core calculus in this paper scale the ideas and techniques of rtg to the level of recursive ml style module calculus called rmc thus establishing that no tradeoff between data abstraction and recursive modules is necessary first describe rmc's typing rules for recursive modules informally and discuss some of the design questions that arose in developing them then present the formal semantics of rmc which is interesting in its own right the formalization synthesizes aspects of both the definition and the harper stone interpretation of standard ml and includes novel two pass algorithm for recursive module typechecking in which the coherence of the two passes is emphasized by their representation in terms of the same set of inference rules
we present feasibility study for performing virtual address translation without specialized translation hardware removing address translation hardware and instead managing address translation in software has the potential to make the processor design simpler smaller and more energy efficient at little or no cost in performance the purpose of this study is to describe the design and quantify its performance impact trace driven simulations show that software managed address translation is just as efficient as hardware managed address translation moreover mechanisms to support such features as shared memory superpages fine grained protection and sparse address spaces can be defined completely in software allowing for more flexibility than in hardware defined mechanisms
abstract this paper provides an extensive overview of existing research in the field of software refactoring this research is compared and discussed based on number of different criteria the refactoring activities that are supported the specific techniques and formalisms that are used for supporting these activities the types of software artifacts that are being refactored the important issues that need to be taken into account when building refactoring tool support and the effect of refactoring on the software process running example is used throughout the paper to explain and illustrate the main concepts
we study oblivious routing in fat tree based system area networks with deterministic routing under the assumption that the traffic demand is uncertain the performance of routing algorithm under uncertain traffic demands is characterized by the oblivious performance ratio that bounds the relative performance of the routing algorithm with respect to the optimal algorithm for any given traffic demand we consider both single path routing where only one path is used to carry the traffic between each source destination pair and multipath routing where multiple paths are allowed for single path routing we derive lower bounds of the oblivious performance ratio for different fat trees and develop routing schemes that achieve the optimal oblivious performance ratios for commonly used topologies our evaluation results indicate that the proposed oblivious routing schemes not only provide the optimal worst case performance guarantees but also outperform existing schemes in average cases for multipath routing we show that it is possible to obtain an optimal scheme for all traffic demands an oblivious performance ratio of these results quantitatively demonstrate the performance difference between single path routing and multipath routing in fat trees
programming paradigms are designed to express algorithms elegantly and efficiently there are many parallel programming paradigms each suited to certain class of problems selecting the best parallel programming paradigm for problem minimizes programming effort and maximizes performance given the increasing complexity of parallel applications no one paradigm may be suitable for all components of an application today most parallel scientific applications are programmed with single paradigm and the challenge of multi paradigm parallel programming remains unmet in the broader communitywe believe that each component of parallel program should be programmed using the most suitable paradigm furthermore it is not sufficient to simply bolt modules together programmers should be able to switch between paradigms easily and resource management across paradigms should be automatic we present pre existing adaptive runtime system arts and show how it can be used to meet these challenges by allowing the simultaneous use of multiple parallel programming paradigms and supporting resource management across all of them we discuss the implementation of some common paradigms within the arts and demonstrate the use of multiple paradigms within our featurerich unstructured mesh framework we show how this approach boosts performance and productivity for an application developed using this framework
this paper presents the design principles implementation and evaluation of the retos operating system which is specifically developed for micro sensor nodes retos has four distinct objectives which are to provide multithreaded programming interface system resiliency kernel extensibility with dynamic reconfiguration and wsn oriented network abstraction retos is multithreaded operating system hence it provides the commonly used thread model of programming interface to developers we have used various implementation techniques to optimize the performance and resource usage of multithreading retos also provides software solutions to separate kernel from user applications and supports their robust execution on mmu less hardware the retos kernel can be dynamically reconfigured via loadable kernel framework so application optimized and resource efficient kernel is constructed finally the networking architecture in retos is designed with layering concept to provide wsn specific network abstraction retos currently supports atmel atmega ti msp and chipcon cc family of microcontrollers several real world wsn applications are developed for retos and the overall evaluation of the systems is described in the paper
some software defects trigger failures only when certain complex information flows occur within the software profiling and analyzing such flows therefore provides potentially important basis for filtering test cases we report the results of an empirical evaluation of several test case filtering techniques that are based on exercising complex information flows both coverage based and profile distribution based filtering techniques are considered they are compared to filtering techniques based on exercising basic blocks branches function calls and def use pairs with respect to their effectiveness for revealing defects
if kolmogorov complexity measures information in one object and information distance measures information shared by two objects how do we measure information shared by many objects this paper provides an initial pragmatic study of this fundamental data mining question firstly em xn is defined to be the minimum amount of thermodynamic energy needed to convert from any xi to any xj with this definition several theoretical problems have been solved second our newly proposed theory is applied to select comprehensive review and specialized review from many reviews core feature words expanded words and dependent words are extracted respectively comprehensive and specialized reviews are selected according to the information among them this method of selecting single review can be extended to select multiple reviews as well finally experiments show that this comprehensive and specialized review mining method based on our new theory can do the job efficiently
this paper proposes statistical tree to tree model for producing translations two main contributions are as follows method for the extraction of syntactic structures with alignment information from parallel corpus of translations and use of discriminative feature based model for prediction of these target language syntactic structures which we call aligned extended projections or aeps an evaluation of the method on translation from german to english shows similar performance to the phrase based model of koehn et al
this paper presents and validates performance models for varietyvof high performance collective communication algorithms for systems with cell processors the systems modeled include single cell processor two cell chips on cell blade and cluster of cell blades the models extend plogp the well known point topoint performance model by accounting for the unique hardware characteristics of the cell eg heterogeneous interconnects and dma engines and by applying the model to collective communication this paper also presents micro benchmark suite to accurately measure the extended plogp parameters on the cell blade and then uses these parameters to model different algorithms for the barrier broadcast reduce all reduce and all gather collective operations out of total performance predictions of them see less than error compared to the actual execution time and all of them see less than
while association rule mining is one of the most popular data mining techniques it usually results in many rules some of which are not considered as interesting or significant for the application at hand in this paper we conduct systematic approach to ascertain the discovered rules and provide rigorous statistical approach supporting this framework the strategy proposed combines data mining and statistical measurement techniques including redundancy analysis sampling and multivariate statistical analysis to discard the non significant rules real world dataset is used to demonstrate how the proposed unified framework can discard many of the redundant or non significant rules and still preserve high accuracy of the rule set as whole
the implementation and maintenance of industrial applications have continuously become more and more difficult in this context one problem is the evaluation of complex systems the ieee defines prototyping as development approach promoting the implementation of pilot version of the intended product this approach is potential solution to the early evaluation of system it can also be used to avoid the shift between the description specification of system and its implementation this brief introduction to the special section on rapid system prototyping illustrates current picture of prototyping
in this paper we investigate the difference between wikipedia and web link structure with respect to their value as indicators of the relevance of page for given topic of request our experimental evidence is from two ir test collections the gov collection used at the trec web tracks and the wikipedia xml corpus used at inex we first perform comparative analysis of wikipedia and gov link structure and then investigate the value of link evidence for improving search on wikipedia and on the gov domain our main findings are first wikipedia link structure is similar to the web but more densely linked second wikipedia's outlinks behave similar to inlinks and both are good indicators of relevance whereas on the web the inlinks are more important third when incorporating link evidence in the retrieval model for wikipedia the global link evidence fails and we have to take the local context into account
we study the problem of updating xml repository through security views users are provided with the view of the repository schema they are entitled to see they write update requests over their view using the xupdate language each request is processed in two rewriting steps first the xpath expression selecting the nodes to update from the view is rewritten to another expression that only selects nodes the user is permitted to see second the xupdate query is refined according to the write privileges held by the user
high end computing is suffering data deluge from experiments simulations and apparatus that creates overwhelming application dataset sizes end user workstations despite more processing power than ever before are ill equipped to cope with such data demands due to insufficient secondary storage space and rates meanwhile large portion of desktop storage is unused we present the freeloader framework which aggregates unused desktop storage space and bandwidth into shared cache scratch space for hosting large immutable datasets and exploiting data access locality our experiments show that freeloader is an appealing low cost solution to storing massive datasets by delivering higher data access rates than traditional storage facilities in particular we present novel data striping techniques that allow freeloader to efficiently aggregate workstation's network communication bandwidth and local bandwidth in addition the performance impact on the native workload of donor machines is small and can be effectively controlled
specialization of heap objects is critical for pointer analysis to effectively analyze complex memory activity this paper discusses heap specialization with respect to call chains due to the sheer number of distinct call chains exhaustive specialization can be cumbersome on the other hand insufficient specialization can miss valuable opportunities to prevent spurious data flow which results in not only reduced accuracy but also increased overheadin determining whether further specialization will be fruitful an object's escape information can be exploited from empirical study we found that restriction based on escape information is often but not always sufficient at prohibiting the explosive nature of specializationfor in depth case study four representative benchmarks are selected for each benchmark we vary the degree of heap specialization and examine its impact on analysis results and time to provide better visibility into the impact we present the points to set and pointed to by set sizes in the form of histograms
we introduce method for extracting boundary surfaces from volumetric models of mechanical parts by ray ct scanning when the volumetric model is composed of two materials one for the object and the other for the background air these boundary surfaces can be extracted as isosurfaces using contouring method such as marching cubes lorensen and cline for volumetric model composed of more than two materials we need to classify the voxel types into segments by material and use generalized marching cubes algorithm that can deal with both ct values and material types here we propose method for precisely classifying the volumetric model into its component materials using modified and combined method of two well known algorithms in image segmentation region growing and graph cut we then apply the generalized marching cubes algorithm to generate triangulated mesh surfaces in addition we demonstrate the effectiveness of our method by constructing high quality triangular mesh models of the segmented parts
this study proposes an extended geographical database based on photo shooting history to enable the suggestion of candidate captions to newly shot photos the extended geographical database consists of not only subject positions but also the likely shooting positions and directions estimated using the histories of the shooting positions and directions of the subjects user can add caption to photo by selecting an appropriate one from the candidate captions the candidate captions are acquired using the shooting position and direction as key to the extended geographical database in this paper we present the results of experiments for constructing the extended geographical database using prototype system
this paper explores using information about program branch probabilities to optimize the results of hardware compilation the basic premise is to promote utilization by dedicating more resources to branches which execute more frequently new hardware compilation and flow control scheme are presented which enable the computation rate of different branches to be matched to the observed branch probabilities we propose an analytical queuing network performance model to determine the optimal settings for basic block computation rates given set of observed branch probabilities an experimental hardware compilation system has been developed to evaluate this approach the branch optimization design space is characterized in an experimental study for xilinx virtex fpgas of two complex applications video feature extraction and progressive refinement radiosity for designs of equal performance branch optimized designs require percent and percent less area for designs of equal area branch optimized designs run up to three times faster our analytical performance model is shown to be highly accurate with relative error between and times
we present an architecture designed for alert verification ie to reduce false positives in network intrusion detection systems our technique is based on systematic and automatic anomaly based analysis of the system output which provides useful context information regarding the network services the false positives raised by the nids analyzing the incoming traffic which can be either signature or anomaly based are reduced by correlating them with the output anomalies we designed our architecture for tcp based network services which have client server architecture such as http benchmarks show substantial reduction of false positives between and
xml has become the lingua franca for data exchange and integration across administrative and enterprise boundaries nearly all data providers are adding xml import or export capabilities and standard xml schemas and dtds are being promoted for all types of data sharing the ubiquity of xml has removed one of the major obstacles to integrating data from widely disparate sources namely the heterogeneity of data formats however general purpose integration of data across the wide are also requires query processor that can query data sources on demand receive streamed xml data from them and combine and restructure the data into new xml output while providing good performance for both batch oriented and ad hoc interactive queries this is the goal of the tukwila data integration system the first system that focuses on network bound dynamic xml data sources in contrast to previous approaches which must read parse and often store entire xml objects before querying them tukwila can return query results even as the data is streaming into the system tukwila is built with new system architecture that extends adaptive query processing and relational engine techniques into the xml realm as facilitated by pair of operators that incrementally evaluate query's input path expressions as data is read in this paper we describe the tukwila architecture and its novel aspects and we experimentally demonstrate that tukwila provides better overall query performance and faster initial answers than existing systems and has excellent scalability
new dynamic cache resizing scheme for low power cam tag caches is introduced control algorithm that is only activated on cache misses uses duplicate set of tags the miss tags to minimize active cache size while sustaining close to the same hit rate as full size cache the cache partitioning mechanism saves both switching and leakage energy in unused partitions with little impact on cycle time simulation results show that the scheme saves of data cache energy and of instruction cache energy with minimal performance impact
we present constructs that create manage and verify digital audit trails for versioning file systems based upon small amount of data published to third party file system commits to version history at later date an auditor uses the published data to verify the contents of the file system at any point in time audit trails create an analog of the paper audit process for file data helping to meet the requirements of electronic record legislation such as sarbanes oxley our techniques address the and computational efficiency of generating and verifying audit trails the aggregation of audit information in directory hierarchies and constructing verifiable audit trails in the presence of lost data
the performance of modern machines is increasingly limited by insufficient memory bandwidth one way to alleviate this bandwidth limitation for given program is to minimize the aggregate data volume the program transfers from memory in this article we present compiler strategies for accomplishing this minimization following discussion of the underlying causes of bandwidth limitations we present two step strategy to exploit global cache reuse the temporal reuse across the whole program and the spatial reuse across the entire data set used in that program in the first step we fuse computation on the same data using technique called reuse based loop fusion to integrate loops with different control structures we prove that optimal fusion for bandwidth is np hard and we explore the limitations of computation fusion using perfect program information in the second step we group data used by the same computation through the technique of affinity based data regrouping which intermixes the storage assignments of program data elements at different granularities we show that the method is compile time optimal and can be used on array and structure data we prove that two extensions partial and dynamic data regrouping are np hard problems finally we describe our compiler implementation and experiments demonstrating that the new global strategy on average reduces memory traffic by over and improves execution speed by over on two high end workstations
it has been proposed that email clients could be improved if they presented messages grouped into conversations an email conversation is the tree of related messages that arises from the use of the reply operation we propose two models of conversation the first model characterizes conversation as chronological sequence of messages the second as tree based on the reply relationship we show how existing email clients and prior research projects implicitly support each model to greater or lesser degree depending on their design but none fully supports both models simultaneously we present mixed model visualization that simultaneously presents sequence and reply relationships among the messages of conversation making both visible at glance we describe the integration of the visualization into working prototype email client usability study indicates that the system meets our usability goals and verifies that the visualization fully conveys both types of relationships within the messages of an email conversation
we collected mobility traces of avatars spanning multiple regions in second life popular user created virtual world we analyzed the traces to characterize the dynamics of the avatars mobility and behavior both temporally and spatially we discuss the implications of our findings on the design of peer to peer architecture interest management mobility modeling of avatars server load balancing and zone partitioning caching and prefetching for user created virtual worlds
requirement for rendering realistic images interactively is efficiently simulating material properties recent techniques have improved the quality for interactively rendering dielectric materials but have mostly neglected phenomenon associated with refraction namely total internal reflection we present an algorithm to approximate total internal reflection on commodity graphics hardware using ray depth map intersection technique that is interactive and requires no precomputation our results compare favorably with ray traced images and improve upon approaches that avoid total internal reflection
during the last two decades wide variety of advanced methods for the visual exploration of large data sets have been proposed for most of these techniques user interaction has become crucial element since there are many situations in which users or analysts have to select the right parameter settings from among many in order to construct insightful visualizations the right choice of input parameters is essential since suboptimal parameter settings or the investigation of irrelevant data dimensions make the exploration process more time consuming and may result in wrong conclusions but finding the right parameters is often tedious process and it becomes almost impossible for an analyst to find an optimal parameter setting manually because of the volume and complexity of today's data sets therefore we propose novel approach for automatically determining meaningful parameter and attribute settings based on the combined analysis of the data space and the resulting visualizations with respect to given task our technique automatically analyzes pixel images resulting from visualizations created from diverse parameter mappings and ranks them according to the potential value for the user this allows more effective and more efficient visual data analysis process since the attribute parameter space is reduced to meaningful selections and thus the analyst obtains faster insight into the data real world applications are provided to show the benefit of the proposed approach
procrastination scheduling has gained importance for energy efficiency due to the rapid increase in the leakage power consumption under procrastination scheduling task executions are delayed to extend processor shutdown intervals thereby reducing the idle energy consumption we propose algorithms to compute the maximum procrastination intervals for tasks scheduled by either the fixed priority or the dual priority scheduling policy we show that dual priority scheduling always guarantees longer shutdown intervals than fixed priority scheduling we further combine procrastination scheduling with dynamic voltage scaling to minimize the total static and dynamic energy consumption of the system our simulation experiments show that the proposed algorithms can extend the sleep intervals up to times while meeting the timing requirements the results show up to energy gains over dynamic voltage scaling
this paper studies virus inoculation game on social networks framework is presented which allows the measuring of the windfall of friendship ie how much players benefit if they care about the welfare of their direct neighbors in the social network graph compared to purely selfish environments we analyze the corresponding equilibria and show that the computation of the worst and best nash equilibrium is np hard intriguingly even though the windfall of friendship can never be negative the social welfare does not increase monotonically with the extent to which players care for each other while these phenomena are known on an anecdotal level our framework allows us to quantify these effects analytically
in this paper we present nocee fast and accurate method for extracting energy models for packet switched network on chip noc routers linear regression is used to model the relationship between events occurring in the noc and energy consumption the resulting models are cycle accurate and can be applied to different technology libraries we verify the individual router estimation models with many different synthetically generated traffic patterns and data inputs characterization of small library takes about two hours the mean absolute energy estimation error of the resultant models is max against complete gate level simulation we also apply this method to number of complete nocs with inputs extracted from synthetic application traces and compare our estimated results to the gate level power simulations mean absolute error is our estimation methodology has been integrated with commercial logic synthesis flow and power estimation tools synopsys design compiler and primepower allowing application across different designs the extracted models show the different trends across various parameterizations of network on chip routers and have been integrated into an architecture exploration framework
data cube has been playing an essential role in fast olap online analytical processing in many multi dimensional data warehouses however there exist data sets in applications like bioinformatics statistics and text processing that are characterized by high dimensionality eg over dimensions and moderate size eg around tuples no feasible data cube can be constructed with such data sets in this paper we will address the problem of developing an efficient algorithm to perform olap on such data sets experience tells us that although data analysis tasks may involve high dimensional space most olap operations are performed only on small number of dimensions at time based on this observation we propose novel method that computes thin layer of the data cube together with associated value list indices this layer while being manageable in size will be capable of supporting flexible and fast olap operations in the original high dimensional space through experiments we will show that the method has costs that scale nicely with dimensionality furthermore the costs are comparable to that of accessing an existing data cube when full materialization is possible
this paper presents domain specific dependency constraint language that allows software architects to restrict the spectrum of structural dependencies which can be established in object oriented systems the ultimate goal is to provide architects with means to define acceptable and unacceptable dependencies according to the planned architecture of their systems once defined such restrictions are statically enforced by tool thus avoiding silent erosions in the architecture the paper also presents results from applying the proposed approach to different versions of real world human resource management system copyright copy john wiley sons ltd
the processing of xml queries can result in evaluation of various structural relationships efficient algorithms for evaluating ancestor descendant and parent child relationships have been proposed whereas the problems of evaluating preceding sibling following sibling and preceding following relationships are still open in this paper we studied the structural join and staircase join for sibling relationship first the idea of how to filter out and minimize unnecessary reads of elements using parent's structural information is introduced which can be used to accelerate structural joins of parent child and preceding sibling following sibling relationships second two efficient structural join algorithms of sibling relationship are proposed these algorithms lead to optimal join performance nodes that do not participate in the join can be judged beforehand and then skipped using tree index besides each element list joined is scanned sequentially once at most furthermore output of join results is sorted in document order we also discussed the staircase join algorithm for sibling axes studies show that staircase join for sibling axes is close to the structural join for sibling axes and shares the same characteristic of high efficiency our experimental results not only demonstrate the effectiveness of our optimizing techniques for sibling axes but also validate the efficiency of our algorithms as far as we know this is the first work addressing this problem specially
computing clusters cc consisting of several connected machines could provide high performance multiuser time sharing environment for executing parallel and sequential jobs in order to achieve good performance in such an environment it is necessary to assign processes to machines in manner that ensures efficient allocation of resources among the jobs this paper presents opportunity cost algorithms for online assignment of jobs to machines in cc these algorithms are designed to improve the overall cpu utilization of the cluster and to reduces the and the interprocess communication ipc overhead our approach is based on known theoretical results on competitive algorithms the main contribution of the paper is how to adapt this theory into working algorithms that can assign jobs to machines in manner that guarantees near optimal utilization of the cpu resource for jobs that perform and ipc operations the developed algorithms are easy to implement we tested the algorithms by means of simulations and executions in real system and show that they outperform existing methods for process allocation that are based on ad hoc heuristics
we present novel algorithm called clicks that finds clusters in categorical datasets based on search for partite maximal cliques unlike previous methods clicks mines subspace clusters it uses selective vertical method to guarantee complete search clicks outperforms previous approaches by over an order of magnitude and scales better than any of the existing method for high dimensional datasets these results are demonstrated in comprehensive performance study on real and synthetic datasets
we propose new instruction branch on random that is like standard conditional branch except rather than specifying the condition on which the branch should be taken it specifies frequency at which the branch should be taken we show that branch on random is useful for reducing the overhead of program instrumentation via sampling specifically branch on random provides an order of magnitude reduction in execution time overhead compared to previously proposed software only frameworks for instrumentation sampling furthermore we demonstrate that branch on random can be cleanly architected and implemented simply and efficiently for simple processors we estimate that branch on random can be implemented with bits of state and less than gates for aggressive superscalars this grows to less than bits of state and at most few hundred gates
sharir and welzl introduced an abstract framework for optimization problems called lp type problems or also generalized linear programming problems which proved useful in algorithm design we define new and as we believe simpler and more natural framework violator spaces which constitute proper generalization of lp type problems we show that clarkson's randomized algorithms for low dimensional linear programming work in the context of violator spaces for example in this way we obtain the fastest known algorithm for the matrix generalized linear complementarity problem with constant number of blocks we also give two new characterizations of lp type problems they are equivalent to acyclic violator spaces as well as to concrete lp type problems informally the constraints in concrete lp type problem are subsets of linearly ordered ground set and the value of set of constraints is the minimum of its intersection
sql extensions that allow queries to explicitly specify data quality requirements in terms of currency and consistency were proposed in an earlier paper this paper develops data quality aware finer grained cache model and studies cache design in terms of four fundamental properties presence consistency completeness and currency the model provides an abstract view of the cache to the query processing layer and opens the door for adaptive cache management we describe an implementation approach that builds on the mtcache framework for partially materialized views the optimizer checks most consistency constraints and generates dynamic plan that includes currency checks and inexpensive checks for dynamic consistency constraints that cannot be validated during optimization our solution not only supports transparent caching but also provides fine grained data currency and consistency guarantees
significant fraction of the software and resource usage of modern handheld computer is devoted to its graphical user interface gui moreover guis are direct users of the display and also determine how users interact with software given that displays consume significant fraction of system energy it is very important to optimize guis for energy consumption this work presents the first gui energy characterization methodology energy consumption is characterized for three popular gui platforms windows window system and qt from the hardware software and application perspectives based on this characterization insights are offered for improving gui platforms and designing guis in an energy efficient and aware fashion such characterization also provides firm basis for further research on gui energy optimization
in this work we analyze the complexity of local broadcasting in the physical interference model we present two distributed randomized algorithms one that assumes that each node knows how many nodes there are in its geographical proximity and another which makes no assumptions about topology knowledge we show that if the transmission probability of each node meets certain characteristics the analysis can be decoupled from the global nature of the physical interference model and each node performs successful local broadcast in time proportional to the number of neighbors in its physical proximity we also provide worst case optimality guarantees for both algorithms and demonstrate their behavior in average scenarios through simulations
we consider the problem if given program satisfies specified safety property interesting programs have infinite state spaces with inputs ranging over infinite domains and for these programs the property checking problem is undecidable two broad approaches to property checking are testing and verification testing tries to find inputs and executions which demonstrate violations of the property verification tries to construct formal proof which shows that all executions of the program satisfy the property testing works best when errors are easy to find but it is often difficult to achieve sufficient coverage for correct programs on the other hand verification methods are most successful when proofs are easy to find but they are often inefficient at discovering errors we propose new algorithm synergy which combines testing and verification synergy unifies several ideas from the literature including counterexample guided model checking directed testing and partition refinementthis paper presents description of the synergy algorithm its theoretical properties comparison with related algorithms and prototype implementation called yogi
this paper presents our toolkit for developing java bytecode translator bytecode translation is getting important in various domains such as generative programming and aspect oriented programming to help the users easily develop translator the design of our toolkit is based on the reflective architecture however the previous implementations of this architecture involved serious runtime penalties to address this problem our toolkit uses custom compiler so that the runtime penalties are minimized since the previous version of our toolkit named javassist has been presented in another paper this paper focuses on this new compiler support for performance improvement this feature was not included in the previous version
click fraud is jeopardizing the industry of internet advertising internet advertising is crucial for the thriving of the entire internet since it allows producers to advertise their products and hence contributes to the well being of commerce moreover advertising supports the intellectual value of the internet by covering the running expenses of publishing content some content publishers are dishonest and use automation to generate traffic to defraud the advertisers similarly some advertisers automate clicks on the advertisements of their competitors to deplete their competitors advertising budgets this paper describes the advertising network model and focuses on the most sophisticated type of fraud which involves coalitions among fraudsters we build on several published theoretical results to devise the similarity seeker algorithm that discovers coalitions made by pairs of fraudsters we then generalize the solution to coalitions of arbitrary sizes before deploying our system on real network we conducted comprehensive experiments on data samples for proof of concept the results were very accurate we detected several coalitions formed using various techniques and spanning numerous sites this reveals the generality of our model and approach
regular path queries are the building blocks of almost any mechanism for querying semistructured data despite the fact that the main applications of such data are distributed there are only few works dealing with distributed evaluation of regular path queries in this paper we present message efficient and truly distributed algorithm for computing the answer to regular path queries in multi source semistructured database setting our algorithm is general as it works for the larger class of weighted regular path queries on weighted semistructured databases also we show how to make our algorithm fault tolerant to smoothly work in environments prone to process or machine failures this is very desirable in grid setting which is today's new paradigm of distributed computing and where one does not have full control over machines that can unexpectedly leave in the middle of computation
in this paper we address the problem of garbage collection in single failure fault tolerant home based lazy release consistency hlrc distributed shared memory dsm system based on independent checkpointing and logging our solution uses laziness in garbage collection and exploits consistency constraints of the hlrc memory model for low overhead and scalability we prove safe bounds on the state that must be retained in the system to guarantee correct recovery after failure we devise two algorithms for garbage collection of checkpoints and logs checkpoint garbage collection cgc and lazy log trimming llt the proposed approach targets large scale distributed shared memory computing on local area clusters of computers in such systems using global synchronization or extra communication for garbage collection is inefficient or simply impractical due to system scale and temporary disconnections in communication the challenge lies in controlling the size of the logs and the number of checkpoints without global synchronization while tolerating transient disruptions in communication our garbage collection scheme is completely distributed does not force processes to synchronize does not add extra messages to the base dsm protocol and uses only the available dsm protocol information evaluation results for real applications show that it effectively bounds the number of past checkpoints to be retained and the size of the logs in stable storage
in on line analytical processing olap users explore database cube with roll up and drill down operations in order to find interesting results most approaches rely on simple aggregations and value comparisons in order to validate findings in this work we propose to combine olap dimension lattice traversal and statistical tests to discover significant metric differences between highly similar groups parametric statistical test allows pair wise comparison of neighboring cells in cuboids providing statistical evidence about the validity of findings we introduce two dimensional checkerboard visualization of the cube that allows interactive exploration to understand significant measure differences between two cuboids differing in one dimension along with associated image data our system is tightly integrated into relational dbms by dynamically generating sql code which incorporates several optimizations to efficiently explore the cube to visualize discovered cell pairs and to view associated images we present an experimental evaluation with medical data sets focusing on finding significant relationships between risk factors and disease
gaifman shapiro style architecture of program modules is introduced in the case of normal logic programs under stable model semantics the composition of program modules is suitably limited by module conditions which ensure the compatibility of the module system with stable models the resulting module theorem properly strengthens lifschitz and turner's splitting set theorem for normal logic programs consequently the respective notion of equivalence between modules ie modular equivalence proves to be congruence relation moreover it is shown how our translation based verification method is accommodated to the case of modular equivalence and how the verification of weak visible equivalence can be optimized as sequence of module level tests
