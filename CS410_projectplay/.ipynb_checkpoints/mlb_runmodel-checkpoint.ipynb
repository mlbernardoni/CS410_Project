{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a topic coverage matrix preset to 0\n",
    "topiccoverage = []\n",
    "i = 0\n",
    "while i < len(docs_per_timeslice) :\n",
    "    y = 0\n",
    "    thistopic = []\n",
    "    while y < num_topics:\n",
    "        thistopic.append(0.0)\n",
    "        y += 1\n",
    "    topiccoverage.append(thistopic.copy())\n",
    "    i += 1\n",
    "# get the topic coverage per timeslice per doc\n",
    "timeslice = 0\n",
    "for timeslicedocs in docs_per_timeslice :\n",
    "    # for each doc in this timeslice\n",
    "    for doc in timeslicedocs :\n",
    "        # get the probability matrix\n",
    "        probs = model.get_document_topics(bow[doc]).copy()\n",
    "        #its a sparse array, prob[0] is the topic and prob[1] is the probabiltiy\n",
    "        for prob in probs :\n",
    "            topiccoverage[timeslice][prob[0]] += prob[1]\n",
    "    timeslice += 1\n",
    "\n",
    "# ##################################################\n",
    "# INPUT:\n",
    "# timeslicetokencounts - we have the word coverage for each model token\n",
    "# topiccoverage - and now we have the topic coverage\n",
    "# topics - the current topic word probabilities\n",
    "#\n",
    "# OUTPUT:\n",
    "# updated array newtopics = []\n",
    "#\n",
    "# run the iteration\n",
    "# ##################################################   \n",
    "newtopics = []\n",
    "purity = 0.0\n",
    "purity_count = 0\n",
    "lowtopics = []\n",
    "causality_confidence = 0.0\n",
    "causality_count = 0\n",
    "for ii in range(0, num_topics) :\n",
    "    tempgc = grangercausalitytests([[bets[i],topiccoverage[i][ii]] for i in range(0, len(bets))], the_lag, verbose=False)\n",
    "\n",
    "    lowpvalue = 2.0\n",
    "    lowlag = 0\n",
    "    for yy in range(1,the_lag+1) :\n",
    "        stats = tempgc.get(yy)[0].get('ssr_ftest')\n",
    "        if (stats[1] < lowpvalue) :\n",
    "            lowlag = yy\n",
    "            lowpvalue = stats[1]\n",
    "    causality_confidence += lowpvalue\n",
    "    causality_count += 1\n",
    "    if lowpvalue < low_threshold :\n",
    "        causality_confidence += lowpvalue\n",
    "        causality_count += 1\n",
    "        lowtopics.append(ii)\n",
    "print (\"Significant Topics\", lowtopics)\n",
    "for topic in lowtopics :\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    neg_percent = 0.0\n",
    "    pos_percent = 0.0\n",
    "    neg_wordprob = []\n",
    "    pos_wordprob = []\n",
    "    wordprobstats = []\n",
    "    tot = np.longdouble(0)\n",
    "    count = 0\n",
    "\n",
    "    for wordprob in topics[topic] :\n",
    "        sig = float(wordprob * pgranger[count])\n",
    "        tot += wordprob\n",
    "        wordlist = [abs(sig), pgranger[count], count, wordprob]\n",
    "        wordprobstats.append(wordlist.copy())\n",
    "        count += 1\n",
    "    for wordprobstat in wordprobstats :\n",
    "        wordprobstat[0] = wordprobstat[0]/tot # normalize\n",
    "\n",
    "    wordprobstats = sorted(wordprobstats, key = lambda x: float(x[0]), reverse = True)\n",
    "\n",
    "    hit_limit = 0\n",
    "    for wordprobstat in wordprobstats :\n",
    "        if wordprobstat[1] > 0  :\n",
    "            if hit_limit == 0 :\n",
    "                pos_wordprob.append(wordprobstat.copy())\n",
    "                pos_percent += wordprobstat[3]\n",
    "                pos_count += 1\n",
    "        if wordprobstat[1] < 0   :\n",
    "            if hit_limit == 0 :\n",
    "                neg_wordprob.append(wordprobstat.copy())\n",
    "                neg_percent += wordprobstat[3]\n",
    "                neg_count += 1\n",
    "        if neg_percent + pos_percent > drop_percent or wordprobstat[0] < .0005:\n",
    "            hit_limit = 1\n",
    "    addpurity = 0\n",
    "    if pos_count > neg_count * ignore_little_counts :\n",
    "        # create new topics, start with zero prob\n",
    "        createtopic = zeroprobs.copy()\n",
    "        # add in these words\n",
    "        #print (\"OY Pos Words = \", len(pos_wordprob))\n",
    "        pos_count = len(pos_wordprob)\n",
    "        addpurity = 1\n",
    "        for wordprob in pos_wordprob :\n",
    "            createtopic[wordprob[2]] = wordprob[3]/pos_percent\n",
    "            #createtopic[wordprob[2]] = 1.0\n",
    "        newtopics.append(createtopic.copy())\n",
    "\n",
    "    if neg_count > pos_percent * ignore_little_counts :\n",
    "        # create new topics\n",
    "        # create new topics, start with zero prob\n",
    "        createtopic = zeroprobs.copy()\n",
    "        # add in these words\n",
    "        #print (\"OY Neg Words = \", len(neg_wordprob))\n",
    "        neg_count = len(neg_wordprob)\n",
    "        add_purity = 1\n",
    "        for wordprob in neg_wordprob :\n",
    "            createtopic[wordprob[2]] = wordprob[3]/neg_percent\n",
    "            #createtopic[wordprob[2]] = 1.0\n",
    "        newtopics.append(createtopic.copy())\n",
    "    purity_count += addpurity\n",
    "    ppurity = 0\n",
    "    npurity = 0\n",
    "    if pos_count + neg_count != 0 :\n",
    "        ppurity = pos_count / (pos_count + neg_count)\n",
    "    if pos_count + neg_count != 0 :\n",
    "        npurity = neg_count / (pos_count + neg_count)\n",
    "    if (ppurity != 0) :\n",
    "        ppurity = (ppurity * math.log(ppurity, 10))\n",
    "    if (npurity != 0) :\n",
    "        npurity = (npurity * math.log(npurity, 10))\n",
    "    entropy =  npurity + ppurity\n",
    "    #print(float(100 + (100 * entropy)))\n",
    "    purity += float(100 + (100 * entropy))\n",
    "\n",
    "#print (newtopics)\n",
    "causality_confidence = 1-causality_confidence/causality_count \n",
    "print(\"causality_confidence = \", causality_confidence)\n",
    "run_confidence.append(causality_confidence.copy())\n",
    "\n",
    "purity = purity/purity_count\n",
    "print(\"purity = \", purity)\n",
    "run_purity.append(purity)\n",
    "\n",
    "#%run ITMTF_iterate.ipynb\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# using the returned topics probabilies\n",
    "# and adding buffers - num_buffers using bufferprob\n",
    "# create the prior\n",
    "# update num_topics\n",
    "# and run the model\n",
    "# ##################################################\n",
    "\n",
    "# add the buffers\n",
    "z = 0\n",
    "while z < num_buffers:\n",
    "    newtopics.append(bufferprob.copy())\n",
    "    z += 1\n",
    "num_topics = len(newtopics)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
