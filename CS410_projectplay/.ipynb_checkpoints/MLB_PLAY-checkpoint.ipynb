{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time slices with docs: 123\n",
      "Number of time slices: 123\n",
      "Number of time vocab: 12517\n",
      "Size of doctokens: 2673\n",
      "Number of documents: 2673\n",
      "12517\n",
      "Number of unique tokens: 12517\n",
      "Pearsons correlation: 12517\n",
      "Pearsons/Granger correlation: 12517\n"
     ]
    }
   ],
   "source": [
    "%run load_helper.ipynb\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# ##################################################\n",
    "#             SET PARAMETERS\n",
    "#\n",
    "# pick a baseline\n",
    "#        baselines = [\"10topics\", \"15topics\", \"20topics\", \"25topics\", \"30topics\"]\n",
    "#\n",
    "# name your run (used to store the iterations); models will be saved myrun1.sav myrun2.save etc\n",
    "#\n",
    "# set your parameters\n",
    "#\n",
    "# ##################################################\n",
    "runname = \"D50T30baseline2\"\n",
    "mybaseline = \"30baseline2\"\n",
    "\n",
    "#$$$$$$\n",
    "passes = 10\n",
    "iterations = 100\n",
    "#passes = 10\n",
    "#iterations = 100\n",
    "\n",
    "# below are default params that can be played with\n",
    "num_buffers = 0    # how many buffers to add each iteration\n",
    "lda_decay = .00001     # how much the prior influences the iteration 0 - 1 \n",
    "                   #      anything less than .5 is not guartenteed to converge\n",
    "                   #      the model will still run, but the visualization might not work\n",
    "num_iterations = 4 # NOTE this is in addition to baseline the article used 4\n",
    "\n",
    "# lag of 5 is mentioned in the paper, and seems to work the best with trial runs\n",
    "the_lag = 5\n",
    "drop_percent = .95\n",
    "#drop_percent = .95\n",
    "low_threshold = .05\n",
    "ignore_little_counts = .2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration start time =  19:33:38\n",
      "Number of Topics =  30\n",
      "Significant Topics [0, 4, 5, 8, 12, 18, 26]\n",
      "OY Pos Words =  2\n",
      "OY Neg Words =  4\n",
      "OY Pos Words =  4\n",
      "OY Neg Words =  4\n",
      "OY Pos Words =  3\n",
      "OY Neg Words =  4\n",
      "OY Pos Words =  3\n",
      "OY Neg Words =  6\n",
      "OY Neg Words =  5\n",
      "OY Pos Words =  2\n",
      "OY Neg Words =  6\n",
      "OY Pos Words =  2\n",
      "OY Neg Words =  5\n",
      "causality_confidence =  0.796833578978903\n",
      "purity =  85.37590811826513\n",
      "new topics = 13\n"
     ]
    }
   ],
   "source": [
    "# ##################################################\n",
    "#\n",
    "# The ITMFT algorithm\n",
    "#\n",
    "#  Run cell 1\n",
    "#  and either cell 2 (to create a baseline)\n",
    "#      or cell 3 to load the baseline\n",
    "# ##################################################\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "run_purity = []\n",
    "run_confidence = []\n",
    "\n",
    "\n",
    "file_name = save_path + mybaseline + \".sav\"\n",
    "model = LdaModel.load(file_name)\n",
    "    \n",
    "topics = model.get_topics().copy()\n",
    "topics = topics.copy()\n",
    "num_topics = len(topics)\n",
    "while iteration < num_iterations  :\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Iteration start time = \", current_time)\n",
    "    print(\"Number of Topics = \", num_topics)\n",
    "    %run mlb_runmodel.ipynb   \n",
    "    \n",
    "    print(\"new topics =\", num_topics)\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=tokentoword,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',               \n",
    "        eta=newtopics,                 # preset topic/word\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,         # added buffer topics\n",
    "        passes=passes,\n",
    "        decay = lda_decay,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "\n",
    "    topics = model.get_topics().copy()\n",
    "    topics = topics.copy()\n",
    "    num_topics = len(topics)\n",
    " \n",
    "    file_name = runname + str(iteration) \n",
    "    path_name = save_path + file_name + \".sav\"\n",
    "    print(file_name + \" - saved for visualization\")\n",
    "    #model.save(path_name )\n",
    "    iteration += 1\n",
    "    \n",
    "%run mlb_runmodel.ipynb   \n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Run Complete = \", current_time)\n",
    "\n",
    "    \n",
    "file_name = runname + str(iteration) \n",
    "path_name = save_path + runname + \".confidence.csv\"\n",
    "fo = open(path_name, \"w\")\n",
    "firstime = 0\n",
    "for num in run_confidence :\n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))                 \n",
    "fo.close()   \n",
    "firstime = 0\n",
    "path_name = save_path + runname + \".purity.csv\"\n",
    "fo = open(path_name, \"w\") \n",
    "for num in run_purity :\n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))\n",
    "fo.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = runname + str(iteration) \n",
    "path_name = save_path + runname + \".confidence.csv\"\n",
    "fo = open(path_name, \"w\")\n",
    "firstime = 0\n",
    "for num in run_confidence :\n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))                 \n",
    "fo.close()   \n",
    "firstime = 0\n",
    "path_name = save_path + runname + \".purity.csv\"\n",
    "fo = open(path_name, \"w\") \n",
    "for num in run_purity :\n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))\n",
    "fo.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# calculate Pearson's correlation\n",
    "pearsoncorr = []\n",
    "readfile = save_path + \"\\\\\" + \"pearson4.csv\"\n",
    "fr = open(readfile, \"r\")\n",
    "for line in fr:\n",
    "    #pearsoncorr.append(float(line))\n",
    "    print (float(line))\n",
    "fr.close()\n",
    "print('Pearsons correlation: %d' % len(pearsoncorr))\n",
    "\n",
    "bets = [] \n",
    "with open(\".\\\\LDA_data\\\\betdataclean.csv\") as swf:\n",
    "    for line in swf:\n",
    "        bets.append(np.longdouble(float(line)))\n",
    "\n",
    "swf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# calculate Pearson's correlation\n",
    "print (len(bets))\n",
    "print(bets[0].dtype)\n",
    "trans = np.array(timeslicetokencounts).transpose()\n",
    "wordcorr = []\n",
    "print (len(trans[1]))\n",
    "print (len(timeslicetokencounts[1]))\n",
    "for yy in range(0,num_words) :\n",
    "    corr, _ = pearsonr(bets, trans[yy])\n",
    "    wordcorr.append(corr)\n",
    "    print('Pearsons correlation: %.3f' % corr)\n",
    "readfile = save_path + \"\\\\\" + \"pearson4.csv\"\n",
    "firstime = 0\n",
    "fo = open(readfile, \"w\") \n",
    "for num in wordcorr :\n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))\n",
    "fo.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hightopicsgc)\n",
    "print(\"great\")\n",
    "print(lowtopicsgc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sigtopics))\n",
    "print(sigtopicsgc)\n",
    "print(len(sigwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100) :\n",
    "     print( sigwordsgc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100) :\n",
    "     print( dictionary[sigwords[i]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary[61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "lowtopicsgc = []\n",
    "lowtopics = []\n",
    "trans = np.array(timeslicetokencounts).transpose()\n",
    "wordcorr = []\n",
    "print (len(trans[1]))\n",
    "print (len(timeslicetokencounts[1]))\n",
    "for yy in range(0,num_words) :\n",
    "    tempgc = grangercausalitytests([[bets[i],trans[yy][i]] for i in range(0, len(bets))], the_lag, verbose=False)\n",
    "    #tempgc = grangercausalitytests([[bets[i],topiccoverage[i][ii]] for i in range(0, len(bets))], the_lag, verbose=False)\n",
    "    #print(tempgc)\n",
    "    lowpvalue = 2\n",
    "    lowlag = 0\n",
    "    for yyy in range(1,the_lag+1) :\n",
    "        stats = tempgc.get(yyy)[0].get('ssr_ftest')\n",
    "        if (stats[1] < lowpvalue) :\n",
    "            lowlag = yyy\n",
    "            lowpvalue = stats[1]\n",
    "    stats = tempgc.get(lowlag)[0].get('ssr_ftest')\n",
    "    lowtopics.append(1-stats[1])\n",
    "print (\"Low Topics \", len(lowtopics))\n",
    "\n",
    "\n",
    "\n",
    "readfile = save_path + \"\\\\\" + \"grangerword.csv\"\n",
    "firstime = 0\n",
    "fo = open(readfile, \"w\") \n",
    "for num in lowtopics :\n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))\n",
    "fo.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readfile = save_path + \"\\\\\" + \"grangerword.csv\"\n",
    "granger = [] \n",
    "fg = open(readfile, \"r\") \n",
    "for line in fg:\n",
    "    granger.append(float(line))\n",
    "fg.close()\n",
    "print(len(granger))\n",
    "                       \n",
    "pearson = []\n",
    "readfile = save_path + \"\\\\\" + \"pearson4.csv\"\n",
    "fp = open(readfile, \"r\") \n",
    "for line in fp:\n",
    "    pearson.append(float(line))\n",
    "fg.close()\n",
    "print(len(pearson))\n",
    "                       \n",
    "                       \n",
    "readfile = save_path + \"\\\\\" + \"pgranger.csv\"\n",
    "fo = open(readfile, \"w\") \n",
    "firstime = 0\n",
    "\n",
    "yy = 0\n",
    "while yy < len(granger) :\n",
    "    num = float(granger[yy])\n",
    "    if float(pearson[yy]) < 0 :\n",
    "        num = num * -1\n",
    "                       \n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))\n",
    "    yy += 1\n",
    "fo.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
