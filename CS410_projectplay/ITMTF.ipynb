{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time slices with docs: 123\n",
      "Number of time slices: 123\n",
      "Number of time vocab: 12517\n",
      "Number of documents: 2673\n",
      "Number of unique tokens: 12517\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import smart_open\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "#  Always run this cell\n",
    "#\n",
    "#  and either cell 2 (to create a baseline)\n",
    "#      or cell 3 to load the baseline\n",
    "#\n",
    "# Before you run the iterations\n",
    "# ##################################################\n",
    "\n",
    "\n",
    "# ##################################################\n",
    "#             PARAMETERS TO PLAY WITH\n",
    "#\n",
    "# decay = used like μ in the algorithm (see notes below)\n",
    "# num_topics = number of topics to start with\n",
    "# num_iterations = max number of iterations to run the ITMTF algorithm\n",
    "# ##################################################\n",
    "\n",
    "\n",
    "# decay from 0 to 1, .5 - 1 guarenteed to converge\n",
    "# .5 is model's default\n",
    "# closer to 1, like a lower μ\n",
    "#      decay = 1 is like μ = 0\n",
    "lda_decay = .5    \n",
    "\n",
    "# number of topics to start with, per the article, 30 is a good start\n",
    "num_topics = 30\n",
    "num_buffers = 5   # how many buffers to add each iteration\n",
    "\n",
    "# max number of iterations to run - the article used 5\n",
    "num_iterations = 1\n",
    "\n",
    "# ##################################################\n",
    "#             Other parameters \n",
    "#  used to load the data\n",
    "#  or default values for the LDA algorithm\n",
    "# ##################################################\n",
    "#input parameters\n",
    "documents_path = \".\\\\LDA_data\\\\LDAreduced.csv\"\n",
    "vocab_path = \".\\\\LDA_data\\\\LDAwordseries.csv\"\n",
    "save_path = \".\\\\LDA_data\\\\\"\n",
    "\n",
    "# model parameters\n",
    "num_docs = 0\n",
    "num_words = 0\n",
    "chunksize = 2000\n",
    "passes = 100\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "#docs = []\n",
    "#bow = []     # arrray of bow for doc, used to get probability\n",
    "\n",
    "#docs_per_timeslice = []\n",
    "\n",
    "#tokentoword = {}  # used to visualize the results of the model\n",
    "\n",
    "# as the model's vocab list is not in the same order as our predefined counts we will create these look up tables\n",
    "#   to create timeslicetokencounts\n",
    "#\n",
    "#vocabtowordindex = {}     # dict of the preped vocabulary words to INDEX used to create wordindextotoken\n",
    "#wordindextotoken = {}     # will be used to create timeslicetokencounts\n",
    "#timeslicevocabcounts = [] # an array of timeslices, each element contains an array of word counts for that timeslice\n",
    "#                          # used to create timeslicetokencounts\n",
    "#timeslicetokencounts = [] # an array of timeslices, each element contains an array of dictionary token counts for that timeslice\n",
    "#                          # this one will be used by the iteration\n",
    "\n",
    "# ##################################################\n",
    "# load the cleansed data into an array of docs\n",
    "# ##################################################\n",
    "docs = []\n",
    "with open(documents_path) as swf:\n",
    "    docs_per_timeslice = []\n",
    "    tempslice = []\n",
    "    count = 0\n",
    "    curtimeslice = \"2000.7.1\"\n",
    "    tempslice.append(curtimeslice)\n",
    "    curdocs = []\n",
    "    firstime = 0\n",
    "    for line in swf:\n",
    "        cells = line.split(',')\n",
    "        docslice = cells[0] + \".\" + cells[1] + \".\" + cells[2]\n",
    "        if firstime == 0 :\n",
    "            firstime = 1\n",
    "            curtimeslice = docslice\n",
    "        if docslice != curtimeslice :\n",
    "            curtimeslice = docslice\n",
    "            docs_per_timeslice.append(curdocs)\n",
    "            curdocs = []\n",
    "        curdocs.append(count) \n",
    "        count += 1\n",
    "        \n",
    "        docs.append(cells[3])\n",
    "    docs_per_timeslice.append(curdocs)\n",
    "swf.close\n",
    "print('Number of time slices with docs: %d' % len(docs_per_timeslice))\n",
    "\n",
    "# ##################################################\n",
    "# load the cleansed vocabulary into vocabtowordindex\n",
    "#      and timeslicevocabcounts\n",
    "# ##################################################\n",
    "# load the cleansed data into an array of docs\n",
    "header = 0\n",
    "timeslicevocabcounts = []\n",
    "vocabtowordindex = {}\n",
    "with open(vocab_path) as vwf:\n",
    "    for line in vwf:\n",
    "        linenumber = 1 # skip the header row\n",
    "        cells = line.split(',')\n",
    "        if header == 0 :\n",
    "            header = 1\n",
    "            i = 1 # skip header column\n",
    "            while i < len(cells) - 1:  # the cleansing process adds a black cell at the end\n",
    "                vocabtowordindex[cells[i]] = i-1  \n",
    "                #print(cells[i])\n",
    "                i += 1  \n",
    "        else :\n",
    "            wordcount = []\n",
    "            i = 1 # skip header column\n",
    "            while i < len(cells)-1:  # the cleansing process adds a black cell at the end\n",
    "                wordcount.append(cells[i])  # create an array of vocab counts at this timeslice\n",
    "                i += 1 \n",
    "            timeslicevocabcounts.append(wordcount)\n",
    "            \n",
    "vwf.close\n",
    "print('Number of time slices: %d' % len(timeslicevocabcounts))\n",
    "print('Number of time vocab: %d' % len(vocabtowordindex))\n",
    "\n",
    "# ##################################################\n",
    "# create the dictionary\n",
    "# ##################################################\n",
    "doctokens = [doc.split() for doc in docs]\n",
    "dictionary = Dictionary(doctokens)\n",
    "bow = []\n",
    "# Bag-of-words representation of the documents.\n",
    "for doc in doctokens :\n",
    "    bow.append(dictionary.doc2bow(doc))\n",
    "    \n",
    "    \n",
    "# ##################################################\n",
    "# create the corpus\n",
    "# ##################################################\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doctokens]\n",
    "#print (corpus)\n",
    "\n",
    "num_docs = len(corpus)\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# ##################################################\n",
    "# create the wordindextotoken\n",
    "# a dict so we can take a vocab word and find the dict index\n",
    "#   use this dict to create timeslicetokencounts for the iteration\n",
    "# ##################################################\n",
    "wordindextotoken = {}\n",
    "i = 0\n",
    "while i < len(dictionary):  \n",
    "    wordindextotoken[vocabtowordindex[dictionary[i]]] = i\n",
    "    i += 1\n",
    "tokentoword = dictionary.id2token\n",
    "num_words = len(tokentoword)\n",
    "print('Number of unique tokens: %d' % len(tokentoword))\n",
    "\n",
    "# ok we now have wordindextotoken map\n",
    "# let's use that to create timeslicetokencounts from timeslicevocabcounts\n",
    "timeslicetokencounts = timeslicevocabcounts\n",
    "timeslicecount = 0\n",
    "while timeslicecount < len(timeslicevocabcounts) :\n",
    "    wordcount = 0\n",
    "    while wordcount < len(timeslicevocabcounts[timeslicecount]) :\n",
    "        timeslicetokencounts[timeslicecount][wordindextotoken[wordcount]] = timeslicevocabcounts[timeslicecount][wordcount]\n",
    "        wordcount += 1  \n",
    "    timeslicecount += 1\n",
    "        \n",
    "\n",
    "# create a probabiltiy array for buffer topics that are added\n",
    "zeroprobs = []\n",
    "bufferprob = []\n",
    "i = 0\n",
    "while i < len(tokentoword) :\n",
    "    bufferprob.append(1/len(tokentoword))\n",
    "    zeroprobs.append(0.0)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "#             Runs a baseline\n",
    "# ##################################################\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=tokentoword,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "print('Model Finished')\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "# ##################################################\n",
    "#             Saves the baseline\n",
    "# ##################################################\n",
    "\n",
    "file_name = save_path + \"baseline.sav\"\n",
    "print (file_name)\n",
    "model.save(file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "#             Loads the baseline\n",
    "# ##################################################\n",
    "file_name = save_path + \"baseline.sav\"\n",
    "model = LdaModel.load(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "#\n",
    "# The ITMFT algorithm\n",
    "#\n",
    "#  Run cell 1\n",
    "#  and either cell 2 (to create a baseline)\n",
    "#      or cell 3 to load the baseline\n",
    "# ##################################################\n",
    "\n",
    "iteration = 0\n",
    "oldsignificanttopics = 0\n",
    "significanttopcs = 1\n",
    "\n",
    "while iteration < num_iterations and significanttopcs > oldsignificanttopics :\n",
    "    # create a topic coverage matrix preset to 0\n",
    "    topiccoverage = []\n",
    "    i = 0\n",
    "    while i < len(docs_per_timeslice) :\n",
    "        y = 0\n",
    "        thistopic = []\n",
    "        while y < num_topics:\n",
    "            thistopic.append(0.0)\n",
    "            y += 1\n",
    "        topiccoverage.append(thistopic)\n",
    "        i += 1\n",
    "            \n",
    "    # get the topic coverage per timeslice per doc\n",
    "    timeslice = 0\n",
    "    for timeslicedocs in docs_per_timeslice :\n",
    "        # for each doc in this timeslice\n",
    "        for doc in timeslicedocs :\n",
    "            # get the probability matrix\n",
    "            probs = model.get_document_topics(bow[doc])\n",
    "            #its a sparse array, prob[0] is the topic and prob[1] is the probabiltiy\n",
    "            for prob in probs :\n",
    "                topiccoverage[timeslice][prob[0]] += prob[1]\n",
    "        timeslice += 1\n",
    "        \n",
    "    topics = model.get_topics()\n",
    "    # ##################################################\n",
    "    # timeslicetokencounts - we have the word coverage for each model token\n",
    "    # topiccoverage - and now we have the topic coverage\n",
    "    # topics - the current topic word probabilities\n",
    "    #\n",
    "    # run the iteration\n",
    "    # ##################################################\n",
    "    #      $$$$$$$$$$$$$$$$$\n",
    "    #      ADD ALGORITHM HERE \n",
    "    #      RETURN A LIST OF newtopics = []\n",
    "    #          topic word probabilities\n",
    "    #      AND UPDATE THE VARIABLE significanttopcs\n",
    "    #\n",
    "    \n",
    "    # ##################################################\n",
    "    #\n",
    "    # using the returned topics probabilies\n",
    "    # and adding buffers - num_buffers using bufferprob\n",
    "    # create the prior\n",
    "    # update num_topics\n",
    "    # and run the model\n",
    "    # ##################################################\n",
    "    \n",
    "    \n",
    "    #$$$ remove this, for now just creating a topic prob list of X from old model as the return\n",
    "    topics = model.get_topics()\n",
    "    print(len(topics))\n",
    "    newtopics = []\n",
    "    for topic in topics :\n",
    "        newtopics.append(topic)\n",
    "    mylambda = model.state.get_lambda()\n",
    "    print(len(mylambda))\n",
    "    print (mylambda)\n",
    "    #$$$ remove this, for now just creating a topic prob list of X from old model as the return\n",
    "        \n",
    "    # add the buffers\n",
    "    z = 0\n",
    "    while z < num_buffers:\n",
    "        newtopics.append(bufferprob)\n",
    "        z += 1\n",
    "    num_topics = len(newtopics)\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=tokentoword,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',               \n",
    "        eta=newtopics,                 # preset topic/word\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,       # added a topic\n",
    "        passes=passes,\n",
    "        decay = lda_decay,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "\n",
    "    mylambda = model.state.get_lambda()\n",
    "    print(len(mylambda))\n",
    "    print (mylambda)\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "\n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.get_document_topics(bow[0])\n",
    "print (probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "mylambda = newmodel.state.get_lambda()\n",
    "print (mylambda)\n",
    "\n",
    "\n",
    "topics = newmodel.get_topics()\n",
    "\n",
    "newtopics = []\n",
    "for topic in topics :\n",
    "    newtopics.append(topic)\n",
    "\n",
    "#lets add a new topic\n",
    "newprob = []\n",
    "i = 0\n",
    "while i < 12517 :\n",
    "    newprob.append(0)\n",
    "    i += 1\n",
    "newprob[0] = .5\n",
    "newprob[1] = .5\n",
    "\n",
    "\n",
    "newtopics.append(newprob)\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "it1model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=tokentoword,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',               \n",
    "    eta=newtopics,                 # preset topic/word\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics+1,       # added a topic\n",
    "    passes=passes,\n",
    "    decay = .05,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "mylambda = it1model.state.get_lambda()\n",
    "print (mylambda)\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylambda = it1model.state.get_lambda()\n",
    "print (mylambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(\"baseline_model\")\n",
    "newmodel = LdaModel.load(temp_file)\n",
    "\n",
    "mylambda = newmodel.state.get_lambda()\n",
    "print (mylambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(\"baseline_model\")\n",
    "newmodel = LdaModel.load(temp_file)\n",
    "\n",
    "topics = model.get_topics()\n",
    "#print(topics[0])\n",
    "\n",
    "\n",
    "count = 0\n",
    "newtopics = []\n",
    "for topic in topics :\n",
    "    newtopics.append(topic)\n",
    "    count += 1\n",
    "#print(count)\n",
    "\n",
    "#lets add a new topic\n",
    "newprob = []\n",
    "i = 0\n",
    "while i < 12517 :\n",
    "    newprob.append(0)\n",
    "    i += 1\n",
    "newprob[0] = .5\n",
    "newprob[1] = .5\n",
    "print(tokentoword[0])\n",
    "print(tokentoword[1])\n",
    "newtopics.append(newprob)\n",
    "\n",
    "num_topics += 1\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=tokentoword,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',               \n",
    "    eta=newtopics,                 # preset topic/word\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,       # added a topic\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "temp_file = datapath(\"it1_model\")\n",
    "model.save(temp_file)\n",
    "model.show_topic(30, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_file = datapath(\"baseline_model\")\n",
    "#newmodel = LdaModel.load(temp_file)\n",
    "#newmodel.show_topic(29, 50) # will blow up if 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of probs\n",
    "doctopic = {}\n",
    "count = 0\n",
    "for onebag in bow :\n",
    "    doctopic[count] = model.get_document_topics(onebag)\n",
    "    count += 1\n",
    "print ( len(doctopic))\n",
    "\n",
    "# get the doc list for this iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (doctopic[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
