{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import smart_open\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "#  Always run this cell\n",
    "#\n",
    "#  and either cell 2 (to create a baseline)\n",
    "#      or cell 3 to load the baseline\n",
    "#\n",
    "# Before you run the iterations\n",
    "# ##################################################\n",
    "\n",
    "\n",
    "# ##################################################\n",
    "#             PARAMETERS TO PLAY WITH\n",
    "#\n",
    "# decay = used like μ in the algorithm (see notes below)\n",
    "# num_topics = number of topics to start with\n",
    "# num_iterations = max number of iterations to run the ITMTF algorithm\n",
    "# ##################################################\n",
    "\n",
    "\n",
    "# decay from 0 to 1, .5 - 1 guarenteed to converge\n",
    "# .5 is model's default\n",
    "# closer to 1, like a lower μ\n",
    "#      decay = 1 is like μ = 0\n",
    "lda_decay = .5    \n",
    "\n",
    "# number of topics to start with, per the article, 30 is a good start\n",
    "num_topics = 30\n",
    "num_buffers = 5   # how many buffers to add each iteration\n",
    "\n",
    "# max number of iterations to run - the article used 5\n",
    "num_iterations = 1\n",
    "\n",
    "# ##################################################\n",
    "#             Other parameters \n",
    "#  used to load the data\n",
    "#  or default values for the LDA algorithm\n",
    "# ##################################################\n",
    "#input parameters\n",
    "documents_path = \".\\\\LDA_data\\\\LDAreduced.csv\"\n",
    "vocab_path = \".\\\\LDA_data\\\\LDAwordseries.csv\"\n",
    "save_path = \".\\\\LDA_data\\\\\"\n",
    "\n",
    "# model parameters\n",
    "num_docs = 0\n",
    "num_words = 0\n",
    "chunksize = 2000\n",
    "passes = 100\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "#docs = []\n",
    "#bow = []     # arrray of bow for doc, used to get probability\n",
    "\n",
    "#docs_per_timeslice = []\n",
    "\n",
    "#tokentoword = {}  # used to visualize the results of the model\n",
    "\n",
    "# as the model's vocab list is not in the same order as our predefined counts we will create these look up tables\n",
    "#   to create timeslicetokencounts\n",
    "#\n",
    "#vocabtowordindex = {}     # dict of the preped vocabulary words to INDEX used to create wordindextotoken\n",
    "#wordindextotoken = {}     # will be used to create timeslicetokencounts\n",
    "#timeslicevocabcounts = [] # an array of timeslices, each element contains an array of word counts for that timeslice\n",
    "#                          # used to create timeslicetokencounts\n",
    "#timeslicetokencounts = [] # an array of timeslices, each element contains an array of dictionary token counts for that timeslice\n",
    "#                          # this one will be used by the iteration\n",
    "\n",
    "# ##################################################\n",
    "# load the cleansed data into an array of docs\n",
    "# ##################################################\n",
    "docs = []\n",
    "with open(documents_path) as swf:\n",
    "    docs_per_timeslice = []\n",
    "    tempslice = []\n",
    "    count = 0\n",
    "    curtimeslice = \"2000.7.1\"\n",
    "    tempslice.append(curtimeslice)\n",
    "    curdocs = []\n",
    "    firstime = 0\n",
    "    for line in swf:\n",
    "        cells = line.split(',')\n",
    "        docslice = cells[0] + \".\" + cells[1] + \".\" + cells[2]\n",
    "        if firstime == 0 :\n",
    "            firstime = 1\n",
    "            curtimeslice = docslice\n",
    "        if docslice != curtimeslice :\n",
    "            curtimeslice = docslice\n",
    "            docs_per_timeslice.append(curdocs)\n",
    "            curdocs = []\n",
    "        curdocs.append(count) \n",
    "        count += 1\n",
    "        \n",
    "        docs.append(cells[3])\n",
    "    docs_per_timeslice.append(curdocs)\n",
    "swf.close\n",
    "print('Number of time slices with docs: %d' % len(docs_per_timeslice))\n",
    "\n",
    "# ##################################################\n",
    "# load the cleansed vocabulary into vocabtowordindex\n",
    "#      and timeslicevocabcounts\n",
    "# ##################################################\n",
    "# load the cleansed data into an array of docs\n",
    "header = 0\n",
    "timeslicevocabcounts = []\n",
    "vocabtowordindex = {}\n",
    "with open(vocab_path) as vwf:\n",
    "    for line in vwf:\n",
    "        linenumber = 1 # skip the header row\n",
    "        cells = line.split(',')\n",
    "        if header == 0 :\n",
    "            header = 1\n",
    "            i = 1 # skip header column\n",
    "            while i < len(cells) - 1:  # the cleansing process adds a black cell at the end\n",
    "                vocabtowordindex[cells[i]] = i-1  \n",
    "                #print(cells[i])\n",
    "                i += 1  \n",
    "        else :\n",
    "            wordcount = []\n",
    "            i = 1 # skip header column\n",
    "            while i < len(cells)-1:  # the cleansing process adds a black cell at the end\n",
    "                wordcount.append(cells[i])  # create an array of vocab counts at this timeslice\n",
    "                i += 1 \n",
    "            timeslicevocabcounts.append(wordcount)\n",
    "            \n",
    "vwf.close\n",
    "print('Number of time slices: %d' % len(timeslicevocabcounts))\n",
    "print('Number of time vocab: %d' % len(vocabtowordindex))\n",
    "\n",
    "# ##################################################\n",
    "# create the dictionary\n",
    "# ##################################################\n",
    "doctokens = [doc.split() for doc in docs]\n",
    "dictionary = Dictionary(doctokens)\n",
    "bow = []\n",
    "# Bag-of-words representation of the documents.\n",
    "for doc in doctokens :\n",
    "    bow.append(dictionary.doc2bow(doc))\n",
    "    \n",
    "    \n",
    "# ##################################################\n",
    "# create the corpus\n",
    "# ##################################################\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doctokens]\n",
    "#print (corpus)\n",
    "\n",
    "num_docs = len(corpus)\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# ##################################################\n",
    "# create the wordindextotoken\n",
    "# a dict so we can take a vocab word and find the dict index\n",
    "#   use this dict to create timeslicetokencounts for the iteration\n",
    "# ##################################################\n",
    "wordindextotoken = {}\n",
    "i = 0\n",
    "while i < len(dictionary):  \n",
    "    wordindextotoken[vocabtowordindex[dictionary[i]]] = i\n",
    "    i += 1\n",
    "tokentoword = dictionary.id2token\n",
    "num_words = len(tokentoword)\n",
    "print('Number of unique tokens: %d' % len(tokentoword))\n",
    "\n",
    "# ok we now have wordindextotoken map\n",
    "# let's use that to create timeslicetokencounts from timeslicevocabcounts\n",
    "timeslicetokencounts = timeslicevocabcounts\n",
    "timeslicecount = 0\n",
    "while timeslicecount < len(timeslicevocabcounts) :\n",
    "    wordcount = 0\n",
    "    while wordcount < len(timeslicevocabcounts[timeslicecount]) :\n",
    "        timeslicetokencounts[timeslicecount][wordindextotoken[wordcount]] = timeslicevocabcounts[timeslicecount][wordcount]\n",
    "        wordcount += 1  \n",
    "    timeslicecount += 1\n",
    "        \n",
    "\n",
    "# create a probabiltiy array for buffer topics that are added\n",
    "zeroprobs = []\n",
    "bufferprob = []\n",
    "i = 0\n",
    "while i < len(tokentoword) :\n",
    "    bufferprob.append(1/len(tokentoword))\n",
    "    zeroprobs.append(0.0)\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
