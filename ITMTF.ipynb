{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Vocabulary size:6\n",
      "Number of documents:1000\n",
      "EM iteration begins...\n",
      "Iteration #113...\n",
      "Converge\n",
      "[[9.16386594e-093 1.00000000e+000]\n",
      " [3.99687129e-107 1.00000000e+000]\n",
      " [1.00000000e+000 1.51816742e-020]\n",
      " ...\n",
      " [1.00000000e+000 2.52048252e-046]\n",
      " [8.88601242e-001 1.11398758e-001]\n",
      " [2.82042602e-001 7.17957398e-001]]\n",
      "[[5.45000047e-02 1.88570009e-07 5.07744111e-02 3.00398355e-01\n",
      "  2.96457572e-01 2.97869468e-01]\n",
      " [2.99127175e-01 3.96992446e-01 3.03876153e-01 1.02849670e-08\n",
      "  6.35946693e-08 4.15156992e-06]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.likelihoods = []\n",
    "        self.documents_path = documents_path\n",
    "        self.term_doc_matrix = None \n",
    "        self.document_topic_prob = None  # P(z | d) - INITIALIZE TO RANDOM\n",
    "        self.topic_word_prob = None  # P(w | z) - INITiALIZE TO RANDOM\n",
    "        self.topic_prob = None  # P(z | d, w) - NORMALIZED document_topic_prob * topic_word_prob for each doc, each word \n",
    "\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
    "        for example: [\"rain\", \"the\", ...]\n",
    "\n",
    "        Update self.vocabulary_size\n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = dict()\n",
    "        docnumber = 0\n",
    "        with open(self.documents_path, encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line.strip()\n",
    "                words = line.split()\n",
    "                self.documents.append([])\n",
    "                for word in words:\n",
    "                    if word != \"0\" and word != \"1\": # ignore the first word, it is the 0 or 1\n",
    "                        self.documents[docnumber].append(word)\n",
    "                        if vocab.get(word):\n",
    "                            vocab[word] += 1\n",
    "                        else:\n",
    "                            vocab[word] = 1\n",
    "                docnumber = docnumber + 1\n",
    "            self.number_of_documents = len(self.documents)\n",
    "            print(self.number_of_documents)\n",
    "            self.vocabulary = list(vocab.keys())\n",
    "            self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "    def build_term_doc_matrix(self):\n",
    "        \"\"\"\n",
    "        Construct the term-document matrix where each row represents a document, \n",
    "        and each column represents a vocabulary term.\n",
    "\n",
    "        self.term_doc_matrix[i][j] is the count of term j in document i\n",
    "        \"\"\"\n",
    "        doccount = 0\n",
    "        mymatrix = []\n",
    "        for document in self.documents:\n",
    "            # initialize the variables for this doc\n",
    "            mymatrix.append([])\n",
    "            # count the words for this doc\n",
    "            vocab = dict()\n",
    "            for word in document:\n",
    "                if vocab.get(word):\n",
    "                    vocab[word] += 1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "            wordcount = 0\n",
    "            for uniqueword in self.vocabulary:\n",
    "                if vocab.get(uniqueword):\n",
    "                    mymatrix[doccount].append(vocab.get(uniqueword))\n",
    "                else:\n",
    "                    mymatrix[doccount].append(0)\n",
    "            \n",
    "            doccount = doccount + 1\n",
    "        self.term_doc_matrix = mymatrix\n",
    "\n",
    "        \n",
    "        #pass    # REMOVE THIS\n",
    "\n",
    "\n",
    "    def initialize_randomly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Randomly initialize the matrices: document_topic_prob and topic_word_prob\n",
    "        which hold the probability distributions for P(z | d) and P(w | z): self.document_topic_prob, and self.topic_word_prob\n",
    "\n",
    "        Don't forget to normalize! \n",
    "        HINT: you will find numpy's random matrix useful [https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html]\n",
    "        #np.random.random_sample((3, 2)) \n",
    "        \"\"\"\n",
    "        np.random.RandomState()\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        #print(self.document_topic_prob)\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "\n",
    "        #pass    # REMOVE THIS\n",
    "        \n",
    "\n",
    "    def initialize_uniformly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Initializes the matrices: self.document_topic_prob and self.topic_word_prob with a uniform \n",
    "        probability distribution. This is used for testing purposes.\n",
    "\n",
    "        DO NOT CHANGE THIS FUNCTION\n",
    "        \"\"\"\n",
    "        self.document_topic_prob = np.ones((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.ones((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "    def initialize(self, number_of_topics, random=False):\n",
    "        \"\"\" Call the functions to initialize the matrices document_topic_prob and topic_word_prob\n",
    "        \"\"\"\n",
    "        #print(\"Initializing...\")\n",
    "\n",
    "        if random:\n",
    "            self.initialize_randomly(number_of_topics)\n",
    "        else:\n",
    "            self.initialize_uniformly(number_of_topics)\n",
    "\n",
    "\n",
    "    def iterate(self, number_of_topics):\n",
    "        #print(\"E step:\")\n",
    "        \n",
    "        #self.topic_prob = np.ones((self.number_of_documents, number_of_topics, self.vocabulary_size))\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                #print(self.topic_prob[docindex,topicindex])\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex][topicindex][wordindex] = self.topic_word_prob[topicindex, wordindex] * self.document_topic_prob[docindex, topicindex]\n",
    "                    mysum += self.topic_prob[docindex][topicindex][wordindex]\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex,topicindex,wordindex] = self.topic_prob[docindex,topicindex,wordindex] / mysum\n",
    "\n",
    "        #print(\"M step:\")\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for topicindex in range(0, number_of_topics):\n",
    "                mysum = 0\n",
    "                for wordindex in range(0, self.vocabulary_size):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.document_topic_prob[docindex][topicindex] = mysum\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "            \n",
    "        # update P(z | d) self.document_topic_prob\n",
    "        for topicindex in range(0, number_of_topics):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                for docindex in range(0, self.number_of_documents):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.topic_word_prob[topicindex][wordindex] = mysum\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "        \"\"\" Calculate the current log-likelihood of the model using\n",
    "        the model's updated probability matrices\n",
    "        \n",
    "        Append the calculated log-likelihood to self.likelihoods\n",
    "\n",
    "        Likelihood:\n",
    "        For each doc sum:\n",
    "        C(w,d) * log (sum(Prob of that topic * prob of that word in topic)\n",
    "        \n",
    "        loop over docs (variable in self) - docnumber\n",
    "            loop over words (variable in self) - wordnumber\n",
    "                multiply Prob of that topic * prob of that word in topic\n",
    "        log of this\n",
    "                \n",
    "       \"\"\"\n",
    "        newlikely = 0\n",
    "        #print(self.document_topic_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            docsum = 0\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    mysum += self.document_topic_prob[docindex][topicindex] * self.topic_word_prob[topicindex][wordindex]\n",
    "                mysum = math.log10(mysum)\n",
    "                mysum = mysum * self.term_doc_matrix[docindex][wordindex]\n",
    "                docsum += mysum\n",
    "            #print(docsum)\n",
    "            newlikely += docsum\n",
    "        self.likelihoods.append(newlikely)\n",
    "        \n",
    "\n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "\n",
    "        \"\"\"\n",
    "        Model topics.\n",
    "        \"\"\"\n",
    "        print (\"EM iteration begins...\")\n",
    "        \n",
    "        # build term-doc matrix\n",
    "        self.build_term_doc_matrix()\n",
    "        \n",
    "        # Create the counter arrays.\n",
    "        \n",
    "        # P(z | d, w)\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "\n",
    "        # P(z | d) P(w | z)\n",
    "        self.initialize(number_of_topics, random=True)\n",
    "\n",
    "        # Run the EM algorithm\n",
    "        self.calculate_likelihood(number_of_topics)\n",
    "\n",
    "        current_likelihood = self.likelihoods[-1]\n",
    "\n",
    "        for iteration in range(0, max_iter):\n",
    "            self.iterate(number_of_topics)\n",
    "            self.calculate_likelihood(number_of_topics)\n",
    "            \n",
    "            new_likelihood = self.likelihoods[-1]\n",
    "            #print(current_likelihood)\n",
    "            #print(new_likelihood)\n",
    "            #return\n",
    "            newepsilon = abs(new_likelihood - current_likelihood)\n",
    "            #print(newepsilon)\n",
    "            if (newepsilon <= epsilon):\n",
    "                #return\n",
    "                print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "                print(\"Converge\")\n",
    "                break\n",
    "            current_likelihood = new_likelihood\n",
    "\n",
    "       \n",
    "\n",
    "def main():\n",
    "    documents_path = 'data/test.txt' \n",
    "    #documents_path = 'data/DBLP2.txt'\n",
    "    #documents_path = 'data/test5.txt'\n",
    "    corpus = Corpus(documents_path)  # instantiate corpus\n",
    "    corpus.build_vocabulary()\n",
    "    #print(corpus.vocabulary)\n",
    "    print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "    print(\"Number of documents:\" + str(len(corpus.documents)))\n",
    "    #corpus.build_term_doc_matrix()  # testing only REMOVE\n",
    "    number_of_topics = 2\n",
    "    #max_iterations = 500\n",
    "    max_iterations = 500\n",
    "    epsilon = 0.001\n",
    "    corpus.plsa(number_of_topics, max_iterations, epsilon)\n",
    "    print (corpus.document_topic_prob)\n",
    "    print (corpus.topic_word_prob)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\programs\\CS410_data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Pres\\\\2000.07.01.00000000.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1184-0901c6736ccf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#Javascript(\"Jupyter.notebook.execute_cells([0])\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1184-0901c6736ccf>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m#documents_path = '1211543.xml'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mcleandata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCleanData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# instantiate cleandata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mcleandata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleanxml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m#from IPython.display import Javascript\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1184-0901c6736ccf>\u001b[0m in \u001b[0;36mcleanxml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Gore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Bush'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                                 \u001b[0mmystring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Pres\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmystring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Pres\\\\2000.07.01.00000000.txt'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "\n",
    "class CleanData(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents_path = documents_path\n",
    "        \n",
    "    def cleanxml(self):\n",
    "        print(self.documents_path)\n",
    "        count = 0;\n",
    "        for subdir, dirs, files in os.walk(self.documents_path):\n",
    "            for filename in files:\n",
    "                filepath = subdir + os.sep + filename\n",
    "                #print (filepath)        \n",
    "                day = 0\n",
    "                month = 0\n",
    "                year = 0\n",
    "                docs = []\n",
    "                # create element tree object \n",
    "                tree = ET.parse(filepath) \n",
    "                # get root element \n",
    "                root = tree.getroot() \n",
    "                head = tree.find('head')\n",
    "                meta = head.findall('meta')\n",
    "                for metadata in meta:\n",
    "                    if metadata.attrib['name'] == 'publication_day_of_month' :\n",
    "                        day = metadata.attrib['content']\n",
    "                    if metadata.attrib['name'] == 'publication_month' :\n",
    "                        month = metadata.attrib['content']\n",
    "                    if metadata.attrib['name'] == 'publication_year' :\n",
    "                        year = metadata.attrib['content']\n",
    "                #print(year, month, day) \n",
    "\n",
    "                body = root.find('body')\n",
    "                content = body.find('body.content')\n",
    "                for block in content:\n",
    "                    if block.attrib['class'] == 'full_text' :\n",
    "                        for para in block :\n",
    "                            if (para.text.find('Gore') != -1) or (para.text.find('Bush') != -1) :\n",
    "                                mystring = 'Pres\\\\' + str(year) + \".\"  + str(month).zfill(2) + \".\" + str(day).zfill(2) + \".\" + str(count).zfill(8) + '.txt'\n",
    "                                f = open(mystring, \"w\")\n",
    "                                f.write(para.text)\n",
    "                                f.close()                                \n",
    "                                count = count + 1\n",
    "                                #docs.append(para.text)\n",
    "                            #print(para.text)\n",
    "        print(count)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = \"C:\\\\programs\\\\CS410_data\\\\2000\\\\07\\\\01\"\n",
    "    documents_path = \"C:\\\\programs\\\\CS410_data\"\n",
    "    #documents_path = '1211543.xml' \n",
    "    cleandata = CleanData(documents_path)  # instantiate cleandata\n",
    "    cleandata.cleanxml()\n",
    "\n",
    "#from IPython.display import Javascript\n",
    "#Javascript(\"Jupyter.notebook.execute_cells([0])\")\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.execute_cells([0])"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 1098,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.execute_cells([0])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
