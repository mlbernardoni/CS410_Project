{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "C:\\programs\\CS410_Pres_raw\n",
      "402\n",
      "Vocabulary size:4529\n",
      "Number of documents:402\n",
      "EM iteration begins...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-fcd945b0b0c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-fcd945b0b0c8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[0mmax_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_topic_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopic_word_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-fcd945b0b0c8>\u001b[0m in \u001b[0;36mplsa\u001b[1;34m(self, number_of_topics, max_iter, epsilon)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;31m# Run the EM algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mcurrent_likelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihoods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-fcd945b0b0c8>\u001b[0m in \u001b[0;36mcalculate_likelihood\u001b[1;34m(self, number_of_topics)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mmysum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtopicindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                     \u001b[0mmysum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_topic_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopicindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopic_word_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopicindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m                 \u001b[0mmysum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmysum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mmysum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmysum\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterm_doc_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents = []\n",
    "        self.vocabulary = []\n",
    "        self.likelihoods = []\n",
    "        self.documents_path = documents_path\n",
    "        self.term_doc_matrix = None \n",
    "        self.document_topic_prob = None  # P(z | d) - INITIALIZE TO RANDOM\n",
    "        self.topic_word_prob = None  # P(w | z) - INITiALIZE TO RANDOM\n",
    "        self.topic_prob = None  # P(z | d, w) - NORMALIZED document_topic_prob * topic_word_prob for each doc, each word \n",
    "\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        bad_chars = [';', ':', '!', '*', '\"', ',', '.', '?', '-', '_', '@', '[', ']', '(', ')', '{', '}', '/', ',', \"'\"]\n",
    "        \"\"\"\n",
    "        Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
    "        for example: [\"rain\", \"the\", ...]\n",
    "\n",
    "        Update self.vocabulary_size\n",
    "        \"\"\"\n",
    "        stopwords = dict()\n",
    "        with open(\"stopwords.txt\") as swf:\n",
    "            for line in swf:\n",
    "                if not stopwords.get(line):\n",
    "                    stopwords[line] = 1\n",
    "        print(len(stopwords))\n",
    "        vocab = dict()\n",
    "        docnumber = 0\n",
    "        self.documents.clear()\n",
    "\n",
    "        print(self.documents_path)\n",
    "        datadate = \"2000.11.05\"\n",
    "        count = 0\n",
    "        for subdir, dirs, files in os.walk(self.documents_path):\n",
    "            for filename in files:\n",
    "                filepath = subdir + os.sep + filename\n",
    "                #print (filepath)        \n",
    "                if (filepath.find(datadate) != -1)  :\n",
    "                    count = count + 1\n",
    "                    with open(filepath) as f:\n",
    "                        for line in f:\n",
    "                            line.strip(\",;.?!-:@[](){}_*/'\")\n",
    "                            words = line.split()\n",
    "                            self.documents.append([])\n",
    "                            for word in words:\n",
    "                                word = word.lower()\n",
    "                                word = ''.join((filter(lambda i: i not in bad_chars, word)))\n",
    "                                if not stopwords.get(word):\n",
    "                                    self.documents[docnumber].append(word)\n",
    "                                    if vocab.get(word):\n",
    "                                        vocab[word] += 1\n",
    "                                    else:\n",
    "                                        vocab[word] = 1\n",
    "                            docnumber = docnumber + 1\n",
    "        self.number_of_documents = len(self.documents)\n",
    "        self.vocabulary = list(vocab.keys())\n",
    "        #log = open(\"vocab.csv\", \"w\")\n",
    "        #print(self.vocabulary, file = log) \n",
    "        #print(self.vocabulary) \n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        print(count)     \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        with open(self.documents_path, encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line.strip()\n",
    "                words = line.split()\n",
    "                self.documents.append([])\n",
    "                for word in words:\n",
    "                    if word != \"0\" and word != \"1\": # ignore the first word, it is the 0 or 1\n",
    "                        self.documents[docnumber].append(word)\n",
    "                        if vocab.get(word):\n",
    "                            vocab[word] += 1\n",
    "                        else:\n",
    "                            vocab[word] = 1\n",
    "                docnumber = docnumber + 1\n",
    "            self.number_of_documents = len(self.documents)\n",
    "            print(self.number_of_documents)\n",
    "            self.vocabulary = list(vocab.keys())\n",
    "            self.vocabulary_size = len(self.vocabulary)\n",
    "        \"\"\"\n",
    "\n",
    "    def build_term_doc_matrix(self):\n",
    "        \"\"\"\n",
    "        Construct the term-document matrix where each row represents a document, \n",
    "        and each column represents a vocabulary term.\n",
    "\n",
    "        self.term_doc_matrix[i][j] is the count of term j in document i\n",
    "        \"\"\"\n",
    "        doccount = 0\n",
    "        mymatrix = []\n",
    "        for document in self.documents:\n",
    "            # initialize the variables for this doc\n",
    "            mymatrix.append([])\n",
    "            # count the words for this doc\n",
    "            vocab = dict()\n",
    "            for word in document:\n",
    "                if vocab.get(word):\n",
    "                    vocab[word] += 1\n",
    "                else:\n",
    "                    vocab[word] = 1\n",
    "            wordcount = 0\n",
    "            for uniqueword in self.vocabulary:\n",
    "                if vocab.get(uniqueword):\n",
    "                    mymatrix[doccount].append(vocab.get(uniqueword))\n",
    "                else:\n",
    "                    mymatrix[doccount].append(0)\n",
    "            \n",
    "            doccount = doccount + 1\n",
    "        self.term_doc_matrix = mymatrix\n",
    "\n",
    "        \n",
    "        #pass    # REMOVE THIS\n",
    "\n",
    "    def initialize_randomly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Randomly initialize the matrices: document_topic_prob and topic_word_prob\n",
    "        which hold the probability distributions for P(z | d) and P(w | z): self.document_topic_prob, and self.topic_word_prob\n",
    "\n",
    "        Don't forget to normalize! \n",
    "        HINT: you will find numpy's random matrix useful [https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html]\n",
    "        #np.random.random_sample((3, 2)) \n",
    "        \"\"\"\n",
    "        np.random.RandomState()\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        #print(self.document_topic_prob)\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "\n",
    "        #pass    # REMOVE THIS\n",
    "        \n",
    "\n",
    "    def initialize_uniformly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Initializes the matrices: self.document_topic_prob and self.topic_word_prob with a uniform \n",
    "        probability distribution. This is used for testing purposes.\n",
    "\n",
    "        DO NOT CHANGE THIS FUNCTION\n",
    "        \"\"\"\n",
    "        self.document_topic_prob = np.ones((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "\n",
    "        self.topic_word_prob = np.ones((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "\n",
    "    def initialize(self, number_of_topics, random=False):\n",
    "        \"\"\" Call the functions to initialize the matrices document_topic_prob and topic_word_prob\n",
    "        \"\"\"\n",
    "        #print(\"Initializing...\")\n",
    "\n",
    "        if random:\n",
    "            self.initialize_randomly(number_of_topics)\n",
    "        else:\n",
    "            self.initialize_uniformly(number_of_topics)\n",
    "\n",
    "\n",
    "    def iterate(self, number_of_topics):\n",
    "        #print(\"E step:\")\n",
    "        \n",
    "        #self.topic_prob = np.ones((self.number_of_documents, number_of_topics, self.vocabulary_size))\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                #print(self.topic_prob[docindex,topicindex])\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex][topicindex][wordindex] = self.topic_word_prob[topicindex, wordindex] * self.document_topic_prob[docindex, topicindex]\n",
    "                    mysum += self.topic_prob[docindex][topicindex][wordindex]\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    self.topic_prob[docindex,topicindex,wordindex] = self.topic_prob[docindex,topicindex,wordindex] / mysum\n",
    "\n",
    "        #print(\"M step:\")\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            for topicindex in range(0, number_of_topics):\n",
    "                mysum = 0\n",
    "                for wordindex in range(0, self.vocabulary_size):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.document_topic_prob[docindex][topicindex] = mysum\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
    "        #print(self.document_topic_prob)\n",
    "            \n",
    "        # update P(z | d) self.document_topic_prob\n",
    "        for topicindex in range(0, number_of_topics):\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                for docindex in range(0, self.number_of_documents):\n",
    "                    mysum += self.topic_prob[docindex,topicindex,wordindex] * self.term_doc_matrix[docindex][wordindex]\n",
    "                self.topic_word_prob[topicindex][wordindex] = mysum\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "        \"\"\" Calculate the current log-likelihood of the model using\n",
    "        the model's updated probability matrices\n",
    "        \n",
    "        Append the calculated log-likelihood to self.likelihoods\n",
    "\n",
    "        Likelihood:\n",
    "        For each doc sum:\n",
    "        C(w,d) * log (sum(Prob of that topic * prob of that word in topic)\n",
    "        \n",
    "        loop over docs (variable in self) - docnumber\n",
    "            loop over words (variable in self) - wordnumber\n",
    "                multiply Prob of that topic * prob of that word in topic\n",
    "        log of this\n",
    "                \n",
    "       \"\"\"\n",
    "        newlikely = 0\n",
    "        #print(self.document_topic_prob)\n",
    "        #print(self.topic_word_prob)\n",
    "        for docindex in range(0, self.number_of_documents):\n",
    "            docsum = 0\n",
    "            for wordindex in range(0, self.vocabulary_size):\n",
    "                mysum = 0\n",
    "                for topicindex in range(0, number_of_topics):\n",
    "                    mysum += self.document_topic_prob[docindex][topicindex] * self.topic_word_prob[topicindex][wordindex]\n",
    "                mysum = math.log10(mysum)\n",
    "                mysum = mysum * self.term_doc_matrix[docindex][wordindex]\n",
    "                docsum += mysum\n",
    "            #print(docsum)\n",
    "            newlikely += docsum\n",
    "        self.likelihoods.append(newlikely)\n",
    "        \n",
    "\n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "\n",
    "        \"\"\"\n",
    "        Model topics.\n",
    "        \"\"\"\n",
    "        print (\"EM iteration begins...\")\n",
    "        \n",
    "        # build term-doc matrix\n",
    "        self.build_term_doc_matrix()\n",
    "        \n",
    "        # Create the counter arrays.\n",
    "        \n",
    "        # P(z | d, w)\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "\n",
    "        # P(z | d) P(w | z)\n",
    "        self.initialize(number_of_topics, random=True)\n",
    "\n",
    "        # Run the EM algorithm\n",
    "        self.calculate_likelihood(number_of_topics)\n",
    "\n",
    "        current_likelihood = self.likelihoods[-1]\n",
    "\n",
    "        for iteration in range(0, max_iter):\n",
    "            print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "            self.iterate(number_of_topics)\n",
    "            self.calculate_likelihood(number_of_topics)\n",
    "            \n",
    "            new_likelihood = self.likelihoods[-1]\n",
    "            print(current_likelihood)\n",
    "            print(new_likelihood)\n",
    "            #return\n",
    "            newepsilon = abs(new_likelihood - current_likelihood)\n",
    "            #print(newepsilon)\n",
    "            if (newepsilon <= epsilon):\n",
    "                #return\n",
    "                print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "                print(\"Converge\")\n",
    "                break\n",
    "            current_likelihood = new_likelihood\n",
    "\n",
    "       \n",
    "\n",
    "def main():\n",
    "    #documents_path = 'data/test.txt' \n",
    "    documents_path = \"C:\\\\programs\\\\CS410_Pres_raw\"\n",
    "    \n",
    "    #documents_path = 'data/DBLP2.txt'\n",
    "    #documents_path = 'data/test5.txt'\n",
    "    corpus = Corpus(documents_path)  # instantiate corpus\n",
    "    corpus.build_vocabulary()\n",
    "    #return\n",
    "    #print(corpus.vocabulary)\n",
    "    print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
    "    print(\"Number of documents:\" + str(len(corpus.documents)))\n",
    "    #corpus.build_term_doc_matrix()  # testing only REMOVE\n",
    "    number_of_topics = 2\n",
    "    #max_iterations = 500\n",
    "    max_iterations = 500\n",
    "    epsilon = 0.001\n",
    "    corpus.plsa(number_of_topics, max_iterations, epsilon)\n",
    "    print (corpus.document_topic_prob)\n",
    "    print (corpus.topic_word_prob)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\programs\\CS410_data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Pres\\\\2000.07.01.00000000.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1215-0901c6736ccf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#Javascript(\"Jupyter.notebook.execute_cells([0])\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1215-0901c6736ccf>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m#documents_path = '1211543.xml'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mcleandata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCleanData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# instantiate cleandata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mcleandata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleanxml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m#from IPython.display import Javascript\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1215-0901c6736ccf>\u001b[0m in \u001b[0;36mcleanxml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Gore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Bush'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                                 \u001b[0mmystring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Pres\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmystring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Pres\\\\2000.07.01.00000000.txt'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "\n",
    "class CleanData(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents_path = documents_path\n",
    "        \n",
    "    def cleanxml(self):\n",
    "        print(self.documents_path)\n",
    "        count = 0;\n",
    "        for subdir, dirs, files in os.walk(self.documents_path):\n",
    "            for filename in files:\n",
    "                filepath = subdir + os.sep + filename\n",
    "                #print (filepath)        \n",
    "                day = 0\n",
    "                month = 0\n",
    "                year = 0\n",
    "                docs = []\n",
    "                # create element tree object \n",
    "                tree = ET.parse(filepath) \n",
    "                # get root element \n",
    "                root = tree.getroot() \n",
    "                head = tree.find('head')\n",
    "                meta = head.findall('meta')\n",
    "                for metadata in meta:\n",
    "                    if metadata.attrib['name'] == 'publication_day_of_month' :\n",
    "                        day = metadata.attrib['content']\n",
    "                    if metadata.attrib['name'] == 'publication_month' :\n",
    "                        month = metadata.attrib['content']\n",
    "                    if metadata.attrib['name'] == 'publication_year' :\n",
    "                        year = metadata.attrib['content']\n",
    "                #print(year, month, day) \n",
    "\n",
    "                body = root.find('body')\n",
    "                content = body.find('body.content')\n",
    "                for block in content:\n",
    "                    if block.attrib['class'] == 'full_text' :\n",
    "                        for para in block :\n",
    "                            if (para.text.find('Gore') != -1) or (para.text.find('Bush') != -1) :\n",
    "                                mystring = 'Pres\\\\' + str(year) + \".\"  + str(month).zfill(2) + \".\" + str(day).zfill(2) + \".\" + str(count).zfill(8) + '.txt'\n",
    "                                f = open(mystring, \"w\")\n",
    "                                f.write(para.text)\n",
    "                                f.close()                                \n",
    "                                count = count + 1\n",
    "                                #docs.append(para.text)\n",
    "                            #print(para.text)\n",
    "        print(count)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = \"C:\\\\programs\\\\CS410_data\\\\2000\\\\07\\\\01\"\n",
    "    documents_path = \"C:\\\\programs\\\\CS410_data\"\n",
    "    #documents_path = '1211543.xml' \n",
    "    cleandata = CleanData(documents_path)  # instantiate cleandata\n",
    "    cleandata.cleanxml()\n",
    "\n",
    "#from IPython.display import Javascript\n",
    "#Javascript(\"Jupyter.notebook.execute_cells([0])\")\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "C:\\programs\\CS410_Pres_raw\n",
      "C:\\programs\\CS410_Pres_cleanalpha\n",
      "number of docs: 57422\n",
      "vocabulary size: 56769\n",
      "Total vocabulary:56769\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "       \n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path, clean_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.vocabulary = dict()\n",
    "        self.documents_path = documents_path\n",
    "        self.clean_path = clean_path\n",
    "\n",
    "    def build_clean(self):\n",
    "        stopwords = dict()\n",
    "        with open(\"stopwords.txt\") as swf:\n",
    "            for line in swf:\n",
    "                line = re.sub(r'[^a-zA-Z]','', line)\n",
    "                if not stopwords.get(line):\n",
    "                    stopwords[line] = 1\n",
    "        print(len(stopwords))\n",
    "        print(self.documents_path)\n",
    "        print(self.clean_path)\n",
    "\n",
    "        count = 0\n",
    "        for subdir, dirs, files in os.walk(self.documents_path):\n",
    "            for filename in files:\n",
    "                filepath = subdir + os.sep + filename\n",
    "                writefile = self.clean_path + '\\\\' + filename\n",
    "                writefile = writefile.replace(\"txt\", \"csv\")\n",
    "                with open(filepath) as f:\n",
    "                    document = dict()\n",
    "                    for line in f:\n",
    "                        line = line.lower()\n",
    "                        #print(line)\n",
    "                        words = line.split()\n",
    "                        for word in words:\n",
    "                            #word = re.sub(r'\\W+', '', word)\n",
    "                            word = re.sub(r'[^a-zA-Z]','', word)\n",
    "                            #word = re.sub(r'[^a-zA-Z0-9]','', word)\n",
    "                            #print(word + \" \")\n",
    "                            if (len(word) > 0) :\n",
    "                                if not stopwords.get(word):\n",
    "                                    if self.vocabulary.get(word):\n",
    "                                        self.vocabulary[word] += 1\n",
    "                                    else:\n",
    "                                        self.vocabulary[word] = 1\n",
    "                                    if document.get(word):\n",
    "                                        document[word] += 1\n",
    "                                    else:\n",
    "                                        document[word] = 1\n",
    "                        count = count + 1\n",
    "                        fo = open(writefile, \"w\")\n",
    "                        for key, value in document.items():\n",
    "                            fo.write(key + ',' + str(value) + \"\\n\")\n",
    "                        fo.close()                            \n",
    "        vocabfile = self.clean_path + '\\\\vocabulary.csv'\n",
    "        fv = open(vocabfile, \"w\")\n",
    "        for key, value in self.vocabulary.items():\n",
    "            fv.write(key + ',' + str(value) + \"\\n\")\n",
    "        fv.close()                            \n",
    "                           \n",
    "        #self.vocabulary = list(vocab.keys()) \n",
    "        #log = open(\"vocab.csv\", \"w\")\n",
    "        #print(self.vocabulary, file = log) \n",
    "        #print(self.vocabulary) \n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        print(\"number of docs: \" + str(count))     \n",
    "        print(\"vocabulary size: \" + str(len(self.vocabulary)))     \n",
    "\n",
    "\n",
    "def main():\n",
    "    #documents_path = 'data/test.txt' \n",
    "    documents_path = \"C:\\\\programs\\\\CS410_Pres_raw\"\n",
    "    clean_path = \"C:\\\\programs\\\\CS410_Pres_cleanalpha\"\n",
    "    \n",
    "    corpus = Corpus(documents_path, clean_path)  # instantiate corpus\n",
    "    corpus.build_clean()\n",
    "    print(\"Total vocabulary:\" + str(len(corpus.vocabulary)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.execute_cells([0])"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.execute_cells([0])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
